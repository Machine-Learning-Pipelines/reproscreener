
\section{Conclusion}
We develop a new RL formulation for text generation based on soft $Q$-learning and path consistency learning.
We conduct experiments on learning with noisy and negative data, black box adversarial attack, prompting a pretrained language model for controllable generation, and  standard supervised tasks. This formulation opens up new opportunities to integrate more advances made in the fertile RL literature to improve text
generation problems. 

