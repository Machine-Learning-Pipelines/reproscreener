

\section{Appendix}




\subsection{Applications and Experiments}

\begin{figure*}
    \centering
    \includegraphics[width=0.86\linewidth]{figures/202101004-entailment-generation.all.pdf}
    \vspace{-4pt}
    \caption{Entailment generation performance plotted against diversity (average of $H_1$ and $H_2$).}
    \label{appendix-fig:entailment-generation}
    \vspace{-5pt}
\end{figure*}

\subsubsection{Learning from Noisy (Negative) Text}
\label{appendix-subsubsec:experiments-noisy-data}
Please see Table~\ref{table:entailment-generation} for beam search results, Figure~\ref{appendix-fig:entailment-generation} for additional results for \texttt{MLE+reward}, and Table~\ref{table:entailment-generation-examples-sql} for examples.


\subsubsection{\textit{Universal} Adversarial Attacks}
Please see Table~\ref{table:entailment-attack-examples} for examples.

\subsubsection{Prompt Generation for Controlling Pretrained Language Models}
\label{appendix-subsubsec:experiments-prompt-generation}
Please see Table~\ref{table:prompt-generation-full} for detailed results breakdown, and Table~\ref{table:prompt-examples-sql}-\ref{table:prompt-examples-pplm} for examples. Examples are in the format: \textcolor{red}{topic}: \textcolor{blue}{[prompt]} \textcolor{brown}{input sentence} generated text.

\input{tables/experiments-standard-tasks}




\subsection{Setup Details}
\label{appendix-subsec:setup-details}

Our evaluation follows the GEM Benchmark~\citep{gehrmann2021gem} when applicable,\footnote{\url{https://github.com/GEM-benchmark/GEM-metrics}} and otherwise same with the reward function used in training. 
We use a transformer model~\citep{vaswani2017attention} based on Texar-Pytorch~\citep{hu2019texar} by default, with $64$ hidden dimension, $3$ blocks, and $4$ heads. 
For experiments that involve policy gradient training, we initialize the model with maximum likelihood training by default unless specified otherwise. We train soft $Q$-learning model from scratch with both off-policy (using data) and on-policy (using samples) by default except in \S\ref{subsec:noisy-data} and \S\ref{subsec:prompt-generation}, in which we find it beneficial to warm-up the model with just off-policy training. We apply similar tuning budgets to both soft $Q$-learning model, and policy-gradient (mostly the reward scale and top-$k$), based on performance on the validation dataset and sample qualities. Most of the experiments are conducted using Nvidia 1080 or 2080 series GPUs with around $12$GB memory. Most of the datasets are based in English.











\paragraph{Reward Functions}
We use the robust entailment classifier~\citep{nie2020adversarial} in \S\ref{subsec:noisy-data},\footnote{\url{https://huggingface.co/ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli}. $355$M parameters.} one of the most used entailment classifiers on HuggingFaceHub in \S\ref{subsec:adversarial-attack},\footnote{\url{https://github.com/pytorch/fairseq/tree/master/examples/roberta}. This classifier is \textbf{ranked \#1} (as of May 20, 2021) based on \url{https://huggingface.co/models?search=nli}. $355$M parameters.} and a zero-shot classifier based on BART~\citep{lewis2020bart} to compute the topic score in \S\ref{subsec:prompt-generation}.\footnote{\url{https://huggingface.co/facebook/bart-large-mnli}. $407$M parameters.} To compute perplexities, we use a GPT-2 model ($124$M parameters)~\citep{radford2019language} fine-tuned on the corresponding datasets for computing perplexity in \S\ref{subsec:noisy-data} and~\ref{subsec:adversarial-attack}, and a distilled GPT-2 model in \S\ref{subsec:prompt-generation} without fine-tuning.\footnote{\url{https://huggingface.co/distilgpt2}. $82$M parameters.} We simply set reward weights to $1.0$, except in \S\ref{subsec:adversarial-attack}, where we changed the entailment weight to $0.5$, log-likelihood and repetition penalty weight to $5.0$. 




\input{tables/noisy-data-appendix}

\input{tables/entailment-attack-samples}
\input{tables/attack-appendix}

For top-$p$ sampling results, we sample a hypothesis for each premise and measure the average attack rate across the dataset. This is because sampling multiple hypotheses, each for all premises, and measure performance are expensive. Since the hypotheses are sampled input-independently, this should be a good approximation.

\input{tables/prompt-generation-appendix}
We use the paraphrase generation model based on~\citet{zhang2019pegasus}.\footnote{\url{https://huggingface.co/tuner007/pegasus_paraphrase}} During decoding, we include \texttt{no\_repeat\_ngram\_size}$=2$, which improves readability.\footnote{\url{https://huggingface.co/blog/how-to-generate}}




\subsection{The Soft $Q$-Learning Framework}
\input{tables/methods-appendix}

\input{tables/entailment-generation}
\input{tables/prompt-generation}
\input{tables/entailment-generation-samples}
\input{tables/prompt-samples}