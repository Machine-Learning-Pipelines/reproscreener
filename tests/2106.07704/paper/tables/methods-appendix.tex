\subsubsection{Comparison with MLE Objective}
\label{comparison-with-mle-objective}
It is interesting to take a closer look at the above objective and compare with the common MLE training. 
Specifically, we notice the relations between the optimal $Q^*$, $V^*$, and $A^*$ functions: $A^*\left(\bm{s}_{t}, a_{t}\right) = Q^*(\s_t, a_t) - V^*(\s_t) = r_t + \gamma V^*(\s_{t+1}) -V^*\left(\bm{s}_{t}\right)$, where the first equation is the definition of $A^*$ (see Eq.\ref{eq:sql-state-value}) and the second equation is due to Eqs.\eqref{eq:q-and-next-v} and \eqref{eq:sql-state-value}. We thus can see the regression target in the above objective as an approximation to the advantage function: $\tilde{A}_{\bar{{\theta}}}\left(\bm{s}_{t}, a_{t}\right) := -V_{\bar{{\theta}}}\left(\bm{s}_{t}\right)+ \gamma V_{\bar{{\theta}}}\left(\bm{s}_{t+1}\right)+r_{t}$. Therefore, by optimizing the regression objective, $\log \pi_\theta(a_t|\s_t)$, which is the log probability of generating token $a_t$ given preceding tokens $\s_t$, is encouraged to match the approximate advantage value $\tilde{A}_{\bar{{\theta}}}\left(\bm{s}_{t}, a_{t}\right)$, no more and no less. This is different from the objective of MLE where the model is trained to (blindly) increase the probability of the observed token $a_t$ given $\s_t$ and decrease the probability of the rest.



\subsubsection{Vanilla Training with Temporal Consistency}
\label{appendix-subsubsec:vanilla-training-with-temporal-consistency}
Much like the Bellman temporal consistency in standard $Q$-learning, in SQL, the optimal action-value function follows the \emph{softmax} form of the temporal consistency~\citep{ziebart2008maximum,ziebart2010modeling,fox2016taming,nachum2017bridging}:
\begin{equation}
\small
    Q^{*}\left(\bm{s}_{t}, a_{t}\right) =r_{t} + \gamma \log \sum\nolimits_{a_{t+1}} \exp Q^*\left(\bm{s}_{t+1}, a_{t+1}\right) .
    \label{eq:q-and-next-v}
\end{equation}
We thus can derive a regression objective similar to the standard $Q$-learning (Eq.\ref{eq:q-regression}):
\begin{equation}
\small
\begin{aligned}
&\loss_{\text{SQL, vanilla}} (\bm{\theta}) \\
&= \mathbb{E}_{\pi'} \Bigg[ 0.5 \cdot \Bigg( r_t + \gamma \log \sum_{a_{t+1}} \exp Q_{\bar{{\theta}}}\left(\bm{s}_{t+1}, a_{t+1}\right) - Q_{{\theta}}\left(\bm{s}_{t}, a_{t}\right)\Bigg)^{2}\,\Bigg].
\label{eq:optimal-Q-2}
\end{aligned}
\end{equation}
Recall that $\pi'$ is an arbitrary behavior policy (e.g., data distribution), and $Q_{\bar{{\theta}}}$ is the target $Q$-network which is a slow copy of the $Q_\theta$ to be learned and is held fixed during the gradient updates. However, the above objective is inefficient due to exact the same reasons as in standard $Q$-learning discussed earlier, namely the unstable per-step bootstrapping-style training with sparse reward signals, plus the slow updates w.r.t only one token $a_t$ out of the large vocabulary (action space).
