{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install arxiv-miner rpyc nltk cso-classifier arxivscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cso_classifier import CSOClassifier as cc\n",
    "cc.setup()\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_miner import KeywordsTextSearch,TextSearchFilter\n",
    "ELASTICARGS= dict(\n",
    "    index_name=None,\n",
    "    host='localhost',\n",
    "    port=9200,\n",
    "    auth=None\n",
    ")\n",
    "database = KeywordsTextSearch(**ELASTICARGS)\n",
    "# Pagination Style retrieval\n",
    "text_filter = TextSearchFilter(\n",
    "    string_match_query=\"out of distribution generalization\",\n",
    "    start_date_key='04/04/2015',\n",
    "    end_date_key = '04/04/2021',\n",
    "    page_size=100,\n",
    ")\n",
    "paginated_search_results = database.text_search(text_filter)\n",
    "# Iterator style retrieval.\n",
    "scan_text_filter = TextSearchFilter(\n",
    "    string_match_query=\"out of distribution generalization\",\n",
    "    start_date_key='04/04/2015',\n",
    "    end_date_key = '04/04/2021',\n",
    "    scan=True\n",
    ")\n",
    "for doc in database.text_search_scan(scan_text_filter):\n",
    "    handlestuff(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxivscraper\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "scraper = arxivscraper.Scraper(category='cs',date_from='2022-10-24',date_until='2022-10-25',t=10, filters={'categories':['cs.lg']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching up to  1000 records...\n",
      "fetching up to  2000 records...\n",
      "fetching is completed in 15.3 seconds.\n",
      "Total number of records 602\n"
     ]
    }
   ],
   "source": [
    "output = scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'progressive neural networks',\n",
       "  'id': '1606.04671',\n",
       "  'abstract': 'learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. the progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. we evaluate this architecture extensively on a wide variety of reinforcement learning tasks (atari and 3d maze games), and show that it outperforms common baselines based on pretraining and finetuning. using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2016-06-15',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['andrei a. rusu',\n",
       "   'neil c. rabinowitz',\n",
       "   'guillaume desjardins',\n",
       "   'hubert soyer',\n",
       "   'james kirkpatrick',\n",
       "   'koray kavukcuoglu',\n",
       "   'razvan pascanu',\n",
       "   'raia hadsell'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/1606.04671'},\n",
       " {'title': 'data augmentation for bayesian deep learning',\n",
       "  'id': '1903.09668',\n",
       "  'abstract': 'deep learning (dl) methods have emerged as one of the most powerful tools for functional approximation and prediction. while the representation properties of dl have been well studied, uncertainty quantification remains challenging and largely unexplored. data augmentation techniques are a natural approach to provide uncertainty quantification and to incorporate stochastic monte carlo search into stochastic gradient descent (sgd) methods. the purpose of our paper is to show that training dl architectures with data augmentation leads to efficiency gains. we use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning. this allows variants of the expectation-maximization and mcmc algorithms to be brought to bear on these high dimensional nonlinear deep learning models. to demonstrate our methodology, we develop data augmentation algorithms for a variety of commonly used activation functions: logit, relu, leaky relu and svm. our methodology is compared to traditional stochastic gradient descent with back-propagation. our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods providing substantial improvement in speed. we illustrate our methodology on a number of standard datasets. finally, we conclude with directions for future research.',\n",
       "  'categories': 'stat.ml cs.lg stat.me',\n",
       "  'doi': '10.1214/22-ba1331',\n",
       "  'created': '2019-03-22',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['yuexi wang', 'nicholas g. polson', 'vadim o. sokolov'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/1903.09668'},\n",
       " {'title': 'deep q-learning for nash equilibria: nash-dqn',\n",
       "  'id': '1904.10554',\n",
       "  'abstract': 'model-free learning for multi-agent stochastic games is an active area of research. existing reinforcement learning algorithms, however, are often restricted to zero-sum games, and are applicable only in small state-action spaces or other simplified settings. here, we develop a new data efficient deep-q-learning methodology for model-free learning of nash equilibria for general-sum stochastic games. the algorithm uses a local linear-quadratic expansion of the stochastic game, which leads to analytically solvable optimal actions. the expansion is parametrized by deep neural networks to give it sufficient flexibility to learn the environment without the need to experience all state-action pairs. we study symmetry properties of the algorithm stemming from label-invariant stochastic games and as a proof of concept, apply our algorithm to learning optimal trading strategies in competitive electronic markets.',\n",
       "  'categories': 'cs.lg cs.gt q-fin.cp stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2019-04-23',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['philippe casgrain', 'brian ning', 'sebastian jaimungal'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/1904.10554'},\n",
       " {'title': 'distributionally robust optimization: a review',\n",
       "  'id': '1908.05659',\n",
       "  'abstract': 'the concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. a modeling framework, called distributionally robust optimization (dro), has recently received significant attention in both the operations research and statistical learning communities. this paper surveys main concepts and contributions to dro, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.',\n",
       "  'categories': 'math.oc cs.lg stat.ml',\n",
       "  'doi': '10.5802/ojmo.15',\n",
       "  'created': '2019-08-12',\n",
       "  'updated': '',\n",
       "  'authors': ['hamed rahimian', 'sanjay mehrotra'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/1908.05659'},\n",
       " {'title': 'transfer fine-tuning: a bert case study',\n",
       "  'id': '1909.00931',\n",
       "  'abstract': \"a semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). it constitutes a set of tasks crucial for research on natural language understanding. recently, bert realized a breakthrough in sentence representation learning (devlin et al., 2019), which is broadly transferable to various nlp tasks. while bert's performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. herein, we propose to inject phrasal paraphrase relations into bert in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller bert model while maintaining the model size. the generated model exhibits superior performance compared to a larger bert model on semantic equivalence assessment tasks. furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.\",\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '10.18653/v1/d19-1542',\n",
       "  'created': '2019-09-02',\n",
       "  'updated': '',\n",
       "  'authors': ['yuki arase', 'junichi tsujii'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/1909.00931'},\n",
       " {'title': 'a modular deep learning pipeline for galaxy-scale strong gravitational   lens detection and modeling',\n",
       "  'id': '1911.03867',\n",
       "  'abstract': \"upcoming large astronomical surveys are expected to capture an unprecedented number of strong gravitational lensing systems. deep learning is emerging as a promising practical tool for the detection and quantification of these galaxy-scale image distortions. the absence of large quantities of representative data from current astronomical surveys motivates the development of a robust forward-modeling approach using synthetic lensing images. using a mock sample of strong lenses created upon a state-of-the-art extragalactic catalogs, we train a modular deep learning pipeline for uncertainty-quantified detection and modeling with intermediate image processing components for denoising and deblending the lensing systems. we demonstrate a high degree of interpretability and controlled systematics due to domain-specific task modules trained with different stages of synthetic image generation. for lens detection and modeling, we obtain semantically meaningful latent spaces that separate classes of strong lens images and yield uncertainty estimates that explain the origin of misclassified images and provide probabilistic predictions for the lens parameters. validation of the inference pipeline has been carried out using images from the subaru telescope's hyper suprime-cam camera, and lsst desc simulated dc2 sky survey catalogues.\",\n",
       "  'categories': 'astro-ph.im astro-ph.co cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2019-11-10',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['sandeep madireddy',\n",
       "   'nesar ramachandra',\n",
       "   'nan li',\n",
       "   'james butler',\n",
       "   'prasanna balaprakash',\n",
       "   'salman habib',\n",
       "   'katrin heitmann',\n",
       "   'the lsst dark energy science collaboration'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/1911.03867'},\n",
       " {'title': 'magneto: fingerprinting usb flash drives via unintentional magnetic   emissions',\n",
       "  'id': '2002.05905',\n",
       "  'abstract': 'universal serial bus (usb) flash drives are nowadays one of the most convenient and diffused means to transfer files, especially when no internet connection is available. however, usb flash drives are also one of the most common attack vectors used to gain unauthorized access to host devices. for instance, it is possible to replace a usb drive so that when the usb key is connected, it would install passwords stealing tools, root-kit software, and other disrupting malware. in such a way, an attacker can steal sensitive information via the usb-connected devices, as well as inject any kind of malicious software into the host.   to thwart the above-cited raising threats, we propose magneto, an efficient, non-interactive, and privacy-preserving framework to verify the authenticity of a usb flash drive, rooted in the analysis of its unintentional magnetic emissions. we show that the magnetic emissions radiated during boot operations on a specific host are unique for each device, and sufficient to uniquely fingerprint both the brand and the model of the usb flash drive, or the specific usb device, depending on the used equipment. our investigation on 59 different usb flash drives---belonging to 17 brands, including the top brands purchased on amazon in mid-2019---, reveals a minimum classification accuracy of 98.2% in the identification of both brand and model, accompanied by a negligible time and computational overhead. magneto can also identify the specific usb flash drive, with a minimum classification accuracy of 91.2%. overall, magneto proves that unintentional magnetic emissions can be considered as a viable and reliable means to fingerprint read-only usb flash drives. finally, future research directions in this domain are also discussed.',\n",
       "  'categories': 'cs.cr cs.lg',\n",
       "  'doi': '10.1145/3422308',\n",
       "  'created': '2020-02-14',\n",
       "  'updated': '2020-09-12',\n",
       "  'authors': ['omar adel ibrahim',\n",
       "   'savio sciancalepore',\n",
       "   'gabriele oligeri',\n",
       "   'roberto di pietro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2002.05905'},\n",
       " {'title': 'exploring optimal deep learning models for image-based malware variant   classification',\n",
       "  'id': '2004.05258',\n",
       "  'abstract': 'analyzing a huge amount of malware is a major burden for security analysts. since emerging malware is often a variant of existing malware, automatically classifying malware into known families greatly reduces a part of their burden. image-based malware classification with deep learning is an attractive approach for its simplicity, versatility, and affinity with the latest technologies. however, the impact of differences in deep learning models and the degree of transfer learning on the classification accuracy of malware variants has not been fully studied. in this paper, we conducted an exhaustive survey of deep learning models using 24 imagenet pre-trained models and five fine-tuning parameters, totaling 120 combinations, on two platforms. as a result, we found that the highest classification accuracy was obtained by fine-tuning one of the latest deep learning models with a relatively low degree of transfer learning, and we achieved the highest classification accuracy ever in cross-validation on the malimg and drebin datasets. we also confirmed that this trend holds true for the recent malware variants using the virustotal 2020 windows and android datasets. the experimental results suggest that it is effective to periodically explore optimal deep learning models with the latest models and malware datasets by gradually reducing the degree of transfer learning from half.',\n",
       "  'categories': 'cs.cr cs.lg',\n",
       "  'doi': '10.1109/compsac54236.2022.00128',\n",
       "  'created': '2020-04-10',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['rikima mitsuhashi', 'takahiro shinagawa'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2004.05258'},\n",
       " {'title': 'towards quantum advantage via topological data analysis',\n",
       "  'id': '2005.02607',\n",
       "  'abstract': 'even after decades of quantum computing development, examples of generally useful quantum algorithms with exponential speedups over classical counterparts are scarce. recent progress in quantum algorithms for linear-algebra positioned quantum machine learning (qml) as a potential source of such useful exponential improvements. yet, in an unexpected development, a recent series of \"dequantization\" results has equally rapidly removed the promise of exponential speedups for several qml algorithms. this raises the critical question whether exponential speedups of other linear-algebraic qml algorithms persist. in this paper, we study the quantum-algorithmic methods behind the algorithm for topological data analysis of lloyd, garnerone and zanardi through this lens. we provide evidence that the problem solved by this algorithm is classically intractable by showing that its natural generalization is as hard as simulating the one clean qubit model -- which is widely believed to require superpolynomial time on a classical computer -- and is thus very likely immune to dequantizations. based on this result, we provide a number of new quantum algorithms for problems such as rank estimation and complex network analysis, along with complexity-theoretic evidence for their classical intractability. furthermore, we analyze the suitability of the proposed quantum algorithms for near-term implementations. our results provide a number of useful applications for full-blown, and restricted quantum computers with a guaranteed exponential speedup over classical methods, recovering some of the potential for linear-algebraic qml to become one of quantum computing\\'s killer applications.',\n",
       "  'categories': 'quant-ph cs.cc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2020-05-06',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['casper gyurik', 'chris cade', 'vedran dunjko'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2005.02607'},\n",
       " {'title': 'wide boosting',\n",
       "  'id': '2007.09855',\n",
       "  'abstract': \"gradient boosting (gb) is a popular methodology used to solve prediction problems by minimizing a differentiable loss function, $l$. gb performs very well on tabular machine learning (ml) problems; however, as a pure ml solver it lacks the ability to fit models with probabilistic but correlated multi-dimensional outputs, for example, multiple correlated bernoulli outputs. gb also does not form intermediate abstract data embeddings, one property of deep learning that gives greater flexibility and performance on other types of problems. this paper presents a simple adjustment to gb motivated in part by artificial neural networks. specifically, our adjustment inserts a matrix multiplication between the output of a gb model and the loss, $l$. this allows the output of a gb model to have increased dimension prior to being fed into the loss and is thus ``wider'' than standard gb implementations. we call our method wide boosting (wb) and show that wb outperforms gb on mult-dimesional output tasks and that the embeddings generated by wb contain are more useful in downstream prediction tasks than gb output predictions alone.\",\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2020-07-19',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['michael t. horrell'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2007.09855'},\n",
       " {'title': 'practical and parallelizable algorithms for non-monotone submodular   maximization with size constraint',\n",
       "  'id': '2009.01947',\n",
       "  'abstract': 'we present combinatorial and parallelizable algorithms for maximization of a submodular function, not necessarily monotone, with respect to a size constraint. we improve the best approximation factor achieved by an algorithm that has optimal adaptivity and nearly optimal query complexity to $0.193 - \\\\varepsilon$. the conference version of this work mistakenly employed a subroutine that does not work for non-monotone, submodular functions. in this version, we propose a fixed and improved subroutine to add a set with high average marginal gain, \\\\threseq, which returns a solution in $o( \\\\log(n) )$ adaptive rounds with high probability. moreover, we provide two approximation algorithms. the first has approximation ratio $1/6 - \\\\varepsilon$, adaptivity $o( \\\\log (n) )$, and query complexity $o( n \\\\log (k) )$, while the second has approximation ratio $0.193 - \\\\varepsilon$, adaptivity $o( \\\\log^2 (n) )$, and query complexity $o(n \\\\log (k))$. our algorithms are empirically validated to use a low number of adaptive rounds and total queries while obtaining solutions with high objective value in comparison with state-of-the-art approximation algorithms, including continuous algorithms that use the multilinear extension.',\n",
       "  'categories': 'cs.ds cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2020-09-03',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['yixin chen', 'alan kuhnle'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2009.01947'},\n",
       " {'title': 'dissecting hessian: understanding common structure of hessian in neural   networks',\n",
       "  'id': '2010.04261',\n",
       "  'abstract': 'hessian captures important properties of the deep neural network loss landscape. previous works have observed low rank structure in the hessians of neural networks. in this paper, we propose a decoupling conjecture that decomposes the layer-wise hessians of a network as the kronecker product of two smaller matrices. we can analyze the properties of these smaller matrices and prove the structure of top eigenspace random 2-layer networks. the decoupling conjecture has several other interesting implications - top eigenspaces for different models have surprisingly high overlap, and top eigenvectors form low rank matrices when they are reshaped into the same shape as the corresponding weight matrix. all of these can be verified empirically for deeper networks. finally, we use the structure of layer-wise hessian to get better explicit generalization bounds for neural networks.',\n",
       "  'categories': 'cs.lg cs.ne stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2020-10-08',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['yikai wu', 'xingyu zhu', 'chenwei wu', 'annie wang', 'rong ge'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2010.04261'},\n",
       " {'title': 'kernel methods for causal functions: dose, heterogeneous, and   incremental response curves',\n",
       "  'id': '2010.04855',\n",
       "  'abstract': 'we propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves. treatment and covariates may be discrete or continuous in general spaces. due to a decomposition property specific to the rkhs, our estimators have simple closed form solutions. we prove uniform consistency with finite sample rates via original analysis of generalized kernel ridge regression. we extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria. we achieve state-of-the-art performance in nonlinear simulations with many covariates, and conduct a policy evaluation of the us job corps training program for disadvantaged youths.',\n",
       "  'categories': 'econ.em cs.lg math.st stat.ml stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2020-10-09',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['rahul singh', 'liyuan xu', 'arthur gretton'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2010.04855'},\n",
       " {'title': 'conjecturing-based computational discovery of patterns in data',\n",
       "  'id': '2011.11576',\n",
       "  'abstract': 'we propose the use of a conjecturing machine that generates feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features. the proposed \\\\textsc{conjecturing} framework recovers known nonlinear and boolean relationships among features from data. in both settings, true underlying relationships are revealed. we then compare the method to a previously-proposed framework for symbolic regression and demonstrate that it can also be used to recover equations that are satisfied among features in a dataset. the framework is then applied to patient-level data regarding covid-19 outcomes to suggest possible risk factors that are confirmed in medical literature.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2020-11-23',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['j. p. brooks',\n",
       "   'd. j. edwards',\n",
       "   'c. e. larson',\n",
       "   'n. van cleemput'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2011.11576'},\n",
       " {'title': 'trojanzoo: towards unified, holistic, and practical evaluation of neural   backdoors',\n",
       "  'id': '2012.09302',\n",
       "  'abstract': 'neural backdoors represent one primary threat to the security of deep learning systems. the intensive research has produced a plethora of backdoor attacks/defenses, resulting in a constant arms race. however, due to the lack of evaluation benchmarks, many critical questions remain under-explored: (i) what are the strengths and limitations of different attacks/defenses? (ii) what are the best practices to operate them? and (iii) how can the existing attacks/defenses be further improved?   to bridge this gap, we design and implement trojanzoo, the first open-source platform for evaluating neural backdoor attacks/defenses in a unified, holistic, and practical manner. thus far, focusing on the computer vision domain, it has incorporated 8 representative attacks, 14 state-of-the-art defenses, 6 attack performance metrics, 10 defense utility metrics, as well as rich tools for in-depth analysis of the attack-defense interactions. leveraging trojanzoo, we conduct a systematic study on the existing attacks/defenses, unveiling their complex design spectrum: both manifest intricate trade-offs among multiple desiderata (e.g., the effectiveness, evasiveness, and transferability of attacks). we further explore improving the existing attacks/defenses, leading to a number of interesting findings: (i) one-pixel triggers often suffice; (ii) training from scratch often outperforms perturbing benign models to craft trojan models; (iii) optimizing triggers and trojan models jointly greatly improves both attack effectiveness and evasiveness; (iv) individual defenses can often be evaded by adaptive attacks; and (v) exploiting model interpretability significantly improves defense robustness. we envision that trojanzoo will serve as a valuable platform to facilitate future research on neural backdoors.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '10.1109/eurosp53844.2022.00048',\n",
       "  'created': '2020-12-16',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['ren pang',\n",
       "   'zheng zhang',\n",
       "   'xiangshan gao',\n",
       "   'zhaohan xi',\n",
       "   'shouling ji',\n",
       "   'peng cheng',\n",
       "   'xiapu luo',\n",
       "   'ting wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2012.09302'},\n",
       " {'title': 'exact recovery of community structures using deepwalk and node2vec',\n",
       "  'id': '2101.07354',\n",
       "  'abstract': 'random-walk based network embedding algorithms like deepwalk and node2vec are widely used to obtain euclidean representation of the nodes in a network prior to performing downstream inference tasks. however, despite their impressive empirical performance, there is a lack of theoretical results explaining their large-sample behavior. in this paper, we study node2vec and deepwalk through the perspective of matrix factorization. in particular, we analyze these algorithms in the setting of community detection for stochastic blockmodel graphs (and their degree-corrected variants). by exploiting the row-wise uniform perturbation bound for leading singular vectors, we derive high-probability error bounds between the matrix factorization-based node2vec/deepwalk embeddings and their true counterparts, uniformly over all node embeddings. based on strong concentration results, we further show the perfect membership recovery by node2vec/deepwalk, followed by $k$-means/medians algorithms. specifically, as the network becomes sparser, our results guarantee that with large enough window size and vertices number, applying $k$-means/medians on the matrix factorization-based node2vec embeddings can, with high probability, correctly recover the memberships of all vertices in a network generated from the stochastic blockmodel (or its degree-corrected variants). the theoretical justifications are mirrored in the numerical experiments and real data applications, for both the original node2vec and its matrix factorization variant.',\n",
       "  'categories': 'stat.ml cs.lg cs.si',\n",
       "  'doi': '',\n",
       "  'created': '2021-01-18',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['yichi zhang', 'minh tang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2101.07354'},\n",
       " {'title': 'quantum cross entropy and maximum likelihood principle',\n",
       "  'id': '2102.11887',\n",
       "  'abstract': 'quantum machine learning is an emerging field at the intersection of machine learning and quantum computing. classical cross entropy plays a central role in machine learning. we define its quantum generalization, the quantum cross entropy, prove its lower bounds, and investigate its relation to quantum fidelity. in the classical case, minimizing cross entropy is equivalent to maximizing likelihood. in the quantum case, when the quantum cross entropy is constructed from quantum data undisturbed by quantum measurements, this relation holds. classical cross entropy is equal to negative log-likelihood. when we obtain quantum cross entropy through empirical density matrix based on measurement outcomes, the quantum cross entropy is lower-bounded by negative log-likelihood. these two different scenarios illustrate the information loss when making quantum measurements. we conclude that to achieve the goal of full quantum machine learning, it is crucial to utilize the deferred measurement principle.',\n",
       "  'categories': 'quant-ph cs.it cs.lg hep-th math.it stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2021-02-23',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['zhou shangnan', 'yixu wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2102.11887'},\n",
       " {'title': 'lgd-gcn: local and global disentangled graph convolutional networks',\n",
       "  'id': '2104.11893',\n",
       "  'abstract': 'disentangled graph convolutional network (disengcn) is an encouraging framework to disentangle the latent factors arising in a real-world graph. however, it relies on disentangling information heavily from a local range (i.e., a node and its 1-hop neighbors), while the local information in many cases can be uneven and incomplete, hindering the interpretabiliy power and model performance of disengcn. in this paper, we introduce a novel local and global disentangled graph convolutional network (lgd-gcn) to capture both local and global information for graph disentanglement. lgd-gcn performs a statistical mixture modeling to derive a factor-aware latent continuous space, and then constructs different structures w.r.t. different factors from the revealed space. in this way, the global factor-specific information can be efficiently and selectively encoded via a message passing along these built structures, strengthening the intra-factor consistency. we also propose a novel diversity promoting regularizer employed with the latent space modeling, to encourage inter-factor diversity. evaluations of the proposed lgd-gcn on the synthetic and real-world datasets show a better interpretability and improved performance in node classification over the existing competitive models.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-04-24',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['jingwei guo', 'kaizhu huang', 'xinping yi', 'rui zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2104.11893'},\n",
       " {'title': 'the effects of air quality on the spread of the covid-19 pandemic in   italy: an artificial intelligence approach',\n",
       "  'id': '2104.12546',\n",
       "  'abstract': 'the covid-19 pandemic considerably affects public health systems around the world. the lack of knowledge about the virus, the extension of this phenomenon, and the speed of the evolution of the infection are all factors that highlight the necessity of employing new approaches to study these events. artificial intelligence techniques may be useful in analyzing data related to areas affected by the virus. the aim of this work is to investigate any possible relationships between air quality and confirmed cases of covid-19 in italian districts. specifically, we report an analysis of the correlation between daily covid-19 cases and environmental factors, such as temperature, relative humidity, and atmospheric pollutants. our analysis confirms a significant association of some environmental parameters with the spread of the virus. this suggests that machine learning models trained on the environmental parameters to predict the number of future infected cases may be accurate. predictive models may be useful for helping institutions in making decisions for protecting the population and contrasting the pandemic.',\n",
       "  'categories': 'cs.cy cs.ai cs.lg',\n",
       "  'doi': '10.1016/j.procs.2022.09.112',\n",
       "  'created': '2021-04-09',\n",
       "  'updated': '2021-08-31',\n",
       "  'authors': ['andrea loreggia', 'anna passarelli', 'maria silvia pini'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2104.12546'},\n",
       " {'title': 'reinforcement learning for ridesharing: an extended survey',\n",
       "  'id': '2105.01099',\n",
       "  'abstract': 'in this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to decision optimization problems in a typical ridesharing system. papers on the topics of rideshare matching, vehicle repositioning, ride-pooling, routing, and dynamic pricing are covered. most of the literature has appeared in the last few years, and several core challenges are to continue to be tackled: model complexity, agent coordination, and joint optimization of multiple levers. hence, we also introduce popular data sets and open simulation environments to facilitate further research and development. subsequently, we discuss a number of challenges and opportunities for reinforcement learning research on this important domain.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '10.1016/j.trc.2022.103852',\n",
       "  'created': '2021-05-03',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['zhiwei qin', 'hongtu zhu', 'jieping ye'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2105.01099'},\n",
       " {'title': 'flex: extrinsic parameters-free multi-view 3d human motion   reconstruction',\n",
       "  'id': '2105.01937',\n",
       "  'abstract': 'the increasing availability of video recordings made by multiple cameras has offered new means for mitigating occlusion and depth ambiguities in pose and motion reconstruction methods. yet, multi-view algorithms strongly depend on camera parameters; particularly, the relative transformations between the cameras. such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings. we introduce flex (free multi-view reconstruxion), an end-to-end extrinsic parameter-free multi-view model. flex is extrinsic parameter-free (dubbed ep-free) in the sense that it does not require extrinsic camera parameters. our key idea is that the 3d angles between skeletal parts, as well as bone lengths, are invariant to the camera position. hence, learning 3d rotations and bone lengths rather than locations allows predicting common values for all camera views. our network takes multiple video streams, learns fused deep features through a novel multi-view fusion layer, and reconstructs a single consistent skeleton with temporally coherent joint rotations. we demonstrate quantitative and qualitative results on three public datasets, and on synthetic multi-person video streams captured by dynamic cameras. we compare our model to state-of-the-art methods that are not ep-free and show that in the absence of camera parameters, we outperform them by a large margin while obtaining comparable results when camera parameters are available. code, trained models, and other materials are available on our project page.',\n",
       "  'categories': 'cs.cv cs.gr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-05-05',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['brian gordon',\n",
       "   'sigal raab',\n",
       "   'guy azov',\n",
       "   'raja giryes',\n",
       "   'daniel cohen-or'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2105.01937'},\n",
       " {'title': 'a simple and general debiased machine learning theorem with finite   sample guarantees',\n",
       "  'id': '2105.15197',\n",
       "  'abstract': 'debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e. scalar summaries, of machine learning algorithms. for example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network. we provide a nonasymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. formally, we prove consistency, gaussian approximation, and semiparametric efficiency by finite sample arguments. the rate of convergence is $n^{-1/2}$ for global functionals, and it degrades gracefully for local functionals. our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. the conditions reveal a general double robustness property for ill posed inverse problems.',\n",
       "  'categories': 'stat.ml cs.lg econ.em math.st stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2021-05-31',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['victor chernozhukov', 'whitney k. newey', 'rahul singh'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2105.15197'},\n",
       " {'title': 'normalizing flows for knockoff-free controlled feature selection',\n",
       "  'id': '2106.01528',\n",
       "  'abstract': 'controlled feature selection aims to discover the features a response depends on while limiting the false discovery rate (fdr) to a predefined level. recently, multiple deep-learning-based methods have been proposed to perform controlled feature selection through the model-x knockoff framework. we demonstrate, however, that these methods often fail to control the fdr for two reasons. first, these methods often learn inaccurate models of features. second, the \"swap\" property, which is required for knockoffs to be valid, is often not well enforced. we propose a new procedure called flowselect to perform controlled feature selection that does not suffer from either of these two problems. to more accurately model the features, flowselect uses normalizing flows, the state-of-the-art method for density estimation. instead of enforcing the \"swap\" property, flowselect uses a novel mcmc-based procedure to calculate p-values for each feature directly. asymptotically, flowselect computes valid p-values. empirically, flowselect consistently controls the fdr on both synthetic and semi-synthetic benchmarks, whereas competing knockoff-based approaches do not. flowselect also demonstrates greater power on these benchmarks. additionally, flowselect correctly infers the genetic variants associated with specific soybean traits from gwas data.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-02',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['derek hansen', 'brian manzo', 'jeffrey regier'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.01528'},\n",
       " {'title': \"self-supervision is all you need for solving rubik's cube\",\n",
       "  'id': '2106.03157',\n",
       "  'abstract': \"existing combinatorial search methods are often complex and require some expertise. in this work, we propose a simple and performant deep learning method, especially for goal-predefined combinatorial problems represented by rubik's cube. we show that, for solving such problems with high optimality, it can be sufficient to train a deep neural network on random scrambles branching from the goal state. when tested on rubik's cube, our method outperformed the previous state-of-the-art method deepcubea in solution optimality, being 10 times more efficient in training and 2.0 times in inference. one key assumption is that, when viewed from scrambled states, random moves from the goal are biased to be optimal. we also demonstrate the proposed method on 15 puzzle and lights out.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-06',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['kyo takano'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.03157'},\n",
       " {'title': 'stability to deformations of manifold filters and manifold neural   networks',\n",
       "  'id': '2106.03725',\n",
       "  'abstract': 'the paper defines and studies manifold (m) convolutional filters and neural networks (nns). \\\\emph{manifold} filters and mnns are defined in terms of the laplace-beltrami operator exponential and are such that \\\\emph{graph} (g) filters and neural networks (nns) are recovered as discrete approximations when the manifold is sampled. these filters admit a spectral representation which is a generalization of both the spectral representation of graph filters and the frequency response of standard convolutional filters in continuous time. the main technical contribution of the paper is to analyze the stability of manifold filters and mnns to smooth deformations of the manifold. this analysis generalizes known stability properties of graph filters and gnns and it is also a generalization of known stability properties of standard convolutional filters and neural networks in continuous time. the most important observation that follows from this analysis is that manifold filters, same as graph filters and standard continuous time filters, have difficulty discriminating high frequency components in the presence of deformations. this is a challenge that can be ameliorated with the use of manifold, graph, or continuous time neural networks. the most important practical consequence of this analysis is to shed light on the behavior of graph filters and gnns in large scale graphs.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-07',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['zhiyang wang', 'luana ruiz', 'alejandro ribeiro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.03725'},\n",
       " {'title': 'scalars are universal: equivariant machine learning, structured like   classical physics',\n",
       "  'id': '2106.06610',\n",
       "  'abstract': \"there has been enormous progress in the last few years in designing neural networks that respect the fundamental symmetries and coordinate freedoms of physical law. some of these frameworks make use of irreducible representations, some make use of high-order tensor objects, and some apply symmetry-enforcing constraints. different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the euclidean, lorentz, and poincar\\\\'e groups, at any dimensionality $d$. the key observation is that nonlinear o($d$)-equivariant (and related-group-equivariant) functions can be universally expressed in terms of a lightweight collection of scalars -- scalar products and scalar contractions of the scalar, vector, and tensor inputs. we complement our theory with numerical examples that show that the scalar-based method is simple, efficient, and scalable.\",\n",
       "  'categories': 'cs.lg math-ph math.mp stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-11',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['soledad villar',\n",
       "   'david w. hogg',\n",
       "   'kate storey-fisher',\n",
       "   'weichi yao',\n",
       "   'ben blum-smith'],\n",
       "  'affiliation': ['jhu', 'flatiron, nyu', 'nyu', 'nyu', 'nyu'],\n",
       "  'url': 'https://arxiv.org/abs/2106.06610'},\n",
       " {'title': 'inverting adversarially robust networks for image synthesis',\n",
       "  'id': '2106.06927',\n",
       "  'abstract': 'despite unconditional feature inversion being the foundation of many image synthesis applications, training an inverter demands a high computational budget, large decoding capacity and imposing conditions such as autoregressive priors. to address these limitations, we propose the use of adversarially robust representations as a perceptual primitive for feature inversion. we train an adversarially robust encoder to extract disentangled and perceptually-aligned image representations, making them easily invertible. by training a simple generator with the mirror architecture of the encoder, we achieve superior reconstruction quality and generalization over standard models. based on this, we propose an adversarially robust autoencoder and demonstrate its improved performance on style transfer, image denoising and anomaly detection tasks. compared to recent imagenet feature inversion methods, our model attains improved performance with significantly less complexity.',\n",
       "  'categories': 'cs.cv cs.lg cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-13',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['renan a. rojas-gomez',\n",
       "   'raymond a. yeh',\n",
       "   'minh n. do',\n",
       "   'anh nguyen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.06927'},\n",
       " {'title': 'efficient (soft) q-learning for text generation with limited good data',\n",
       "  'id': '2106.07704',\n",
       "  'abstract': 'maximum likelihood estimation (mle) is the predominant algorithm for training text generation models. this paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. reinforcement learning (rl) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. yet previous rl algorithms for text generation, such as policy gradient (on-policy rl) and q-learning (off-policy rl), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. in this paper, we introduce a new rl formulation for text generation from the soft q-learning (sql) perspective. it enables us to draw from the latest rl advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. we apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. experiments show our approach consistently outperforms both task-specialized algorithms and the previous rl methods.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-14',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['han guo',\n",
       "   'bowen tan',\n",
       "   'zhengzhong liu',\n",
       "   'eric p. xing',\n",
       "   'zhiting hu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.07704'},\n",
       " {'title': 'banditmf: multi-armed bandit based matrix factorization recommender   system',\n",
       "  'id': '2106.10898',\n",
       "  'abstract': 'multi-armed bandits (mab) provide a principled online learning approach to attain the balance between exploration and exploitation. due to the superior performance and low feedback learning without the learning to act in multiple situations, multi-armed bandits drawing widespread attention in applications ranging such as recommender systems. likewise, within the recommender system, collaborative filtering (cf) is arguably the earliest and most influential method in the recommender system. crucially, new users and an ever-changing pool of recommended items are the challenges that recommender systems need to address. for collaborative filtering, the classical method is training the model offline, then perform the online testing, but this approach can no longer handle the dynamic changes in user preferences which is the so-called cold start. so how to effectively recommend items to users in the absence of effective information? to address the aforementioned problems, a multi-armed bandit based collaborative filtering recommender system has been proposed, named banditmf. banditmf is designed to address two challenges in the multi-armed bandits algorithm and collaborative filtering: (1) how to solve the cold start problem for collaborative filtering under the condition of scarcity of valid information, (2) how to solve the sub-optimal problem of bandit algorithms in strong social relations domains caused by independently estimating unknown parameters associated with each user and ignoring correlations between users.',\n",
       "  'categories': 'cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-21',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['shenghao xu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.10898'},\n",
       " {'title': 'imitation learning: progress, taxonomies and challenges',\n",
       "  'id': '2106.12177',\n",
       "  'abstract': \"imitation learning aims to extract knowledge from human experts' demonstrations or artificially created agents in order to replicate their behaviors. its success has been demonstrated in areas such as video games, autonomous driving, robotic simulations and object manipulation. however, this replicating process could be problematic, such as the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments. in this survey, we provide a systematic review on imitation learning. we first introduce the background knowledge from development history and preliminaries, followed by presenting different taxonomies within imitation learning and key milestones of the field. we then detail challenges in learning strategies and present research opportunities with learning policy from suboptimal demonstration, voice instructions and other associated optimization schemes.\",\n",
       "  'categories': 'cs.lg cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-23',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['boyuan zheng',\n",
       "   'sunny verma',\n",
       "   'jianlong zhou',\n",
       "   'ivor tsang',\n",
       "   'fang chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.12177'},\n",
       " {'title': 'fundamental limits for learning hidden markov model parameters',\n",
       "  'id': '2106.12936',\n",
       "  'abstract': 'we study the frontier between learnable and unlearnable hidden markov models (hmms). hmms are flexible tools for clustering dependent data coming from unknown populations. the model parameters are known to be fully identifiable (up to label-switching) without any modeling assumption on the distributions of the populations as soon as the clusters are distinct and the hidden chain is ergodic with a full rank transition matrix. in the limit as any one of these conditions fails, it becomes impossible in general to identify parameters. for a chain with two hidden states we prove nonasymptotic minimax upper and lower bounds, matching up to constants, which exhibit thresholds at which the parameters become learnable. we also provide an upper bound on the relative entropy rate for parameters in a neighbourhood of the unlearnable region which may have interest in itself.',\n",
       "  'categories': 'stat.ml cs.lg math.st stat.th',\n",
       "  'doi': '10.1109/tit.2022.3213429',\n",
       "  'created': '2021-06-24',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['kweku abraham', 'zacharie naulet', 'elisabeth gassiat'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.12936'},\n",
       " {'title': 'quantum data compression and quantum cross entropy',\n",
       "  'id': '2106.13823',\n",
       "  'abstract': 'quantum machine learning is an emerging field at the intersection of machine learning and quantum computing. a central quantity for the theoretical foundation of quantum machine learning is the quantum cross entropy. in this paper, we present one operational interpretation of this quantity, that the quantum cross entropy is the compression rate for sub-optimal quantum source coding. to do so, we give a simple, universal quantum data compression protocol, which is developed based on quantum generalization of variable-length coding, as well as quantum strong typicality.',\n",
       "  'categories': 'quant-ph cond-mat.stat-mech cs.it cs.lg hep-th math.it',\n",
       "  'doi': '',\n",
       "  'created': '2021-06-25',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['zhou shangnan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2106.13823'},\n",
       " {'title': 'tight mutual information estimation with contrastive fenchel-legendre   optimization',\n",
       "  'id': '2107.01131',\n",
       "  'abstract': 'successful applications of infonce and its variants have popularized the use of contrastive variational mutual information (mi) estimators in machine learning. while featuring superior stability, these estimators crucially depend on costly large-batch training, and they sacrifice bound tightness for variance reduction. to overcome these limitations, we revisit the mathematics of popular variational mi bounds from the lens of unnormalized statistical modeling and convex optimization. our investigation not only yields a new unified theoretical framework encompassing popular variational mi bounds but also leads to a novel, simple, and powerful contrastive mi estimator named as flo. theoretically, we show that the flo estimator is tight, and it provably converges under stochastic gradient descent. empirically, our flo estimator overcomes the limitations of its predecessors and learns more efficiently. the utility of flo is verified using an extensive set of benchmarks, which also reveals the trade-offs in practical mi estimation.',\n",
       "  'categories': 'stat.ml cs.ai cs.it cs.lg math.it',\n",
       "  'doi': '',\n",
       "  'created': '2021-07-02',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['qing guo',\n",
       "   'junya chen',\n",
       "   'dong wang',\n",
       "   'yuewei yang',\n",
       "   'xinwei deng',\n",
       "   'lawrence carin',\n",
       "   'fan li',\n",
       "   'jing huang',\n",
       "   'chenyang tao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2107.01131'},\n",
       " {'title': 'recurrent parameter generators',\n",
       "  'id': '2107.07110',\n",
       "  'abstract': \"deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. we propose a drastically different approach to compact and optimal deep learning: we decouple the degrees of freedom (dof) and the actual number of parameters of a model, optimize a small dof with predefined random linear constraints for a large model of arbitrary architecture, in one-stage end-to-end learning. specifically, we create a recurrent parameter generator (rpg), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. we show that gradient descent can automatically find the best model under constraints with faster convergence. our extensive experimentation reveals a log-linear relationship between model dof and accuracy. our rpg demonstrates remarkable dof reduction and can be further pruned and quantized for additional run-time performance gain. for example, in terms of top-1 accuracy on imagenet, rpg achieves $96\\\\%$ of resnet18's performance with only $18\\\\%$ dof (the equivalent of one convolutional layer) and $52\\\\%$ of resnet34's performance with only $0.25\\\\%$ dof! our work shows a significant potential of constrained neural optimization in compact and optimal deep learning.\",\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-07-15',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['jiayun wang',\n",
       "   'yubei chen',\n",
       "   'stella x. yu',\n",
       "   'brian cheung',\n",
       "   'yann lecun'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2107.07110'},\n",
       " {'title': 'transferring dexterous manipulation from gpu simulation to a remote   real-world trifinger',\n",
       "  'id': '2108.09779',\n",
       "  'abstract': \"we present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary 6-dof pose with only 3-fingers trained with nvidia's isaacgym simulator. we show empirical benefits, both in simulation and sim-to-real transfer, of using keypoints as opposed to position+quaternion representations for the object pose in 6-dof for policy observations and in reward calculation to train a model-free reinforcement learning agent. by utilizing domain randomization strategies along with the keypoint representation of the pose of the manipulated object, we achieve a high success rate of 83% on a remote trifinger system maintained by the organizers of the real robot challenge. with the aim of assisting further research in learning in-hand manipulation, we make the codebase of our system, along with trained checkpoints that come with billions of steps of experience available, at https://s2r2-ig.github.io\",\n",
       "  'categories': 'cs.ro cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-08-22',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['arthur allshire',\n",
       "   'mayank mittal',\n",
       "   'varun lodaya',\n",
       "   'viktor makoviychuk',\n",
       "   'denys makoviichuk',\n",
       "   'felix widmaier',\n",
       "   'manuel wüthrich',\n",
       "   'stefan bauer',\n",
       "   'ankur handa',\n",
       "   'animesh garg'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2108.09779'},\n",
       " {'title': 'sample noise impact on active learning',\n",
       "  'id': '2109.01372',\n",
       "  'abstract': 'this work explores the effect of noisy sample selection in active learning strategies. we show on both synthetic problems and real-life use-cases that knowledge of the sample noise can significantly improve the performance of active learning strategies. building on prior work, we propose a robust sampler, incremental weighted k-means that brings significant improvement on the synthetic tasks but only a marginal uplift on real-life ones. we hope that the questions raised in this paper are of interest to the community and could open new paths for active learning research.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-09-03',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['alexandre abraham', 'léo dreyfus-schmidt'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2109.01372'},\n",
       " {'title': 'learning from few samples: transformation-invariant svms with   composition and locality at multiple scales',\n",
       "  'id': '2109.12784',\n",
       "  'abstract': 'motivated by the problem of learning with small sample sizes, this paper shows how to incorporate into support-vector machines (svms) those properties that have made convolutional neural networks (cnns) successful. particularly important is the ability to incorporate domain knowledge of invariances, e.g., translational invariance of images. kernels based on the \\\\textit{maximum} similarity over a group of transformations are not generally positive definite. perhaps it is for this reason that they have not been studied theoretically. we address this lacuna and show that positive definiteness indeed holds \\\\textit{with high probability} for kernels based on the maximum similarity in the small training sample set regime of interest, and that they do yield the best results in that regime. we also show how additional properties such as their ability to incorporate local features at multiple spatial scales, e.g., as done in cnns through max pooling, and to provide the benefits of composition through the architecture of multiple layers, can also be embedded into svms. we verify through experiments on widely available image sets that the resulting svms do provide superior accuracy in comparison to well-established deep neural network benchmarks for small sample sizes.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2021-09-27',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['tao liu', 'p. r. kumar', 'ruida zhou', 'xi liu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2109.12784'},\n",
       " {'title': 'sim2real for soft robotic fish via differentiable simulation',\n",
       "  'id': '2109.14855',\n",
       "  'abstract': \"accurate simulation of soft mechanisms under dynamic actuation is critical for the design of soft robots. we address this gap with our differentiable simulation tool by learning the material parameters of our soft robotic fish. on the example of a soft robotic fish, we demonstrate an experimentally-verified, fast optimization pipeline for learning the material parameters from quasi-static data via differentiable simulation and apply it to the prediction of dynamic performance. our method identifies physically plausible young's moduli for various soft silicone elastomers and stiff acetal copolymers used in creation of our three different robotic fish tail designs. we show that our method is compatible with varying internal geometry of the actuators, such as the number of hollow cavities. our framework allows high fidelity prediction of dynamic behavior for composite bi-morph bending structures in real hardware to millimeter-accuracy and within 3 percent error normalized to actuator length. we provide a differentiable and robust estimate of the thrust force using a neural network thrust predictor; this estimate allows for accurate modeling of our experimental setup measuring bollard pull. this work presents a prototypical hardware and simulation problem solved using our differentiable framework; the framework can be applied to higher dimensional parameter inference, learning control policies, and computational design due to its differentiable character.\",\n",
       "  'categories': 'cs.ro cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-09-30',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['john z. zhang',\n",
       "   'yu zhang',\n",
       "   'pingchuan ma',\n",
       "   'elvis nava',\n",
       "   'tao du',\n",
       "   'philip arm',\n",
       "   'wojciech matusik',\n",
       "   'robert k. katzschmann'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2109.14855'},\n",
       " {'title': 'quantum semi-supervised learning with quantum supremacy',\n",
       "  'id': '2110.02343',\n",
       "  'abstract': 'quantum machine learning promises to efficiently solve important problems. there are two persistent challenges in classical machine learning: the lack of labeled data, and the limit of computational power. we propose a novel framework that resolves both issues: quantum semi-supervised learning. moreover, we provide a protocol in systematically designing quantum machine learning algorithms with quantum supremacy, which can be extended beyond quantum semi-supervised learning. in the meantime, we show that naive quantum matrix product estimation algorithm outperforms the best known classical matrix multiplication algorithm. we showcase two concrete quantum semi-supervised learning algorithms: a quantum self-training algorithm named the propagating nearest-neighbor classifier, and the quantum semi-supervised k-means clustering algorithm. by doing time complexity analysis, we conclude that they indeed possess quantum supremacy.',\n",
       "  'categories': 'quant-ph cond-mat.stat-mech cs.it cs.lg hep-th math.it',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-05',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['zhou shangnan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.02343'},\n",
       " {'title': 'can an ai agent hit a moving target?',\n",
       "  'id': '2110.02474',\n",
       "  'abstract': 'i model the belief formation and decision making processes of economic agents during a monetary policy regime change (an acceleration in the money supply) with a deep reinforcement learning algorithm in the ai literature. i show that when the money supply accelerates, the learning agents only adjust their actions, which include consumption and demand for real balance, after gathering learning experience for many periods. this delayed adjustments leads to low returns during transition periods. once they start adjusting to the new environment, their welfare improves. their changes in beliefs and actions lead to temporary inflation volatility. i also show that, 1. the ai agents who explores their environment more adapt to the policy regime change quicker, which leads to welfare improvements and less inflation volatility, and 2. the ai agents who have experienced a structural change adjust their beliefs and behaviours quicker than an inexperienced learning agent.',\n",
       "  'categories': 'econ.th cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-05',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['n/a rui', 'n/a shi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.02474'},\n",
       " {'title': 'label noise in adversarial training: a novel perspective to study robust   overfitting',\n",
       "  'id': '2110.03135',\n",
       "  'abstract': 'we show that label noise exists in adversarial training. such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-06',\n",
       "  'updated': '2022-10-19',\n",
       "  'authors': ['chengyu dong', 'liyuan liu', 'jingbo shang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.03135'},\n",
       " {'title': 'learning a subspace of policies for online adaptation in reinforcement   learning',\n",
       "  'id': '2110.05169',\n",
       "  'abstract': \"deep reinforcement learning (rl) is mainly studied in a setting where the training and the testing environments are similar. but in many practical applications, these environments may differ. for instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. it can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). there is a need to develop rl methods that generalize well to variations of the training conditions. in this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. this online adaptation process can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-rl techniques since there is just a single train environment. to do so, we propose an approach where we learn a subspace of policies within the parameter space. this subspace contains an infinite number of policies that are trained to solve the training environment while having different parameter values. as a consequence, two policies in that subspace process information differently and exhibit different behaviors when facing variations of the train environment. our experiments carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods. in comparison, our approach is simple to tune, does not need any extra component (e.g., discriminator) and learns policies able to gather a high reward on unseen environments.\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-11',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['jean-baptiste gaya', 'laure soulier', 'ludovic denoyer'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.05169'},\n",
       " {'title': 'yformer: u-net inspired transformer architecture for far horizon time   series forecasting',\n",
       "  'id': '2110.08255',\n",
       "  'abstract': 'time series data is ubiquitous in research as well as in a wide variety of industrial applications. effectively analyzing the available historical data and providing insights into the far future allows us to make effective decisions. recent research has witnessed the superior performance of transformer-based architectures, especially in the regime of far horizon time series forecasting. however, the current state of the art sparse transformer architectures fail to couple down- and upsampling procedures to produce outputs in a similar resolution as the input. we propose the yformer model, based on a novel y-shaped encoder-decoder architecture that (1) uses direct connection from the downscaled encoder layer to the corresponding upsampled decoder layer in a u-net inspired architecture, (2) combines the downscaling/upsampling with sparse attention to capture long-range effects, and (3) stabilizes the encoder-decoder stacks with the addition of an auxiliary reconstruction loss. extensive experiments have been conducted with relevant baselines on four benchmark datasets, demonstrating an average improvement of 19.82, 18.41 percentage mse and 13.62, 11.85 percentage mae in comparison to the current state of the art for the univariate and the multivariate settings respectively.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-13',\n",
       "  'updated': '2022-08-25',\n",
       "  'authors': ['kiran madhusudhanan',\n",
       "   'johannes burchert',\n",
       "   'nghia duong-trung',\n",
       "   'stefan born',\n",
       "   'lars schmidt-thieme'],\n",
       "  'affiliation': ['university of hildesheim',\n",
       "   'university of hildesheim',\n",
       "   'technische universität berlin',\n",
       "   'technische universität berlin',\n",
       "   'university of hildesheim'],\n",
       "  'url': 'https://arxiv.org/abs/2110.08255'},\n",
       " {'title': 'meta-learning with adjoint methods',\n",
       "  'id': '2110.08432',\n",
       "  'abstract': 'model agnostic meta learning (maml) is widely used to find a good initialization for a family of tasks. despite its success, a critical challenge in maml is to calculate the gradient w.r.t. the initialization of a long training trajectory for the sampled tasks, because the computation graph can rapidly explode and the computational cost is very expensive. to address this problem, we propose adjoint maml (a-maml). we view gradient descent in the inner optimization as the evolution of an ordinary differential equation (ode). to efficiently compute the gradient of the validation loss w.r.t. the initialization, we use the adjoint method to construct a companion, backward ode. to obtain the gradient w.r.t. the initialization, we only need to run the standard ode solver twice -- one is forward in time that evolves a long trajectory of gradient flow for the sampled task; the other is backward and solves the adjoint ode. we need not create or expand any intermediate computational graphs, adopt aggressive approximations, or impose proximal regularizers in the training loss. our approach is cheap, accurate, and adaptable to different trajectory lengths. we demonstrate the advantage of our approach in both synthetic and real-world meta-learning tasks.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-15',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['shibo li',\n",
       "   'zheng wang',\n",
       "   'akil narayan',\n",
       "   'robert kirby',\n",
       "   'shandian zhe'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.08432'},\n",
       " {'title': 'toward understanding convolutional neural networks from volterra   convolution perspective',\n",
       "  'id': '2110.09902',\n",
       "  'abstract': 'we make an attempt to understanding convolutional neural network by exploring the relationship between (deep) convolutional neural networks and volterra convolutions. we propose a novel approach to explain and study the overall characteristics of neural networks without being disturbed by the horribly complex architectures. specifically, we attempt to convert the basic structures of a convolutional neural network (cnn) and their combinations to the form of volterra convolutions. the results show that most of convolutional neural networks can be approximated in the form of volterra convolution, where the approximated proxy kernels preserve the characteristics of the original network. analyzing these proxy kernels may give valuable insight about the original network. base on this setup, we presented methods to approximating the order-zero and order-one proxy kernels, and verified the correctness and effectiveness of our results.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-19',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['tenghui li', 'guoxu zhou', 'yuning qiu', 'qibin zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.09902'},\n",
       " {'title': 'differentially private coordinate descent for composite empirical risk   minimization',\n",
       "  'id': '2110.11688',\n",
       "  'abstract': 'machine learning models can leak information about the data used to train them. to mitigate this issue, differentially private (dp) variants of optimization algorithms like stochastic gradient descent (dp-sgd) have been designed to trade-off utility for privacy in empirical risk minimization (erm) problems. in this paper, we propose differentially private proximal coordinate descent (dp-cd), a new method to solve composite dp-erm problems. we derive utility guarantees through a novel theoretical analysis of inexact coordinate descent. our results show that, thanks to larger step sizes, dp-cd can exploit imbalance in gradient coordinates to outperform dp-sgd. we also prove new lower bounds for composite dp-erm under coordinate-wise regularity assumptions, that are nearly matched by dp-cd. for practical implementations, we propose to clip gradients using coordinate-wise thresholds that emerge from our theory, avoiding costly hyperparameter tuning. experiments on real and synthetic data support our results, and show that dp-cd compares favorably with dp-sgd.',\n",
       "  'categories': 'cs.lg cs.cr stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-22',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['paul mangold',\n",
       "   'aurélien bellet',\n",
       "   'joseph salmon',\n",
       "   'marc tommasi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.11688'},\n",
       " {'title': 'technology fitness landscape for design innovation: a deep neural   embedding approach based on patent data',\n",
       "  'id': '2110.13624',\n",
       "  'abstract': \"technology is essential to innovation and economic prosperity. understanding technological changes can guide innovators to find new directions of design innovation and thus make breakthroughs. in this work, we construct a technology fitness landscape via deep neural embeddings of patent data. the landscape consists of 1,757 technology domains and their respective improvement rates. in the landscape, we found a high hill related to information and communication technologies (ict) and a vast low plain of the remaining domains. the landscape presents a bird's eye view of the structure of the total technology space, providing a new way for innovators to interpret technology evolution with a biological analogy, and a biologically-inspired inference to the next innovation.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-21',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['shuo jiang', 'jianxi luo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.13624'},\n",
       " {'title': 'dynamic population-based meta-learning for multi-agent communication   with natural language',\n",
       "  'id': '2110.14241',\n",
       "  'abstract': 'in this work, our goal is to train agents that can coordinate with seen, unseen as well as human partners in a multi-agent communication environment involving natural language. previous work using a single set of agents has shown great progress in generalizing to known partners, however it struggles when coordinating with unfamiliar agents. to mitigate that, recent work explored the use of population-based approaches, where multiple agents interact with each other with the goal of learning more generic protocols. these methods, while able to result in good coordination between unseen partners, still only achieve so in cases of simple languages, thus failing to adapt to human partners using natural language. we attribute this to the use of static populations and instead propose a dynamic population-based meta-learning approach that builds such a population in an iterative manner. we perform a holistic evaluation of our method on two different referential games, and show that our agents outperform all prior work when communicating with seen partners and humans. furthermore, we analyze the natural language generation skills of our agents, where we find that our agents also outperform strong baselines. finally, we test the robustness of our agents when communicating with out-of-population agents and carefully test the importance of each component of our method through ablation studies.',\n",
       "  'categories': 'cs.lg cs.ai cs.cl cs.ma stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2021-10-27',\n",
       "  'updated': '',\n",
       "  'authors': ['abhinav gupta', 'marc lanctot', 'angeliki lazaridou'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2110.14241'},\n",
       " {'title': 'global optimality and finite sample analysis of softmax off-policy actor   critic under state distribution mismatch',\n",
       "  'id': '2111.02997',\n",
       "  'abstract': 'in this paper, we establish the global optimality and convergence rate of an off-policy actor critic algorithm in the tabular setting without using density ratio to correct the discrepancy between the state distribution of the behavior policy and that of the target policy. our work goes beyond existing works on the optimality of policy gradient methods in that existing works use the exact policy gradient for updating the policy parameters while we use an approximate and stochastic update step. our update step is not a gradient update because we do not use a density ratio to correct the state distribution, which aligns well with what practitioners do. our update is approximate because we use a learned critic instead of the true value function. our update is stochastic because at each step the update is done for only the current state action pair. moreover, we remove several restrictive assumptions from existing works in our analysis. central to our work is the finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on time-inhomogeneous markov chains, based on its uniform contraction properties.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-11-04',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['shangtong zhang', 'remi tachet', 'romain laroche'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2111.02997'},\n",
       " {'title': 'improved regret analysis for variance-adaptive linear bandits and   horizon-free linear mixture mdps',\n",
       "  'id': '2111.03289',\n",
       "  'abstract': \"in online learning problems, exploiting low variance plays an important role in obtaining tight performance guarantees yet is challenging because variances are often not known a priori. recently, considerable progress has been made by zhang et al. (2021) where they obtain a variance-adaptive regret bound for linear bandits without knowledge of the variances and a horizon-free regret bound for linear mixture markov decision processes (mdps). in this paper, we present novel analyses that improve their regret bounds significantly. for linear bandits, we achieve $\\\\tilde o(\\\\min\\\\{d\\\\sqrt{k}, d^{1.5}\\\\sqrt{\\\\sum_{k=1}^k \\\\sigma_k^2}\\\\} + d^2)$ where $d$ is the dimension of the features, $k$ is the time horizon, and $\\\\sigma_k^2$ is the noise variance at time step $k$, and $\\\\tilde o$ ignores polylogarithmic dependence, which is a factor of $d^3$ improvement. for linear mixture mdps with the assumption of maximum cumulative reward in an episode being in $[0,1]$, we achieve a horizon-free regret bound of $\\\\tilde o(d \\\\sqrt{k} + d^2)$ where $d$ is the number of base models and $k$ is the number of episodes. this is a factor of $d^{3.5}$ improvement in the leading term and $d^7$ in the lower order term. our analysis critically relies on a novel peeling-based regret analysis that leverages the elliptical potential `count' lemma.\",\n",
       "  'categories': 'stat.ml cs.lg math.st stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2021-11-05',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['yeoneung kim', 'insoon yang', 'kwang-sung jun'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2111.03289'},\n",
       " {'title': 'oracle teacher: leveraging target information for better knowledge   distillation of ctc models',\n",
       "  'id': '2111.03664',\n",
       "  'abstract': \"knowledge distillation (kd), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). conventional kd methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (ctc)-based sequence models, namely oracle teacher, that leverages both the source inputs and the output labels as the teacher model's input. since the oracle teacher learns a more accurate ctc alignment by referring to the target information, it can provide the student with more optimal guidance. one potential risk for the proposed approach is a trivial solution that the model's output directly copies the target input. based on a many-to-one mapping property of the ctc algorithm, we present a training strategy that can effectively prevent the trivial solution and thus enables utilizing both source and target inputs for model training. extensive experiments are conducted on two sequence learning tasks: speech recognition and scene text recognition. from the experimental results, we empirically show that the proposed model improves the students across these tasks while achieving a considerable speed-up in the teacher model's training time.\",\n",
       "  'categories': 'cs.lg eess.as eess.iv',\n",
       "  'doi': '',\n",
       "  'created': '2021-11-05',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['ji won yoon',\n",
       "   'hyung yong kim',\n",
       "   'hyeonseung lee',\n",
       "   'sunghwan ahn',\n",
       "   'nam soo kim'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2111.03664'},\n",
       " {'title': 'on-demand unlabeled personalized federated learning',\n",
       "  'id': '2111.08356',\n",
       "  'abstract': 'in federated learning (fl), multiple clients collaborate to learn a shared model through a central server while keeping data decentralized. personalized federated learning (pfl) further extends fl by learning a personalized model per client. in both fl and pfl, all clients participate in the training process and their labeled data are used for training. however, in reality, novel clients may wish to join a prediction service after it has been deployed, obtaining predictions for their own \\\\textbf{unlabeled} data.   here, we introduce a new learning setup, on-demand unlabeled pfl (od-pfl), where a system trained on a set of clients, needs to be later applied to novel unlabeled clients at inference time. we propose a novel approach to this problem, odpfl-hn, which learns to produce a new model for the late-to-the-party client. specifically, we train an encoder network that learns a representation for a client given its unlabeled data. that client representation is fed to a hypernetwork that generates a personalized model for that client. evaluated on five benchmark datasets, we find that odpfl-hn generalizes better than the current fl and pfl methods, especially when the novel client has a large shift from training clients. we also analyzed the generalization error for novel clients, and showed analytically and experimentally how novel clients can apply differential privacy.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-11-16',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['ohad amosy', 'gal eyal', 'gal chechik'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2111.08356'},\n",
       " {'title': 'adaptively calibrated critic estimates for deep reinforcement learning',\n",
       "  'id': '2111.12673',\n",
       "  'abstract': 'accurate value estimates are important for off-policy reinforcement learning. algorithms based on temporal difference learning typically are prone to an over- or underestimation bias building up over time. in this paper, we propose a general method called adaptively calibrated critics (acc) that uses the most recent high variance but unbiased on-policy rollouts to alleviate the bias of the low variance temporal difference targets. we apply acc to truncated quantile critics, which is an algorithm for continuous control that allows regulation of the bias with a hyperparameter tuned per environment. the resulting algorithm adaptively adjusts the parameter during training rendering hyperparameter search unnecessary and sets a new state of the art on the openai gym continuous control benchmark among all algorithms that do not tune hyperparameters for each environment. acc further achieves improved results on different tasks from the meta-world robot benchmark. additionally, we demonstrate the generality of acc by applying it to td3 and showing an improved performance also in this setting.',\n",
       "  'categories': 'cs.lg cs.ai cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2021-11-24',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['nicolai dorka',\n",
       "   'tim welschehold',\n",
       "   'joschka boedecker',\n",
       "   'wolfram burgard'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2111.12673'},\n",
       " {'title': 'a softmax-free loss function based on predefined optimal-distribution of   latent features for deep learning classifier',\n",
       "  'id': '2111.15449',\n",
       "  'abstract': 'in the field of pattern classification, the training of deep learning classifiers is mostly end-to-end learning, and the loss function is the constraint on the final output (posterior probability) of the network, so the existence of softmax is essential. in the case of end-to-end learning, there is usually no effective loss function that completely relies on the features of the middle layer to restrict learning, resulting in the distribution of sample latent features is not optimal, so there is still room for improvement in classification accuracy. based on the concept of predefined evenly-distributed class centroids (pedcc), this article proposes a softmax-free loss function based on predefined optimal-distribution of latent features-pod loss. the loss function only restricts the latent features of the samples, including the norm-adaptive cosine distance between the latent feature vector of the sample and the center of the predefined evenly-distributed class, and the correlation between the latent features of the samples. finally, cosine distance is used for classification. compared with the commonly used softmax loss, some typical softmax related loss functions and pedcc-loss, experiments on several commonly used datasets on several typical deep learning classification networks show that the classification performance of pod loss is always significant better and easier to converge. code is available in https://github.com/tianyuzu/pod-loss.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '10.1109/tcsvt.2022.3212426',\n",
       "  'created': '2021-11-25',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['qiuyu zhu', 'xuewen zu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2111.15449'},\n",
       " {'title': 'learning knot invariants across dimensions',\n",
       "  'id': '2112.00016',\n",
       "  'abstract': \"we use deep neural networks to machine learn correlations between knot invariants in various dimensions. the three-dimensional invariant of interest is the jones polynomial $j(q)$, and the four-dimensional invariants are the khovanov polynomial $\\\\text{kh}(q,t)$, smooth slice genus $g$, and rasmussen's $s$-invariant. we find that a two-layer feed-forward neural network can predict $s$ from $\\\\text{kh}(q,-q^{-4})$ with greater than $99\\\\%$ accuracy. a theoretical explanation for this performance exists in knot theory via the now disproven knight move conjecture, which is obeyed by all knots in our dataset. more surprisingly, we find similar performance for the prediction of $s$ from $\\\\text{kh}(q,-q^{-2})$, which suggests a novel relationship between the khovanov and lee homology theories of a knot. the network predicts $g$ from $\\\\text{kh}(q,t)$ with similarly high accuracy, and we discuss the extent to which the machine is learning $s$ as opposed to $g$, since there is a general inequality $|s| \\\\leq 2g$. the jones polynomial, as a three-dimensional invariant, is not obviously related to $s$ or $g$, but the network achieves greater than $95\\\\%$ accuracy in predicting either from $j(q)$. moreover, similar accuracy can be achieved by evaluating $j(q)$ at roots of unity. this suggests a relationship with $su(2)$ chern--simons theory, and we review the gauge theory construction of khovanov homology which may be relevant for explaining the network's performance.\",\n",
       "  'categories': 'hep-th cs.lg math.gt',\n",
       "  'doi': '',\n",
       "  'created': '2021-11-30',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['jessica craven', 'mark hughes', 'vishnu jejjala', 'arjun kar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2112.00016'},\n",
       " {'title': 'first-order regret in reinforcement learning with linear function   approximation: a robust estimation approach',\n",
       "  'id': '2112.03432',\n",
       "  'abstract': 'obtaining first-order regret bounds -- regret bounds scaling not as the worst-case but with some measure of the performance of the optimal policy on a given instance -- is a core question in sequential decision-making. while such bounds exist in many settings, they have proven elusive in reinforcement learning with large state spaces. in this work we address this gap, and show that it is possible to obtain regret scaling as $\\\\widetilde{\\\\mathcal{o}}(\\\\sqrt{d^3 h^3 \\\\cdot v_1^\\\\star \\\\cdot k} + d^{3.5}h^3\\\\log k )$ in reinforcement learning with large state spaces, namely the linear mdp setting. here $v_1^\\\\star$ is the value of the optimal policy and $k$ is the number of episodes. we demonstrate that existing techniques based on least squares estimation are insufficient to obtain this result, and instead develop a novel robust self-normalized concentration bound based on the robust catoni mean estimator, which may be of independent interest.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2021-12-06',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['andrew wagenmaker',\n",
       "   'yifang chen',\n",
       "   'max simchowitz',\n",
       "   'simon s. du',\n",
       "   'kevin jamieson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2112.03432'},\n",
       " {'title': 'kge-cl: contrastive learning of tensor decomposition based knowledge   graph embeddings',\n",
       "  'id': '2112.04871',\n",
       "  'abstract': 'learning the embeddings of knowledge graphs (kg) is vital in artificial intelligence, and can benefit various downstream applications, such as recommendation and question answering. in recent years, many research efforts have been proposed for knowledge graph embedding (kge). however, most previous kge methods ignore the semantic similarity between the related entities and entity-relation couples in different triples since they separately optimize each triple with the scoring function. to address this problem, we propose a simple yet efficient contrastive learning framework for tensor decomposition based (tdb) kge, which can shorten the semantic distance of the related entities and entity-relation couples in different triples and thus improve the performance of kge. we evaluate our proposed method on three standard kge datasets: wn18rr, fb15k-237 and yago3-10. our method can yield some new state-of-the-art results, achieving 51.2% mrr, 46.8% hits@1 on the wn18rr dataset, 37.8% mrr, 28.6% hits@1 on fb15k-237 dataset, and 59.1% mrr, 51.8% hits@1 on the yago3-10 dataset.',\n",
       "  'categories': 'cs.ai cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2021-12-09',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['zhiping luo',\n",
       "   'wentao xu',\n",
       "   'weiqing liu',\n",
       "   'jiang bian',\n",
       "   'jian yin',\n",
       "   'tie-yan liu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2112.04871'},\n",
       " {'title': 'a prediction-based approach for online dynamic patient scheduling: a   case study in radiotherapy treatment',\n",
       "  'id': '2112.08549',\n",
       "  'abstract': \"patient scheduling is a difficult task involving stochastic factors such as the unknown arrival times of patients. similarly, the scheduling of radiotherapy for cancer treatments needs to handle patients with different urgency levels when allocating resources. high priority patients may arrive at any time, and there must be resources available to accommodate them. a common solution is to reserve a flat percentage of treatment capacity for emergency patients. however, this solution can result in overdue treatments for urgent patients, a failure to fully exploit treatment capacity, and delayed treatments for low-priority patients. this problem is especially severe in large and crowded hospitals. in this paper, we propose a prediction-based approach for online dynamic radiotherapy scheduling that dynamically adapts the present scheduling decision based on each incoming patient and the current allocation of resources. our approach is based on a regression model trained to recognize the links between patients' arrival patterns, and their ideal waiting time in optimal offline solutions where all future arrivals are known in advance. when our prediction-based approach is compared to flat-reservation policies, it does a better job of preventing overdue treatments for emergency patients, while also maintaining comparable waiting times for the other patients. we also demonstrate how our proposed approach supports explainability and interpretability in scheduling decisions using shap values.\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2021-12-15',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['tu-san pham',\n",
       "   'antoine legrain',\n",
       "   'patrick de causmaecker',\n",
       "   'louis-martin rousseau'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2112.08549'},\n",
       " {'title': 'robust natural language processing: recent advances, challenges, and   future directions',\n",
       "  'id': '2201.00768',\n",
       "  'abstract': \"recent natural language processing (nlp) techniques have accomplished high performance on benchmark datasets, primarily due to the significant improvement in the performance of deep learning. the advances in the research community have led to great enhancements in state-of-the-art production systems for nlp tasks, such as virtual assistants, speech recognition, and sentiment analysis. however, such nlp systems still often fail when tested with adversarial attacks. the initial lack of robustness exposed troubling gaps in current models' language understanding capabilities, creating problems when nlp systems are deployed in real life. in this paper, we present a structured overview of nlp robustness research by summarizing the literature in a systemic way across various dimensions. we then take a deep-dive into the various dimensions of robustness, across techniques, metrics, embeddings, and benchmarks. finally, we argue that robustness should be multi-dimensional, provide insights into current research, identify gaps in the literature to suggest directions worth pursuing to address these gaps.\",\n",
       "  'categories': 'cs.cl cs.ai cs.cr cs.hc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-01-03',\n",
       "  'updated': '',\n",
       "  'authors': ['marwan omar',\n",
       "   'soohyeon choi',\n",
       "   'daehun nyang',\n",
       "   'david mohaisen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.00768'},\n",
       " {'title': 'diversity-boosted generalization-specialization balancing for zero-shot   learning',\n",
       "  'id': '2201.01961',\n",
       "  'abstract': \"zero-shot learning (zsl) aims to transfer classification capability from seen to unseen classes. recent methods have proved that generalization and specialization are two essential abilities to achieve good performance in zsl. however, focusing on only one of the abilities may result in models that are either too general with degraded classification ability or too specialized to generalize to unseen classes. in this paper, we propose an end-to-end network, termed as bgsnet, which equips and balances generalization and specialization abilities at the instance and dataset level. specifically, bgsnet consists of two branches: the generalization network (gnet), which applies episodic meta-learning to learn generalized knowledge, and the balanced specialization network (bsnet), which adopts multiple attentive extractors to extract discriminative features and achieve instance-level balance. a novel self-adjusted diversity loss is designed to optimize bsnet with redundancy reduced and diversity boosted. we further propose a differentiable dataset-level balance and update the weights in a linear annealing schedule to simulate network pruning and thus obtain the optimal structure for bsnet with dataset-level balance achieved. experiments on four benchmark datasets demonstrate our model's effectiveness. sufficient component ablations prove the necessity of integrating and balancing generalization and specialization abilities.\",\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-01-06',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['yun li',\n",
       "   'zhe liu',\n",
       "   'xiaojun chang',\n",
       "   'julian mcauley',\n",
       "   'lina yao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.01961'},\n",
       " {'title': 'noisy neonatal chest sound separation for high-quality heart and lung   sounds',\n",
       "  'id': '2201.03211',\n",
       "  'abstract': 'stethoscope-recorded chest sounds provide the opportunity for remote cardio-respiratory health monitoring of neonates. however, reliable monitoring requires high-quality heart and lung sounds. this paper presents novel non-negative matrix factorisation (nmf) and non-negative matrix co-factorisation (nmcf) methods for neonatal chest sound separation. to assess these methods and compare with existing single-source separation methods, an artificial mixture dataset was generated comprising of heart, lung and noise sounds. signal-to-noise ratios were then calculated for these artificial mixtures. these methods were also tested on real-world noisy neonatal chest sounds and assessed based on vital sign estimation error and a signal quality score of 1-5 developed in our previous works. additionally, the computational cost of all methods was assessed to determine the applicability for real-time processing. overall, both the proposed nmf and nmcf methods outperform the next best existing method by 2.7db to 11.6db for the artificial dataset and 0.40 to 1.12 signal quality improvement for the real-world dataset. the median processing time for the sound separation of a 10s recording was found to be 28.3s for nmcf and 342ms for nmf. because of stable and robust performance, we believe that our proposed methods are useful to denoise neonatal heart and lung sound in a real-world environment. codes for proposed and existing methods can be found at: https://github.com/egrooby-monash/heart-and-lung-sound-separation.',\n",
       "  'categories': 'eess.as cs.lg cs.sd eess.sp',\n",
       "  'doi': '10.1109/jbhi.2022.3215995',\n",
       "  'created': '2022-01-10',\n",
       "  'updated': '',\n",
       "  'authors': ['ethan grooby',\n",
       "   'chiranjibi sitaula',\n",
       "   'davood fattahi',\n",
       "   'reza sameni',\n",
       "   'kenneth tan',\n",
       "   'lindsay zhou',\n",
       "   'arrabella king',\n",
       "   'ashwin ramanathan',\n",
       "   'atul malhotra',\n",
       "   'guy a. dumont',\n",
       "   'faezeh marzbanrad'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.03211'},\n",
       " {'title': 'knock detection in combustion engine time series using a theory-guided   1d convolutional neural network approach',\n",
       "  'id': '2201.06990',\n",
       "  'abstract': 'this paper introduces a method for the detection of knock occurrences in an internal combustion engine (ice) using a 1d convolutional neural network trained on in-cylinder pressure data. the model architecture was based on considerations regarding the expected frequency characteristics of knocking combustion. to aid the feature extraction, all cycles were reduced to 60{\\\\deg} ca long windows, with no further processing applied to the pressure traces. the neural networks were trained exclusively on in-cylinder pressure traces from multiple conditions and labels provided by human experts. the best-performing model architecture achieves an accuracy of above 92% on all test sets in a tenfold cross-validation when distinguishing between knocking and non-knocking cycles. in a multi-class problem where each cycle was labeled by the number of experts who rated it as knocking, 78% of cycles were labeled perfectly, while 90% of cycles were classified at most one class from ground truth. they thus considerably outperform the broadly applied mapo (maximum amplitude of pressure oscillation) detection method, as well as other references reconstructed from previous works. our analysis indicates that the neural network learned physically meaningful features connected to engine-characteristic resonance frequencies, thus verifying the intended theory-guided data science approach. deeper performance investigation further shows remarkable generalization ability to unseen operating points. in addition, the model proved to classify knocking cycles in unseen engines with increased accuracy of 89% after adapting to their features via training on a small number of exclusively non-knocking cycles. the algorithm takes below 1 ms (on cpu) to classify individual cycles, effectively making it suitable for real-time engine control.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '10.1109/tmech.2022.3144832',\n",
       "  'created': '2022-01-18',\n",
       "  'updated': '',\n",
       "  'authors': ['andreas b. ofner',\n",
       "   'achilles kefalas',\n",
       "   'stefan posch',\n",
       "   'bernhard c. geiger'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.06990'},\n",
       " {'title': 'texthacker: learning based hybrid local search algorithm for text   hard-label adversarial attack',\n",
       "  'id': '2201.08193',\n",
       "  'abstract': 'existing textual adversarial attacks usually utilize the gradient or prediction confidence to generate adversarial examples, making it hard to be deployed in real-world applications. to this end, we consider a rarely investigated but more rigorous setting, namely hard-label attack, in which the attacker can only access the prediction label. in particular, we find we can learn the importance of different words via the change on prediction label caused by word substitutions on the adversarial examples. based on this observation, we propose a novel adversarial attack, termed text hard-label attacker (texthacker). texthacker randomly perturbs lots of words to craft an adversarial example. then, texthacker adopts a hybrid local search algorithm with the estimation of word importance from the attack history to minimize the adversarial perturbation. extensive evaluations for text classification and textual entailment show that texthacker significantly outperforms existing hard-label attacks regarding the attack performance as well as adversary quality.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-01-20',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['zhen yu', 'xiaosen wang', 'wanxiang che', 'kun he'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.08193'},\n",
       " {'title': 'multimodal data matters: language model pre-training over structured and   unstructured electronic health records',\n",
       "  'id': '2201.10113',\n",
       "  'abstract': 'as two important textual modalities in electronic health records (ehr), both structured data (clinical codes) and unstructured data (clinical narratives) have recently been increasingly applied to the healthcare domain. most existing ehr-oriented studies, however, either focus on a particular modality or integrate data from different modalities in a straightforward manner, which usually treats structured and unstructured data as two independent sources of information about patient admission and ignore the intrinsic interactions between them. in fact, the two modalities are documented during the same encounter where structured data inform the documentation of unstructured data and vice versa. in this paper, we proposed a medical multimodal pre-trained language model, named medm-plm, to learn enhanced ehr representations over structured and unstructured data and explore the interaction of two modalities. in medm-plm, two transformer-based neural network components are firstly adopted to learn representative characteristics from each modality. a cross-modal module is then introduced to model their interactions. we pre-trained medm-plm on the mimic-iii dataset and verified the effectiveness of the model on three downstream clinical tasks, i.e., medication recommendation, 30-day readmission prediction and icd coding. extensive experiments demonstrate the power of medm-plm compared with state-of-the-art methods. further analyses and visualizations show the robustness of our model, which could potentially provide more comprehensive interpretations for clinical decision-making.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-01-25',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['sicen liu',\n",
       "   'xiaolong wang',\n",
       "   'yongshuai hou',\n",
       "   'ge li',\n",
       "   'hui wang',\n",
       "   'hui xu',\n",
       "   'yang xiang',\n",
       "   'buzhou tang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.10113'},\n",
       " {'title': 'learning proximal operators to discover multiple optima',\n",
       "  'id': '2201.11945',\n",
       "  'abstract': 'finding multiple solutions of non-convex optimization problems is a ubiquitous yet challenging task. most past algorithms either apply single-solution optimization methods from multiple random initial guesses or search in the vicinity of found solutions using ad hoc heuristics. we present an end-to-end method to learn the proximal operator of a family of training problems so that multiple local minima can be quickly obtained from initial guesses by iterating the learned operator, emulating the proximal-point algorithm that has fast convergence. the learned proximal operator can be further generalized to recover multiple optima for unseen problems at test time, enabling applications such as object detection. the key ingredient in our formulation is a proximal regularization term, which elevates the convexity of our training loss: by applying recent theoretical results, we show that for weakly-convex objectives with lipschitz gradients, training of the proximal operator converges globally with a practical degree of over-parameterization. we further present an exhaustive benchmark for multi-solution optimization to demonstrate the effectiveness of our method.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-01-28',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['lingxiao li',\n",
       "   'noam aigerman',\n",
       "   'vladimir g. kim',\n",
       "   'jiajin li',\n",
       "   'kristjan greenewald',\n",
       "   'mikhail yurochkin',\n",
       "   'justin solomon'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.11945'},\n",
       " {'title': 'faster convergence of local sgd for over-parameterized models',\n",
       "  'id': '2201.12719',\n",
       "  'abstract': 'modern machine learning architectures are often highly expressive. they are usually over-parameterized and can interpolate the data by driving the empirical loss close to zero. we analyze the convergence of local sgd (or fedavg) for such over-parameterized models in the heterogeneous data setting and improve upon the existing literature by establishing the following convergence rates. we show an error bound of $\\\\o(\\\\exp(-t))$ for strongly-convex loss functions, where $t$ is the total number of iterations. for general convex loss functions, we establish an error bound of $\\\\o(1/t)$ under a mild data similarity assumption and an error bound of $\\\\o(k/t)$ otherwise, where $k$ is the number of local steps. we also extend our results for non-convex loss functions by proving an error bound of $\\\\o(k/t)$. before our work, the best-known convergence rate for strongly-convex loss functions was $\\\\o(\\\\exp(-t/k))$, and none existed for general convex or non-convex loss functions under the overparameterized setting. we complete our results by providing problem instances in which such convergence rates are tight to a constant factor under a reasonably small stepsize scheme. finally, we validate our theoretical results using numerical experiments on real and synthetic data.',\n",
       "  'categories': 'cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-01-29',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['tiancheng qin', 's. rasoul etesami', 'césar a. uribe'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2201.12719'},\n",
       " {'title': 'layoutenhancer: generating good indoor layouts from imperfect data',\n",
       "  'id': '2202.00185',\n",
       "  'abstract': 'we address the problem of indoor layout synthesis, which is a topic of continuing research interest in computer graphics. the newest works made significant progress using data-driven generative methods; however, these approaches rely on suitable datasets. in practice, desirable layout properties may not exist in a dataset, for instance, specific expert knowledge can be missing in the data. we propose a method that combines expert knowledge, for example, knowledge about ergonomics, with a data-driven generator based on the popular transformer architecture. the knowledge is given as differentiable scalar functions, which can be used both as weights or as additional terms in the loss function. using this knowledge, the synthesized layouts can be biased to exhibit desirable properties, even if these properties are not present in the dataset. our approach can also alleviate problems of lack of data and imperfections in the data. our work aims to improve generative machine learning for modeling and provide novel tools for designers and amateurs for the problem of interior layout creation.',\n",
       "  'categories': 'cs.gr cs.ai cs.cv cs.lg',\n",
       "  'doi': '10.1145/3550469.3555425',\n",
       "  'created': '2022-01-31',\n",
       "  'updated': '2022-10-05',\n",
       "  'authors': ['kurt leimer',\n",
       "   'paul guerrero',\n",
       "   'tomer weiss',\n",
       "   'przemyslaw musialski'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.00185'},\n",
       " {'title': 'phase diagram of stochastic gradient descent in high-dimensional   two-layer neural networks',\n",
       "  'id': '2202.00293',\n",
       "  'abstract': 'despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. the picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of saad & solla. focusing on the case of gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (sgd). our work builds on a deterministic description of sgd in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates.',\n",
       "  'categories': 'stat.ml cond-mat.dis-nn cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-01',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['rodrigo veiga',\n",
       "   'ludovic stephan',\n",
       "   'bruno loureiro',\n",
       "   'florent krzakala',\n",
       "   'lenka zdeborová'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.00293'},\n",
       " {'title': 'learning physics-consistent particle interactions',\n",
       "  'id': '2202.00299',\n",
       "  'abstract': 'interacting particle systems play a key role in science and engineering. access to the governing particle interaction law is fundamental for a complete understanding of such systems. however, the inherent system complexity keeps the particle interaction hidden in many cases. machine learning methods have the potential to learn the behavior of interacting particle systems by combining experiments with data analysis methods. however, most existing algorithms focus on learning the kinetics at the particle level. learning pairwise interaction, e.g., pairwise force or pairwise potential energy, remains an open challenge. here, we propose an algorithm that adapts the graph networks framework, which contains an edge part to learn the pairwise interaction and a node part to model the dynamics at particle level. different from existing approaches that use neural networks in both parts, we design a deterministic operator in the node part that allows to precisely infer the pairwise interactions that are consistent with underlying physical laws by only being trained to predict the particle acceleration. we test the proposed methodology on multiple datasets and demonstrate that it achieves superior performance in inferring correctly the pairwise interactions while also being consistent with the underlying physics on all the datasets. the proposed framework is scalable to larger systems and transferable to any type of particle interactions, contrary to the previously proposed purely data-driven solutions. the developed methodology can support a better understanding and discovery of the underlying particle interaction laws, and hence guide the design of materials with targeted properties.',\n",
       "  'categories': 'cs.lg physics.comp-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-01',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['zhichao han', 'david s. kammer', 'olga fink'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.00299'},\n",
       " {'title': 'identifying pauli spin blockade using deep learning',\n",
       "  'id': '2202.00574',\n",
       "  'abstract': 'pauli spin blockade (psb) can be employed as a great resource for spin qubit initialisation and readout even at elevated temperatures but it can be difficult to identify. we present a machine learning algorithm capable of automatically identifying psb using charge transport measurements. the scarcity of psb data is circumvented by training the algorithm with simulated data and by using cross-device validation. we demonstrate our approach on a silicon field-effect transistor device and report an accuracy of 96% on different test devices, giving evidence that the approach is robust to device variability. the approach is expected to be employable across all types of quantum dot devices.',\n",
       "  'categories': 'cond-mat.mes-hall cs.lg quant-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-01',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['jonas schuff',\n",
       "   'dominic t. lennon',\n",
       "   'simon geyer',\n",
       "   'david l. craig',\n",
       "   'federico fedele',\n",
       "   'florian vigneau',\n",
       "   'leon c. camenzind',\n",
       "   'andreas v. kuhlmann',\n",
       "   'g. andrew d. briggs',\n",
       "   'dominik m. zumbühl',\n",
       "   'dino sejdinovic',\n",
       "   'natalia ares'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.00574'},\n",
       " {'title': 'pso-pinn: physics-informed neural networks trained with particle swarm   optimization',\n",
       "  'id': '2202.01943',\n",
       "  'abstract': 'physics-informed neural networks (pinn) have recently emerged as a promising application of deep learning in a wide range of engineering and scientific problems based on partial differential equation (pde) models. however, evidence shows that pinn training by gradient descent displays pathologies that often prevent convergence when solving pdes with irregular solutions. in this paper, we propose the use of a particle swarm optimization (pso) approach to train pinns. the resulting pso-pinn algorithm not only mitigates the undesired behaviors of pinns trained with standard gradient descent but also presents an ensemble approach to pinn that affords the possibility of robust predictions with quantified uncertainty. we also propose pso-bp-cd (pso with back-propagation and coefficient decay), a hybrid pso variant that combines swarm optimization with gradient descent, putting more weight on the latter as training progresses and the swarm zeros in on a good local optimum. comprehensive experimental results show that pso-pinn with the proposed pso-bp-cd algorithm outperforms pinn ensembles trained with other pso variants or with pure gradient descent.',\n",
       "  'categories': 'cs.lg physics.comp-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-03',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['caio davi', 'ulisses braga-neto'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.01943'},\n",
       " {'title': 'parallel successive learning for dynamic distributed model training over   heterogeneous wireless networks',\n",
       "  'id': '2202.02947',\n",
       "  'abstract': 'federated learning (fedl) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). in this paper, we develop parallel successive learning (psl), which expands the fedl architecture along three dimensions: (i) network, allowing decentralized cooperation among the devices via device-to-device (d2d) communications. (ii) heterogeneity, interpreted at three levels: (ii-a) learning: psl considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) data: psl presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) device: psl considers devices with different computation and communication capabilities. (iii) proximity, where devices have different distances to each other and the access point. psl considers the realistic scenario where global aggregations are conducted with idle times in-between them for resource efficiency improvements, and incorporates data dispersion and model dispersion with local model condensation into fedl. our analysis sheds light on the notion of cold vs. warmed up models, and model inertia in distributed machine learning. we then propose network-aware dynamic model tracking to optimize the model learning vs. resource efficiency tradeoff, which we show is an np-hard signomial programming problem. we finally solve this problem through proposing a general optimization solver. our numerical results reveal new findings on the interdependencies between the idle times in-between the global aggregations, model/concept drift, and d2d cooperation configuration.',\n",
       "  'categories': 'cs.lg cs.ai cs.ni cs.sy eess.sy',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-07',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['seyyedali hosseinalipour',\n",
       "   'su wang',\n",
       "   'nicolo michelusi',\n",
       "   'vaneet aggarwal',\n",
       "   'christopher g. brinton',\n",
       "   'david j. love',\n",
       "   'mung chiang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.02947'},\n",
       " {'title': 'generative multitask learning mitigates target-causing confounding',\n",
       "  'id': '2202.04136',\n",
       "  'abstract': 'we propose generative multitask learning (gmtl), a simple and scalable approach to causal representation learning for multitask learning. our approach makes a minor change to the conventional multitask inference objective, and improves robustness to target shift. since gmtl only modifies the inference objective, it can be used with existing multitask learning methods without requiring additional training. the improvement in robustness comes from mitigating unobserved confounders that cause the targets, but not the input. we refer to them as \\\\emph{target-causing confounders}. these confounders induce spurious dependencies between the input and targets. this poses a problem for conventional multitask learning, due to its assumption that the targets are conditionally independent given the input. gmtl mitigates target-causing confounding at inference time, by removing the influence of the joint target distribution, and predicting all targets jointly. this removes the spurious dependencies between the input and targets, where the degree of removal is adjustable via a single hyperparameter. this flexibility is useful for managing the trade-off between in- and out-of-distribution generalization. our results on the attributes of people and taskonomy datasets reflect an improved robustness to target shift across four multitask learning methods.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-08',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['taro makino', 'krzysztof j. geras', 'kyunghyun cho'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.04136'},\n",
       " {'title': 'exploring the limits of domain-adaptive training for detoxifying   large-scale language models',\n",
       "  'id': '2202.04173',\n",
       "  'abstract': 'pre-trained language models (lms) are shown to easily generate toxic language. in this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. we conduct this study on three dimensions: training corpus, model size, and parameter efficiency. for the training corpus, we propose to leverage the generative power of lms and generate nontoxic datasets for domain-adaptive training, which mitigates the exposure bias and is shown to be more data-efficient than using a curated pre-training corpus. we demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. we then comprehensively study detoxifying lms with parameter sizes ranging from 126m up to 530b (3x larger than gpt-3), a scale that has never been studied before. we find that i) large lms have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large lms require more endeavor to detoxify. we also explore parameter-efficient training methods for detoxification. we demonstrate that adding and training adapter-only layers in lms not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.',\n",
       "  'categories': 'cs.cl cs.ai cs.cy cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-08',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['boxin wang',\n",
       "   'wei ping',\n",
       "   'chaowei xiao',\n",
       "   'peng xu',\n",
       "   'mostofa patwary',\n",
       "   'mohammad shoeybi',\n",
       "   'bo li',\n",
       "   'anima anandkumar',\n",
       "   'bryan catanzaro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.04173'},\n",
       " {'title': 'neural sheaf diffusion: a topological perspective on heterophily and   oversmoothing in gnns',\n",
       "  'id': '2202.04579',\n",
       "  'abstract': 'cellular sheaves equip graphs with a \"geometrical\" structure by assigning vector spaces and linear maps to nodes and edges. graph neural networks (gnns) implicitly assume a graph with a trivial underlying sheaf. this choice is reflected in the structure of the graph laplacian operator, the properties of the associated diffusion equation, and the characteristics of the convolutional models that discretise this equation. in this paper, we use cellular sheaf theory to show that the underlying geometry of the graph is deeply linked with the performance of gnns in heterophilic settings and their oversmoothing behaviour. by considering a hierarchy of increasingly general sheaves, we study how the ability of the sheaf diffusion process to achieve linear separation of the classes in the infinite time limit expands. at the same time, we prove that when the sheaf is non-trivial, discretised parametric diffusion processes have greater control than gnns over their asymptotic behaviour. on the practical side, we study how sheaves can be learned from data. the resulting sheaf diffusion models have many desirable properties that address the limitations of classical graph diffusion equations (and corresponding gnn models) and obtain competitive results in heterophilic settings. overall, our work provides new connections between gnns and algebraic topology and would be of interest to both fields.',\n",
       "  'categories': 'cs.lg math.at',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-09',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['cristian bodnar',\n",
       "   'francesco di giovanni',\n",
       "   'benjamin paul chamberlain',\n",
       "   'pietro liò',\n",
       "   'michael m. bronstein'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.04579'},\n",
       " {'title': 'target-aware molecular graph generation',\n",
       "  'id': '2202.04829',\n",
       "  'abstract': 'generating molecules with desired biological activities has attracted growing attention in drug discovery. previous molecular generation models are designed as chemocentric methods that hardly consider the drug-target interaction, limiting their practical applications. in this paper, we aim to generate molecular drugs in a target-aware manner that bridges biological activity and molecular design. to solve this problem, we compile a benchmark dataset from several publicly available datasets and build baselines in a unified framework. building on the recent advantages of flow-based molecular generation models, we propose siamflow, which forces the flow to fit the distribution of target sequence embeddings in latent space. specifically, we employ an alignment loss and a uniform loss to bring target sequence embeddings and drug graph embeddings into agreements while avoiding collapse. furthermore, we formulate the alignment into a one-to-many problem by learning spaces of target sequence embeddings. experiments quantitatively show that our proposed method learns meaningful representations in the latent space toward the target-aware molecular graph generation and provides an alternative approach to bridge biology and chemistry in drug discovery.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-09',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['cheng tan', 'zhangyang gao', 'stan z. li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.04829'},\n",
       " {'title': 'supa: a lightweight diagnostic simulator for machine learning in   particle physics',\n",
       "  'id': '2202.05012',\n",
       "  'abstract': 'deep learning methods have gained popularity in high energy physics for fast modeling of particle showers in detectors. detailed simulation frameworks such as the gold standard geant4 are computationally intensive, and current deep generative architectures work on discretized, lower resolution versions of the detailed simulation. the development of models that work at higher spatial resolutions is currently hindered by the complexity of the full simulation data, and by the lack of simpler, more interpretable benchmarks. our contribution is supa, the surrogate particle propagation simulator, an algorithm and software package for generating data by simulating simplified particle propagation, scattering and shower development in matter. the generation is extremely fast and easy to use compared to geant4, but still exhibits the key characteristics and challenges of the detailed simulation. we support this claim experimentally by showing that performance of generative models on data from our simulator reflects the performance on a dataset generated with geant4. the proposed simulator generates thousands of particle showers per second on a desktop machine, a speed up of up to 6 orders of magnitudes over geant4, and stores detailed geometric information about the shower propagation. supa provides much greater flexibility for setting initial conditions and defining multiple benchmarks for the development of models. moreover, interpreting particle showers as point clouds creates a connection to geometric machine learning and provides challenging and fundamentally new datasets for the field.   the code for supa is available at https://github.com/itsdaniele/supa.',\n",
       "  'categories': 'physics.data-an astro-ph.im cs.lg hep-ex physics.acc-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-10',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['atul kumar sinha',\n",
       "   'daniele paliotta',\n",
       "   'bálint máté',\n",
       "   'sebastian pina-otey',\n",
       "   'john a. raine',\n",
       "   'tobias golling',\n",
       "   'françois fleuret'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.05012'},\n",
       " {'title': 'locating and editing factual associations in gpt',\n",
       "  'id': '2202.05262',\n",
       "  'abstract': \"we analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. we first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. this reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. to test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using rank-one model editing (rome). we find that rome is effective on a standard zero-shot relation extraction (zsre) model-editing task, comparable to existing methods. to perform a more sensitive evaluation, we also evaluate rome on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. the code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/\",\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-10',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['kevin meng', 'david bau', 'alex andonian', 'yonatan belinkov'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.05262'},\n",
       " {'title': 'transformer memory as a differentiable search index',\n",
       "  'id': '2202.06991',\n",
       "  'abstract': 'in this paper, we demonstrate that information retrieval can be accomplished with a single transformer, in which all information about the corpus is encoded in the parameters of the model. to this end, we introduce the differentiable search index (dsi), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a dsi model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. we study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. experiments demonstrate that given appropriate design choices, dsi significantly outperforms strong baselines such as dual encoder models. moreover, dsi demonstrates strong generalization capabilities, outperforming a bm25 baseline in a zero-shot setup.',\n",
       "  'categories': 'cs.cl cs.ai cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-14',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['yi tay',\n",
       "   'vinh q. tran',\n",
       "   'mostafa dehghani',\n",
       "   'jianmo ni',\n",
       "   'dara bahri',\n",
       "   'harsh mehta',\n",
       "   'zhen qin',\n",
       "   'kai hui',\n",
       "   'zhe zhao',\n",
       "   'jai gupta',\n",
       "   'tal schuster',\n",
       "   'william w. cohen',\n",
       "   'donald metzler'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.06991'},\n",
       " {'title': 'debiased self-training for semi-supervised learning',\n",
       "  'id': '2202.07136',\n",
       "  'abstract': 'deep neural networks achieve remarkable performances on a wide range of tasks with the aid of large-scale labeled datasets. yet these datasets are time-consuming and labor-exhaustive to obtain on realistic tasks. to mitigate the requirement for labeled data, self-training is widely used in semi-supervised learning by iteratively assigning pseudo labels to unlabeled samples. despite its popularity, self-training is well-believed to be unreliable and often leads to training instability. our experimental studies further reveal that the bias in semi-supervised learning arises from both the problem itself and the inappropriate training with potentially incorrect pseudo labels, which accumulates the error in the iterative self-training process. to reduce the above bias, we propose debiased self-training (dst). first, the generation and utilization of pseudo labels are decoupled by two parameter-independent classifier heads to avoid direct error accumulation. second, we estimate the worst case of self-training bias, where the pseudo labeling function is accurate on labeled samples, yet makes as many mistakes as possible on unlabeled samples. we then adversarially optimize the representations to improve the quality of pseudo labels by avoiding the worst case. extensive experiments justify that dst achieves an average improvement of 6.3% against state-of-the-art methods on standard semi-supervised learning benchmark datasets and 18.9%$ against fixmatch on 13 diverse tasks. furthermore, dst can be seamlessly adapted to other self-training methods and help stabilize their training and balance performance across classes in both cases of training from scratch and finetuning from pre-trained models.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-14',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['baixu chen',\n",
       "   'junguang jiang',\n",
       "   'ximei wang',\n",
       "   'pengfei wan',\n",
       "   'jianmin wang',\n",
       "   'mingsheng long'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.07136'},\n",
       " {'title': 'a multi-agent reinforcement learning framework for off-policy evaluation   in two-sided markets',\n",
       "  'id': '2202.10574',\n",
       "  'abstract': 'the two-sided markets such as ride-sharing companies often involve a group of subjects who are making sequential decisions across time and/or location. with the rapid development of smart phones and internet of things, they have substantially transformed the transportation landscape of human beings. in this paper we consider large-scale fleet management in ride-sharing companies that involve multiple units in different areas receiving sequences of products (or treatments) over time. major technical challenges, such as policy evaluation, arise in those studies because (i) spatial and temporal proximities induce interference between locations and times; and (ii) the large number of locations results in the curse of dimensionality. to address both challenges simultaneously, we introduce a multi-agent reinforcement learning (marl) framework for carrying policy evaluation in these studies. we propose novel estimators for mean outcomes under different products that are consistent despite the high-dimensionality of state-action space. the proposed estimator works favorably in simulation experiments. we further illustrate our method using a real dataset obtained from a two-sided marketplace company to evaluate the effects of applying different subsidizing policies. a python implementation of our proposed method is available at https://github.com/runzhestat/causalmarl.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-21',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['chengchun shi',\n",
       "   'runzhe wan',\n",
       "   'ge song',\n",
       "   'shikai luo',\n",
       "   'rui song',\n",
       "   'hongtu zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.10574'},\n",
       " {'title': 'listen to interpret: post-hoc interpretability for audio networks with   nmf',\n",
       "  'id': '2202.11479',\n",
       "  'abstract': \"this paper tackles post-hoc interpretability for audio processing networks. our goal is to interpret decisions of a network in terms of high-level audio objects that are also listenable for the end-user. to this end, we propose a novel interpreter design that incorporates non-negative matrix factorization (nmf). in particular, a carefully regularized interpreter module is trained to take hidden layer representations of the targeted network as input and produce time activations of pre-learnt nmf components as intermediate outputs. our methodology allows us to generate intuitive audio-based interpretations that explicitly enhance parts of the input signal most relevant for a network's decision. we demonstrate our method's applicability on popular benchmarks, including a real-world multi-label classification task.\",\n",
       "  'categories': 'cs.sd cs.lg eess.as',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-23',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['jayneel parekh',\n",
       "   'sanjeel parekh',\n",
       "   'pavlo mozharovskyi',\n",
       "   \"florence d'alché-buc\",\n",
       "   'gaël richard'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.11479'},\n",
       " {'title': 'robust federated learning with connectivity failures: a   semi-decentralized framework with collaborative relaying',\n",
       "  'id': '2202.11850',\n",
       "  'abstract': \"intermittent connectivity of clients to the parameter server (ps) is a major bottleneck in federated edge learning frameworks. the lack of constant connectivity induces a large generalization gap, especially when the local data distribution amongst clients exhibits heterogeneity. to overcome intermittent communication outages between clients and the central ps, we introduce the concept of collaborative relaying wherein the participating clients relay their neighbors' local updates to the ps in order to boost the participation of clients with poor connectivity to the ps. we propose a semi-decentralized federated learning framework in which at every communication round, each client initially computes a local consensus of a subset of its neighboring clients' updates, and eventually transmits to the ps a weighted average of its own update and those of its neighbors'. we appropriately optimize these local consensus weights to ensure that the global update at the ps is unbiased with minimal variance - consequently improving the convergence rate. numerical evaluations on the cifar-10 dataset demonstrate that our collaborative relaying approach outperforms federated averaging-based benchmarks for learning over intermittently-connected networks such as when the clients communicate over millimeter wave channels with intermittent blockages.\",\n",
       "  'categories': 'cs.dc cs.lg cs.ma',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-23',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['michal yemini',\n",
       "   'rajarshi saha',\n",
       "   'emre ozfatura',\n",
       "   'deniz gündüz',\n",
       "   'andrea j. goldsmith'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.11850'},\n",
       " {'title': 'magnitude-aware probabilistic speaker embeddings',\n",
       "  'id': '2202.13826',\n",
       "  'abstract': 'recently, hyperspherical embeddings have established themselves as a dominant technique for face and voice recognition. specifically, euclidean space vector embeddings are learned to encode person-specific information in their direction while ignoring the magnitude. however, recent studies have shown that the magnitudes of the embeddings extracted by deep neural networks may indicate the quality of the corresponding inputs. this paper explores the properties of the magnitudes of the embeddings related to quality assessment and out-of-distribution detection. we propose a new probabilistic speaker embedding extractor using the information encoded in the embedding magnitude and leverage it in the speaker verification pipeline. we also propose several quality-aware diarization methods and incorporate the magnitudes in those. our results indicate significant improvements over magnitude-agnostic baselines both in speaker verification and diarization tasks.',\n",
       "  'categories': 'eess.as cs.lg cs.sd',\n",
       "  'doi': '10.21437/odyssey.2022-1',\n",
       "  'created': '2022-02-28',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['nikita kuzmin', 'igor fedorov', 'alexey sholokhov'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.13826'},\n",
       " {'title': 'domain knowledge-informed self-supervised representations for workout   form assessment',\n",
       "  'id': '2202.14019',\n",
       "  'abstract': \"maintaining proper form while exercising is important for preventing injuries and maximizing muscle mass gains. detecting errors in workout form naturally requires estimating human's body pose. however, off-the-shelf pose estimators struggle to perform well on the videos recorded in gym scenarios due to factors such as camera angles, occlusion from gym equipment, illumination, and clothing. to aggravate the problem, the errors to be detected in the workouts are very subtle. to that end, we propose to learn exercise-oriented image and video representations from unlabeled samples such that a small dataset annotated by experts suffices for supervised error detection. in particular, our domain knowledge-informed self-supervised approaches (pose contrastive learning and motion disentangling) exploit the harmonic motion of the exercise actions, and capitalize on the large variances in camera angles, clothes, and illumination to learn powerful representations. to facilitate our self-supervised pretraining, and supervised finetuning, we curated a new exercise dataset, \\\\emph{fitness-aqa} (\\\\url{https://github.com/paritoshparmar/fitness-aqa}), comprising of three exercises: backsquat, barbellrow, and overheadpress. it has been annotated by expert trainers for multiple crucial and typically occurring exercise errors. experimental results show that our self-supervised representations outperform off-the-shelf 2d- and 3d-pose estimators and several other baselines. we also show that our approaches can be applied to other domains/tasks such as pose estimation and dive quality assessment.\",\n",
       "  'categories': 'cs.cv cs.ai cs.hc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-02-28',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['paritosh parmar', 'amol gharat', 'helge rhodin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2202.14019'},\n",
       " {'title': 'nestanets: stable, accurate and efficient neural networks for   analysis-sparse inverse problems',\n",
       "  'id': '2203.00804',\n",
       "  'abstract': 'solving inverse problems is a fundamental component of science, engineering and mathematics. with the advent of deep learning, deep neural networks have significant potential to outperform existing state-of-the-art, model-based methods for solving inverse problems. however, it is known that current data-driven approaches face several key issues, notably hallucinations, instabilities and unpredictable generalization, with potential impact in critical tasks such as medical imaging. this raises the key question of whether or not one can construct deep neural networks for inverse problems with explicit stability and accuracy guarantees. in this work, we present a novel construction of accurate, stable and efficient neural networks for inverse problems with general analysis-sparse models, termed nestanets. to construct the network, we first unroll nesta, an accelerated first-order method for convex optimization. the slow convergence of this method leads to deep networks with low efficiency. therefore, to obtain shallow, and consequently more efficient, networks we combine nesta with a novel restart scheme. we then use compressed sensing techniques to demonstrate accuracy and stability. we showcase this approach in the case of fourier imaging, and verify its stability and performance via a series of numerical experiments. the key impact of this work is demonstrating the construction of efficient neural networks based on unrolling with guaranteed stability and accuracy.',\n",
       "  'categories': 'cs.lg cs.cv cs.it cs.na eess.iv math.it math.na',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-01',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['maksym neyra-nesterenko', 'ben adcock'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.00804'},\n",
       " {'title': 'nemo: guiding and contextualizing weak supervision for interactive data   programming',\n",
       "  'id': '2203.01382',\n",
       "  'abstract': 'weak supervision (ws) techniques allow users to efficiently create large training datasets by programmatically labeling data with heuristic sources of supervision. while the success of ws relies heavily on the provided labeling heuristics, the process of how these heuristics are created in practice has remained under-explored. in this work, we formalize the development process of labeling heuristics as an interactive procedure, built around the existing workflow where users draw ideas from a selected set of development data for designing the heuristic sources. with the formalism, we study two core problems of how to strategically select the development data to guide users in efficiently creating informative heuristics, and how to exploit the information within the development process to contextualize and better learn from the resultant heuristics. building upon two novel methodologies that effectively tackle the respective problems considered, we present nemo, an end-to-end interactive system that improves the overall productivity of ws learning pipeline by an average 20% (and up to 47% in one task) compared to the prevailing ws approach.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-02',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['cheng-yu hsieh', 'jieyu zhang', 'alexander ratner'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.01382'},\n",
       " {'title': 'interventions, where and how? experimental design for causal models at   scale',\n",
       "  'id': '2203.02016',\n",
       "  'abstract': 'causal discovery from observational and interventional data is challenging due to limited data and non-identifiability: factors that introduce uncertainty in estimating the underlying structural causal model (scm). selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identification of the scm. existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the scm or select only the intervention target. this work incorporates recent advances in bayesian causal discovery into the bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear scms while selecting both the interventional target and the value. we demonstrate the performance of the proposed method on synthetic graphs (erdos-r\\\\`enyi, scale free) for both linear and nonlinear scms as well as on the \\\\emph{in-silico} single-cell gene regulatory network dataset, dream.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-03',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['panagiotis tigas',\n",
       "   'yashas annadani',\n",
       "   'andrew jesson',\n",
       "   'bernhard schölkopf',\n",
       "   'yarin gal',\n",
       "   'stefan bauer'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.02016'},\n",
       " {'title': 'distributionally robust bayesian optimization with $\\\\phi$-divergences',\n",
       "  'id': '2203.02128',\n",
       "  'abstract': 'the study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. one such example of concern is bayesian optimization (bo), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. in particular, there is the work of kirschner et al. (2020), which bridges the existing literature of distributionally robust optimization (dro) by casting the bo problem from the lens of dro. while this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question can one devise a computationally tractable algorithm for solving this dro-bo problem? in this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $\\\\phi$-divergences, which subsumes many popular choices, such as the $\\\\chi^2$-divergence, total variation, and the extant kullback-leibler (kl) divergence. we show that the dro-bo problem in this setting is equivalent to a finite-dimensional optimization problem which, even in the continuous context setting, can be easily implemented with provable sublinear regret bounds. we then show experimentally that our method surpasses existing methods, attesting to the theoretical results.',\n",
       "  'categories': 'cs.lg math.oc stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-03',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['hisham husain', 'vu nguyen', 'anton van den hengel'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.02128'},\n",
       " {'title': 'race: retrieval-augmented commit message generation',\n",
       "  'id': '2203.02700',\n",
       "  'abstract': 'commit messages are important for software development and maintenance. many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. however, the generated commit messages could be repetitive or redundant. in this paper, we propose race, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. as the retrieved commit message may not always accurately describe the content/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. we conduct extensive experiments on a large public dataset with five programming languages. experimental results show that race can outperform all baselines. furthermore, race can boost the performance of existing seq2seq models in commit message generation.',\n",
       "  'categories': 'cs.se cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-05',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['ensheng shi',\n",
       "   'yanlin wang',\n",
       "   'wei tao',\n",
       "   'lun du',\n",
       "   'hongyu zhang',\n",
       "   'shi han',\n",
       "   'dongmei zhang',\n",
       "   'hongbin sun'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.02700'},\n",
       " {'title': 'biological sequence design with gflownets',\n",
       "  'id': '2203.04115',\n",
       "  'abstract': 'design of de novo biological sequences with desired properties, like protein and dna sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. these experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. this makes the diversity of proposed candidates a key consideration in the ideation phase. in this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed gflownets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. we also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in gflownets. we present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.',\n",
       "  'categories': 'q-bio.bm cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-02',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['moksh jain',\n",
       "   'emmanuel bengio',\n",
       "   'alex-hernandez garcia',\n",
       "   'jarrid rector-brooks',\n",
       "   'bonaventure f. p. dossou',\n",
       "   'chanakya ekbote',\n",
       "   'jie fu',\n",
       "   'tianyu zhang',\n",
       "   'micheal kilgour',\n",
       "   'dinghuai zhang',\n",
       "   'lena simine',\n",
       "   'payel das',\n",
       "   'yoshua bengio'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.04115'},\n",
       " {'title': 'less is more: summary of long instructions is better for program   synthesis',\n",
       "  'id': '2203.08597',\n",
       "  'abstract': 'despite the success of large pre-trained language models (lms) such as codex, they show below-par performance on the larger and more complicated programming related questions. we show that lms benefit from the summarized version of complicated questions. our findings show that superfluous information often present in problem description such as human characters, background stories, and names (which are included to help humans in understanding a task) does not help models in understanding a task. to this extent, we create a meta-dataset from the frequently used apps dataset and the newly created codecontests dataset for the program synthesis task. our meta-dataset consists of human and synthesized summaries of the long and complicated programming questions. experimental results on codex show that our proposed approach outperforms baseline by 8.13% on the apps dataset and 11.88% on the codecontests dataset on average in terms of strict accuracy. our analysis shows that summaries significantly improve performance for introductory (9.86%) and interview (11.48%) programming questions. however, it shows improvement by a small margin (~ 2%) for competitive programming questions, implying scope for future research in this direction.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-16',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['kirby kuznia',\n",
       "   'swaroop mishra',\n",
       "   'mihir parmar',\n",
       "   'chitta baral'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.08597'},\n",
       " {'title': 'near instance-optimal pac reinforcement learning for deterministic mdps',\n",
       "  'id': '2203.09251',\n",
       "  'abstract': 'in probably approximately correct (pac) reinforcement learning (rl), an agent is required to identify an $\\\\epsilon$-optimal policy with probability $1-\\\\delta$. while minimax optimal algorithms exist for this problem, its instance-dependent complexity remains elusive in episodic markov decision processes (mdps). in this paper, we propose the first nearly matching (up to a horizon squared factor and logarithmic terms) upper and lower bounds on the sample complexity of pac rl in deterministic episodic mdps with finite state and action spaces. in particular, our bounds feature a new notion of sub-optimality gap for state-action pairs that we call the deterministic return gap. while our instance-dependent lower bound is written as a linear program, our algorithms are very simple and do not require solving such an optimization problem during learning. their design and analyses employ novel ideas, including graph-theoretical concepts (minimum flows) and a new maximum-coverage exploration strategy.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-17',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['andrea tirinzoni', 'aymen al-marjani', 'emilie kaufmann'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.09251'},\n",
       " {'title': 'enhanced temporal knowledge embeddings with contextualized language   representations',\n",
       "  'id': '2203.09590',\n",
       "  'abstract': 'world knowledge exists in both structured (tables, knowledge graphs) and unstructured forms (texts). recently, there have been extensive research efforts in the integration of structured factual knowledge and unstructured textual knowledge. however, most studies focus on incorporating static factual knowledge into pre-trained language models, while there is less work on enhancing temporal knowledge graph embedding using textual knowledge. existing integration approaches can not apply to temporal knowledge graphs (tkgs) since they often assume knowledge embedding is time-invariant. in fact, the entity embedding in tkg embedding models usually evolves over time, which poses the challenge of aligning temporally relevant textual information with entities. to this end, we propose enhanced temporal knowledge embeddings with contextualized language representations (ecola), which uses tkg quadruple as an implicit measure to temporally align textual data and the time-evolving entity representations and uses a novel knowledge-text prediction task to inject textual information into temporal knowledge embedding. ecola jointly optimizes the knowledge-text prediction objective and the temporal knowledge embedding objective, and thus, can simultaneously take full advantage of textual and structured knowledge. since existing datasets do not provide tkgs with aligned textual data, we introduce three new datasets for training and evaluating ecola. experimental results on the temporal knowledge graph completion task show that ecola outperforms state-of-the-art tkg embedding models by a large margin.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-17',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['zhen han',\n",
       "   'ruotong liao',\n",
       "   'beiyan liu',\n",
       "   'yao zhang',\n",
       "   'zifeng ding',\n",
       "   'jindong gu',\n",
       "   'heinz köppl',\n",
       "   'hinrich schütze',\n",
       "   'volker tresp'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.09590'},\n",
       " {'title': 'multi-edge server-assisted dynamic federated learning with an optimized   floating aggregation point',\n",
       "  'id': '2203.13950',\n",
       "  'abstract': \"we propose cooperative edge-assisted dynamic federated learning (ce-fl). ce-fl introduces a distributed machine learning (ml) architecture, where data collection is carried out at the end devices, while the model training is conducted cooperatively at the end devices and the edge servers, enabled via data offloading from the end devices to the edge servers through base stations. ce-fl also introduces floating aggregation point, where the local models generated at the devices and the servers are aggregated at an edge server, which varies from one model training round to another to cope with the network evolution in terms of data distribution and users' mobility. ce-fl considers the heterogeneity of network elements in terms of communication/computation models and the proximity to one another. ce-fl further presumes a dynamic environment with online variation of data at the network devices which causes a drift at the ml model performance. we model the processes taken during ce-fl, and conduct analytical convergence analysis of its ml model training. we then formulate network-aware ce-fl which aims to adaptively optimize all the network elements via tuning their contribution to the learning process, which turns out to be a non-convex mixed integer problem. motivated by the large scale of the system, we propose a distributed optimization solver to break down the computation of the solution across the network elements. we finally demonstrate the effectiveness of our framework with the data collected from a real-world testbed.\",\n",
       "  'categories': 'cs.lg cs.ni',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-25',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['bhargav ganguly',\n",
       "   'seyyedali hosseinalipour',\n",
       "   'kwang taik kim',\n",
       "   'christopher g. brinton',\n",
       "   'vaneet aggarwal',\n",
       "   'david j. love',\n",
       "   'mung chiang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.13950'},\n",
       " {'title': 'dual-path style learning for end-to-end noise-robust speech recognition',\n",
       "  'id': '2203.14838',\n",
       "  'abstract': 'automatic speech recognition (asr) systems degrade significantly in face of noisy conditions. recently, speech enhancement (se) has been introduced as front-end module to reduce noise and improve speech quality for asr, but it would also suppress some important speech information, i.e., over-suppression problem. to alleviate this, we propose a dual-path style learning approach for end-to-end noise-robust automatic speech recognition (dpsl-asr). specifically, we first introduce clean speech feature along with the fused feature from previously proposed iff-net as dual-path inputs to recover the over-suppressed information. then, we propose a style learning method to map the fused feature close to clean feature, in order to learn latent speech information from the latter, i.e., clean \"speech style\". furthermore, we employ consistency loss to minimize the distance of asr outputs in two paths to improve noise-robustness. experimental results show that the proposed approach achieves relative word error rate (wer) reductions of 10.6% and 8.6% over the best iff-net baseline, on rats channel-a and chime-4 1-channel track datasets, respectively. visualizations of intermediate embeddings indicate that dpsl-asr can recover abundant over-suppressed information in enhanced speech. our code is available at github: https://github.com/yuchen005/dpsl-asr.',\n",
       "  'categories': 'eess.as cs.lg cs.sd',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-28',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['yuchen hu', 'nana hou', 'chen chen', 'eng siong chng'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.14838'},\n",
       " {'title': 'protein language models trained on multiple sequence alignments learn   phylogenetic relationships',\n",
       "  'id': '2203.15465',\n",
       "  'abstract': \"self-supervised neural language models with attention have recently been applied to biological sequence data, advancing structure, function and mutational effect prediction. some protein language models, including msa transformer and alphafold's evoformer, take multiple sequence alignments (msas) of evolutionarily related proteins as inputs. simple combinations of msa transformer's row attentions have led to state-of-the-art unsupervised structural contact prediction. we demonstrate that similarly simple, and universal, combinations of msa transformer's column attentions strongly correlate with hamming distances between sequences in msas. therefore, msa-based language models encode detailed phylogenetic relationships. we further show that these models can separate coevolutionary signals encoding functional and structural constraints from phylogenetic correlations reflecting historical contingency. to assess this, we generate synthetic msas, either without or with phylogeny, from potts models trained on natural msas. we find that unsupervised contact prediction is substantially more resilient to phylogenetic noise when using msa transformer versus inferred potts models.\",\n",
       "  'categories': 'q-bio.bm cs.lg q-bio.qm',\n",
       "  'doi': '10.1038/s41467-022-34032-y',\n",
       "  'created': '2022-03-29',\n",
       "  'updated': '2022-09-21',\n",
       "  'authors': ['umberto lupo', 'damiano sgarbossa', 'anne-florence bitbol'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.15465'},\n",
       " {'title': 'adaptive divergence-based non-negative latent factor analysis',\n",
       "  'id': '2203.16214',\n",
       "  'abstract': 'high-dimensional and incomplete (hdi) data are frequently found in various industrial applications with complex interactions among numerous nodes, which are commonly non-negative for representing the inherent non-negativity of node interactions. a non-negative latent factor (nlf) model is able to extract intrinsic features from such data efficiently. however, existing nlf models all adopt a static divergence metric like euclidean distance or {\\\\alpha}-\\\\b{eta} divergence to build its learning objective, which greatly restricts its scalability of accurately representing hdi data from different domains. aiming at addressing this issue, this study presents an adaptive divergence-based non-negative latent factor (adnlf) model with three-fold ideas: a) generalizing the objective function with the {\\\\alpha}-\\\\b{eta}-divergence to expand its potential of representing various hdi data; b) adopting a non-negative bridging function to connect the optimization variables with output latent factors for fulfilling the non-negativity constraints constantly; and c) making the divergence parameters adaptive through particle swarm optimization, thereby facilitating adaptive divergence in the learning objective to achieve high scalability. empirical studies are conducted on four hdi datasets from real applications, whose results demonstrate that in comparison with state-of-the-art nlf models, an adnlf model achieves significantly higher estimation accuracy for missing data of an hdi dataset with high computational efficiency.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-30',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['ye yuan', 'guangxiao yuan', 'renfang wang', 'xin luo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2203.16214'},\n",
       " {'title': 'reproducibility issues for bert-based evaluation metrics',\n",
       "  'id': '2204.00004',\n",
       "  'abstract': 'reproducibility is of utmost concern in machine learning and natural language processing (nlp). in the field of natural language generation (especially machine translation), the seminal paper of post (2018) has pointed out problems of reproducibility of the dominant metric, bleu, at the time of publication. nowadays, bert-based evaluation metrics considerably outperform bleu. in this paper, we ask whether results and claims from four recent bert-based metrics can be reproduced. we find that reproduction of claims and results often fails because of (i) heavy undocumented preprocessing involved in the metrics, (ii) missing code and (iii) reporting weaker results for the baseline metrics. (iv) in one case, the problem stems from correlating not to human scores but to a wrong column in the csv file, inflating scores by 5 points. motivated by the impact of preprocessing, we then conduct a second study where we examine its effects more closely (for one of the metrics). we find that preprocessing can have large effects, especially for highly inflectional languages. in this case, the effect of preprocessing may be larger than the effect of the aggregation mechanism (e.g., greedy alignment vs. word mover distance).',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-03-30',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['yanran chen', 'jonas belouadi', 'steffen eger'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.00004'},\n",
       " {'title': 'perceive, represent, generate: translating multimodal information to   robotic motion trajectories',\n",
       "  'id': '2204.03051',\n",
       "  'abstract': 'we present perceive-represent-generate (prg), a novel three-stage framework that maps perceptual information of different modalities (e.g., visual or sound), corresponding to a sequence of instructions, to an adequate sequence of movements to be executed by a robot. in the first stage, we perceive and pre-process the given inputs, isolating individual commands from the complete instruction provided by a human user. in the second stage we encode the individual commands into a multimodal latent space, employing a deep generative model. finally, in the third stage we convert the multimodal latent values into individual trajectories and combine them into a single dynamic movement primitive, allowing its execution in a robotic platform. we evaluate our pipeline in the context of a novel robotic handwriting task, where the robot receives as input a word through different perceptual modalities (e.g., image, sound), and generates the corresponding motion trajectory to write it, creating coherent and readable handwritten words.',\n",
       "  'categories': 'cs.ro cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-04-06',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['fábio vital',\n",
       "   'miguel vasco',\n",
       "   'alberto sardinha',\n",
       "   'francisco melo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.03051'},\n",
       " {'title': 'evolving generalizable actor-critic algorithms',\n",
       "  'id': '2204.04292',\n",
       "  'abstract': \"deploying reinforcement learning (rl) agents in the real world requires designing and tuning algorithms for problem-specific objectives such as performance, robustness, or stability. these objectives can frequently change, which will then necessitate further painstaking design and tuning. this paper presents metapg, an evolutionary method for designing new loss functions for actor-critic rl algorithms that optimize for different objectives. in particular, we focus on the objectives of final performance in training regime, policy robustness to unseen environment configurations, and training curve stability over random seeds. we initialize our algorithm population from soft actor-critic (sac) and optimize for these objectives over a set of continuous control tasks from the real-world rl benchmark suite. we find that our method evolves algorithms that, using a single environment during evolution, improve upon sac's performance and generalizability by 3% and 17%, respectively, and reduce instability up to 65% in that same environment. then, we scale up to more complex environments from the brax physics simulator and replicate conditions that can be encountered in practical settings (such as different friction coefficients). metapg evolves algorithms that can obtain 9% better policy robustness within the same meta-training environment without loss of performance and robustness when doing cross-domain evaluations in other brax environments. lastly, we analyze the structure of the best algorithms in the population and interpret the specific elements that help the algorithm optimize for a certain objective, such as regularizing the critic loss.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-04-08',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['juan jose garau-luis',\n",
       "   'yingjie miao',\n",
       "   'john d. co-reyes',\n",
       "   'aaron parisi',\n",
       "   'jie tan',\n",
       "   'esteban real',\n",
       "   'aleksandra faust'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.04292'},\n",
       " {'title': 'refined convergence and topology learning for decentralized sgd with   heterogeneous data',\n",
       "  'id': '2204.04452',\n",
       "  'abstract': 'one of the key challenges in decentralized and federated learning is to design algorithms that efficiently deal with highly heterogeneous data distributions across agents. in this paper, we revisit the analysis of the popular decentralized stochastic gradient descent algorithm (d-sgd) under data heterogeneity. we exhibit the key role played by a new quantity, called neighborhood heterogeneity, on the convergence rate of d-sgd. by coupling the communication topology and the heterogeneity, our analysis sheds light on the poorly understood interplay between these two concepts. we then argue that neighborhood heterogeneity provides a natural criterion to learn data-dependent topologies that reduce (and can even eliminate) the otherwise detrimental effect of data heterogeneity on the convergence time of d-sgd. for the important case of classification with label skew, we formulate the problem of learning such a good topology as a tractable optimization problem that we solve with a frank-wolfe algorithm. as illustrated over a set of simulated and real-world experiments, our approach provides a principled way to design a sparse topology that balances the convergence speed and the per-iteration communication costs of d-sgd under data heterogeneity.',\n",
       "  'categories': 'cs.lg math.oc stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-04-09',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['batiste le bars',\n",
       "   'aurélien bellet',\n",
       "   'marc tommasi',\n",
       "   'erick lavoie',\n",
       "   'anne-marie kermarrec'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.04452'},\n",
       " {'title': 'investigation of a data split strategy involving the time axis in   adverse event prediction using machine learning',\n",
       "  'id': '2204.08682',\n",
       "  'abstract': 'adverse events are a serious issue in drug development and many prediction methods using machine learning have been developed. the random split cross-validation is the de facto standard for model building and evaluation in machine learning, but care should be taken in adverse event prediction because this approach does not match to the real-world situation. the time split, which uses the time axis, is considered suitable for real-world prediction. however, the differences in model performance obtained using the time and random splits are not clear due to the lack of the comparable studies. to understand the differences, we compared the model performance between the time and random splits using nine types of compound information as input, eight adverse events as targets, and six machine learning algorithms. the random split showed higher area under the curve values than did the time split for six of eight targets. the chemical spaces of the training and test datasets of the time split were similar, suggesting that the concept of applicability domain is insufficient to explain the differences derived from the splitting. the area under the curve differences were smaller for the protein interaction than for the other datasets. subsequent detailed analyses suggested the danger of confounding in the use of knowledge-based information in the time split. these findings indicate the importance of understanding the differences between the time and random splits in adverse event prediction and strongly suggest that appropriate use of the splitting strategies and interpretation of results are necessary for the real-world prediction of adverse events. we provide analysis code and datasets used in the present study (https://github.com/mizuno-group/ae_prediction).',\n",
       "  'categories': 'cs.lg q-bio.qm',\n",
       "  'doi': '10.1021/acs.jcim.2c00765',\n",
       "  'created': '2022-04-19',\n",
       "  'updated': '2022-07-19',\n",
       "  'authors': ['katsuhisa morita', 'tadahaya mizuno', 'hiroyuki kusuhara'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.08682'},\n",
       " {'title': \"from spoken thoughts to automated driving commentary: predicting and   explaining intelligent vehicles' actions\",\n",
       "  'id': '2204.09109',\n",
       "  'abstract': \"in commentary driving, drivers verbalise their observations, assessments and intentions. by speaking out their thoughts, both learning and expert drivers are able to create a better understanding and awareness of their surroundings. in the intelligent vehicle context, automated driving commentary can provide intelligible explanations about driving actions, thereby assisting a driver or an end-user during driving operations in challenging and safety-critical scenarios. in this paper, we conducted a field study in which we deployed a research vehicle in an urban environment to obtain data. while collecting sensor data of the vehicle's surroundings, we obtained driving commentary from a driving instructor using the think-aloud protocol. we analysed the driving commentary and uncovered an explanation style; the driver first announces his observations, announces his plans, and then makes general remarks. he also makes counterfactual comments. we successfully demonstrated how factual and counterfactual natural language explanations that follow this style could be automatically generated using a transparent tree-based approach. generated explanations for longitudinal actions (e.g., stop and move) were deemed more intelligible and plausible by human judges compared to lateral actions, such as lane changes. we discussed how our approach can be built on in the future to realise more robust and effective explainability for driver assistance as well as partial and conditional automation of driving functions.\",\n",
       "  'categories': 'cs.ai cs.cy cs.hc cs.lg cs.ro',\n",
       "  'doi': '10.1109/iv51971.2022.9827345',\n",
       "  'created': '2022-04-19',\n",
       "  'updated': '2022-06-04',\n",
       "  'authors': ['daniel omeiza',\n",
       "   'sule anjomshoae',\n",
       "   'helena webb',\n",
       "   'marina jirotka',\n",
       "   'lars kunze'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.09109'},\n",
       " {'title': 'k-lite: learning transferable visual models with external knowledge',\n",
       "  'id': '2204.09222',\n",
       "  'abstract': 'the new generation of state-of-the-art computer vision systems are trained from natural language supervision, ranging from simple object category names to descriptive captions. this form of supervision ensures high generality and usability of the learned visual models, due to the broad concept coverage achieved via large-scale data collection process. alternatively, we argue that learning with external knowledge is a promising way which leverages a much more structured source of supervision and offers sample efficiency. we propose k-lite, a simple strategy to leverage external knowledge for building transferable visual systems: in training, it enriches entities in text with wordnet and wiktionary knowledge, leading to an efficient and scalable approach to learning image representations that uses knowledge about the visual concepts. in evaluation, the text is also augmented with external knowledge and then used to reference learned visual concepts (or describe new ones) to enable zero-shot and few-shot transfer of the pre-trained models. we study the performance of k-lite on two important computer vision problems, image classification and object detection, benchmarking on 20 and 13 different existing datasets, respectively. the proposed knowledge-augmented models show significant improvement in transfer learning performance over existing methods. our code is available at https://github.com/microsoft/klite.',\n",
       "  'categories': 'cs.cv cs.ai cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-04-20',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['sheng shen',\n",
       "   'chunyuan li',\n",
       "   'xiaowei hu',\n",
       "   'jianwei yang',\n",
       "   'yujia xie',\n",
       "   'pengchuan zhang',\n",
       "   'zhe gan',\n",
       "   'lijuan wang',\n",
       "   'lu yuan',\n",
       "   'ce liu',\n",
       "   'kurt keutzer',\n",
       "   'trevor darrell',\n",
       "   'anna rohrbach',\n",
       "   'jianfeng gao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.09222'},\n",
       " {'title': 'transher: translating knowledge graph embedding with hyper-ellipsoidal   restriction',\n",
       "  'id': '2204.13221',\n",
       "  'abstract': 'knowledge graph embedding methods are important for the knowledge graph completion (or link prediction) task. one existing efficient method, pairre, leverages two separate vectors to model complex relations (i.e., 1-to-n, n-to-1, and n-to-n) in knowledge graphs. however, such a method strictly restricts entities on the hyper-ellipsoid surfaces which limits the optimization of entity distribution, leading to suboptimal performance of knowledge graph completion. to address this issue, we propose a novel score function transher, which leverages relation-specific translations between head and tail entities to relax the constraint of hyper-ellipsoid restrictions. by introducing an intuitive and simple relation-specific translation, transher can provide more direct guidance on optimization and capture more semantic characteristics of entities with complex relations. experimental results show that transher achieves significant performance improvements on link prediction and generalizes well to datasets in different domains and scales. our codes are public available at https://github.com/yizhilll/transher.',\n",
       "  'categories': 'cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-04-27',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['yizhi li', 'wei fan', 'chao liu', 'chenghua lin', 'jiang qian'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.13221'},\n",
       " {'title': 'backdoor attacks in federated learning by rare embeddings and gradient   ensembling',\n",
       "  'id': '2204.14017',\n",
       "  'abstract': 'recent advances in federated learning have demonstrated its promising capability to learn on decentralized datasets. however, a considerable amount of work has raised concerns due to the potential risks of adversaries participating in the framework to poison the global model for an adversarial purpose. this paper investigates the feasibility of model poisoning for backdoor attacks through rare word embeddings of nlp models. in text classification, less than 1% of adversary clients suffices to manipulate the model output without any drop in the performance on clean sentences. for a less complex dataset, a mere 0.1% of adversary clients is enough to poison the global model effectively. we also propose a technique specialized in the federated learning scheme called gradient ensemble, which enhances the backdoor performance in all our experimental settings.',\n",
       "  'categories': 'cs.lg cs.ai cs.cl',\n",
       "  'doi': '',\n",
       "  'created': '2022-04-29',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['kiyoon yoo', 'nojun kwak'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2204.14017'},\n",
       " {'title': 'neuralef: deconstructing kernels by deep neural networks',\n",
       "  'id': '2205.00165',\n",
       "  'abstract': 'learning the principal eigenfunctions of an integral operator defined by a kernel and a data distribution is at the core of many machine learning problems. traditional nonparametric solutions based on the nystr{\\\\\"o}m formula suffer from scalability issues. recent work has resorted to a parametric approach, i.e., training neural networks to approximate the eigenfunctions. however, the existing method relies on an expensive orthogonalization step and is difficult to implement. we show that these problems can be fixed by using a new series of objective functions that generalizes the eigengame~\\\\citep{gemp2020eigengame} to function space. we test our method on a variety of supervised and unsupervised learning problems and show it provides accurate approximations to the eigenfunctions of polynomial, radial basis, neural network gaussian process, and neural tangent kernels. finally, we demonstrate our method can scale up linearised laplace approximation of deep neural networks to modern image classification datasets through approximating the gauss-newton matrix. code is available at \\\\url{https://github.com/thudzj/neuraleigenfunction}.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-04-30',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['zhijie deng', 'jiaxin shi', 'jun zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.00165'},\n",
       " {'title': 'pi-nlf: a proportional-integral approach for non-negative latent factor   analysis',\n",
       "  'id': '2205.02591',\n",
       "  'abstract': 'a high-dimensional and incomplete (hdi) matrix frequently appears in various big-data-related applications, which demonstrates the inherently non-negative interactions among numerous nodes. a non-negative latent factor (nlf) model performs efficient representation learning to an hdi matrix, whose learning process mostly relies on a single latent factor-dependent, non-negative and multiplicative update (slf-nmu) algorithm. however, an slf-nmu algorithm updates a latent factor based on the current update increment only without appropriate considerations of past learning information, resulting in slow convergence. inspired by the prominent success of a proportional-integral (pi) controller in various applications, this paper proposes a proportional-integral-incorporated non-negative latent factor (pi-nlf) model with two-fold ideas: a) establishing an increment refinement (ir) mechanism via considering the past update increments following the principle of a pi controller; and b) designing an ir-based slf-nmu (isn) algorithm to accelerate the convergence rate of a resultant model. empirical studies on four hdi datasets demonstrate that a pi-nlf model outperforms the state-of-the-art models in both computational efficiency and estimation accuracy for missing data of an hdi matrix. hence, this study unveils the feasibility of boosting the performance of a non-negative learning algorithm through an error feedback controller.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-05',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['ye yuan', 'xin luo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.02591'},\n",
       " {'title': 'mode reduction for markov jump systems',\n",
       "  'id': '2205.02697',\n",
       "  'abstract': 'switched systems are capable of modeling processes with underlying dynamics that may change abruptly over time. to achieve accurate modeling in practice, one may need a large number of modes, but this may in turn increase the model complexity drastically. existing work on reducing system complexity mainly considers state space reduction, yet reducing the number of modes is less studied. in this work, we consider markov jump linear systems (mjss), a special class of switched systems where the active mode switches according to a markov chain, and several issues associated with its mode complexity. specifically, inspired by clustering techniques from unsupervised learning, we are able to construct a reduced mjs with fewer modes that approximates well the original mjs under various metrics. furthermore, both theoretically and empirically, we show how one can use the reduced mjs to analyze stability and design controllers with significant reduction in computational cost while achieving guaranteed accuracy.',\n",
       "  'categories': 'eess.sy cs.lg cs.sy',\n",
       "  'doi': '10.1109/ojcsys.2022.3212613',\n",
       "  'created': '2022-05-05',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['zhe du', 'laura balzano', 'necmiye ozay'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.02697'},\n",
       " {'title': 'longitudinal cardio-respiratory fitness prediction through wearables in   free-living environments',\n",
       "  'id': '2205.03116',\n",
       "  'abstract': \"cardiorespiratory fitness is an established predictor of metabolic disease and mortality. fitness is directly measured as maximal oxygen consumption (vo$_{2}max$), or indirectly assessed using heart rate responses to standard exercise tests. however, such testing is costly and burdensome because it requires specialized equipment such as treadmills and oxygen masks, limiting its utility. modern wearables capture dynamic real-world data which could improve fitness prediction. in this work, we design algorithms and models that convert raw wearable sensor data into cardiorespiratory fitness estimates. we validate these estimates' ability to capture fitness profiles in free-living conditions using the fenland study (n=11,059), along with its longitudinal cohort (n=2,675), and a third external cohort using the uk biobank validation study (n=181) who underwent maximal vo$_{2}max$ testing, the gold standard measurement of fitness. our results show that the combination of wearables and other biomarkers as inputs to neural networks yields a strong correlation to ground truth in a holdout sample (r = 0.82, 95ci 0.80-0.83), outperforming other approaches and models and detects fitness change over time (e.g., after 7 years). we also show how the model's latent space can be used for fitness-aware patient subtyping paving the way to scalable interventions and personalized trial recruitment. these results demonstrate the value of wearables for fitness estimation that today can be measured only with laboratory tests.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-06',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['dimitris spathis',\n",
       "   'ignacio perez-pozuelo',\n",
       "   'tomas i. gonzales',\n",
       "   'yu wu',\n",
       "   'soren brage',\n",
       "   'nicholas wareham',\n",
       "   'cecilia mascolo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.03116'},\n",
       " {'title': 'ef-bv: a unified theory of error feedback and variance reduction   mechanisms for biased and unbiased compression in distributed optimization',\n",
       "  'id': '2205.04180',\n",
       "  'abstract': \"in distributed or federated optimization and learning, communication between the different computing units is often the bottleneck and gradient compression is widely used to reduce the number of bits sent within each communication round of iterative methods. there are two classes of compression operators and separate algorithms making use of them. in the case of unbiased random compressors with bounded variance (e.g., rand-k), the diana algorithm of mishchenko et al. (2019), which implements a variance reduction technique for handling the variance introduced by compression, is the current state of the art. in the case of biased and contractive compressors (e.g., top-k), the ef21 algorithm of richt\\\\'arik et al. (2021), which instead implements an error-feedback mechanism, is the current state of the art. these two classes of compression schemes and algorithms are distinct, with different analyses and proof techniques. in this paper, we unify them into a single framework and propose a new algorithm, recovering diana and ef21 as particular cases. our general approach works with a new, larger class of compressors, which has two parameters, the bias and the variance, and includes unbiased and biased compressors as particular cases. this allows us to inherit the best of the two worlds: like ef21 and unlike diana, biased compressors, like top-k, whose good performance in practice is recognized, can be used. and like diana and unlike ef21, independent randomness at the compressors allows to mitigate the effects of compression, with the convergence rate improving when the number of parallel workers is large. this is the first time that an algorithm with all these features is proposed. we prove its linear convergence under certain conditions. our approach takes a step towards better understanding of two so-far distinct worlds of communication-efficient distributed learning.\",\n",
       "  'categories': 'cs.lg cs.dc math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-09',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['laurent condat', 'kai yi', 'peter richtárik'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.04180'},\n",
       " {'title': 'feature extractor stacking for cross-domain few-shot meta-learning',\n",
       "  'id': '2205.05831',\n",
       "  'abstract': 'cross-domain few-shot meta-learning (cdfsml) addresses learning problems where knowledge needs to be transferred from several source domains into an instance-scarce target domain with an explicitly different distribution. recently published cdfsml methods generally construct a \"universal model\" that combines knowledge of multiple source domains into one backbone feature extractor. this enables efficient inference but necessitates re-computation of the backbone whenever a new source domain is added. moreover, these methods often derive their universal model from a collection of backbones -- normally one for each source domain -- where these backbones are constrained to have the same architecture as the universal model. we propose feature extractor stacking (fes), a new cdfsml method for combining information from a collection of backbones that imposes no constraints on the backbones\\' architecture and does not require re-computing a universal model when a backbone for a new source domain becomes available. we present the basic fes algorithm, which is inspired by the classic stacking approach to meta-learning, and also introduce two variants: convolutional fes (confes) and regularised fes (refes). given a target-domain task, these algorithms fine-tune each backbone independently, use cross-validation to extract meta training data from the support set available for the task, and learn a simple linear meta-classifier from this data. we evaluate our fes methods on the well-known meta-dataset benchmark, targeting image classification with convolutional neural networks, and show that they can achieve state-of-the-art performance.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-11',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['hongyu wang',\n",
       "   'eibe frank',\n",
       "   'bernhard pfahringer',\n",
       "   'michael mayo',\n",
       "   'geoffrey holmes'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.05831'},\n",
       " {'title': 'large neural networks learning from scratch with very few data and   without explicit regularization',\n",
       "  'id': '2205.08836',\n",
       "  'abstract': 'recent findings have shown that highly over-parameterized neural networks generalize without pretraining or explicit regularization. it is achieved with zero training error, i.e., complete over-fitting by memorizing the training data. this is surprising, since it is completely against traditional machine learning wisdom. in our empirical study we fortify these findings in the domain of fine-grained image classification. we show that very large convolutional neural networks with millions of weights do learn with only a handful of training samples and without image augmentation, explicit regularization or pretraining. we train the architectures resnet018, resnet101 and vgg19 on subsets of the difficult benchmark datasets caltech101, cub_200_2011, fgvcaircraft, flowers102 and stanfordcars with 100 classes and more, perform a comprehensive comparative study and draw implications for the practical application of cnns. finally, we show that a randomly initialized vgg19 with 140 million weights learns to distinguish airplanes and motorbikes with up to 95% accuracy using only 20 training samples per class.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-18',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['christoph linse', 'thomas martinetz'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.08836'},\n",
       " {'title': 'masked autoencoders as spatiotemporal learners',\n",
       "  'id': '2205.09113',\n",
       "  'abstract': 'this paper studies a conceptually simple extension of masked autoencoders (mae) to spatiotemporal representation learning from videos. we randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. interestingly, we show that our mae method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. we observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. a high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. we report competitive results on several challenging video datasets using vanilla vision transformers. we observe that mae can outperform supervised pre-training by large margins. we further report encouraging results of training on real-world, uncurated instagram data. our study suggests that the general framework of masked autoencoding (bert, mae, etc.) can be a unified methodology for representation learning with minimal domain knowledge.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-18',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['christoph feichtenhofer',\n",
       "   'haoqi fan',\n",
       "   'yanghao li',\n",
       "   'kaiming he'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.09113'},\n",
       " {'title': 'clcnet: rethinking of ensemble modeling with classification confidence   network',\n",
       "  'id': '2205.09612',\n",
       "  'abstract': 'in this paper, we propose a classification confidence network (clcnet) that can determine whether the classification model classifies input samples correctly. it can take a classification result in the form of vector in any dimension, and return a confidence score as output, which represents the probability of an instance being classified correctly. we can utilize clcnet in a simple cascade structure system consisting of several sota (state-of-the-art) classification models, and our experiments show that the system can achieve the following advantages: 1. the system can customize the average computation requirement (flops) per image while inference. 2. under the same computation requirement, the performance of the system can exceed any model that has identical structure with the model in the system, but different in size. in fact, this is a new type of ensemble modeling. like general ensemble modeling, it can achieve higher performance than single classification model, yet our system requires much less computation than general ensemble modeling. we have uploaded our code to a github repository: https://github.com/yaoching0/clcnet-rethinking-of-ensemble-modeling.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-19',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['yao-ching yu', 'shi-jinn horng'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.09612'},\n",
       " {'title': 'summarization as indirect supervision for relation extraction',\n",
       "  'id': '2205.09837',\n",
       "  'abstract': 'relation extraction (re) models have been challenged by their reliance on training data with expensive annotations. considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of re, i.e., extracting a kind of synoptical information that describes the relation of entity mentions. we present sure, which converts re into a summarization formulation. sure leads to more precise and resource-efficient re based on indirect supervision from summarization tasks. to achieve this goal, we develop sentence and relation conversion techniques that essentially bridge the formulation of summarization and re tasks. we also incorporate constraint decoding techniques with trie scoring to further enhance summarization-based re with robust inference. experiments on three re datasets demonstrate the effectiveness of sure in both full-dataset and low-resource settings, showing that summarization is a promising source of indirect supervision to improve re models.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-19',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['keming lu',\n",
       "   'i-hung hsu',\n",
       "   'wenxuan zhou',\n",
       "   'mingyu derek ma',\n",
       "   'muhao chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.09837'},\n",
       " {'title': 'non-autoregressive neural machine translation: a call for clarity',\n",
       "  'id': '2205.10577',\n",
       "  'abstract': 'non-autoregressive approaches aim to improve the inference speed of translation models by only requiring a single forward pass to generate the output sequence instead of iteratively producing each predicted token. consequently, their translation quality still tends to be inferior to their autoregressive counterparts due to several issues involving output token interdependence. in this work, we take a step back and revisit several techniques that have been proposed for improving non-autoregressive translation models and compare their combined translation quality and speed implications under third-party testing environments. we provide novel insights for establishing strong baselines using length prediction or ctc-based architecture variants and contribute standardized bleu, chrf++, and ter scores using sacrebleu on four translation tasks, which crucially have been missing as inconsistencies in the use of tokenized bleu lead to deviations of up to 1.7 bleu points. our open-sourced code is integrated into fairseq for reproducibility.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-21',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['robin m. schmidt',\n",
       "   'telmo pires',\n",
       "   'stephan peitz',\n",
       "   'jonas lööf'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.10577'},\n",
       " {'title': 'fast instrument learning with faster rates',\n",
       "  'id': '2205.10772',\n",
       "  'abstract': 'we investigate nonlinear instrumental variable (iv) regression given high-dimensional instruments. we propose a simple algorithm which combines kernelized iv methods and an arbitrary, adaptive regression algorithm, accessed as a black box. our algorithm enjoys faster-rate convergence and adapts to the dimensionality of informative latent features, while avoiding an expensive minimax optimization procedure, which has been necessary to establish similar guarantees. it further brings the benefit of flexible machine learning models to quasi-bayesian uncertainty quantification, likelihood-based model selection, and model averaging. simulation studies demonstrate the competitive performance of our method.',\n",
       "  'categories': 'stat.ml cs.lg econ.em',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-22',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['ziyu wang', 'yuhao zhou', 'jun zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.10772'},\n",
       " {'title': 'what do compressed multilingual machine translation models forget?',\n",
       "  'id': '2205.10828',\n",
       "  'abstract': 'recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (nlp) tasks, but their size makes it more challenging to apply them in resource-constrained environments. compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. however, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. in this work, we assess the impact of compression methods on multilingual neural machine translation models (mnmt) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. flores-101, mt-gender, and dibimt. we show that the performance of under-represented languages drops significantly, while the average bleu metric only slightly decreases. interestingly, the removal of noisy memorization with compression leads to a significant improvement for some medium-resource languages. finally, we demonstrate that compression amplifies intrinsic gender and semantic biases, even in high-resource languages. code: https://github.com/alirezamshi/bias-compressedmt',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-22',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['alireza mohammadshahi',\n",
       "   'vassilina nikoulina',\n",
       "   'alexandre berard',\n",
       "   'caroline brun',\n",
       "   'james henderson',\n",
       "   'laurent besacier'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.10828'},\n",
       " {'title': 'on elimination strategies for bandit fixed-confidence identification',\n",
       "  'id': '2205.10936',\n",
       "  'abstract': 'elimination algorithms for bandit identification, which prune the plausible correct answers sequentially until only one remains, are computationally convenient since they reduce the problem size over time. however, existing elimination strategies are often not fully adaptive (they update their sampling rule infrequently) and are not easy to extend to combinatorial settings, where the set of answers is exponentially large in the problem dimension. on the other hand, most existing fully-adaptive strategies to tackle general identification problems are computationally demanding since they repeatedly test the correctness of every answer, without ever reducing the problem size. we show that adaptive methods can be modified to use elimination in both their stopping and sampling rules, hence obtaining the best of these two worlds: the algorithms (1) remain fully adaptive, (2) suffer a sample complexity that is never worse of their non-elimination counterpart, and (3) provably eliminate certain wrong answers early. we confirm these benefits experimentally, where elimination improves significantly the computational complexity of adaptive methods on common tasks like best-arm identification in linear bandits.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-22',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['andrea tirinzoni', 'rémy degenne'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.10936'},\n",
       " {'title': 'rl with kl penalties is better viewed as bayesian inference',\n",
       "  'id': '2205.11275',\n",
       "  'abstract': 'reinforcement learning (rl) is frequently employed in fine-tuning large language models (lms), such as gpt-3, to penalize them for undesirable features of generated sequences, such as offensiveness, social bias, harmfulness or falsehood. the rl formulation involves treating the lm as a policy and updating it to maximise the expected value of a reward function which captures human preferences, such as non-offensiveness. in this paper, we analyze challenges associated with treating a language model as an rl policy and show how avoiding those challenges requires moving beyond the rl paradigm. we start by observing that the standard rl approach is flawed as an objective for fine-tuning lms because it leads to distribution collapse: turning the lm into a degenerate distribution. then, we analyze kl-regularised rl, a widely used recipe for fine-tuning lms, which additionally constrains the fine-tuned lm to stay close to its original distribution in terms of kullback-leibler (kl) divergence. we show that kl-regularised rl is equivalent to variational inference: approximating a bayesian posterior which specifies how to update a prior lm to conform with evidence provided by the reward function. we argue that this bayesian inference view of kl-regularised rl is more insightful than the typically employed rl perspective. the bayesian inference view explains how kl-regularised rl avoids the distribution collapse problem and offers a first-principles derivation for its objective. while this objective happens to be equivalent to rl (with a particular choice of parametric reward), there exist other objectives for fine-tuning lms which are no longer equivalent to rl. that observation leads to a more general point: rl is not an adequate formal framework for problems such as fine-tuning language models. these problems are best viewed as bayesian inference: approximating a pre-defined target distribution.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-23',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['tomasz korbak', 'ethan perez', 'christopher l buckley'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.11275'},\n",
       " {'title': 'chaotic regularization and heavy-tailed limits for deterministic   gradient descent',\n",
       "  'id': '2205.11361',\n",
       "  'abstract': \"recent studies have shown that gradient descent (gd) can achieve improved generalization when its dynamics exhibits a chaotic behavior. however, to obtain the desired effect, the step-size should be chosen sufficiently large, a task which is problem dependent and can be difficult in practice. in this study, we incorporate a chaotic component to gd in a controlled manner, and introduce multiscale perturbed gd (mpgd), a novel optimization framework where the gd recursion is augmented with chaotic perturbations that evolve via an independent dynamical system. we analyze mpgd from three different angles: (i) by building up on recent advances in rough paths theory, we show that, under appropriate assumptions, as the step-size decreases, the mpgd recursion converges weakly to a stochastic differential equation (sde) driven by a heavy-tailed l\\\\'evy-stable process. (ii) by making connections to recently developed generalization bounds for heavy-tailed processes, we derive a generalization bound for the limiting sde and relate the worst-case generalization error over the trajectories of the process to the parameters of mpgd. (iii) we analyze the implicit regularization effect brought by the dynamical regularization and show that, in the weak perturbation regime, mpgd introduces terms that penalize the hessian of the loss function. empirical results are provided to demonstrate the advantages of mpgd.\",\n",
       "  'categories': 'stat.ml cs.lg math.ds math.pr',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-23',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['soon hoe lim', 'yijun wan', 'umut şimşekli'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.11361'},\n",
       " {'title': 'logical reasoning with span-level predictions for interpretable and   robust nli models',\n",
       "  'id': '2205.11432',\n",
       "  'abstract': 'current natural language inference (nli) models achieve impressive results, sometimes outperforming humans when evaluating on in-distribution test sets. however, as these models are known to learn from annotation artefacts and dataset biases, it is unclear to what extent the models are learning the task of nli instead of learning from shallow heuristics in their training data. we address this issue by introducing a logical reasoning framework for nli, creating highly transparent model decisions that are based on logical rules. unlike prior work, we show that improved interpretability can be achieved without decreasing the predictive accuracy. we almost fully retain performance on snli, while also identifying the exact hypothesis spans that are responsible for each model prediction. using the e-snli human explanations, we verify that our model makes sensible decisions at a span level, despite not using any span labels during training. we can further improve model performance and span-level decisions by using the e-snli explanations during training. finally, our model is more robust in a reduced data setting. when training with only 1,000 examples, out-of-distribution performance improves on the mnli matched and mismatched validation sets by 13% and 16% relative to the baseline. training with fewer observations yields further improvements, both in-distribution and out-of-distribution.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-23',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['joe stacey',\n",
       "   'pasquale minervini',\n",
       "   'haim dubossarsky',\n",
       "   'marek rei'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.11432'},\n",
       " {'title': 'generating natural language proofs with verifier-guided search',\n",
       "  'id': '2205.12443',\n",
       "  'abstract': 'reasoning over natural language is a challenging problem in nlp. in this work, we focus on proof generation: given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. instead, they tend to hallucinate invalid steps given the hypothesis. in this paper, we present a novel stepwise method, nlproofs (natural language proof search), which learns to generate relevant steps conditioning on the hypothesis. at the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. nlproofs achieves state-of-the-art performance on entailmentbank and ruletaker. specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of entailmentbank, demonstrating the effectiveness of nlproofs in generating challenging human-authored proofs.',\n",
       "  'categories': 'cs.cl cs.lg cs.lo',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-24',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['kaiyu yang', 'jia deng', 'danqi chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.12443'},\n",
       " {'title': 'rlprompt: optimizing discrete text prompts with reinforcement learning',\n",
       "  'id': '2205.12548',\n",
       "  'abstract': 'prompting has shown impressive success in enabling large pretrained language models (lms) to perform diverse nlp tasks, especially when only few downstream data are available. automatically finding the optimal prompt for each task, however, is challenging. most existing work resorts to tuning soft prompt (e.g., embeddings) which falls short of interpretability, reusability across lms, and applicability when gradients are not accessible. discrete prompt, on the other hand, is difficult to optimize, and is often created by \"enumeration (e.g., paraphrasing)-then-selection\" heuristics that do not explore the prompt space systematically. this paper proposes rlprompt, an efficient discrete prompt optimization approach with reinforcement learning (rl). rlprompt formulates a parameter-efficient policy network that generates the desired discrete prompt after training with reward. to overcome the complexity and stochasticity of reward signals by the large lm environment, we incorporate effective reward stabilization that substantially enhances the training efficiency. rlprompt is flexibly applicable to different types of lms, such as masked (e.g., bert) and left-to-right models (e.g., gpts), for both classification and generation tasks. experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing finetuning or prompting methods. interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different lms to retain significant performance, indicating lm prompting may not follow human language patterns.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-25',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['mingkai deng',\n",
       "   'jianyu wang',\n",
       "   'cheng-ping hsieh',\n",
       "   'yihan wang',\n",
       "   'han guo',\n",
       "   'tianmin shu',\n",
       "   'meng song',\n",
       "   'eric p. xing',\n",
       "   'zhiting hu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.12548'},\n",
       " {'title': 'ground-truth labels matter: a deeper look into input-label   demonstrations',\n",
       "  'id': '2205.12685',\n",
       "  'abstract': 'despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive. intuitively, ground-truth labels should have as much impact in in-context learning (icl) as supervised learning, but recent work reported that the input-label correspondence is significantly less important than previously thought. intrigued by this counter-intuitive observation, we re-examine the importance of ground-truth labels in in-context learning. with the introduction of two novel metrics, namely label-correctness sensitivity and ground-truth label effect ratio (gler), we were able to conduct quantifiable analysis on the impact of ground-truth label demonstrations. through extensive analyses, we find that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration. through additional studies, we identify key components, such as the verbosity of prompt templates and the language model size, as the controlling factor to achieve more noise-resilient icl.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-25',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['kang min yoo',\n",
       "   'junyeob kim',\n",
       "   'hyuhng joon kim',\n",
       "   'hyunsoo cho',\n",
       "   'hwiyeol jo',\n",
       "   'sang-woo lee',\n",
       "   'sang-goo lee',\n",
       "   'taeuk kim'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.12685'},\n",
       " {'title': 'train flat, then compress: sharpness-aware minimization learns more   compressible models',\n",
       "  'id': '2205.12694',\n",
       "  'abstract': 'model compression by way of parameter pruning, quantization, or distillation has recently gained popularity as an approach for reducing the computational requirements of modern deep neural network models for nlp. inspired by prior works suggesting a connection between simpler, more generalizable models and those that lie within wider loss basins, we hypothesize that optimizing for flat minima should lead to simpler parameterizations and thus more compressible models. we propose to combine sharpness-aware minimization (sam) with various task-specific model compression methods, including iterative magnitude pruning (imp), structured pruning with a distillation objective, and post-training dynamic quantization. empirically, we show that optimizing for flatter minima consistently leads to greater compressibility of parameters compared to vanilla adam when fine-tuning bert models, with little to no loss in accuracy on the glue text classification and squad question answering benchmarks. moreover, sam finds superior winning tickets during imp that 1) are amenable to vanilla adam optimization, and 2) transfer more effectively across tasks.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-25',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['clara na', 'sanket vaibhav mehta', 'emma strubell'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.12694'},\n",
       " {'title': 'differentially private data generation needs better features',\n",
       "  'id': '2205.12900',\n",
       "  'abstract': \"training even moderately-sized generative models with differentially-private stochastic gradient descent (dp-sgd) is difficult: the required level of noise for reasonable levels of privacy is simply too large. we advocate instead building off a good, relevant representation on an informative public dataset, then learning to model the private data with that representation. in particular, we minimize the maximum mean discrepancy (mmd) between private target data and a generator's distribution, using a kernel based on perceptual features learned from a public dataset. with the mmd, we can simply privatize the data-dependent term once and for all, rather than introducing noise at each step of optimization as in dp-sgd. our algorithm allows us to generate cifar10-level images with $\\\\epsilon \\\\approx 2$ which capture distinctive features in the distribution, far surpassing the current state of the art, which mostly focuses on datasets such as mnist and fashionmnist at a large $\\\\epsilon \\\\approx 10$. our work introduces simple yet powerful foundations for reducing the gap between private and non-private deep generative models.\",\n",
       "  'categories': 'stat.ml cs.cr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-25',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['fredrik harder',\n",
       "   'milad jalali asadabadi',\n",
       "   'danica j. sutherland',\n",
       "   'mijung park'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.12900'},\n",
       " {'title': 'scalable and low-latency federated learning with cooperative mobile edge   networking',\n",
       "  'id': '2205.13054',\n",
       "  'abstract': 'federated learning (fl) enables collaborative model training without centralizing data. however, the traditional fl framework is cloud-based and suffers from high communication latency. on the other hand, the edge-based fl framework that relies on an edge server co-located with mobile base station for model aggregation has low communication latency but suffers from degraded model accuracy due to the limited coverage of edge server. in light of high accuracy but high-latency cloud-based fl and low-latency but low-accuracy edge-based fl, this paper proposes a new fl framework based on cooperative mobile edge networking called cooperative federated edge learning (cfel) to enable both high-accuracy and low-latency distributed intelligence at mobile edge networks. considering the unique two-tier network architecture of cfel, a novel federated optimization method dubbed cooperative edge-based federated averaging (ce-fedavg) is further developed, wherein each edge server both coordinates collaborative model training among the devices within its own coverage and cooperates with other edge servers to learn a shared global model through decentralized consensus. experimental results based on benchmark datasets show that cfel can largely reduce the training time to achieve a target model accuracy compared with prior fl frameworks.',\n",
       "  'categories': 'cs.dc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-25',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['zhenxiao zhang', 'zhidong gao', 'yuanxiong guo', 'yanmin gong'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.13054'},\n",
       " {'title': 'censored quantile regression neural networks for distribution-free   survival analysis',\n",
       "  'id': '2205.13496',\n",
       "  'abstract': \"this paper considers doing quantile regression on censored data using neural networks (nns). this adds to the survival analysis toolkit by allowing direct prediction of the target variable, along with a distribution-free characterisation of uncertainty, using a flexible function approximator. we begin by showing how an algorithm popular in linear models can be applied to nns. however, the resulting procedure is inefficient, requiring sequential optimisation of an individual nn at each desired quantile. our major contribution is a novel algorithm that simultaneously optimises a grid of quantiles output by a single nn. to offer theoretical insight into our algorithm, we show firstly that it can be interpreted as a form of expectation-maximisation, and secondly that it exhibits a desirable `self-correcting' property. experimentally, the algorithm produces quantiles that are better calibrated than existing methods on 10 out of 12 real datasets.\",\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-26',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['tim pearce', 'jong-hyeon jeong', 'yichen jia', 'jun zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.13496'},\n",
       " {'title': 'training and inference on any-order autoregressive models the right way',\n",
       "  'id': '2205.13554',\n",
       "  'abstract': 'conditional inference on arbitrary subsets of variables is a core problem in probabilistic inference with important applications such as masked language modeling and image inpainting. in recent years, the family of any-order autoregressive models (ao-arms) -- closely related to popular models such as bert and xlnet -- has shown breakthrough performance in arbitrary conditional tasks across a sweeping range of domains. but, in spite of their success, in this paper we identify significant improvements to be made to previous formulations of ao-arms. first, we show that ao-arms suffer from redundancy in their probabilistic model, i.e., they define the same distribution in multiple different ways. we alleviate this redundancy by training on a smaller set of univariate conditionals that still maintains support for efficient arbitrary conditional inference. second, we upweight the training loss for univariate conditionals that are evaluated more frequently during inference. our method leads to improved performance with no compromises on tractability, giving state-of-the-art likelihoods in arbitrary conditional modeling on text (text8), image (cifar10, imagenet32), and continuous tabular data domains.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-26',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['andy shih', 'dorsa sadigh', 'stefano ermon'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.13554'},\n",
       " {'title': 'learning dynamical systems via koopman operator regression in   reproducing kernel hilbert spaces',\n",
       "  'id': '2205.14027',\n",
       "  'abstract': 'we study a class of dynamical systems modelled as markov chains that admit an invariant distribution via the corresponding transfer, or koopman, operator. while data-driven algorithms to reconstruct such operators are well known, their relationship with statistical learning is largely unexplored. we formalize a framework to learn the koopman operator from finite data trajectories of the dynamical system. we consider the restriction of this operator to a reproducing kernel hilbert space and introduce a notion of risk, from which different estimators naturally arise. we link the risk with the estimation of the spectral decomposition of the koopman operator. these observations motivate a reduced-rank operator regression (rrr) estimator. we derive learning bounds for the proposed estimator, holding both in i.i.d. and non i.i.d. settings, the latter in terms of mixing coefficients. our results suggest rrr might be beneficial over other widely used estimators as confirmed in numerical experiments both for forecasting and mode decomposition.',\n",
       "  'categories': 'cs.lg math.ds',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-27',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['vladimir kostic',\n",
       "   'pietro novelli',\n",
       "   'andreas maurer',\n",
       "   'carlo ciliberto',\n",
       "   'lorenzo rosasco',\n",
       "   'massimiliano pontil'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.14027'},\n",
       " {'title': 'align then fusion: generalized large-scale multi-view clustering with   anchor matching correspondences',\n",
       "  'id': '2205.15075',\n",
       "  'abstract': 'multi-view anchor graph clustering selects representative anchors to avoid full pair-wise similarities and therefore reduce the complexity of graph methods. although widely applied in large-scale applications, existing approaches do not pay sufficient attention to establishing correct correspondences between the anchor sets across views. to be specific, anchor graphs obtained from different views are not aligned column-wisely. such an \\\\textbf{a}nchor-\\\\textbf{u}naligned \\\\textbf{p}roblem (aup) would cause inaccurate graph fusion and degrade the clustering performance. under multi-view scenarios, generating correct correspondences could be extremely difficult since anchors are not consistent in feature dimensions. to solve this challenging issue, we propose the first study of the generalized and flexible anchor graph fusion framework termed \\\\textbf{f}ast \\\\textbf{m}ulti-\\\\textbf{v}iew \\\\textbf{a}nchor-\\\\textbf{c}orrespondence \\\\textbf{c}lustering (fmvacc). specifically, we show how to find anchor correspondence with both feature and structure information, after which anchor graph fusion is performed column-wisely. moreover, we theoretically show the connection between fmvacc and existing multi-view late fusion \\\\cite{liu2018late} and partial view-aligned clustering \\\\cite{huang2020partially}, which further demonstrates our generality. extensive experiments on seven benchmark datasets demonstrate the effectiveness and efficiency of our proposed method. moreover, the proposed alignment module also shows significant performance improvement applying to existing multi-view anchor graph competitors indicating the importance of anchor alignment. our code is available at \\\\url{https://github.com/wangsiwei2010/neurips22-fmvacc}.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-30',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['siwei wang',\n",
       "   'xinwang liu',\n",
       "   'suyuan liu',\n",
       "   'jiaqi jin',\n",
       "   'wenxuan tu',\n",
       "   'xinzhong zhu',\n",
       "   'en zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15075'},\n",
       " {'title': 'few-shot adaptation of pre-trained networks for domain shift',\n",
       "  'id': '2205.15234',\n",
       "  'abstract': 'deep networks are prone to performance degradation when there is a domain shift between the source (training) data and target (test) data. recent test-time adaptation methods update batch normalization layers of pre-trained source models deployed in new target environments with streaming data to mitigate such performance degradation. although such methods can adapt on-the-fly without first collecting a large target domain dataset, their performance is dependent on streaming conditions such as mini-batch size and class-distribution, which can be unpredictable in practice. in this work, we propose a framework for few-shot domain adaptation to address the practical challenges of data-efficient adaptation. specifically, we propose a constrained optimization of feature normalization statistics in pre-trained source models supervised by a small support set from the target domain. our method is easy to implement and improves source model performance with as few as one sample per class for classification tasks. extensive experiments on 5 cross-domain classification and 4 semantic segmentation datasets show that our method achieves more accurate and reliable performance than test-time adaptation, while not being constrained by streaming conditions.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-30',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['wenyu zhang', 'li shen', 'wanyue zhang', 'chuan-sheng foo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15234'},\n",
       " {'title': 'minimax optimal online imitation learning via replay estimation',\n",
       "  'id': '2205.15397',\n",
       "  'abstract': 'online imitation learning is the problem of how best to mimic expert demonstrations, given access to the environment or an accurate simulator. prior work has shown that in the infinite sample regime, exact moment matching achieves value equivalence to the expert policy. however, in the finite sample regime, even if one has no optimization error, empirical variance can lead to a performance gap that scales with $h^2 / n$ for behavioral cloning and $h / \\\\sqrt{n}$ for online moment matching, where $h$ is the horizon and $n$ is the size of the expert dataset. we introduce the technique of replay estimation to reduce this empirical variance: by repeatedly executing cached expert actions in a stochastic simulator, we compute a smoother expert visitation distribution estimate to match. in the presence of general function approximation, we prove a meta theorem reducing the performance gap of our approach to the parameter estimation error for offline classification (i.e. learning the expert policy). in the tabular setting or with linear function approximation, our meta theorem shows that the performance gap incurred by our approach achieves the optimal $\\\\widetilde{o} \\\\left( \\\\min({h^{3/2}} / {n}, {h} / {\\\\sqrt{n}} \\\\right)$ dependency, under significantly weaker assumptions compared to prior work. we implement multiple instantiations of our approach on several continuous control tasks and find that we are able to significantly improve policy performance across a variety of dataset sizes.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-30',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['gokul swamy',\n",
       "   'nived rajaraman',\n",
       "   'matthew peng',\n",
       "   'sanjiban choudhury',\n",
       "   'j. andrew bagnell',\n",
       "   'zhiwei steven wu',\n",
       "   'jiantao jiao',\n",
       "   'kannan ramchandran'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15397'},\n",
       " {'title': 'data banzhaf: a robust data valuation framework for machine learning',\n",
       "  'id': '2205.15466',\n",
       "  'abstract': \"this paper studies the robustness of data valuation to noisy model performance scores. particularly, we find that the inherent randomness of the widely used stochastic gradient descent can cause existing data value notions (e.g., the shapley value and the leave-one-out error) to produce inconsistent data value rankings across different runs. to address this challenge, we first pose a formal framework within which one can measure the robustness of a data value notion. we show that the banzhaf value, a value notion originated from cooperative game theory literature, achieves the maximal robustness among all semivalues -- a class of value notions that satisfy crucial properties entailed by ml applications. we propose an algorithm to efficiently estimate the banzhaf value based on the maximum sample reuse (msr) principle. we derive the lower bound sample complexity for banzhaf value estimation, and we show that our msr algorithm's sample complexity is close to the lower bound. our evaluation demonstrates that the banzhaf value outperforms the existing semivalue-based data value notions on several downstream ml tasks such as learning with weighted samples and noisy label detection. overall, our study suggests that when the underlying ml algorithm is stochastic, the banzhaf value is a promising alternative to the semivalue-based data value schemes given its computational advantage and ability to robustly differentiate data quality.\",\n",
       "  'categories': 'cs.lg cs.gt stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-30',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['jiachen t. wang', 'ruoxi jia'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15466'},\n",
       " {'title': 'provable general function class representation learning in multitask   bandits and mdps',\n",
       "  'id': '2205.15701',\n",
       "  'abstract': 'while multitask representation learning has become a popular approach in reinforcement learning (rl) to boost the sample efficiency, the theoretical understanding of why and how it works is still limited. most previous analytical works could only assume that the representation function is already known to the agent or from linear function class, since analyzing general function class representation encounters non-trivial technical obstacles such as generalization guarantee, formulation of confidence bound in abstract function space, etc. however, linear-case analysis heavily relies on the particularity of linear function class, while real-world practice usually adopts general non-linear representation functions like neural networks. this significantly reduces its applicability. in this work, we extend the analysis to general function class representations. specifically, we consider an agent playing $m$ contextual bandits (or mdps) concurrently and extracting a shared representation function $\\\\phi$ from a specific function class $\\\\phi$ using our proposed generalized functional upper confidence bound algorithm (gfucb). we theoretically validate the benefit of multitask representation learning within general function class for bandits and linear mdp for the first time. lastly, we conduct experiments to demonstrate the effectiveness of our algorithm with neural net representation.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-31',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['rui lu', 'andrew zhao', 'simon s. du', 'gao huang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15701'},\n",
       " {'title': 'symformer: end-to-end symbolic regression using transformer-based   architecture',\n",
       "  'id': '2205.15764',\n",
       "  'abstract': 'many real-world problems can be naturally described by mathematical formulas. the task of finding formulas from a set of observed inputs and outputs is called symbolic regression. recently, neural networks have been applied to symbolic regression, among which the transformer-based ones seem to be the most promising. after training the transformer on a large number of formulas (in the order of days), the actual inference, i.e., finding a formula for new, unseen data, is very fast (in the order of seconds). this is considerably faster than state-of-the-art evolutionary methods. the main drawback of transformers is that they generate formulas without numerical constants, which have to be optimized separately, so yielding suboptimal results. we propose a transformer-based approach called symformer, which predicts the formula by outputting the individual symbols and the corresponding constants simultaneously. this leads to better performance in terms of fitting the available data. in addition, the constants provided by symformer serve as a good starting point for subsequent tuning via gradient descent to further improve the performance. we show on a set of benchmarks that symformer outperforms two state-of-the-art methods while having faster inference.',\n",
       "  'categories': 'cs.lg cs.cv cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-31',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['martin vastl',\n",
       "   'jonáš kulhánek',\n",
       "   'jiří kubalík',\n",
       "   'erik derner',\n",
       "   'robert babuška'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15764'},\n",
       " {'title': 'robust anytime learning of markov decision processes',\n",
       "  'id': '2205.15827',\n",
       "  'abstract': 'markov decision processes (mdps) are formal models commonly used in sequential decision-making. mdps capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. however, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. uncertain mdps (umdps) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. we continuously learn the transition probabilities of an mdp in a robust anytime-learning approach that combines a dedicated bayesian inference scheme with the computation of robust policies. in particular, our method (1) approximates probabilities as intervals, (2) adapts to new data that may be inconsistent with an intermediate model, and (3) may be stopped at any time to compute a robust policy on the umdp that faithfully captures the data so far. furthermore, our method is capable of adapting to changes in the environment. we show the effectiveness of our approach and compare it to robust policies computed on umdps learned by the ucrl2 reinforcement learning algorithm in an experimental evaluation on several benchmarks.',\n",
       "  'categories': 'cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-31',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['marnix suilen',\n",
       "   'thiago d. simão',\n",
       "   'david parker',\n",
       "   'nils jansen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15827'},\n",
       " {'title': 'evaluating robustness to dataset shift via parametric robustness sets',\n",
       "  'id': '2205.15947',\n",
       "  'abstract': 'we give a method for proactively identifying small, plausible shifts in distribution which lead to large differences in model performance. these shifts are defined via parametric changes in the causal mechanisms of observed variables, where constraints on parameters yield a \"robustness set\" of plausible distributions and a corresponding worst-case loss over the set. while the loss under an individual parametric shift can be estimated via reweighting techniques such as importance sampling, the resulting worst-case optimization problem is non-convex, and the estimate may suffer from large variance. for small shifts, however, we can construct a local second-order approximation to the loss under shift and cast the problem of finding a worst-case shift as a particular non-convex quadratic optimization problem, for which efficient algorithms are available. we demonstrate that this second-order approximation can be estimated directly for shifts in conditional exponential family models, and we bound the approximation error. we apply our approach to a computer vision task (classifying gender from images), revealing sensitivity to shifts in non-causal attributes.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-31',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['nikolaj thams', 'michael oberst', 'david sontag'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2205.15947'},\n",
       " {'title': 'instance-specific augmentation: capturing local invariances',\n",
       "  'id': '2206.00051',\n",
       "  'abstract': 'we introduce instaaug, a method for automatically learning input-specific augmentations from data. previous data augmentation methods have generally assumed independence between the original input and the transformation applied to that input. this can be highly restrictive, as the invariances that the augmentations are based on are themselves often highly input dependent; e.g., we can change a leaf from green to yellow while maintaining its label, but not a lime. instaaug instead allows for input dependency by introducing an invariance module that maps inputs to tailored transformation distributions. it can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. we empirically demonstrate that instaaug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-05-31',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['ning miao',\n",
       "   'tom rainforth',\n",
       "   'emile mathieu',\n",
       "   'yann dubois',\n",
       "   'yee whye teh',\n",
       "   'adam foster',\n",
       "   'hyunjik kim'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.00051'},\n",
       " {'title': 'dataset distillation using neural feature regression',\n",
       "  'id': '2206.00719',\n",
       "  'abstract': 'dataset distillation aims to learn a small synthetic dataset that preserves most of the information from the original dataset. dataset distillation can be formulated as a bi-level meta-learning problem where the outer loop optimizes the meta-dataset and the inner loop trains a model on the distilled data. meta-gradient computation is one of the key challenges in this formulation, as differentiating through the inner loop learning procedure introduces significant computation and memory costs. in this paper, we address these challenges using neural feature regression with pooling (frepo), achieving the state-of-the-art performance with an order of magnitude less memory requirement and two orders of magnitude faster training than previous methods. the proposed algorithm is analogous to truncated backpropagation through time with a pool of models to alleviate various types of overfitting in dataset distillation. frepo significantly outperforms the previous methods on cifar100, tiny imagenet, and imagenet-1k. furthermore, we show that high-quality distilled data can greatly improve various downstream applications, such as continual learning and membership inference defense. please check out our webpage at https://sites.google.com/view/frepo.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-01',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['yongchao zhou', 'ehsan nezhadarya', 'jimmy ba'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.00719'},\n",
       " {'title': 'the phenomenon of policy churn',\n",
       "  'id': '2206.00730',\n",
       "  'abstract': 'we identify and study the phenomenon of policy churn, that is, the rapid change of the greedy policy in value-based reinforcement learning. policy churn operates at a surprisingly rapid pace, changing the greedy action in a large fraction of states within a handful of learning updates (in a typical deep rl set-up such as dqn on atari). we characterise the phenomenon empirically, verifying that it is not limited to specific algorithm or environment properties. a number of ablations help whittle down the plausible explanations on why churn occurs to just a handful, all related to deep learning. finally, we hypothesise that policy churn is a beneficial but overlooked form of implicit exploration that casts $\\\\epsilon$-greedy exploration in a fresh light, namely that $\\\\epsilon$-noise plays a much smaller role than expected.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-01',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['tom schaul', 'andré barreto', 'john quan', 'georg ostrovski'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.00730'},\n",
       " {'title': 'a survey on computationally efficient neural architecture search',\n",
       "  'id': '2206.01520',\n",
       "  'abstract': 'neural architecture search (nas) has become increasingly popular in the deep learning community recently, mainly because it can provide an opportunity to allow interested users without rich expertise to benefit from the success of deep neural networks (dnns). however, nas is still laborious and time-consuming because a large number of performance estimations are required during the search process of nas, and training dnns is computationally intensive. to solve this major limitation of nas, improving the computational efficiency is essential in the design of nas. however, a systematic overview of computationally efficient nas (ce-nas) methods still lacks. to fill this gap, we provide a comprehensive survey of the state-of-the-art on ce-nas by categorizing the existing work into proxy-based and surrogate-assisted nas methods, together with a thorough discussion of their design principles and a quantitative comparison of their performances and computational complexities. the remaining challenges and open research questions are also discussed, and promising research topics in this emerging field are suggested.',\n",
       "  'categories': 'cs.lg cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-03',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['shiqing liu', 'haoyu zhang', 'yaochu jin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.01520'},\n",
       " {'title': 'estimating counterfactual treatment outcomes over time in complex   multi-agent scenarios',\n",
       "  'id': '2206.01900',\n",
       "  'abstract': 'evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. estimating the individual treatment effect (ite) using counterfactual long-term prediction is practical to evaluate such interventions. however, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. this may sometimes lead to erroneous assessments of ite and interpretation problems. here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ite estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm under the circumstances under which the intervention is effective. on simulated models of an automated vehicle and biological agents with time-varying confounders, we show that our methods achieved lower estimation errors in counterfactual covariates and the most effective treatment timing than the baselines. furthermore, using real basketball data, our methods performed realistic counterfactual predictions and evaluated the counterfactual passes in shot scenarios.',\n",
       "  'categories': 'cs.ai cs.lg cs.ma stat.me stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-04',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['keisuke fujii',\n",
       "   'koh takeuchi',\n",
       "   'atsushi kuribayashi',\n",
       "   'naoya takeishi',\n",
       "   'yoshinobu kawahara',\n",
       "   'kazuya takeda'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.01900'},\n",
       " {'title': 'rorl: robust offline reinforcement learning via conservative smoothing',\n",
       "  'id': '2206.02829',\n",
       "  'abstract': 'offline reinforcement learning (rl) provides a promising direction to exploit massive amount of offline data for complex decision-making tasks. due to the distribution shift issue, current offline rl algorithms are generally designed to be conservative in value estimation and action selection. however, such conservatism can impair the robustness of learned policies when encountering observation deviation under realistic conditions, such as sensor errors and adversarial attacks. to trade off robustness and conservatism, we propose robust offline reinforcement learning (rorl) with a novel conservative smoothing technique. in rorl, we explicitly introduce regularization on the policy and the value function for states near the dataset, as well as additional conservative value estimation on these states. theoretically, we show rorl enjoys a tighter suboptimality bound than recent theoretical results in linear mdps. we demonstrate that rorl can achieve state-of-the-art performance on the general offline rl benchmark and is considerably robust to adversarial observation perturbations.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-06',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['rui yang',\n",
       "   'chenjia bai',\n",
       "   'xiaoteng ma',\n",
       "   'zhaoran wang',\n",
       "   'chongjie zhang',\n",
       "   'lei han'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.02829'},\n",
       " {'title': 'gretel: a unified framework for graph counterfactual explanation   evaluation',\n",
       "  'id': '2206.02957',\n",
       "  'abstract': 'machine learning (ml) systems are a building part of the modern tools which impact our daily life in several application domains. due to their black-box nature, those systems are hardly adopted in application domains (e.g. health, finance) where understanding the decision process is of paramount importance. explanation methods were developed to explain how the ml model has taken a specific decision for a given case/instance. graph counterfactual explanations (gce) is one of the explanation techniques adopted in the graph learning domain. the existing works of graph counterfactual explanations diverge mostly in the problem definition, application domain, test data, and evaluation metrics, and most existing works do not compare exhaustively against other counterfactual explanation techniques present in the literature. we present gretel, a unified framework to develop and test gce methods in several settings. gretel is a highly extensible evaluation framework which promotes the open science and the evaluations reproducibility by providing a set of well-defined mechanisms to integrate and manage easily: both real and synthetic datasets, ml models, state-of-the-art explanation techniques, and evaluation measures. to present gretel, we show the experiments conducted to integrate and test several synthetic and real datasets with several existing explanation techniques and base ml models.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '10.1145/3511808.3557608',\n",
       "  'created': '2022-06-06',\n",
       "  'updated': '',\n",
       "  'authors': ['mario alfonso prado-romero', 'giovanni stilo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.02957'},\n",
       " {'title': 'vn-transformer: rotation-equivariant attention for vector neurons',\n",
       "  'id': '2206.04176',\n",
       "  'abstract': 'rotation equivariance is a desirable property in many practical applications such as motion forecasting and 3d perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations. vector neurons (vn) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional \"vector neurons.\" we introduce a novel \"vn-transformer\" architecture to address several shortcomings of the current vn models. our contributions are: $(i)$ we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original vector neurons models; $(ii)$ we extend the vn framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; $(iii)$ we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; $(iv)$ we show that small tradeoffs in equivariance ($\\\\epsilon$-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models. finally, we apply our vn-transformer to 3d shape classification and motion forecasting with compelling results.',\n",
       "  'categories': 'cs.cv cs.lg cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-08',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['serge assaad',\n",
       "   'carlton downey',\n",
       "   'rami al-rfou',\n",
       "   'nigamaa nayakanti',\n",
       "   'ben sapp'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.04176'},\n",
       " {'title': 'pragmatically learning from pedagogical demonstrations in multi-goal   environments',\n",
       "  'id': '2206.04546',\n",
       "  'abstract': 'learning from demonstration methods usually leverage close to optimal demonstrations to accelerate training. by contrast, when demonstrating a task, human teachers deviate from optimal demonstrations and pedagogically modify their behavior by giving demonstrations that best disambiguate the goal they want to demonstrate. analogously, human learners excel at pragmatically inferring the intent of the teacher, facilitating communication between the two agents. these mechanisms are critical in the few demonstrations regime, where inferring the goal is more difficult. in this paper, we implement pedagogy and pragmatism mechanisms by leveraging a bayesian model of goal inference from demonstrations (bgi). we highlight the benefits of this model in multi-goal teacher-learner setups with two artificial agents that learn with goal-conditioned reinforcement learning. we show that combining bgi-agents (a pedagogical teacher and a pragmatic learner) results in faster learning and reduced goal ambiguity over standard learning from demonstrations, especially in the few demonstrations regime. we provide the code for our experiments (https://github.com/caselles/neurips22-demonstrations-pedagogy-pragmatism), as well as an illustrative video explaining our approach (https://youtu.be/v4n16ijknyw).',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-09',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['hugo caselles-dupré', 'olivier sigaud', 'mohamed chetouani'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.04546'},\n",
       " {'title': 'factuality enhanced language models for open-ended text generation',\n",
       "  'id': '2206.04624',\n",
       "  'abstract': \"pretrained language models (lms) are susceptible to generate text with nonfactual information. in this work, we measure and improve the factual accuracy of large-scale lms for open-ended text generation. we design the factualityprompts test set and metrics to measure the factuality of lm generations. based on that, we study the factual accuracy of lms with parameter sizes ranging from 126m to 530b. interestingly, we find that larger lms are more factual than smaller ones, although a previous study suggests that larger lms can be less truthful in terms of misconceptions. in addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ''uniform randomness'' introduced at every sampling step. we propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality. furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., wikipedia). we propose a factuality-enhanced training method that uses topicprefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors.\",\n",
       "  'categories': 'cs.cl cs.ai cs.cy cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-09',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['nayeon lee',\n",
       "   'wei ping',\n",
       "   'peng xu',\n",
       "   'mostofa patwary',\n",
       "   'pascale fung',\n",
       "   'mohammad shoeybi',\n",
       "   'bryan catanzaro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.04624'},\n",
       " {'title': 'joint entropy search for maximally-informed bayesian optimization',\n",
       "  'id': '2206.04771',\n",
       "  'abstract': 'information-theoretic bayesian optimization techniques have become popular for optimizing expensive-to-evaluate black-box functions due to their non-myopic qualities. entropy search and predictive entropy search both consider the entropy over the optimum in the input space, while the recent max-value entropy search considers the entropy over the optimal value in the output space. we propose joint entropy search (jes), a novel information-theoretic acquisition function that considers an entirely new quantity, namely the entropy over the joint optimal probability density over both input and output space. to incorporate this information, we consider the reduction in entropy from conditioning on fantasized optimal input/output pairs. the resulting approach primarily relies on standard gp machinery and removes complex approximations typically associated with information-theoretic methods. with minimal computational overhead, jes shows superior decision-making, and yields state-of-the-art performance for information-theoretic approaches across a wide suite of tasks. as a light-weight approach with superior results, jes provides a new go-to acquisition function for bayesian optimization.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-09',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['carl hvarfner', 'frank hutter', 'luigi nardi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.04771'},\n",
       " {'title': 'trimmed maximum likelihood estimation for robust learning in generalized   linear models',\n",
       "  'id': '2206.04777',\n",
       "  'abstract': 'we study the problem of learning generalized linear models under adversarial corruptions. we analyze a classical heuristic called the iterative trimmed maximum likelihood estimator which is known to be effective against label corruptions in practice. under label corruptions, we prove that this simple estimator achieves minimax near-optimal risk on a wide range of generalized linear models, including gaussian regression, poisson regression and binomial regression. finally, we extend the estimator to the more challenging setting of label and covariate corruptions and demonstrate its robustness and optimality in that setting as well.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-09',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['pranjal awasthi', 'abhimanyu das', 'weihao kong', 'rajat sen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.04777'},\n",
       " {'title': 'superiority of gnn over nn in generalizing bandlimited functions',\n",
       "  'id': '2206.05904',\n",
       "  'abstract': 'we constructively show, via rigorous mathematical arguments, that gnn architectures outperform those of nn in approximating bandlimited functions on compact $d$-dimensional euclidean grids. we show that the former only needs $\\\\mathcal{m}$ sampled functional values to achieve a uniform approximation error of $o_{d}(\\\\exp(-c\\\\mathcal{m}^{1/d}))$ and that this error rate is optimal, in the sense that, nns might achieve worse. on the theoretical side, our work demonstrates that ideas from sampling theory can be effectively used in analyzing the expressive capability of neural networks.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-13',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['a. martina neuman', 'rongrong wang', 'yuying xie'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.05904'},\n",
       " {'title': 'unknown-aware domain adversarial learning for open-set domain adaptation',\n",
       "  'id': '2206.07551',\n",
       "  'abstract': 'open-set domain adaptation (osda) assumes that a target domain contains unknown classes, which are not discovered in a source domain. existing domain adversarial learning methods are not suitable for osda because distribution matching with $\\\\textit{unknown}$ classes leads to negative transfer. previous osda methods have focused on matching the source and the target distribution by only utilizing $\\\\textit{known}$ classes. however, this $\\\\textit{known}$-only matching may fail to learn the target-$\\\\textit{unknown}$ feature space. therefore, we propose unknown-aware domain adversarial learning (uadal), which $\\\\textit{aligns}$ the source and the target-$\\\\textit{known}$ distribution while simultaneously $\\\\textit{segregating}$ the target-$\\\\textit{unknown}$ distribution in the feature alignment procedure. we provide theoretical analyses on the optimized state of the proposed $\\\\textit{unknown-aware}$ feature alignment, so we can guarantee both $\\\\textit{alignment}$ and $\\\\textit{segregation}$ theoretically. empirically, we evaluate uadal on the benchmark datasets, which shows that uadal outperforms other methods with better feature alignments by reporting state-of-the-art performances.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-15',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['joonho jang',\n",
       "   'byeonghu na',\n",
       "   'donghyeok shin',\n",
       "   'mingi ji',\n",
       "   'kyungwoo song',\n",
       "   'il-chul moon'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.07551'},\n",
       " {'title': 'maximum class separation as inductive bias in one matrix',\n",
       "  'id': '2206.08704',\n",
       "  'abstract': 'maximizing the separation between classes constitutes a well-known inductive bias in machine learning and a pillar of many traditional algorithms. by default, deep networks are not equipped with this inductive bias and therefore many alternative solutions have been proposed through differential optimization. current approaches tend to optimize classification and separation jointly: aligning inputs with class vectors and separating class vectors angularly. this paper proposes a simple alternative: encoding maximum separation as an inductive bias in the network by adding one fixed matrix multiplication before computing the softmax activations. the main observation behind our approach is that separation does not require optimization but can be solved in closed-form prior to training and plugged into a network. we outline a recursive approach to obtain the matrix consisting of maximally separable vectors for any number of classes, which can be added with negligible engineering effort and computational overhead. despite its simple nature, this one matrix multiplication provides real impact. we show that our proposal directly boosts classification, long-tailed recognition, out-of-distribution detection, and open-set recognition, from cifar to imagenet. we find empirically that maximum separation works best as a fixed bias; making the matrix learnable adds nothing to the performance. the closed-form implementation and code to reproduce the experiments are available on github.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-17',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['tejaswi kasarla',\n",
       "   'gertjan j. burghouts',\n",
       "   'max van spengler',\n",
       "   'elise van der pol',\n",
       "   'rita cucchiara',\n",
       "   'pascal mettes'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.08704'},\n",
       " {'title': 'a theory of learning with constrained weight-distribution',\n",
       "  'id': '2206.08933',\n",
       "  'abstract': 'a central question in computational neuroscience is how structure determines function in neural networks. the emerging high-quality large-scale connectomic datasets raise the question of what general functional principles can be gleaned from structural information such as the distribution of excitatory/inhibitory synapse types and the distribution of synaptic weights. motivated by this question, we developed a statistical mechanical theory of learning in neural networks that incorporates structural information as constraints. we derived an analytical solution for the memory capacity of the perceptron, a basic feedforward model of supervised learning, with constraint on the distribution of its weights. our theory predicts that the reduction in capacity due to the constrained weight-distribution is related to the wasserstein distance between the imposed distribution and that of the standard normal distribution. to test the theoretical predictions, we use optimal transport theory and information geometry to develop an sgd-based algorithm to find weights that simultaneously learn the input-output task and satisfy the distribution constraint. we show that training in our algorithm can be interpreted as geodesic flows in the wasserstein space of probability distributions. we further developed a statistical mechanical theory for teacher-student perceptron rule learning and ask for the best way for the student to incorporate prior knowledge of the rule. our theory shows that it is beneficial for the learner to adopt different prior weight distributions during learning, and shows that distribution-constrained learning outperforms unconstrained and sign-constrained learning. our theory and algorithm provide novel strategies for incorporating prior knowledge about weights into learning, and reveal a powerful connection between structure and function in neural networks.',\n",
       "  'categories': 'q-bio.nc cond-mat.dis-nn cond-mat.stat-mech cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-13',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['weishun zhong',\n",
       "   'ben sorscher',\n",
       "   'daniel d lee',\n",
       "   'haim sompolinsky'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.08933'},\n",
       " {'title': 'on the statistical efficiency of reward-free exploration in non-linear   rl',\n",
       "  'id': '2206.10770',\n",
       "  'abstract': 'we study reward-free reinforcement learning (rl) under general non-linear function approximation, and establish sample efficiency and hardness results under various standard structural assumptions. on the positive side, we propose the rfolive (reward-free olive) algorithm for sample-efficient reward-free exploration under minimal structural assumptions, which covers the previously studied settings of linear mdps (jin et al., 2020b), linear completeness (zanette et al., 2020b) and low-rank mdps with unknown representation (modi et al., 2021). our analyses indicate that the explorability or reachability assumptions, previously made for the latter two settings, are not necessary statistically for reward-free exploration. on the negative side, we provide a statistical hardness result for both reward-free and reward-aware exploration under linear completeness assumptions when the underlying features are unknown, showing an exponential separation between low-rank and linear completeness settings.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-21',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['jinglin chen',\n",
       "   'aditya modi',\n",
       "   'akshay krishnamurthy',\n",
       "   'nan jiang',\n",
       "   'alekh agarwal'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.10770'},\n",
       " {'title': 'descent steps of a relation-aware energy produce heterogeneous graph   neural networks',\n",
       "  'id': '2206.11081',\n",
       "  'abstract': 'heterogeneous graph neural networks (gnns) achieve strong performance on node classification tasks in a semi-supervised learning setting. however, as in the simpler homogeneous gnn case, message-passing-based heterogeneous gnns may struggle to balance between resisting the oversmoothing that may occur in deep models, and capturing long-range dependencies of graph structured data. moreover, the complexity of this trade-off is compounded in the heterogeneous graph case due to the disparate heterophily relationships between nodes of different types. to address these issues, we propose a novel heterogeneous gnn architecture in which layers are derived from optimization steps that descend a novel relation-aware energy function. the corresponding minimizer is fully differentiable with respect to the energy function parameters, such that bilevel optimization can be applied to effectively learn a functional form whose minimum provides optimal node representations for subsequent classification tasks. in particular, this methodology allows us to model diverse heterophily relationships between different node types while avoiding oversmoothing effects. experimental results on 8 heterogeneous graph benchmarks demonstrates that our proposed method can achieve competitive node classification accuracy',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-22',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['hongjoon ahn',\n",
       "   'yongyi yang',\n",
       "   'quan gan',\n",
       "   'taesup moon',\n",
       "   'david wipf'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.11081'},\n",
       " {'title': 'on a class of geodesically convex optimization problems solved via   euclidean mm methods',\n",
       "  'id': '2206.11426',\n",
       "  'abstract': 'we study geodesically convex (g-convex) problems that can be written as a difference of euclidean convex functions. this structure arises in several optimization problems in statistics and machine learning, e.g., for matrix scaling, m-estimators for covariances, and brascamp-lieb inequalities. our work offers efficient algorithms that on the one hand exploit g-convexity to ensure global optimality along with guarantees on iteration complexity. on the other hand, the split structure permits us to develop euclidean majorization-minorization algorithms that help us bypass the need to compute expensive riemannian operations such as exponential maps and parallel transport. we illustrate our results by specializing them to a few concrete optimization problems that have been previously studied in the machine learning literature. ultimately, we hope our work helps motivate the broader search for mixed euclidean-riemannian optimization algorithms',\n",
       "  'categories': 'math.oc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-22',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['melanie weber', 'suvrit sra'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.11426'},\n",
       " {'title': 'single-phase deep learning in cortico-cortical networks',\n",
       "  'id': '2206.11769',\n",
       "  'abstract': 'the error-backpropagation (backprop) algorithm remains the most common solution to the credit assignment problem in artificial neural networks. in neuroscience, it is unclear whether the brain could adopt a similar strategy to correctly modify its synapses. recent models have attempted to bridge this gap while being consistent with a range of experimental observations. however, these models are either unable to effectively backpropagate error signals across multiple layers or require a multi-phase learning process, neither of which are reminiscent of learning in the brain. here, we introduce a new model, bursting cortico-cortical networks (burstccn), which solves these issues by integrating known properties of cortical networks namely bursting activity, short-term plasticity (stp) and dendrite-targeting interneurons. burstccn relies on burst multiplexing via connection-type-specific stp to propagate backprop-like error signals within deep cortical networks. these error signals are encoded at distal dendrites and induce burst-dependent plasticity as a result of excitatory-inhibitory top-down inputs. first, we demonstrate that our model can effectively backpropagate errors through multiple layers using a single-phase learning process. next, we show both empirically and analytically that learning in our model approximates backprop-derived gradients. finally, we demonstrate that our model is capable of learning complex image classification tasks (mnist and cifar-10). overall, our results suggest that cortical features across sub-cellular, cellular, microcircuit and systems levels jointly underlie single-phase efficient deep learning in the brain.',\n",
       "  'categories': 'q-bio.nc cs.lg cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-23',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['will greedy',\n",
       "   'heng wei zhu',\n",
       "   'joseph pemberton',\n",
       "   'jack mellor',\n",
       "   'rui ponte costa'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.11769'},\n",
       " {'title': 'value function decomposition for iterative design of reinforcement   learning agents',\n",
       "  'id': '2206.13901',\n",
       "  'abstract': \"designing reinforcement learning (rl) agents is typically a difficult process that requires numerous design iterations. learning can fail for a multitude of reasons, and standard rl methods provide too few tools to provide insight into the exact cause. in this paper, we show how to integrate value decomposition into a broad class of actor-critic algorithms and use it to assist in the iterative agent-design process. value decomposition separates a reward function into distinct components and learns value estimates for each. these value estimates provide insight into an agent's learning and decision-making process and enable new training methods to mitigate common problems. as a demonstration, we introduce sac-d, a variant of soft actor-critic (sac) adapted for value decomposition. sac-d maintains similar performance to sac, while learning a larger set of value predictions. we also introduce decomposition-based tools that exploit this information, including a new reward influence metric, which measures each reward component's effect on agent decision-making. using these tools, we provide several demonstrations of decomposition's use in identifying and addressing problems in the design of both environments and agents. value decomposition is broadly applicable and easy to incorporate into existing algorithms and workflows, making it a powerful tool in an rl practitioner's toolbox.\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-06-24',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['james macglashan',\n",
       "   'evan archer',\n",
       "   'alisa devlic',\n",
       "   'takuma seno',\n",
       "   'craig sherstan',\n",
       "   'peter r. wurman',\n",
       "   'peter stone'],\n",
       "  'affiliation': ['sony ai',\n",
       "   'sony ai',\n",
       "   'sony ai',\n",
       "   'sony ai',\n",
       "   'sony ai',\n",
       "   'sony ai',\n",
       "   'sony ai'],\n",
       "  'url': 'https://arxiv.org/abs/2206.13901'},\n",
       " {'title': 'classical and learned mr to pseudo-ct mappings for accurate transcranial   ultrasound simulation',\n",
       "  'id': '2206.15441',\n",
       "  'abstract': 'model-based treatment planning for transcranial ultrasound therapy typically involves mapping the acoustic properties of the skull from an x-ray computed tomography (ct) image of the head. here, three methods for generating pseudo-ct images from magnetic resonance (mr) images were compared as an alternative to ct. a convolutional neural network (u-net) was trained on paired mr-ct images to generate pseudo-ct images from either t1-weighted or zero-echo time (zte) mr images (denoted tct and zct, respectively). a direct mapping from zte to pseudo-ct was also implemented (denoted cct). when comparing the pseudo-ct and ground truth ct images for the test set, the mean absolute error was 133, 83, and 145 hounsfield units (hu) across the whole head, and 398, 222, and 336 hu within the skull for the tct, zct, and cct images, respectively. ultrasound simulations were also performed using the generated pseudo-ct images and compared to simulations based on ct. an annular array transducer was used targeting the visual or motor cortex. the mean differences in the simulated focal pressure, focal position, and focal volume were 9.9%, 1.5 mm, and 15.1% for simulations based on the tct images, 5.7%, 0.6 mm, and 5.7% for the zct, and 6.7%, 0.9 mm, and 12.1% for the cct. the improved results for images mapped from zte highlight the advantage of using imaging sequences which improve contrast of the skull bone. overall, these results demonstrate that acoustic simulations based on mr images can give comparable accuracy to those based on ct.',\n",
       "  'categories': 'physics.med-ph cs.lg',\n",
       "  'doi': '10.1109/tuffc.2022.3198522',\n",
       "  'created': '2022-06-30',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['maria miscouridou',\n",
       "   'josé a. pineda-pardo',\n",
       "   'charlotte j. stagg',\n",
       "   'bradley e. treeby',\n",
       "   'antonio stanziola'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2206.15441'},\n",
       " {'title': 'masked autoencoder for self-supervised pre-training on lidar point   clouds',\n",
       "  'id': '2207.00531',\n",
       "  'abstract': 'masked autoencoding has become a successful pretraining paradigm for transformer models for text, images, and, recently, point clouds. raw automotive datasets are suitable candidates for self-supervised pre-training as they generally are cheap to collect compared to annotations for tasks like 3d object detection (od). however, the development of masked autoencoders for point clouds has focused solely on synthetic and indoor data. consequently, existing methods have tailored their representations and models toward small and dense point clouds with homogeneous point densities.in this work, we study masked autoencoding for point clouds in an automotive setting, which are sparse and for which the point density can vary drastically among objects in the same scene. to this end, we propose voxel-mae, a simple masked autoencoding pre-training scheme designed for voxel representations. we pre-train the backbone of a transformer-based 3d object detector to reconstruct masked voxels and to distinguish between empty and non-empty voxels. our method improves the 3d od performance by 1.75 map points and 1.05 nds on the challenging nuscenes dataset. further, we show that by pre-training with voxel-mae, we require only 40% of the annotated data to outperform a randomly initialized equivalent. code available at https://github.com/georghess/voxel-mae',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-01',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['georg hess',\n",
       "   'johan jaxing',\n",
       "   'elias svensson',\n",
       "   'david hagerman',\n",
       "   'christoffer petersson',\n",
       "   'lennart svensson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.00531'},\n",
       " {'title': 'integral probability metrics pac-bayes bounds',\n",
       "  'id': '2207.00614',\n",
       "  'abstract': 'we present a pac-bayes-style generalization bound which enables the replacement of the kl-divergence with a variety of integral probability metrics (ipm). we provide instances of this bound with the ipm being the total variation metric and the wasserstein distance. a notable feature of the obtained bounds is that they naturally interpolate between classical uniform convergence bounds in the worst case (when the prior and posterior are far away from each other), and improved bounds in favorable cases (when the posterior and prior are close). this illustrates the possibility of reinforcing classical generalization bounds with algorithm- and data-dependent components, thus making them more suitable to analyze algorithms that use a large hypothesis space.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-01',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['ron amit', 'baruch epstein', 'shay moran', 'ron meir'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.00614'},\n",
       " {'title': 'infinite-fidelity coregionalization for physical simulation',\n",
       "  'id': '2207.00678',\n",
       "  'abstract': 'multi-fidelity modeling and learning are important in physical simulation-related applications. it can leverage both low-fidelity and high-fidelity examples for training so as to reduce the cost of data generation while still achieving good performance. while existing approaches only model finite, discrete fidelities, in practice, the fidelity choice is often continuous and infinite, which can correspond to a continuous mesh spacing or finite element length. in this paper, we propose infinite fidelity coregionalization (ifc). given the data, our method can extract and exploit rich information within continuous, infinite fidelities to bolster the prediction accuracy. our model can interpolate and/or extrapolate the predictions to novel fidelities, which can be even higher than the fidelities of training data. specifically, we introduce a low-dimensional latent output as a continuous function of the fidelity and input, and multiple it with a basis matrix to predict high-dimensional solution outputs. we model the latent output as a neural ordinary differential equation (ode) to capture the complex relationships within and integrate information throughout the continuous fidelities. we then use gaussian processes or another ode to estimate the fidelity-varying bases. for efficient inference, we reorganize the bases as a tensor, and use a tensor-gaussian variational posterior to develop a scalable inference algorithm for massive outputs. we show the advantage of our method in several benchmark tasks in computational physics.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-01',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['shibo li', 'zheng wang', 'robert m. kirby', 'shandian zhe'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.00678'},\n",
       " {'title': 'high-dimensional private empirical risk minimization by greedy   coordinate descent',\n",
       "  'id': '2207.01560',\n",
       "  'abstract': \"in this paper, we study differentially private empirical risk minimization (dp-erm). it has been shown that the worst-case utility of dp-erm reduces polynomially as the dimension increases. this is a major obstacle to privately learning large machine learning models. in high dimension, it is common for some model's parameters to carry more information than others. to exploit this, we propose a differentially private greedy coordinate descent (dp-gcd) algorithm. at each iteration, dp-gcd privately performs a coordinate-wise gradient step along the gradients' (approximately) greatest entry. we show theoretically that dp-gcd can achieve a logarithmic dependence on the dimension for a wide range of problems by naturally exploiting their structural properties (such as quasi-sparse solutions). we illustrate this behavior numerically, both on synthetic and real datasets.\",\n",
       "  'categories': 'cs.lg cs.cr stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-04',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['paul mangold',\n",
       "   'aurélien bellet',\n",
       "   'joseph salmon',\n",
       "   'marc tommasi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.01560'},\n",
       " {'title': 'pure transformers are powerful graph learners',\n",
       "  'id': '2207.02505',\n",
       "  'abstract': 'we show that standard transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a transformer. with an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-ign) composed of equivariant linear layers, which is already more expressive than all message-passing graph neural networks (gnn). when trained on a large-scale graph dataset (pcqm4mv2), our method coined tokenized graph transformer (tokengt) achieves significantly better results compared to gnn baselines and competitive results compared to transformer variants with sophisticated graph-specific inductive bias. our implementation is available at https://github.com/jw9730/tokengt.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-06',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['jinwoo kim',\n",
       "   'tien dat nguyen',\n",
       "   'seonwoo min',\n",
       "   'sungjun cho',\n",
       "   'moontae lee',\n",
       "   'honglak lee',\n",
       "   'seunghoon hong'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.02505'},\n",
       " {'title': 'probing classifiers are unreliable for concept removal and detection',\n",
       "  'id': '2207.04153',\n",
       "  'abstract': \"neural network models trained on text data have been found to encode undesirable linguistic or sensitive concepts in their representation. removing such concepts is non-trivial because of a complex relationship between the concept, text input, and the learnt representation. recent work has proposed post-hoc and adversarial methods to remove such unwanted concepts from a model's representation. through an extensive theoretical and empirical analysis, we show that these methods can be counter-productive: they are unable to remove the concepts entirely, and in the worst case may end up destroying all task-relevant features. the reason is the methods' reliance on a probing classifier as a proxy for the concept. even under the most favorable conditions for learning a probing classifier when a concept's relevant features in representation space alone can provide 100% accuracy, we prove that a probing classifier is likely to use non-concept features and thus post-hoc or adversarial methods will fail to remove the concept correctly. these theoretical implications are confirmed by experiments on models trained on synthetic, multi-nli, and twitter datasets. for sensitive applications of concept removal such as fairness, we recommend caution against using these methods and propose a spuriousness metric to gauge the quality of the final classifier.\",\n",
       "  'categories': 'cs.lg cs.cl',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-08',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['abhinav kumar', 'chenhao tan', 'amit sharma'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.04153'},\n",
       " {'title': 'grounding aleatoric uncertainty for unsupervised environment design',\n",
       "  'id': '2207.05219',\n",
       "  'abstract': 'adaptive curricula in reinforcement learning (rl) have proven effective for producing policies robust to discrepancies between the train and test environment. recently, the unsupervised environment design (ued) framework generalized rl curricula to generating sequences of entire environments, leading to new methods with robust minimax regret properties. problematically, in partially-observable or stochastic settings, optimal policies may depend on the ground-truth distribution over aleatoric parameters of the environment in the intended deployment setting, while curriculum learning necessarily shifts the training distribution. we formalize this phenomenon as curriculum-induced covariate shift (cics), and describe how its occurrence in aleatoric parameters can lead to suboptimal policies. directly sampling these parameters from the ground-truth distribution avoids the issue, but thwarts curriculum learning. we propose samplr, a minimax regret ued method that optimizes the ground-truth utility function, even when the underlying training data is biased due to cics. we prove, and validate on challenging domains, that our approach preserves optimality under the ground-truth distribution, while promoting robustness across the full range of environment settings.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-11',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['minqi jiang',\n",
       "   'michael dennis',\n",
       "   'jack parker-holder',\n",
       "   'andrei lupu',\n",
       "   'heinrich küttler',\n",
       "   'edward grefenstette',\n",
       "   'tim rocktäschel',\n",
       "   'jakob foerster'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.05219'},\n",
       " {'title': 'the muse 2022 multimodal sentiment analysis challenge: humor, emotional   reactions, and stress',\n",
       "  'id': '2207.05691',\n",
       "  'abstract': \"the multimodal sentiment analysis challenge (muse) 2022 is dedicated to multimodal sentiment and emotion recognition. for this year's challenge, we feature three datasets: (i) the passau spontaneous football coach humor (passau-sfch) dataset that contains audio-visual recordings of german football coaches, labelled for the presence of humour; (ii) the hume-reaction dataset in which reactions of individuals to emotional stimuli have been annotated with respect to seven emotional expression intensities, and (iii) the ulm-trier social stress test (ulm-tsst) dataset comprising of audio-visual data labelled with continuous emotion values (arousal and valence) of people in stressful dispositions. using the introduced datasets, muse 2022 2022 addresses three contemporary affective computing problems: in the humor detection sub-challenge (muse-humor), spontaneous humour has to be recognised; in the emotional reactions sub-challenge (muse-reaction), seven fine-grained `in-the-wild' emotions have to be predicted; and in the emotional stress sub-challenge (muse-stress), a continuous prediction of stressed emotion values is featured. the challenge is designed to attract different research communities, encouraging a fusion of their disciplines. mainly, muse 2022 targets the communities of audio-visual emotion recognition, health informatics, and symbolic sentiment analysis. this baseline paper describes the datasets as well as the feature sets extracted from them. a recurrent neural network with lstm cells is used to set competitive baseline results on the test partitions for each sub-challenge. we report an area under the curve (auc) of .8480 for muse-humor; .2801 mean (from 7-classes) pearson's correlations coefficient for muse-reaction, as well as .4931 concordance correlation coefficient (ccc) and .4761 for valence and arousal in muse-stress, respectively.\",\n",
       "  'categories': 'cs.lg cs.ai cs.cl cs.mm eess.as',\n",
       "  'doi': '10.1145/3551876.3554817',\n",
       "  'created': '2022-06-23',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['lukas christ',\n",
       "   'shahin amiriparian',\n",
       "   'alice baird',\n",
       "   'panagiotis tzirakis',\n",
       "   'alexander kathan',\n",
       "   'niklas müller',\n",
       "   'lukas stappen',\n",
       "   'eva-maria meßner',\n",
       "   'andreas könig',\n",
       "   'alan cowen',\n",
       "   'erik cambria',\n",
       "   'björn w. schuller'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.05691'},\n",
       " {'title': 'a new hope for network model generalization',\n",
       "  'id': '2207.05843',\n",
       "  'abstract': 'generalizing machine learning (ml) models for network traffic dynamics tends to be considered a lost cause. hence for every new task, we design new models and train them on model-specific datasets closely mimicking the deployment environments. yet, an ml architecture called_transformer_ has enabled previously unimaginable generalization in other domains. nowadays, one can download a model pre-trained on massive datasets and only fine-tune it for a specific task and context with comparatively little time and data. these fine-tuned models are now state-of-the-art for many benchmarks.   we believe this progress could translate to networking and propose a network traffic transformer (ntt), a transformer adapted to learn network dynamics from packet traces. our initial results are promising: ntt seems able to generalize to new prediction tasks and environments. this study suggests there is still hope for generalization, though it calls for a lot of future research.',\n",
       "  'categories': 'cs.ni cs.lg',\n",
       "  'doi': '10.1145/3563766.3564104',\n",
       "  'created': '2022-07-12',\n",
       "  'updated': '2022-10-18',\n",
       "  'authors': ['alexander dietmüller',\n",
       "   'siddhant ray',\n",
       "   'romain jacob',\n",
       "   'laurent vanbever'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.05843'},\n",
       " {'title': 'unsupervised learning for combinatorial optimization with principled   objective relaxation',\n",
       "  'id': '2207.05984',\n",
       "  'abstract': 'using machine learning to solve combinatorial optimization (co) problems is challenging, especially when the data is unlabeled. this work proposes an unsupervised learning framework for co problems. our framework follows a standard relaxation-plus-rounding approach and adopts neural networks to parameterize the relaxed solutions so that simple back-propagation can train the model end-to-end. our key contribution is the observation that if the relaxed objective satisfies entry-wise concavity, a low optimization loss guarantees the quality of the final integral solutions. this observation significantly broadens the applicability of the previous framework inspired by erdos\\' probabilistic method. in particular, this observation can guide the design of objective models in applications where the objectives are not given explicitly while requiring being modeled in prior. we evaluate our framework by solving a synthetic graph optimization problem, and two real-world applications including resource allocation in circuit design and approximate computing. our framework largely outperforms the baselines based on na\\\\\"{i}ve relaxation, reinforcement learning, and gumbel-softmax tricks.',\n",
       "  'categories': 'cs.lg cs.ar math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-13',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['haoyu wang', 'nan wu', 'hang yang', 'cong hao', 'pan li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.05984'},\n",
       " {'title': 'a data-efficient deep learning framework for segmentation and   classification of histopathology images',\n",
       "  'id': '2207.06489',\n",
       "  'abstract': 'the current study of cell architecture of inflammation in histopathology images commonly performed for diagnosis and research purposes excludes a lot of information available on the biopsy slide. in autoimmune diseases, major outstanding research questions remain regarding which cell types participate in inflammation at the tissue level, and how they interact with each other. while these questions can be partially answered using traditional methods, artificial intelligence approaches for segmentation and classification provide a much more efficient method to understand the architecture of inflammation in autoimmune disease, holding great promise for novel insights. in this paper, we empirically develop deep learning approaches that use dermatomyositis biopsies of human tissue to detect and identify inflammatory cells. our approach improves classification performance by 26% and segmentation performance by 5%. we also propose a novel post-processing autoencoder architecture that improves segmentation performance by an additional 3%.',\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-13',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['pranav singh', 'jacopo cirrone'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.06489'},\n",
       " {'title': 'benign, tempered, or catastrophic: a taxonomy of overfitting',\n",
       "  'id': '2207.06569',\n",
       "  'abstract': 'the practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods, which perfectly fit their training data. certain interpolating methods, including neural networks, can fit noisy training data without catastrophically bad test performance, in defiance of standard intuitions from statistical learning theory. aiming to explain this, a body of recent work has studied benign overfitting, a phenomenon where some interpolating methods approach bayes optimality, even in the presence of noise. in this work we argue that while benign overfitting has been instructive and fruitful to study, many real interpolating methods like neural networks do not fit benignly: modest noise in the training set causes nonzero (but non-infinite) excess risk at test time, implying these models are neither benign nor catastrophic but rather fall in an intermediate regime. we call this intermediate regime tempered overfitting, and we initiate its systematic study. we first explore this phenomenon in the context of kernel (ridge) regression (kr) by obtaining conditions on the ridge parameter and kernel eigenspectrum under which kr exhibits each of the three behaviors. we find that kernels with powerlaw spectra, including laplace kernels and relu neural tangent kernels, exhibit tempered overfitting. we then empirically study deep neural networks through the lens of our taxonomy, and find that those trained to interpolation are tempered, while those stopped early are benign. we hope our work leads to a more refined understanding of overfitting in modern learning.',\n",
       "  'categories': 'cs.lg cs.ai cs.cv stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-13',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['neil mallinar',\n",
       "   'james b. simon',\n",
       "   'amirhesam abedsoltan',\n",
       "   'parthe pandit',\n",
       "   'mikhail belkin',\n",
       "   'preetum nakkiran'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.06569'},\n",
       " {'title': 'accelerated probabilistic marching cubes by deep learning for   time-varying scalar ensembles',\n",
       "  'id': '2207.07260',\n",
       "  'abstract': 'visualizing the uncertainty of ensemble simulations is challenging due to the large size and multivariate and temporal features of ensemble data sets. one popular approach to studying the uncertainty of ensembles is analyzing the positional uncertainty of the level sets. probabilistic marching cubes is a technique that performs monte carlo sampling of multivariate gaussian noise distributions for positional uncertainty visualization of level sets. however, the technique suffers from high computational time, making interactive visualization and analysis impossible to achieve. this paper introduces a deep-learning-based approach to learning the level-set uncertainty for two-dimensional ensemble data with a multivariate gaussian noise assumption. we train the model using the first few time steps from time-varying ensemble data in our workflow. we demonstrate that our trained model accurately infers uncertainty in level sets for new time steps and is up to 170x faster than that of the original probabilistic model with serial computation and 10x faster than that of the original parallel computation.',\n",
       "  'categories': 'cs.lg cs.hc',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-14',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['mengjiao han',\n",
       "   'tushar m. athawale',\n",
       "   'david pugmire',\n",
       "   'chris r. johnson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.07260'},\n",
       " {'title': 'assaying out-of-distribution generalization in transfer learning',\n",
       "  'id': '2207.09239',\n",
       "  'abstract': 'since out-of-distribution generalization is a generally ill-posed problem, various proxy targets (e.g., calibration, adversarial robustness, algorithmic corruptions, invariance across shifts) were studied across different research programs resulting in different recommendations. while sharing the same aspirational goal, these approaches have never been tested under the same experimental conditions on real data. in this paper, we take a unified view of previous work, highlighting message discrepancies that we address empirically, and providing recommendations on how to measure the robustness of a model and how to improve it. to this end, we collect 172 publicly available dataset pairs for training and out-of-distribution evaluation of accuracy, calibration error, adversarial attacks, environment invariance, and synthetic corruptions. we fine-tune over 31k networks, from nine different architectures in the many- and few-shot setting. our findings confirm that in- and out-of-distribution accuracies tend to increase jointly, but show that their relation is largely dataset-dependent, and in general more nuanced and more complex than posited by previous, smaller scale studies.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-19',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['florian wenzel',\n",
       "   'andrea dittadi',\n",
       "   'peter vincent gehler',\n",
       "   'carl-johann simon-gabriel',\n",
       "   'max horn',\n",
       "   'dominik zietlow',\n",
       "   'david kernert',\n",
       "   'chris russell',\n",
       "   'thomas brox',\n",
       "   'bernt schiele',\n",
       "   'bernhard schölkopf',\n",
       "   'francesco locatello'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.09239'},\n",
       " {'title': 'a sharp uniform-in-time error estimate for stochastic gradient langevin   dynamics',\n",
       "  'id': '2207.09304',\n",
       "  'abstract': 'we establish a sharp uniform-in-time error estimate for the stochastic gradient langevin dynamics (sgld), which is a popular sampling algorithm. under mild assumptions, we obtain a uniform-in-time $o(\\\\eta^2)$ bound for the kl-divergence between the sgld iteration and the langevin diffusion, where $\\\\eta$ is the step size (or learning rate). our analysis is also valid for varying step sizes. based on this, we are able to obtain an $o(\\\\eta)$ bound for the distance between the sgld iteration and the invariant distribution of the langevin diffusion, in terms of wasserstein or total variation distances.',\n",
       "  'categories': 'math.pr cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-19',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['lei li', 'yuliang wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.09304'},\n",
       " {'title': 'unifed: a benchmark for federated learning frameworks',\n",
       "  'id': '2207.10308',\n",
       "  'abstract': 'federated learning (fl) has become a practical and popular paradigm in machine learning. however, currently, there is no systematic solution that covers diverse use cases. practitioners often face the challenge of how to select a matching fl framework for their use case. in this work, we present unifed, the first unified benchmark for standardized evaluation of the existing open-source fl frameworks. with 15 evaluation scenarios, we present both qualitative and quantitative evaluation results of nine existing popular open-sourced fl frameworks, from the perspectives of functionality, usability, and system performance. we also provide suggestions on framework selection based on the benchmark conclusions and point out future improvement directions.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-21',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['xiaoyuan liu',\n",
       "   'tianneng shi',\n",
       "   'chulin xie',\n",
       "   'qinbin li',\n",
       "   'kangping hu',\n",
       "   'haoyu kim',\n",
       "   'xiaojun xu',\n",
       "   'bo li',\n",
       "   'dawn song'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.10308'},\n",
       " {'title': 'instant neural representation for interactive volume rendering',\n",
       "  'id': '2207.11620',\n",
       "  'abstract': 'neural networks have shown great potential in compressing volume data for visualization. however, due to the high cost of training and inference, such volumetric neural representations have thus far only been applied to offline data processing and non-interactive rendering. in this paper, we demonstrate that by simultaneously leveraging modern gpu tensor cores, a native cuda neural network framework, and a well-designed rendering algorithm with macro-cell acceleration, we can interactively ray trace volumetric neural representations (10-60fps). our neural representations are also high-fidelity (psnr > 30db) and compact (10-1000x smaller). additionally, we show that it is possible to fit the entire training step inside a rendering loop and skip the pre-training process completely. to support extreme-scale volume data, we also develop an efficient out-of-core training strategy, which allows our volumetric neural representation training to potentially scale up to terascale using only an nvidia rtx 3090 workstation.',\n",
       "  'categories': 'cs.gr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-23',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['qi wu', 'david bauer', 'michael j. doyle', 'kwan-liu ma'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.11620'},\n",
       " {'title': 'a priority map for vision-and-language navigation with trajectory plans   and feature-location cues',\n",
       "  'id': '2207.11717',\n",
       "  'abstract': 'in a busy city street, a pedestrian surrounded by distractions can pick out a single sign if it is relevant to their route. artificial agents in outdoor vision-and-language navigation (vln) are also confronted with detecting supervisory signal on environment features and location in inputs. to boost the prominence of relevant features in transformer-based architectures without costly preprocessing and pretraining, we take inspiration from priority maps - a mechanism described in neuropsychological studies. we implement a novel priority map module and pretrain on auxiliary tasks using low-sample datasets with high-level representations of routes and environment-related references to urban features. a hierarchical process of trajectory planning - with subsequent parameterised visual boost filtering on visual inputs and prediction of corresponding textual spans - addresses the core challenges of cross-modal alignment and feature-level localisation. the priority map module is integrated into a feature-location framework that doubles the task completion rates of standalone transformers and attains state-of-the-art performance on the touchdown benchmark for vln. code and data are referenced in appendix c.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-24',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['jason armitage', 'leonardo impett', 'rico sennrich'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.11717'},\n",
       " {'title': 'laplacian-based cluster-contractive t-sne for high dimensional data   visualization',\n",
       "  'id': '2207.12214',\n",
       "  'abstract': 'dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces to extract hidden and useful information or facilitate visual understanding and interpretation of the data. however, few of them take into consideration the potential cluster information contained implicitly in the high-dimensional data. in this paper, we propose laptsne, a new graph-layout nonlinear dimensionality reduction method based on t-sne, one of the best techniques for visualizing high-dimensional data as 2d scatter plots. specifically, laptsne leverages the eigenvalue information of the graph laplacian to shrink the potential clusters in the low-dimensional embedding when learning to preserve the local and global structure from high-dimensional space to low-dimensional space. it is nontrivial to solve the proposed model because the eigenvalues of normalized symmetric laplacian are functions of the decision variable. we provide a majorization-minimization algorithm with convergence guarantee to solve the optimization problem of laptsne and show how to calculate the gradient analytically, which may be of broad interest when considering optimization with laplacian-composited objective. we evaluate our method by a formal comparison with state-of-the-art methods on seven benchmark datasets, both visually and via established quantitative measurements. the results demonstrate the superiority of our method over baselines such as t-sne and umap. we also provide out-of-sample extension, large-scale extension and mini-batch extension for our laptsne to facilitate dimensionality reduction in various scenarios.',\n",
       "  'categories': 'cs.lg cs.hc',\n",
       "  'doi': '',\n",
       "  'created': '2022-07-25',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['yan sun', 'yi han', 'jicong fan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2207.12214'},\n",
       " {'title': 'compound density networks for risk prediction using electronic health   records',\n",
       "  'id': '2208.01320',\n",
       "  'abstract': 'electronic health records (ehrs) exhibit a high amount of missing data due to variations of patient conditions and treatment needs. imputation of missing values has been considered an effective approach to deal with this challenge. existing work separates imputation method and prediction model as two independent parts of an ehr-based machine learning system. we propose an integrated end-to-end approach by utilizing a compound density network (cdnet) that allows the imputation method and prediction model to be tuned together within a single framework. cdnet consists of a gated recurrent unit (gru), a mixture density network (mdn), and a regularized attention network (ran). the gru is used as a latent variable model to model ehr data. the mdn is designed to sample latent variables generated by gru. the ran serves as a regularizer for less reliable imputed values. the architecture of cdnet enables gru and mdn to iteratively leverage the output of each other to impute missing values, leading to a more accurate and robust prediction. we validate cdnet on the mortality prediction task on the mimic-iii dataset. our model outperforms state-of-the-art models by significant margins. we also empirically show that regularizing imputed values is a key factor for superior prediction performance. analysis of prediction uncertainty shows that our model can capture both aleatoric and epistemic uncertainties, which offers model users a better understanding of the model results.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-02',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['yuxi liu', 'shaowen qin', 'zhenhao zhang', 'wei shao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.01320'},\n",
       " {'title': 'sequence model imitation learning with unobserved contexts',\n",
       "  'id': '2208.02225',\n",
       "  'abstract': \"we consider imitation learning problems where the learner's ability to mimic the expert increases throughout the course of an episode as more information is revealed. one example of this is when the expert has access to privileged information: while the learner might not be able to accurately reproduce expert behavior early on in an episode, by considering the entire history of states and actions, they might be able to eventually identify the hidden context and act as the expert would. we prove that on-policy imitation learning algorithms (with or without access to a queryable expert) are better equipped to handle these sorts of asymptotically realizable problems than off-policy methods. this is because on-policy algorithms provably learn to recover from their initially suboptimal actions, while off-policy methods treat their suboptimal past actions as though they came from the expert. this often manifests as a latching behavior: a naive repetition of past actions. we conduct experiments in a toy bandit domain that show that there exist sharp phase transitions of whether off-policy approaches are able to match expert performance asymptotically, in contrast to the uniformly good performance of on-policy approaches. we demonstrate that on several continuous control tasks, on-policy approaches are able to use history to identify the context while off-policy approaches actually perform worse when given access to history.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-03',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['gokul swamy',\n",
       "   'sanjiban choudhury',\n",
       "   'j. andrew bagnell',\n",
       "   'zhiwei steven wu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.02225'},\n",
       " {'title': 'bayesian inference with latent hamiltonian neural networks',\n",
       "  'id': '2208.06120',\n",
       "  'abstract': 'when sampling for bayesian inference, one popular approach is to use hamiltonian monte carlo (hmc) and specifically the no-u-turn sampler (nuts) which automatically decides the end time of the hamiltonian trajectory. however, hmc and nuts can require numerous numerical gradients of the target density, and can prove slow in practice. we propose hamiltonian neural networks (hnns) with hmc and nuts for solving bayesian inference problems. once trained, hnns do not require numerical gradients of the target density during sampling. moreover, they satisfy important properties such as perfect time reversibility and hamiltonian conservation, making them well-suited for use within hmc and nuts because stationarity can be shown. we also propose an hnn extension called latent hnns (l-hnns), which are capable of predicting latent variable outputs. compared to hnns, l-hnns offer improved expressivity and reduced integration errors. finally, we employ l-hnns in nuts with an online error monitoring scheme to prevent sample degeneracy in regions of low probability density. we demonstrate l-hnns in nuts with online error monitoring on several examples involving complex, heavy-tailed, and high-local-curvature probability densities. overall, l-hnns in nuts with online error monitoring satisfactorily inferred these probability densities. compared to traditional nuts, l-hnns in nuts with online error monitoring required 1--2 orders of magnitude fewer numerical gradients of the target density and improved the effective sample size (ess) per gradient by an order of magnitude.',\n",
       "  'categories': 'cs.lg stat.co stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-12',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['somayajulu l. n. dhulipala',\n",
       "   'yifeng che',\n",
       "   'michael d. shields'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.06120'},\n",
       " {'title': 'reliable decision from multiple subtasks through threshold optimization:   content moderation in the wild',\n",
       "  'id': '2208.07522',\n",
       "  'abstract': 'social media platforms struggle to protect users from harmful content through content moderation. these platforms have recently leveraged machine learning models to cope with the vast amount of user-generated content daily. since moderation policies vary depending on countries and types of products, it is common to train and deploy the models per policy. however, this approach is highly inefficient, especially when the policies change, requiring dataset re-labeling and model re-training on the shifted data distribution. to alleviate this cost inefficiency, social media platforms often employ third-party content moderation services that provide prediction scores of multiple subtasks, such as predicting the existence of underage personnel, rude gestures, or weapons, instead of directly providing final moderation decisions. however, making a reliable automated moderation decision from the prediction scores of the multiple subtasks for a specific target policy has not been widely explored yet. in this study, we formulate real-world scenarios of content moderation and introduce a simple yet effective threshold optimization method that searches the optimal thresholds of the multiple subtasks to make a reliable moderation decision in a cost-effective way. extensive experiments demonstrate that our approach shows better performance in content moderation compared to existing threshold optimization methods and heuristics.',\n",
       "  'categories': 'cs.lg cs.cy cs.si',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-15',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['donghyun son',\n",
       "   'byounggyu lew',\n",
       "   'kwanghee choi',\n",
       "   'yongsu baek',\n",
       "   'seungwoo choi',\n",
       "   'beomjun shin',\n",
       "   'sungjoo ha',\n",
       "   'buru chang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.07522'},\n",
       " {'title': 'score-based diffusion meets annealed importance sampling',\n",
       "  'id': '2208.07698',\n",
       "  'abstract': 'more than twenty years after its introduction, annealed importance sampling (ais) remains one of the most effective methods for marginal likelihood estimation. it relies on a sequence of distributions interpolating between a tractable initial distribution and the target distribution of interest which we simulate from approximately using a non-homogeneous markov chain. to obtain an importance sampling estimate of the marginal likelihood, ais introduces an extended target distribution to reweight the markov chain proposal. while much effort has been devoted to improving the proposal distribution used by ais, an underappreciated issue is that ais uses a convenient but suboptimal extended target distribution. we here leverage recent progress in score-based generative modeling (sgm) to approximate the optimal extended target distribution minimizing the variance of the marginal likelihood estimate for ais proposals corresponding to the discretization of langevin and hamiltonian dynamics. we demonstrate these novel, differentiable, ais procedures on a number of synthetic benchmark distributions and variational auto-encoders.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-16',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['arnaud doucet',\n",
       "   'will grathwohl',\n",
       "   'alexander g. d. g. matthews',\n",
       "   'heiko strathmann'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.07698'},\n",
       " {'title': 'hyperparameter optimization of generative adversarial network models for   high-energy physics simulations',\n",
       "  'id': '2208.07715',\n",
       "  'abstract': 'the generative adversarial network (gan) is a powerful and flexible tool that can generate high-fidelity synthesized data by learning. it has seen many applications in simulating events in high energy physics (hep), including simulating detector responses and physics events. however, training gans is notoriously hard and optimizing their hyperparameters even more so. it normally requires many trial-and-error training attempts to force a stable training and reach a reasonable fidelity. significant tuning work has to be done to achieve the accuracy required by physics analyses. this work uses the physics-agnostic and high-performance-computer-friendly hyperparameter optimization tool hyppo to optimize and examine the sensitivities of the hyperparameters of a gan for two independent hep datasets. this work provides the first insights into efficiently tuning gans for large hadron collider data. we show that given proper hyperparameter tuning, we can find gans that provide high-quality approximations of the desired quantities. we also provide guidelines for how to go about gan architecture tuning using the analysis tools in hyppo.',\n",
       "  'categories': 'hep-ex cs.ai cs.lg',\n",
       "  'doi': '10.21203/rs.3.rs-2181360/v1',\n",
       "  'created': '2022-08-12',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['vincent dumont', 'xiangyang ju', 'juliane mueller'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.07715'},\n",
       " {'title': 'grato: graph neural network framework tackling over-smoothing with   neural architecture search',\n",
       "  'id': '2208.09027',\n",
       "  'abstract': 'current graph neural networks (gnns) suffer from the over-smoothing problem, which results in indistinguishable node representations and low model performance with more gnn layers. many methods have been put forward to tackle this problem in recent years. however, existing tackling over-smoothing methods emphasize model performance and neglect the over-smoothness of node representations. additional, different approaches are applied one at a time, while there lacks an overall framework to jointly leverage multiple solutions to the over-smoothing challenge. to solve these problems, we propose grato, a framework based on neural architecture search to automatically search for gnns architecture. grato adopts a novel loss function to facilitate striking a balance between model performance and representation smoothness. in addition to existing methods, our search space also includes dropattribute, a novel scheme for alleviating the over-smoothing challenge, to fully leverage diverse solutions. we conduct extensive experiments on six real-world datasets to evaluate grato, which demonstrates that grato outperforms baselines in the over-smoothing metrics and achieves competitive performance in accuracy. grato is especially effective and robust with increasing numbers of gnn layers. further experiments bear out the quality of node representations learned with grato and the effectiveness of model architecture. we make cide of grato available at github (\\\\url{https://github.com/fxsxjtu/grato}).',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '10.1145/3511808.3557337',\n",
       "  'created': '2022-08-18',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['xinshun feng',\n",
       "   'herun wan',\n",
       "   'shangbin feng',\n",
       "   'hongrui wang',\n",
       "   'jun zhou',\n",
       "   'qinghua zheng',\n",
       "   'minnan luo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.09027'},\n",
       " {'title': 'adam can converge without any modification on update rules',\n",
       "  'id': '2208.09632',\n",
       "  'abstract': 'ever since reddi et al. 2018 pointed out the divergence issue of adam, many new variants have been designed to obtain convergence. however, vanilla adam remains exceptionally popular and it works well in practice. why is there a gap between theory and practice? we point out there is a mismatch between the settings of theory and practice: reddi et al. 2018 pick the problem after picking the hyperparameters of adam, i.e., $(\\\\beta_1, \\\\beta_2)$; while practical applications often fix the problem first and then tune $(\\\\beta_1, \\\\beta_2)$. due to this observation, we conjecture that the empirical convergence can be theoretically justified, only if we change the order of picking the problem and hyperparameter. in this work, we confirm this conjecture. we prove that, when $\\\\beta_2$ is large and $\\\\beta_1 < \\\\sqrt{\\\\beta_2}<1$, adam converges to the neighborhood of critical points. the size of the neighborhood is propositional to the variance of stochastic gradients. under an extra condition (strong growth condition), adam converges to critical points. it is worth mentioning that our results cover a wide range of hyperparameters: as $\\\\beta_2$ increases, our convergence result can cover any $\\\\beta_1 \\\\in [0,1)$ including $\\\\beta_1=0.9$, which is the default setting in deep learning libraries. to our knowledge, this is the first result showing that adam can converge without any modification on its update rules. further, our analysis does not require assumptions of bounded gradients or bounded 2nd-order momentum. when $\\\\beta_2$ is small, we further point out a large region of $(\\\\beta_1,\\\\beta_2)$ where adam can diverge to infinity. our divergence result considers the same setting as our convergence result, indicating a phase transition from divergence to convergence when increasing $\\\\beta_2$. these positive and negative results can provide suggestions on how to tune adam hyperparameters.',\n",
       "  'categories': 'cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-20',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['yushun zhang',\n",
       "   'congliang chen',\n",
       "   'naichen shi',\n",
       "   'ruoyu sun',\n",
       "   'zhi-quan luo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.09632'},\n",
       " {'title': 'evaluating and crafting datasets effective for deep learning with data   maps',\n",
       "  'id': '2208.10033',\n",
       "  'abstract': 'rapid development in deep learning model construction has prompted an increased need for appropriate training data. the popularity of large datasets - sometimes known as \"big data\" - has diverted attention from assessing their quality. training on large datasets often requires excessive system resources and an infeasible amount of time. furthermore, the supervised machine learning process has yet to be fully automated: for supervised learning, large datasets require more time for manually labeling samples. we propose a method of curating smaller datasets with comparable out-of-distribution model accuracy after an initial training session using an appropriate distribution of samples classified by how difficult it is for a model to learn from them.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-21',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['jay bishnu', 'andrew gondoputro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.10033'},\n",
       " {'title': 'mix-pooling strategy for attention mechanism',\n",
       "  'id': '2208.10322',\n",
       "  'abstract': 'recently many effective attention modules are proposed to boot the model performance by exploiting the internal information of convolutional neural networks in computer vision. in general, many previous works ignore considering the design of the pooling strategy of the attention mechanism since they adopt the global average pooling for granted, which hinders the further improvement of the performance of the attention mechanism. however, we empirically find and verify a phenomenon that the simple linear combination of global max-pooling and global min-pooling can produce pooling strategies that match or exceed the performance of global average pooling. based on this empirical observation, we propose a simple-yet-effective attention module spem, which adopts a self-adaptive pooling strategy based on global max-pooling and global min-pooling and a lightweight module for producing the attention map. the effectiveness of spem is demonstrated by extensive experiments on widely-used benchmark datasets and popular attention networks.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-22',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['shanshan zhong', 'wushao wen', 'jinghui qin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.10322'},\n",
       " {'title': 'bag of tricks for out-of-distribution generalization',\n",
       "  'id': '2208.10722',\n",
       "  'abstract': 'recently, out-of-distribution (ood) generalization has attracted attention to the robustness and generalization ability of deep learning based models, and accordingly, many strategies have been made to address different aspects related to this issue. however, most existing algorithms for ood generalization are complicated and specifically designed for certain dataset. to alleviate this problem, nicochallenge-2022 provides nico++, a large-scale dataset with diverse context information. in this paper, based on systematic analysis of different schemes on nico++ dataset, we propose a simple but effective learning framework via coupling bag of tricks, including multi-objective framework design, data augmentations, training and inference strategies. our algorithm is memory-efficient and easily-equipped, without complicated modules and does not require for large pre-trained models. it achieves an excellent performance with top-1 accuracy of 88.16% on public test set and 75.65% on private test set, and ranks 1st in domain generalization task of nicochallenge-2022.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-23',\n",
       "  'updated': '',\n",
       "  'authors': ['zining chen',\n",
       "   'weiqiu wang',\n",
       "   'zhicheng zhao',\n",
       "   'aidong men',\n",
       "   'hong chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.10722'},\n",
       " {'title': 'accelerating sgd for highly ill-conditioned huge-scale online matrix   completion',\n",
       "  'id': '2208.11246',\n",
       "  'abstract': 'the matrix completion problem seeks to recover a $d\\\\times d$ ground truth matrix of low rank $r\\\\ll d$ from observations of its individual elements. real-world matrix completion is often a huge-scale optimization problem, with $d$ so large that even the simplest full-dimension vector operations with $o(d)$ time complexity become prohibitively expensive. stochastic gradient descent (sgd) is one of the few algorithms capable of solving matrix completion on a huge scale, and can also naturally handle streaming data over an evolving ground truth. unfortunately, sgd experiences a dramatic slow-down when the underlying ground truth is ill-conditioned; it requires at least $o(\\\\kappa\\\\log(1/\\\\epsilon))$ iterations to get $\\\\epsilon$-close to ground truth matrix with condition number $\\\\kappa$. in this paper, we propose a preconditioned version of sgd that preserves all the favorable practical qualities of sgd for huge-scale online optimization while also making it agnostic to $\\\\kappa$. for a symmetric ground truth and the root mean square error (rmse) loss, we prove that the preconditioned sgd converges to $\\\\epsilon$-accuracy in $o(\\\\log(1/\\\\epsilon))$ iterations, with a rapid linear convergence rate as if the ground truth were perfectly conditioned with $\\\\kappa=1$. in our experiments, we observe a similar acceleration for item-item collaborative filtering on the movielens25m dataset via a pair-wise ranking loss, with 100 million training pairs and 10 million testing pairs. [see supporting code at https://github.com/hong-ming/scaledsgd.]',\n",
       "  'categories': 'cs.lg math.oc stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-23',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['gavin zhang', 'hong-ming chiu', 'richard y. zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.11246'},\n",
       " {'title': 'continuous qa learning with structured prompts',\n",
       "  'id': '2208.14602',\n",
       "  'abstract': \"qa models with lifelong learning (ll) abilities are important for practical qa applications, and architecture-based ll methods are reported to be an effective implementation for these models. however, it is non-trivial to extend previous approaches to qa tasks since they either require access to task identities in the testing phase or do not explicitly model samples from unseen tasks. in this paper, we propose diana: a dynamic architecture-based lifelong qa model that tries to learn a sequence of qa tasks with a prompt enhanced language model. four types of hierarchically organized prompts are used in diana to capture qa knowledge from different granularities. specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high ll performances and maintain instance-level prompts to learn knowledge shared across different input samples to improve the model's generalization performance. moreover, we dedicate separate prompts to explicitly model unseen tasks and introduce a set of prompt key vectors to facilitate knowledge sharing between tasks. extensive experiments demonstrate that diana outperforms state-of-the-art lifelong qa models, especially in handling unseen tasks.\",\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-30',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['yinhe zheng'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2208.14602'},\n",
       " {'title': 'optimizing the performative risk under weak convexity assumptions',\n",
       "  'id': '2209.00771',\n",
       "  'abstract': 'in performative prediction, a predictive model impacts the distribution that generates future data, a phenomenon that is being ignored in classical supervised learning. in this closed-loop setting, the natural measure of performance named performative risk ($\\\\mathrm{pr}$), captures the expected loss incurred by a predictive model \\\\emph{after} deployment. the core difficulty of using the performative risk as an optimization objective is that the data distribution itself depends on the model parameters. this dependence is governed by the environment and not under the control of the learner. as a consequence, even the choice of a convex loss function can result in a highly non-convex $\\\\mathrm{pr}$ minimization problem. prior work has identified a pair of general conditions on the loss and the mapping from model parameters to distributions that implies the convexity of the performative risk. in this paper, we relax these assumptions and focus on obtaining weaker notions of convexity, without sacrificing the amenability of the $\\\\mathrm{pr}$ minimization problem for iterative optimization methods.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-01',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['yulai zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.00771'},\n",
       " {'title': 'index tracking via learning to predict market sensitivities',\n",
       "  'id': '2209.00780',\n",
       "  'abstract': \"a significant number of equity funds are preferred by index funds nowadays, and market sensitivities are instrumental in managing them. index funds might replicate the index identically, which is, however, cost-ineffective and impractical. moreover, to utilize market sensitivities to replicate the index partially, they must be predicted or estimated accurately. accordingly, first, we examine deep learning models to predict market sensitivities. also, we present pragmatic applications of data processing methods to aid training and generate target data for the prediction. then, we propose a partial-index-tracking optimization model controlling the net predicted market sensitivities of the portfolios and index to be the same. these processes' efficacy is corroborated by the korea stock price index 200. our experiments show a significant reduction of the prediction errors compared with historical estimations, and competitive tracking errors of replicating the index using fewer than half of the entire constituents. therefore, we show that applying deep learning to predict market sensitivities is promising and that our portfolio construction methods are practically effective. additionally, to our knowledge, this is the first study that addresses market sensitivities focused on deep learning.\",\n",
       "  'categories': 'q-fin.pm cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-01',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['yoonsik hong', 'yanghoon kim', 'jeonghun kim', 'yongmin choi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.00780'},\n",
       " {'title': 'diffusion models: a comprehensive survey of methods and applications',\n",
       "  'id': '2209.00796',\n",
       "  'abstract': 'diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. in this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. we also discuss the potential for combining diffusion models with other generative models for enhanced results. we further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. this survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. github: https://github.com/yangling0818/diffusion-models-papers-survey-taxonomy.',\n",
       "  'categories': 'cs.lg cs.ai cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-01',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['ling yang',\n",
       "   'zhilong zhang',\n",
       "   'yang song',\n",
       "   'shenda hong',\n",
       "   'runsheng xu',\n",
       "   'yue zhao',\n",
       "   'yingxia shao',\n",
       "   'wentao zhang',\n",
       "   'bin cui',\n",
       "   'ming-hsuan yang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.00796'},\n",
       " {'title': 'a survey of machine unlearning',\n",
       "  'id': '2209.02299',\n",
       "  'abstract': \"today, computer systems hold large amounts of personal data. yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ml), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and ai. recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ml models, i.e. ``the right to be forgotten''). while removing data from back-end databases should be straightforward, it is not sufficient in the ai context as ml models often `remember' the old data. contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. this phenomenon calls for a new paradigm, namely machine unlearning, to make ml models forget about particular data. it turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. in addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. we hope this survey serves as a valuable resource for ml researchers and those seeking to innovate privacy technologies. our resources are publicly available at https://github.com/tamlhp/awesome-machine-unlearning.\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-06',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['thanh tam nguyen',\n",
       "   'thanh trung huynh',\n",
       "   'phi le nguyen',\n",
       "   'alan wee-chung liew',\n",
       "   'hongzhi yin',\n",
       "   'quoc viet hung nguyen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.02299'},\n",
       " {'title': 'inference and learning for generative capsule models',\n",
       "  'id': '2209.03115',\n",
       "  'abstract': 'capsule networks (see e.g. hinton et al., 2018) aim to encode knowledge of and reason about the relationship between an object and its parts. in this paper we specify a generative model for such data, and derive a variational algorithm for inferring the transformation of each model object in a scene, and the assignments of observed parts to the objects. we derive a learning algorithm for the object models, based on variational expectation maximization (jordan et al., 1999). we also study an alternative inference algorithm based on the ransac method of fischler and bolles (1981). we apply these inference methods to (i) data generated from multiple geometric objects like squares and triangles (\"constellations\"), and (ii) data from a parts-based model of faces. recent work by kosiorek et al. (2019) has used amortized inference via stacked capsule autoencoders (scaes) to tackle this problem -- our results show that we significantly outperform them where we can make comparisons (on the constellations data).',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-07',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['alfredo nazabal',\n",
       "   'nikolaos tsagkas',\n",
       "   'christopher k. i. williams'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.03115'},\n",
       " {'title': 'machine learning sensors for diagnosis of covid-19 disease using routine   blood values for internet of things application',\n",
       "  'id': '2209.03522',\n",
       "  'abstract': 'healthcare digitalization requires effective applications of human sensors, when various parameters of the human body are instantly monitored in everyday life due to the internet of things (iot). in particular, machine learning (ml) sensors for the prompt diagnosis of covid-19 are an important option for iot application in healthcare and ambient assisted living (aal). determining a covid-19 infected status with various diagnostic tests and imaging results is costly and time-consuming. this study provides a fast, reliable and cost-effective alternative tool for the diagnosis of covid-19 based on the routine blood values (rbvs) measured at admission. the dataset of the study consists of a total of 5296 patients with the same number of negative and positive covid-19 test results and 51 routine blood values. in this study, 13 popular classifier machine learning models and the lognnet neural network model were exanimated. the most successful classifier model in terms of time and accuracy in the detection of the disease was the histogram-based gradient boosting (hgb) (accuracy: 100%, time: 6.39 sec). the hgb classifier identified the 11 most important features (ldl, cholesterol, hdl-c, mchc, triglyceride, amylase, ua, ldh, ck-mb, alp and mch) to detect the disease with 100% accuracy. in addition, the importance of single, double and triple combinations of these features in the diagnosis of the disease was discussed. we propose to use these 11 features and their binary combinations as important biomarkers for ml sensors in the diagnosis of the disease, supporting edge computing on arduino and cloud iot service.',\n",
       "  'categories': 'cs.lg physics.med-ph q-bio.qm',\n",
       "  'doi': '10.3390/s22207886',\n",
       "  'created': '2022-09-07',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['andrei velichko',\n",
       "   'mehmet tahir huyut',\n",
       "   'maksim belyaev',\n",
       "   'yuriy izotov',\n",
       "   'dmitry korzun'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.03522'},\n",
       " {'title': 'fact-saboteurs: a taxonomy of evidence manipulation attacks against   fact-verification systems',\n",
       "  'id': '2209.03755',\n",
       "  'abstract': \"mis- and disinformation are now a substantial global threat to our security and safety. to cope with the scale of online misinformation, one viable solution is to automate the fact-checking of claims by retrieving and verifying against relevant evidence. while major recent advances have been achieved in pushing forward the automatic fact-verification, a comprehensive evaluation of the possible attack vectors against such systems is still lacking. particularly, the automated fact-verification process might be vulnerable to the exact disinformation campaigns it is trying to combat. in this work, we assume an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouflaging the relevant evidence, or planting a misleading one. we first propose an exploratory taxonomy that spans these two targets and the different threat model dimensions. guided by this, we design and propose several potential attack methods. we show that it is possible to subtly modify claim-salient snippets in the evidence, in addition to generating diverse and claim-aligned evidence. as a result, we highly degrade the fact-checking performance under many different permutations of the taxonomy's dimensions. the attacks are also robust against post-hoc modifications of the claim. our analysis further hints at potential limitations in models' inference when faced with contradicting evidence. we emphasize that these attacks can have harmful implications on the inspectable and human-in-the-loop usage scenarios of such models, and we conclude by discussing challenges and directions for future defenses.\",\n",
       "  'categories': 'cs.cr cs.cl cs.cy cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-07',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['sahar abdelnabi', 'mario fritz'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.03755'},\n",
       " {'title': 'towards confidence-guided shape completion for robotic applications',\n",
       "  'id': '2209.04300',\n",
       "  'abstract': 'many robotic tasks involving some form of 3d visual perception greatly benefit from a complete knowledge of the working environment. however, robots often have to tackle unstructured environments and their onboard visual sensors can only provide incomplete information due to limited workspaces, clutter or object self-occlusion. in recent years, deep learning architectures for shape completion have begun taking traction as effective means of inferring a complete 3d object representation from partial visual data. nevertheless, most of the existing state-of-the-art approaches provide a fixed output resolution in the form of voxel grids, strictly related to the size of the neural network output stage. while this is enough for some tasks, e.g. obstacle avoidance in navigation, grasping and manipulation require finer resolutions and simply scaling up the neural network outputs is computationally expensive. in this paper, we address this limitation by proposing an object shape completion method based on an implicit 3d representation providing a confidence value for each reconstructed point. as a second contribution, we propose a gradient-based method for efficiently sampling such implicit function at an arbitrary resolution, tunable at inference time. we experimentally validate our approach by comparing reconstructed shapes with ground truths, and by deploying our shape completion algorithm in a robotic grasping pipeline. in both cases, we compare results with a state-of-the-art shape completion approach.',\n",
       "  'categories': 'cs.cv cs.lg cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-09',\n",
       "  'updated': '',\n",
       "  'authors': ['andrea rosasco',\n",
       "   'stefano berti',\n",
       "   'fabrizio bottarel',\n",
       "   'michele colledanchise',\n",
       "   'lorenzo natale'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.04300'},\n",
       " {'title': 'investigation of a machine learning methodology for the ska pulsar   search pipeline',\n",
       "  'id': '2209.04430',\n",
       "  'abstract': 'the ska pulsar search pipeline will be used for real time detection of pulsars. modern radio telescopes such as ska will be generating petabytes of data in their full scale of operation. hence experience-based and data-driven algorithms become indispensable for applications such as candidate detection. here we describe our findings from testing a state of the art object detection algorithm called mask r-cnn to detect candidate signatures in the ska pulsar search pipeline. we have trained the mask r-cnn model to detect candidate images. a custom annotation tool was developed to mark the regions of interest in large datasets efficiently. we have successfully demonstrated this algorithm by detecting candidate signatures on a simulation dataset. the paper presents details of this work with a highlight on the future prospects.',\n",
       "  'categories': 'astro-ph.im cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-09',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['shashank sanjay bhat',\n",
       "   'thiagaraj prabu',\n",
       "   'ben stappers',\n",
       "   'atul ghalame',\n",
       "   'snehanshu saha',\n",
       "   't. s. b sudarshan',\n",
       "   'zafiirah hosenie'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.04430'},\n",
       " {'title': 'efficient learning of nonlinear prediction models with time-series   privileged information',\n",
       "  'id': '2209.07067',\n",
       "  'abstract': 'in domains where sample sizes are limited, efficient learning algorithms are critical. learning using privileged information (lupi) offers increased sample efficiency by allowing prediction models access to auxiliary information at training time which is unavailable when the models are used. in recent work, it was shown that for prediction in linear-gaussian dynamical systems, a lupi learner with access to intermediate time series data is never worse and often better in expectation than any unbiased classical learner. we provide new insights into this analysis and generalize it to nonlinear prediction tasks in latent dynamical systems, extending theoretical guarantees to the case where the map connecting latent variables and observations is known up to a linear transform. in addition, we propose algorithms based on random features and representation learning for the case when this map is unknown. a suite of empirical results confirm theoretical findings and show the potential of using privileged time-series information in nonlinear prediction.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-15',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['bastian jung', 'fredrik d johansson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.07067'},\n",
       " {'title': 'sound and complete verification of polynomial networks',\n",
       "  'id': '2209.07235',\n",
       "  'abstract': 'polynomial networks (pns) have demonstrated promising performance on face and image recognition recently. however, robustness of pns is unclear and thus obtaining certificates becomes imperative for enabling their adoption in real-world applications. existing verification algorithms on relu neural networks (nns) based on classical branch and bound (bab) techniques cannot be trivially applied to pn verification. in this work, we devise a new bounding method, equipped with bab for global convergence guarantees, called verification of polynomial networks or vpn for short. one key insight is that we obtain much tighter bounds than the interval bound propagation (ibp) and deept-fast [bonaert et al., 2021] baselines. this enables sound and complete pn verification with empirical validation on mnist, cifar10 and stl10 datasets. we believe our method has its own interest to nn verification. the source code is publicly available at https://github.com/megaelius/pnverification.',\n",
       "  'categories': 'cs.lg cs.ai cs.cr',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-15',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['elias abad rocamora',\n",
       "   'mehmet fatih sahin',\n",
       "   'fanghui liu',\n",
       "   'grigorios g chrysos',\n",
       "   'volkan cevher'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.07235'},\n",
       " {'title': 'extracting biomedical factual knowledge using pretrained language model   and electronic health record context',\n",
       "  'id': '2209.07859',\n",
       "  'abstract': 'language models (lms) have performed well on biomedical natural language processing applications. in this study, we conducted some experiments to use prompt methods to extract knowledge from lms as new knowledge bases (lms as kbs). however, prompting can only be used as a low bound for knowledge extraction, and perform particularly poorly on biomedical domain kbs. in order to make lms as kbs more in line with the actual application scenarios of the biomedical domain, we specifically add ehr notes as context to the prompt to improve the low bound in the biomedical domain. we design and validate a series of experiments for our dynamic-context-biolama task. our experiments show that the knowledge possessed by those language models can distinguish the correct knowledge from the noise knowledge in the ehr notes, and such distinguishing ability can also be used as a new metric to evaluate the amount of knowledge possessed by the model.',\n",
       "  'categories': 'cs.ir cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-08-25',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['zonghai yao',\n",
       "   'yi cao',\n",
       "   'zhichao yang',\n",
       "   'vijeta deshpande',\n",
       "   'hong yu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.07859'},\n",
       " {'title': 'deep learning-based rate-splitting multiple access for reconfigurable   intelligent surface-aided tera-hertz massive mimo',\n",
       "  'id': '2209.08456',\n",
       "  'abstract': 'reconfigurable intelligent surface (ris) can significantly enhance the service coverage of tera-hertz massive multiple-input multiple-output (mimo) communication systems. however, obtaining accurate high-dimensional channel state information (csi) with limited pilot and feedback signaling overhead is challenging, severely degrading the performance of conventional spatial division multiple access. to improve the robustness against csi imperfection, this paper proposes a deep learning (dl)-based rate-splitting multiple access (rsma) scheme for ris-aided tera-hertz multi-user mimo systems. specifically, we first propose a hybrid data-model driven dl-based rsma precoding scheme, including the passive precoding at the ris as well as the analog active precoding and the rsma digital active precoding at the base station (bs). to realize the passive precoding at the ris, we propose a transformer-based data-driven ris reflecting network (rrn). as for the analog active precoding at the bs, we propose a match-filter based analog precoding scheme considering that the bs and ris adopt the los-mimo antenna array architecture. as for the rsma digital active precoding at the bs, we propose a low-complexity approximate weighted minimum mean square error (awmmse) digital precoding scheme. furthermore, for better precoding performance as well as lower computational complexity, a model-driven deep unfolding active precoding network (dfapn) is also designed by combining the proposed awmmse scheme with dl. then, to acquire accurate csi at the bs for the investigated rsma precoding scheme to achieve higher spectral efficiency, we propose a csi acquisition network (can) with low pilot and feedback signaling overhead, where the downlink pilot transmission, csi feedback at the user equipments (ues), and csi reconstruction at the bs are modeled as an end-to-end neural network based on transformer.',\n",
       "  'categories': 'eess.sp cs.ai cs.it cs.lg math.it',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-17',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['minghui wu',\n",
       "   'zhen gao',\n",
       "   'yang huang',\n",
       "   'zhenyu xiao',\n",
       "   'derrick wing kwan ng',\n",
       "   'zhaoyang zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.08456'},\n",
       " {'title': 'learning-augmented algorithms for online linear and semidefinite   programming',\n",
       "  'id': '2209.10614',\n",
       "  'abstract': 'semidefinite programming (sdp) is a unifying framework that generalizes both linear programming and quadratically-constrained quadratic programming, while also yielding efficient solvers, both in theory and in practice. however, there exist known impossibility results for approximating the optimal solution when constraints for covering sdps arrive in an online fashion. in this paper, we study online covering linear and semidefinite programs in which the algorithm is augmented with advice from a possibly erroneous predictor. we show that if the predictor is accurate, we can efficiently bypass these impossibility results and achieve a constant-factor approximation to the optimal solution, i.e., consistency. on the other hand, if the predictor is inaccurate, under some technical conditions, we achieve results that match both the classical optimal upper bounds and the tight lower bounds up to constant factors, i.e., robustness.   more broadly, we introduce a framework that extends both (1) the online set cover problem augmented with machine-learning predictors, studied by bamas, maggiori, and svensson (neurips 2020), and (2) the online covering sdp problem, initiated by elad, kale, and naor (icalp 2016). specifically, we obtain general online learning-augmented algorithms for covering linear programs with fractional advice and constraints, and initiate the study of learning-augmented algorithms for covering sdp problems.   our techniques are based on the primal-dual framework of buchbinder and naor (mathematics of operations research, 34, 2009) and can be further adjusted to handle constraints where the variables lie in a bounded region, i.e., box constraints.',\n",
       "  'categories': 'cs.ds cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-21',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['elena grigorescu',\n",
       "   'young-san lin',\n",
       "   'sandeep silwal',\n",
       "   'maoyuan song',\n",
       "   'samson zhou'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.10614'},\n",
       " {'title': 'batch bayesian optimisation via density-ratio estimation with guarantees',\n",
       "  'id': '2209.10715',\n",
       "  'abstract': \"bayesian optimisation (bo) algorithms have shown remarkable success in applications involving expensive black-box functions. traditionally bo has been set as a sequential decision-making process which estimates the utility of query points via an acquisition function and a prior over functions, such as a gaussian process. recently, however, a reformulation of bo via density-ratio estimation (bore) allowed reinterpreting the acquisition function as a probabilistic binary classifier, removing the need for an explicit prior over functions and increasing scalability. in this paper, we present a theoretical analysis of bore's regret and an extension of the algorithm with improved uncertainty estimates. we also show that bore can be naturally extended to a batch optimisation setting by recasting the problem as approximate bayesian inference. the resulting algorithms come equipped with theoretical performance guarantees and are assessed against other batch and sequential bo baselines in a series of experiments.\",\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-21',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['rafael oliveira', 'louis tiao', 'fabio ramos'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.10715'},\n",
       " {'title': 'turning normalizing flows into monge maps with geodesic gaussian   preserving flows',\n",
       "  'id': '2209.10873',\n",
       "  'abstract': \"normalizing flows (nf) are powerful likelihood-based generative models that are able to trade off between expressivity and tractability to model complex densities. a now well established research avenue leverages optimal transport (ot) and looks for monge maps, i.e. models with minimal effort between the source and target distributions. this paper introduces a method based on brenier's polar factorization theorem to transform any trained nf into a more ot-efficient version without changing the final density. we do so by learning a rearrangement of the source (gaussian) distribution that minimizes the ot cost between the source and the final density. we further constrain the path leading to the estimated monge map to lie on a geodesic in the space of volume-preserving diffeomorphisms thanks to euler's equations. the proposed method leads to smooth flows with reduced ot cost for several existing models without affecting the model performance.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-22',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['guillaume morel',\n",
       "   'lucas drumetz',\n",
       "   'simon benaïchouche',\n",
       "   'nicolas courty',\n",
       "   'françois rousseau'],\n",
       "  'affiliation': ['imt atlantique - iti',\n",
       "   'imt atlantique - mee, lab-sticc\\\\_ose',\n",
       "   'imt atlantique',\n",
       "   'irisa, ubs',\n",
       "   'imt atlantique - iti, latim'],\n",
       "  'url': 'https://arxiv.org/abs/2209.10873'},\n",
       " {'title': 'tradeoffs between convergence rate and noise amplification for   momentum-based accelerated optimization algorithms',\n",
       "  'id': '2209.11920',\n",
       "  'abstract': \"we study momentum-based first-order optimization algorithms in which the iterations utilize information from the two previous steps and are subject to an additive white noise. this class of algorithms includes polyak's heavy-ball and nesterov's accelerated methods as special cases and noise accounts for uncertainty in either gradient evaluation or iteration updates. for strongly convex quadratic problems, we use the steady-state variance of the error in the optimization variable to quantify noise amplification and identify fundamental stochastic performance tradeoffs. our approach utilizes the jury stability criterion to provide a novel geometric characterization of conditions for linear convergence, and it clarifies the relation between the noise amplification and convergence rate as well as their dependence on the condition number and the constant algorithmic parameters. this geometric insight leads to simple alternative proofs of standard convergence results and allows us to establish analytical lower bounds on the product between the settling time and noise amplification that scale quadratically with the condition number. our analysis also identifies a key difference between the gradient and iterate noise models: while the amplification of gradient noise can be made arbitrarily small by sufficiently decelerating the algorithm, the best achievable variance amplification for the iterate noise model increases linearly with the settling time in decelerating regime. furthermore, we introduce two parameterized families of algorithms that strike a balance between noise amplification and settling time while preserving order-wise pareto optimality for both noise models. finally, by analyzing a class of accelerated gradient flow dynamics, whose suitable discretization yields the two-step momentum algorithm, we establish that stochastic performance tradeoffs also extend to continuous time.\",\n",
       "  'categories': 'math.oc cs.lg cs.sy eess.sy math.ds',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-24',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['hesameddin mohammadi',\n",
       "   'meisam razaviyayn',\n",
       "   'mihailo r. jovanović'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.11920'},\n",
       " {'title': 'a survey on graph neural networks and graph transformers in computer   vision: a task-oriented perspective',\n",
       "  'id': '2209.13232',\n",
       "  'abstract': 'graph neural networks (gnns) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining (\\\\emph{e.g.,} social network analysis and recommender systems), computer vision (\\\\emph{e.g.,} object detection and point cloud learning), and natural language processing (\\\\emph{e.g.,} relation extraction and sequence learning), to name a few. with the emergence of transformers in natural language processing and computer vision, graph transformers embed a graph structure into the transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. in this paper, we present a comprehensive review of gnns and graph transformers in computer vision from a task-oriented perspective. specifically, we divide their applications in computer vision into five categories according to the modality of input data, \\\\emph{i.e.,} 2d natural images, videos, 3d data, vision + language, and medical images. in each category, we further divide the applications according to a set of vision tasks. such a task-oriented taxonomy allows us to examine how each task is tackled by different gnn-based approaches and how well these approaches perform. based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-27',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['chaoqi chen',\n",
       "   'yushuang wu',\n",
       "   'qiyuan dai',\n",
       "   'hong-yu zhou',\n",
       "   'mutian xu',\n",
       "   'sibei yang',\n",
       "   'xiaoguang han',\n",
       "   'yizhou yu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.13232'},\n",
       " {'title': 'an efficient encoder-decoder architecture with top-down attention for   speech separation',\n",
       "  'id': '2209.15200',\n",
       "  'abstract': \"deep neural networks have shown excellent prospects in speech separation tasks. however, obtaining good results while keeping a low model complexity remains challenging in real-world applications. in this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain's top-down attention, called tdanet, with decreased model complexity without sacrificing performance. the top-down attention in tdanet is extracted by the global attention (ga) module and the cascaded local attention (la) layers. the ga module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. the la layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. on three benchmark datasets, tdanet consistently achieved competitive separation performance to previous state-of-the-art (sota) methods with higher efficiency. specifically, tdanet's multiply-accumulate operations (macs) are only 5\\\\% of sepformer, one of the previous sota models, and cpu inference time is only 10\\\\% of sepformer. in addition, a large-size version of tdanet obtained sota results on three datasets, with macs still only 10\\\\% of sepformer and the cpu inference time only 24\\\\% of sepformer. our study suggests that top-down attention can be a more efficient strategy for speech separation.\",\n",
       "  'categories': 'cs.sd cs.lg eess.as',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-29',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['kai li', 'runxuan yang', 'xiaolin hu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2209.15200'},\n",
       " {'title': 'cast: concurrent recognition and segmentation with adaptive segment   tokens',\n",
       "  'id': '2210.00314',\n",
       "  'abstract': 'recognizing an image and segmenting it into coherent regions are often treated as separate tasks. human vision, however, has a general sense of segmentation hierarchy before recognition occurs. we are thus inspired to learn image recognition with hierarchical image segmentation based entirely on unlabeled images. our insight is to learn fine-to-coarse features concurrently at superpixels, segments, and full image levels, enforcing consistency and goodness of feature induced segmentations while maximizing discrimination among image instances.   our model innovates vision transformers on three aspects. 1) we use adaptive segment tokens instead of fixed-shape patch tokens. 2) we create a token hierarchy by inserting graph pooling between transformer blocks, naturally producing consistent multi-scale segmentations while increasing the segment size and reducing the number of tokens. 3) we produce hierarchical image segmentation for free while training for recognition by maximizing image-wise discrimination.   our work delivers the first concurrent recognition and hierarchical segmentation model without any supervision. validated on imagenet and pascal voc, it achieves better recognition and segmentation with higher computational efficiency.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-01',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['tsung-wei ke', 'stella x. yu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.00314'},\n",
       " {'title': 'high precision differentiation techniques for data-driven solution of   nonlinear pdes by physics-informed neural networks',\n",
       "  'id': '2210.00518',\n",
       "  'abstract': \"time-dependent partial differential equations with given initial conditions are considered in this paper. new differentiation techniques of the unknown solution with respect to time variable are proposed. it is shown that the proposed techniques allow to generate accurate higher order derivatives simultaneously for a set of spatial points. the calculated derivatives can then be used for data-driven solution in different ways. an application for physics informed neural networks by the well-known deepxde software solution in python under tensorflow background framework has been presented for three real-life pdes: burgers', allen-cahn and schrodinger equations.\",\n",
       "  'categories': 'math.na cs.lg cs.na',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-02',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['marat s. mukhametzhanov'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.00518'},\n",
       " {'title': 'limitations of neural network training due to numerical instability of   backpropagation',\n",
       "  'id': '2210.00805',\n",
       "  'abstract': 'we study the training of deep neural networks by gradient descent where floating-point arithmetic is used to compute the gradients. in this framework and under realistic assumptions, we demonstrate that it is highly unlikely to find relu neural networks that maintain, in the course of training with gradient descent, superlinearly many affine pieces with respect to their number of layers. in virtually all approximation theoretical arguments which yield high order polynomial rates of approximation, sequences of relu neural networks with exponentially many affine pieces compared to their numbers of layers are used. as a consequence, we conclude that approximating sequences of relu neural networks resulting from gradient descent in practice differ substantially from theoretically constructed sequences. the assumptions and the theoretical results are compared to a numerical study, which yields concurring results.',\n",
       "  'categories': 'cs.lg math.fa stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-03',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['clemens karner',\n",
       "   'vladimir kazeev',\n",
       "   'philipp christian petersen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.00805'},\n",
       " {'title': 'analysis of (sub-)riemannian pde-g-cnns',\n",
       "  'id': '2210.00935',\n",
       "  'abstract': 'group equivariant convolutional neural networks (g-cnns) have been successfully applied in geometric deep learning. typically, g-cnns have the advantage over cnns that they do not waste network capacity on training symmetries that should have been hard-coded in the network. the recently introduced framework of pde-based g-cnns (pde-g-cnns) generalises g-cnns. pde-g-cnns have the core advantages that they simultaneously 1) reduce network complexity, 2) increase classification performance, and 3) provide geometric interpretability. their implementations primarily consist of linear and morphological convolutions with kernels.   in this paper we show that the previously suggested approximative morphological kernels do not always accurately approximate the exact kernels accurately. more specifically, depending on the spatial anisotropy of the riemannian metric, we argue that one must resort to sub-riemannian approximations. we solve this problem by providing a new approximative kernel that works regardless of the anisotropy. we provide new theorems with better error estimates of the approximative kernels, and prove that they all carry the same reflectional symmetries as the exact ones.   we test the effectiveness of multiple approximative kernels within the pde-g-cnn framework on two datasets, and observe an improvement with the new approximative kernels. we report that the pde-g-cnns again allow for a considerable reduction of network complexity while having comparable or better performance than g-cnns and cnns on the two datasets. moreover, pde-g-cnns have the advantage of better geometric interpretability over g-cnns, as the morphological kernels are related to association fields from neurogeometry.',\n",
       "  'categories': 'cs.lg cs.cv math.dg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-03',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['gijs bellaard',\n",
       "   'daan l. j. bon',\n",
       "   'gautam pai',\n",
       "   'bart m. n. smets',\n",
       "   'remco duits'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.00935'},\n",
       " {'title': 'one transformer can understand both 2d & 3d molecular data',\n",
       "  'id': '2210.01765',\n",
       "  'abstract': 'unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. one can view a molecule as a 2d graph or define it as a collection of atoms located in a 3d space. for molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. we believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. to achieve this goal, in this work, we develop a novel transformer-based molecular model called transformer-m, which can take molecular data of 2d or 3d formats as input and generate meaningful semantic representations. using the standard transformer as the backbone architecture, transformer-m develops two separated channels to encode 2d and 3d structural information and incorporate them with the atom features in the network modules. when the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. by training on 2d and 3d molecular data with properly designed supervised signals, transformer-m automatically learns to leverage knowledge from different data modalities and correctly capture the representations. we conducted extensive experiments for transformer-m. all empirical results show that transformer-m can simultaneously achieve strong performance on 2d and 3d tasks, suggesting its broad applicability. the code and models will be made publicly available at https://github.com/lsj2408/transformer-m.',\n",
       "  'categories': 'cs.lg q-bio.bm stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-04',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['shengjie luo',\n",
       "   'tianlang chen',\n",
       "   'yixian xu',\n",
       "   'shuxin zheng',\n",
       "   'tie-yan liu',\n",
       "   'di he',\n",
       "   'liwei wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.01765'},\n",
       " {'title': 'a new family of constitutive artificial neural networks towards   automated model discovery',\n",
       "  'id': '2210.02202',\n",
       "  'abstract': 'for more than 100 years, chemical, physical, and material scientists have proposed competing constitutive models to best characterize the behavior of natural and man-made materials in response to mechanical loading. now, computer science offers a universal solution: neural networks. neural networks are powerful function approximators that can learn constitutive relations from large data without any knowledge of the underlying physics. however, classical neural networks ignore a century of research in constitutive modeling, violate thermodynamic considerations, and fail to predict the behavior outside the training regime. here we design a new family of constitutive artificial neural networks that inherently satisfy common kinematic, thermodynamic, and physic constraints and, at the same time, constrain the design space of admissible functions to create robust approximators, even in the presence of sparse data. we revisit the non-linear field theories of mechanics and reverse-engineer the network input to account for material objectivity, symmetry, and incompressibility; the network output to enforce thermodynamic consistency; the activation functions to implement physically reasonable restrictions; and the network architecture to ensure polyconvexity. we demonstrate that this new class of models is a generalization of the classical neo hooke, blatz ko, mooney rivlin, yeoh, and demiray models and that the network weights have a clear physical interpretation. when trained with classical benchmark data for rubber, our network autonomously selects the best constitutive model and learns its parameters. our findings suggests that constitutive artificial neural networks have the potential to induce a paradigm shift in constitutive modeling, from user-defined model selection to automated model discovery. our source code, data, and examples are available at https://github.com/livingmatterlab/cann.',\n",
       "  'categories': 'cs.lg cond-mat.mtrl-sci',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-15',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['kevin linka', 'ellen kuhl'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.02202'},\n",
       " {'title': 'join-chain network: a logical reasoning view of the multi-head attention   in transformer',\n",
       "  'id': '2210.02729',\n",
       "  'abstract': \"developing neural architectures that are capable of logical reasoning has become increasingly important for a wide range of applications (e.g., natural language processing). towards this grand objective, we propose a symbolic reasoning architecture that chains many join operators together to model output logical expressions. in particular, we demonstrate that such an ensemble of join-chains can express a broad subset of ''tree-structured'' first-order logical expressions, named foet, which is particularly useful for modeling natural languages. to endow it with differentiable learning capability, we closely examine various neural operators for approximating the symbolic join-chains. interestingly, we find that the widely used multi-head self-attention module in transformer can be understood as a special neural operator that implements the union bound of the join operator in probabilistic predicate space. our analysis not only provides a new perspective on the mechanism of the pretrained models such as bert for natural language understanding but also suggests several important future improvement directions.\",\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-06',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['jianyi zhang', 'yiran chen', 'jianshu chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.02729'},\n",
       " {'title': 'pqlm -- multilingual decentralized portable quantum language model for   privacy protection',\n",
       "  'id': '2210.03221',\n",
       "  'abstract': 'with careful manipulation, malicious agents can reverse engineer private information encoded in pre-trained language models. security concerns motivate the development of quantum pre-training. in this work, we propose a highly portable quantum language model (pqlm) that can be easily transferred to downstream tasks on classical machines. the framework consists of a cloud pqlm built with random variational quantum classifiers (vqc) and local models for downstream applications. we demonstrate the portability of the quantum model by extracting only the word embeddings and effectively applying them to downstream tasks on classical machines. our pqlm exhibits comparable performance to its classical counterpart on both intrinsic evaluation (loss, perplexity) and extrinsic evaluation (multilingual sentiment analysis accuracy) metrics and achieves an accuracy of 93.4%, outperforming the classical model. we also perform ablation studies on the factors affecting pqlm performance to analyze model stability. our work establishes a theoretical foundation for a portable quantum pre-trained language model that could be trained on private data and made available for public use with privacy protection guarantees.',\n",
       "  'categories': 'cs.lg cs.cl quant-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-06',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['shuyue stella li',\n",
       "   'xiangyu zhang',\n",
       "   'shu zhou',\n",
       "   'hongchao shu',\n",
       "   'ruixing liang',\n",
       "   'hexin liu',\n",
       "   'leibny paola garcia'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.03221'},\n",
       " {'title': 'distillation-resistant watermarking for model protection in nlp',\n",
       "  'id': '2210.03312',\n",
       "  'abstract': \"how can we protect the intellectual property of trained nlp models? modern nlp models are prone to stealing by querying and distilling from their publicly exposed apis. however, existing protection methods such as watermarking only work for images but are not applicable to text. we propose distillation-resistant watermarking (drw), a novel technique to protect nlp models from being stolen via distillation. drw protects a model by injecting watermarks into the victim's prediction probability corresponding to a secret key and is able to detect such a key by probing a suspect model. we prove that a protected model still retains the original accuracy within a certain bound. we evaluate drw on a diverse set of nlp tasks including text classification, part-of-speech tagging, and named entity recognition. experiments show that drw protects the original model and detects stealing suspects at 100% mean average precision for all four tasks while the prior method fails on two.\",\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-07',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['xuandong zhao', 'lei li', 'yu-xiang wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.03312'},\n",
       " {'title': 'detecting label errors in token classification data',\n",
       "  'id': '2210.03920',\n",
       "  'abstract': 'mislabeled examples are a common issue in real-world data, particularly for tasks like token classification where many labels must be chosen on a fine-grained basis. here we consider the task of finding sentences that contain label errors in token classification datasets. we study 11 different straightforward methods that score tokens/sentences based on the predicted class probabilities output by a (any) token classification model (trained via any procedure). in precision-recall evaluations based on real-world label errors in entity recognition data from conll-2003, we identify a simple and effective method that consistently detects those sentences containing label errors when applied with different token classification models.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-08',\n",
       "  'updated': '',\n",
       "  'authors': ['wei-chen wang', 'jonas mueller'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.03920'},\n",
       " {'title': 'almost-lossless compression of a low-rank random tensor',\n",
       "  'id': '2210.04041',\n",
       "  'abstract': 'in this work, we establish an asymptotic limit of almost-lossless compression of a random, finite alphabet tensor which admits a low-rank canonical polyadic decomposition.',\n",
       "  'categories': 'cs.it cs.lg eess.sp math.it',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-08',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['minh thanh vu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.04041'},\n",
       " {'title': 'humset: dataset of multilingual information extraction and   classification for humanitarian crisis response',\n",
       "  'id': '2210.04573',\n",
       "  'abstract': 'timely and effective response to humanitarian crises requires quick and accurate analysis of large amounts of text data - a process that can highly benefit from expert-assisted nlp systems trained on validated and annotated data in the humanitarian response domain. to enable creation of such nlp systems, we introduce and release humset, a novel and rich multilingual dataset of humanitarian response documents annotated by experts in the humanitarian response community. the dataset provides documents in three languages (english, french, spanish) and covers a variety of humanitarian crises from 2018 to 2021 across the globe. for each document, humset provides selected snippets (entries) as well as assigned classes to each entry annotated using common humanitarian information analysis frameworks. humset also provides novel and challenging entry extraction and multi-label entry classification tasks. in this paper, we take a first step towards approaching these tasks and conduct a set of experiments on pre-trained language models (plm) to establish strong baselines for future research in this domain. the dataset is available at https://blog.thedeep.io/humset/.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-10',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['selim fekih',\n",
       "   \"nicolo' tamagnone\",\n",
       "   'benjamin minixhofer',\n",
       "   'ranjan shrestha',\n",
       "   'ximena contla',\n",
       "   'ewan oglethorpe',\n",
       "   'navid rekabsaz'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.04573'},\n",
       " {'title': 'cat-probing: a metric-based approach to interpret how pre-trained models   for programming language attend code structure',\n",
       "  'id': '2210.04633',\n",
       "  'abstract': 'code pre-trained models (codeptms) have recently demonstrated significant success in code intelligence. to interpret these models, some probing methods have been applied. however, these methods fail to consider the inherent characteristics of codes. in this paper, to address the problem, we propose a novel probing method cat-probing to quantitatively interpret how codeptms attend code structure. we first denoise the input code sequences based on the token types pre-defined by the compilers to filter those tokens whose attention scores are too small. after that, we define a new metric cat-score to measure the commonality between the token-level attention scores generated in codeptms and the pair-wise distances between corresponding ast nodes. the higher the cat-score, the stronger the ability of codeptms to capture code structure. we conduct extensive experiments to integrate cat-probing with representative codeptms for different programming languages. experimental results show the effectiveness of cat-probing in codeptm interpretation. our codes and data are publicly available at https://github.com/nchen909/codeattention.',\n",
       "  'categories': 'cs.se cs.ai cs.lg cs.pl',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-07',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['nuo chen',\n",
       "   'qiushi sun',\n",
       "   'renyu zhu',\n",
       "   'xiang li',\n",
       "   'xuesong lu',\n",
       "   'ming gao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.04633'},\n",
       " {'title': 'dhrl: a graph-based approach for long-horizon and sparse hierarchical   reinforcement learning',\n",
       "  'id': '2210.05150',\n",
       "  'abstract': 'hierarchical reinforcement learning (hrl) has made notable progress in complex control tasks by leveraging temporal abstraction. however, previous hrl algorithms often suffer from serious data inefficiency as environments get large. the extended components, $i.e.$, goal space and length of episodes, impose a burden on either one or both high-level and low-level policies since both levels share the total horizon of the episode. in this paper, we present a method of decoupling horizons using a graph in hierarchical reinforcement learning (dhrl) which can alleviate this problem by decoupling the horizons of high-level and low-level policies and bridging the gap between the length of both horizons using a graph. dhrl provides a freely stretchable high-level action interval, which facilitates longer temporal abstraction and faster training in complex tasks. our method outperforms state-of-the-art hrl algorithms in typical hrl environments. moreover, dhrl achieves long and complex locomotion and manipulation tasks.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-11',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['seungjae lee', 'jigang kim', 'inkyu jang', 'h. jin kim'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.05150'},\n",
       " {'title': 'make sharpness-aware minimization stronger: a sparsified perturbation   approach',\n",
       "  'id': '2210.05177',\n",
       "  'abstract': 'deep neural networks often suffer from poor generalization caused by complex and non-convex loss landscapes. one of the popular solutions is sharpness-aware minimization (sam), which smooths the loss landscape via minimizing the maximized change of training loss when adding a perturbation to the weight. however, we find the indiscriminate perturbation of sam on all parameters is suboptimal, which also results in excessive computation, i.e., double the overhead of common optimizers like stochastic gradient descent (sgd). in this paper, we propose an efficient and effective training scheme coined as sparse sam (ssam), which achieves sparse perturbation by a binary mask. to obtain the sparse mask, we provide two solutions which are based onfisher information and dynamic sparse training, respectively. in addition, we theoretically prove that ssam can converge at the same rate as sam, i.e., $o(\\\\log t/\\\\sqrt{t})$. sparse sam not only has the potential for training acceleration but also smooths the loss landscape effectively. extensive experimental results on cifar10, cifar100, and imagenet-1k confirm the superior efficiency of our method to sam, and the performance is preserved or even better with a perturbation of merely 50% sparsity. code is availiable at https://github.com/mi-peng/sparse-sharpness-aware-minimization.',\n",
       "  'categories': 'cs.lg cs.ai cs.cv math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-11',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['peng mi',\n",
       "   'li shen',\n",
       "   'tianhe ren',\n",
       "   'yiyi zhou',\n",
       "   'xiaoshuai sun',\n",
       "   'rongrong ji',\n",
       "   'dacheng tao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.05177'},\n",
       " {'title': 'label noise-robust learning using a confidence-based sieving strategy',\n",
       "  'id': '2210.05330',\n",
       "  'abstract': 'in learning tasks with label noise, boosting model robustness against overfitting is a pivotal challenge because the model eventually memorizes labels including the noisy ones. identifying the samples with corrupted labels and preventing the model from learning them is a promising approach to address this challenge. per-sample training loss is a previously studied metric that considers samples with small loss as clean samples on which the model should be trained. in this work, we first demonstrate the ineffectiveness of this small-loss trick. then, we propose a novel discriminator metric called confidence error and a sieving strategy called confes to effectively differentiate between the clean and noisy samples. we experimentally illustrate the superior performance of our proposed approach compared to recent studies on various settings such as synthetic and real-world label noise. moreover, we show confes can be combined with other approaches such as co-teaching and dividemix to further improve the model performance.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-11',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['reihaneh torkzadehmahani',\n",
       "   'reza nasirigerdeh',\n",
       "   'daniel rueckert',\n",
       "   'georgios kaissis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.05330'},\n",
       " {'title': 'deepperform: an efficient approach for performance testing of   resource-constrained neural networks',\n",
       "  'id': '2210.05370',\n",
       "  'abstract': \"today, an increasing number of adaptive deep neural networks (adnns) are being used on resource-constrained embedded devices. we observe that, similar to traditional software, redundant computation exists in adnns, resulting in considerable performance degradation. the performance degradation is dependent on the input and is referred to as input-dependent performance bottlenecks (idpbs). to ensure an adnn satisfies the performance requirements of resource-constrained applications, it is essential to conduct performance testing to detect idpbs in the adnn. existing neural network testing methods are primarily concerned with correctness testing, which does not involve performance testing. to fill this gap, we propose deepperform, a scalable approach to generate test samples to detect the idpbs in adnns. we first demonstrate how the problem of generating performance test samples detecting idpbs can be formulated as an optimization problem. following that, we demonstrate how deepperform efficiently handles the optimization problem by learning and estimating the distribution of adnns' computational consumption. we evaluate deepperform on three widely used datasets against five popular adnn models. the results show that deepperform generates test samples that cause more severe performance degradation (flops: increase up to 552\\\\%). furthermore, deepperform is substantially more efficient than the baseline methods in generating test inputs(runtime overhead: only 6-10 milliseconds).\",\n",
       "  'categories': 'cs.lg cs.cl cs.se',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-09',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['simin chen', 'mirazul haque', 'cong liu', 'wei yang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.05370'},\n",
       " {'title': 'semi-supervised detection of structural damage using variational   autoencoder and a one-class support vector machine',\n",
       "  'id': '2210.05674',\n",
       "  'abstract': \"in recent years, artificial neural networks (anns) have been introduced in structural health monitoring (shm) systems. a semi-supervised method with a data-driven approach allows the ann training on data acquired from an undamaged structural condition to detect structural damages. in standard approaches, after the training stage, a decision rule is manually defined to detect anomalous data. however, this process could be made automatic using machine learning methods, whom performances are maximised using hyperparameter optimization techniques. the paper proposes a semi-supervised method with a data-driven approach to detect structural anomalies. the methodology consists of: (i) a variational autoencoder (vae) to approximate undamaged data distribution and (ii) a one-class support vector machine (oc-svm) to discriminate different health conditions using damage sensitive features extracted from vae's signal reconstruction. the method is applied to a scale steel structure that was tested in nine damage's scenarios by iasc-asce structural health monitoring task group.\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-11',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['andrea pollastro',\n",
       "   'giusiana testa',\n",
       "   'antonio bilotta',\n",
       "   'roberto prevete'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.05674'},\n",
       " {'title': 'predicting the clinical citation count of biomedical papers using   multilayer perceptron neural network',\n",
       "  'id': '2210.06346',\n",
       "  'abstract': 'the number of clinical citations received from clinical guidelines or clinical trials has been considered as one of the most appropriate indicators for quantifying the clinical impact of biomedical papers. therefore, the early prediction of the clinical citation count of biomedical papers is critical to scientific activities in biomedicine, such as research evaluation, resource allocation, and clinical translation. in this study, we designed a four-layer multilayer perceptron neural network (mpnn) model to predict the clinical citation count of biomedical papers in the future by using 9,822,620 biomedical papers published from 1985 to 2005. we extracted ninety-one paper features from three dimensions as the input of the model, including twenty-one features in the paper dimension, thirty-five in the reference dimension, and thirty-five in the citing paper dimension. in each dimension, the features can be classified into three categories, i.e., the citation-related features, the clinical translation-related features, and the topic-related features. besides, in the paper dimension, we also considered the features that have previously been demonstrated to be related to the citation counts of research papers. the results showed that the proposed mpnn model outperformed the other five baseline models, and the features in the reference dimension were the most important.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-07',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['xin li', 'xuli tang', 'qikai cheng'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.06346'},\n",
       " {'title': \"that's the wrong lung! evaluating and improving the interpretability of   unsupervised multimodal encoders for medical data\",\n",
       "  'id': '2210.06565',\n",
       "  'abstract': 'pretraining multimodal models on electronic health records (ehrs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. recent multimodal models induce soft local alignments between image regions and sentences. this is of particular interest in the medical domain, where alignments might highlight regions in an image relevant to specific phenomena described in free-text. while past work has suggested that attention \"heatmaps\" can be interpreted in this manner, there has been little evaluation of such alignments. we compare alignments from a state-of-the-art multimodal (image and text) model for ehr with human annotations that link image regions to sentences. our main finding is that the text has an often weak or unintuitive influence on attention; alignments do not consistently reflect basic anatomical information. moreover, synthetic modifications -- such as substituting \"left\" for \"right\" -- do not substantially influence highlights. simple techniques such as allowing the model to opt out of attending to the image and few-shot finetuning show promise in terms of their ability to improve alignments with very little or no supervision. we make our code and checkpoints open-source.',\n",
       "  'categories': 'cs.lg cs.ai cs.cv eess.iv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-12',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['denis jered mcinerney',\n",
       "   'geoffrey young',\n",
       "   'jan-willem van de meent',\n",
       "   'byron c. wallace'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.06565'},\n",
       " {'title': 'denoising masked autoencoders are certifiable robust vision learners',\n",
       "  'id': '2210.06983',\n",
       "  'abstract': 'in this paper, we propose a new self-supervised method, which is called denoising masked autoencoders (dmae), for learning certified robust classifiers of images. in dmae, we corrupt each image by adding gaussian noises to each pixel value and randomly masking several patches. a transformer-based encoder-decoder model is then trained to reconstruct the original image from the corrupted one. in this learning paradigm, the encoder will learn to capture relevant semantics for the downstream tasks, which is also robust to gaussian additive noises. we show that the pre-trained encoder can naturally be used as the base classifier in gaussian smoothed models, where we can analytically compute the certified radius for any data point. although the proposed method is simple, it yields significant performance improvement in downstream classification tasks. we show that the dmae vit-base model, which just uses 1/10 parameters of the model developed in recent work arxiv:2206.10550, achieves competitive or better certified accuracy in various settings. the dmae vit-large model significantly surpasses all previous results, establishing a new state-of-the-art on imagenet dataset. we further demonstrate that the pre-trained model has good transferability to the cifar-10 dataset, suggesting its wide adaptability. models and code are available at https://github.com/quanlin-wu/dmae.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-10',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['quanlin wu',\n",
       "   'hang ye',\n",
       "   'yuntian gu',\n",
       "   'huishuai zhang',\n",
       "   'di he',\n",
       "   'liwei wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.06983'},\n",
       " {'title': 'language models of code are few-shot commonsense learners',\n",
       "  'id': '2210.07128',\n",
       "  'abstract': \"we address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event -- or a reasoning-graph. to employ large language models (lms) for this task, existing approaches ``serialize'' the output graph as a flat list of nodes and edges. although feasible, these serialized graphs strongly deviate from the natural language corpora that lms were pre-trained on, hindering lms from generating them correctly. in this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained lms of code are better structured commonsense reasoners than lms of natural language, even when the downstream task does not involve source code at all. we demonstrate our approach across three diverse structured commonsense reasoning tasks. in all these natural language tasks, we show that using our approach, a code generation lm (codex) outperforms natural-lms that are fine-tuned on the target task (e.g., t5) and other strong lms such as gpt-3 in the few-shot setting.\",\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-13',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['aman madaan',\n",
       "   'shuyan zhou',\n",
       "   'uri alon',\n",
       "   'yiming yang',\n",
       "   'graham neubig'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.07128'},\n",
       " {'title': 'monte carlo augmented actor-critic for sparse reward deep reinforcement   learning from suboptimal demonstrations',\n",
       "  'id': '2210.07432',\n",
       "  'abstract': 'providing densely shaped reward functions for rl algorithms is often exceedingly challenging, motivating the development of rl algorithms that can learn from easier-to-specify sparse reward functions. this sparsity poses new exploration challenges. one common way to address this problem is using demonstrations to provide initial signal about regions of the state space with high rewards. however, prior rl from demonstrations algorithms introduce significant complexity and many hyperparameters, making them hard to implement and tune. we introduce monte carlo augmented actor critic (mcac), a parameter free modification to standard actor-critic algorithms which initializes the replay buffer with demonstrations and computes a modified $q$-value by taking the maximum of the standard temporal distance (td) target and a monte carlo estimate of the reward-to-go. this encourages exploration in the neighborhood of high-performing trajectories by encouraging high $q$-values in corresponding regions of the state space. experiments across $5$ continuous control domains suggest that mcac can be used to significantly increase learning efficiency across $6$ commonly used rl and rl-from-demonstrations algorithms. see https://sites.google.com/view/mcac-rl for code and supplementary material.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-13',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['albert wilcox',\n",
       "   'ashwin balakrishna',\n",
       "   'jules dedieu',\n",
       "   'wyame benslimane',\n",
       "   'daniel s. brown',\n",
       "   'ken goldberg'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.07432'},\n",
       " {'title': 'sqa3d: situated question answering in 3d scenes',\n",
       "  'id': '2210.07474',\n",
       "  'abstract': 'we propose a new task to benchmark scene understanding of embodied agents: situated question answering in 3d scenes (sqa3d). given a scene context (e.g., 3d scan), sqa3d requires the tested agent to first understand its situation (position, orientation, etc.) in the 3d scene as described by text, then reason about its surrounding environment and answer a question under that situation. based upon 650 scenes from scannet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. these questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. sqa3d imposes a significant challenge to current multi-modal especially 3d reasoning models. we evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. we believe sqa3d could facilitate future embodied ai research with stronger situation understanding and reasoning capability.',\n",
       "  'categories': 'cs.cv cs.ai cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-13',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['xiaojian ma',\n",
       "   'silong yong',\n",
       "   'zilong zheng',\n",
       "   'qing li',\n",
       "   'yitao liang',\n",
       "   'song-chun zhu',\n",
       "   'siyuan huang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.07474'},\n",
       " {'title': 'prediction of drug effectiveness in rheumatoid arthritis patients based   on machine learning algorithms',\n",
       "  'id': '2210.08016',\n",
       "  'abstract': \"rheumatoid arthritis (ra) is an autoimmune condition caused when patients' immune system mistakenly targets their own tissue. machine learning (ml) has the potential to identify patterns in patient electronic health records (ehr) to forecast the best clinical treatment to improve patient outcomes. this study introduced a drug response prediction (drp) framework with two main goals: 1) design a data processing pipeline to extract information from tabular clinical data, and then preprocess it for functional use, and 2) predict ra patient's responses to drugs and evaluate classification models' performance. we propose a novel two-stage ml framework based on european alliance of associations for rheumatology (eular) criteria cutoffs to model drug effectiveness. our model stacked-ensemble drp was developed and cross-validated using data from 425 ra patients. the evaluation used a subset of 124 patients (30%) from the same data source. in the evaluation of the test set, two-stage drp leads to improved classification accuracy over other end-to-end classification models for binary classification. our proposed method provides a complete pipeline to predict disease activity scores and identify the group that does not respond well to anti-tnf treatments, thus showing promise in supporting clinical decisions based on ehr information.\",\n",
       "  'categories': 'q-bio.qm cs.lg q-bio.bm',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-14',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['shengjia chen',\n",
       "   'nikunj gupta',\n",
       "   'woodward b. galbraith',\n",
       "   'valay shah',\n",
       "   'jacopo cirrone'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.08016'},\n",
       " {'title': 'machine learning based discrimination for excited state promoted readout',\n",
       "  'id': '2210.08574',\n",
       "  'abstract': \"a limiting factor for readout fidelity for superconducting qubits is the relaxation of the qubit to the ground state before the time needed for the resonator to reach its final target state. a technique known as excited state promoted (esp) readout was proposed to reduce this effect and further improve the readout contrast on superconducting hardware. in this work, we use readout data from ibm's five-qubit quantum systems to measure the effectiveness of using deep neural networks, like feedforward neural networks, and various classification algorithms, like k-nearest neighbors, decision trees, and gaussian naive bayes, for single-qubit and multi-qubit discrimination. these methods were compared to standardly used linear and quadratic discriminant analysis algorithms based on their qubit-state-assignment fidelity performance, robustness to readout crosstalk, and training time.\",\n",
       "  'categories': 'quant-ph cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-16',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['utkarsh azad', 'helena zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.08574'},\n",
       " {'title': 'a transformer-based generative model for de novo molecular design',\n",
       "  'id': '2210.08749',\n",
       "  'abstract': \"in the scope of drug discovery, the molecular design aims to identify novel compounds from the chemical space where the potential drug-like molecules are estimated to be in the order of 10^60 - 10^100. since this search task is computationally intractable due to the unbounded search space, deep learning draws a lot of attention as a new way of generating unseen molecules. as we seek compounds with specific target proteins, we propose a transformer-based deep model for de novo target-specific molecular design. the proposed method is capable of generating both drug-like compounds (without specified targets) and target-specific compounds. the latter are generated by enforcing different keys and values of the multi-head attention for each target. in this way, we allow the generation of smiles strings to be conditional on the specified target. experimental results demonstrate that our method is capable of generating both valid drug-like compounds and target-specific compounds. moreover, the sampled compounds from conditional model largely occupy the real target-specific molecules' chemical space and also cover a significant fraction of novel compounds.\",\n",
       "  'categories': 'cs.lg q-bio.bm',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-17',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['wenlu wang', 'ye wang', 'honggang zhao', 'simone sciabola'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.08749'},\n",
       " {'title': 'rethinking trajectory prediction via \"team game\"',\n",
       "  'id': '2210.08793',\n",
       "  'abstract': 'to accurately predict trajectories in multi-agent settings, e.g. team games, it is important to effectively model the interactions among agents. whereas a number of methods have been developed for this purpose, existing methods implicitly model these interactions as part of the deep net architecture. however, in the real world, interactions often exist at multiple levels, e.g. individuals may form groups, where interactions among groups and those among the individuals in the same group often follow significantly different patterns. in this paper, we present a novel formulation for multi-agent trajectory prediction, which explicitly introduces the concept of interactive group consensus via an interactive hierarchical latent space. this formulation allows group-level and individual-level interactions to be captured jointly, thus substantially improving the capability of modeling complex dynamics. on two multi-agent settings, i.e. team sports and pedestrians, the proposed framework consistently achieves superior performance compared to existing methods.',\n",
       "  'categories': 'cs.cv cs.lg cs.ma',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-17',\n",
       "  'updated': '',\n",
       "  'authors': ['zikai wei', 'xinge zhu', 'bo dai', 'dahua lin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.08793'},\n",
       " {'title': 'probabilistic categorical adversarial attack & adversarial training',\n",
       "  'id': '2210.09364',\n",
       "  'abstract': 'the existence of adversarial examples brings huge concern for people to apply deep neural networks (dnns) in safety-critical tasks. however, how to generate adversarial examples with categorical data is an important problem but lack of extensive exploration. previously established methods leverage greedy search method, which can be very time-consuming to conduct successful attack. this also limits the development of adversarial training and potential defenses for categorical data. to tackle this problem, we propose probabilistic categorical adversarial attack (pcaa), which transfers the discrete optimization problem to a continuous problem that can be solved efficiently by projected gradient descent. in our paper, we theoretically analyze its optimality and time complexity to demonstrate its significant advantage over current greedy based attacks. moreover, based on our attack, we propose an efficient adversarial training framework. through a comprehensive empirical study, we justify the effectiveness of our proposed attack and defense algorithms.',\n",
       "  'categories': 'cs.lg cs.cr',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-17',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['pengfei he',\n",
       "   'han xu',\n",
       "   'jie ren',\n",
       "   'yuxuan wan',\n",
       "   'zitao liu',\n",
       "   'jiliang tang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.09364'},\n",
       " {'title': 'ceip: combining explicit and implicit priors for reinforcement learning   with demonstrations',\n",
       "  'id': '2210.09496',\n",
       "  'abstract': 'although reinforcement learning has found widespread use in dense reward settings, training autonomous agents with sparse rewards remains challenging. to address this difficulty, prior work has shown promising results when using not only task-specific demonstrations but also task-agnostic albeit somewhat related demonstrations. in most cases, the available demonstrations are distilled into an implicit prior, commonly represented via a single deep net. explicit priors in the form of a database that can be queried have also been shown to lead to encouraging results. to better benefit from available demonstrations, we develop a method to combine explicit and implicit priors (ceip). ceip exploits multiple implicit priors in the form of normalizing flows in parallel to form a single complex prior. moreover, ceip uses an effective explicit retrieval and push-forward mechanism to condition the implicit priors. in three challenging environments, we find the proposed ceip method to improve upon sophisticated state-of-the-art techniques.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-17',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['kai yan', 'alexander g. schwing', 'yu-xiong wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.09496'},\n",
       " {'title': 'generalized many-body dispersion correction through random-phase   approximation for chemically accurate density functional theory',\n",
       "  'id': '2210.09784',\n",
       "  'abstract': 'we extend our recently proposed deep learning-aided many-body dispersion (dnn-mbd) model to quadrupole polarizability (q) terms using a generalized random phase approximation (rpa) formalism enabling to include van der waals contributions beyond dipole. the resulting dnn-mbdq model only relies on ab initio-derived quantities as the introduced quadrupole polarizabilities are recursively retrieved from dipole ones, in turn modelled via the tkatchenko-scheffler method. a transferable and efficient deep-neuronal network (dnn) provides atom in molecule volumes, while a single range-separation parameter is used to couple the model to density functional theory (dft). since it can be computed at negligible cost, the dnn-mbdq approach can be coupled with dft functionals such as pbe/pbe0 or b86bpbe(dispersionless). dnn-mbq-pbe/pbe0 reaches chemical accuracy exhibiting superior accuracy compared to other dispersion-corrected models, especially at near-equilibrium ranges where errors are lowered by nearly 25% compared to our dipole-only approach while gains reach nearly 50% compared to other corrected schemes.',\n",
       "  'categories': 'physics.chem-ph cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-18',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['pier paolo poier', 'louis lagardère', 'jean-philip piquemal'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.09784'},\n",
       " {'title': 'near real-time co$_2$ emissions based on carbon satellite and artificial   intelligence',\n",
       "  'id': '2210.09850',\n",
       "  'abstract': 'to limit global warming to pre-industrial levels, global governments, industry and academia are taking aggressive efforts to reduce carbon emissions. the evaluation of anthropogenic carbon dioxide (co$_2$) emissions, however, depends on the self-reporting information that is not always reliable. society need to develop an objective, independent, and generalized system to meter co$_2$ emissions. satellite co$_2$ observation from space that reports column-average regional co$_2$ dry-air mole fractions has gradually indicated its potential to build such a system. nevertheless, estimating anthropogenic co$_2$ emissions from co$_2$ observing satellite is bottlenecked by the influence of the highly complicated physical characteristics of atmospheric activities. here we provide the first method that combines the advanced artificial intelligence (ai) techniques and the carbon satellite monitor to quantify anthropogenic co$_2$ emissions. we propose an integral ai based pipeline that contains both a data retrieval algorithm and a two-step data-driven solution. first, the data retrieval algorithm can generate effective datasets from multi-modal data including carbon satellite, the information of carbon sources, and several environmental factors. second, the two-step data-driven solution that applies the powerful representation of deep learning techniques to learn to quantify anthropogenic co$_2$ emissions from satellite co$_2$ observation with other factors. our work unmasks the potential of quantifying co$_2$ emissions based on the combination of deep learning algorithms and the carbon satellite monitor.',\n",
       "  'categories': 'physics.ao-ph cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-11',\n",
       "  'updated': '2022-10-22',\n",
       "  'authors': ['zhengwen zhang',\n",
       "   'jinjin gu',\n",
       "   'junhua zhao',\n",
       "   'jianwei huang',\n",
       "   'haifeng wu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.09850'},\n",
       " {'title': 'mass: multi-attribute selective suppression',\n",
       "  'id': '2210.09904',\n",
       "  'abstract': \"the recent rapid advances in machine learning technologies largely depend on the vast richness of data available today, in terms of both the quantity and the rich content contained within. for example, biometric data such as images and voices could reveal people's attributes like age, gender, sentiment, and origin, whereas location/motion data could be used to infer people's activity levels, transportation modes, and life habits. along with the new services and applications enabled by such technological advances, various governmental policies are put in place to regulate such data usage and protect people's privacy and rights. as a result, data owners often opt for simple data obfuscation (e.g., blur people's faces in images) or withholding data altogether, which leads to severe data quality degradation and greatly limits the data's potential utility.   aiming for a sophisticated mechanism which gives data owners fine-grained control while retaining the maximal degree of data utility, we propose multi-attribute selective suppression, or mass, a general framework for performing precisely targeted data surgery to simultaneously suppress any selected set of attributes while preserving the rest for downstream machine learning tasks. mass learns a data modifier through adversarial games between two sets of networks, where one is aimed at suppressing selected attributes, and the other ensures the retention of the rest of the attributes via general contrastive loss as well as explicit classification metrics. we carried out an extensive evaluation of our proposed method using multiple datasets from different domains including facial images, voice audio, and video clips, and obtained promising results in mass' generalizability and capability of suppressing targeted attributes without negatively affecting the data's usability in other downstream ml tasks.\",\n",
       "  'categories': 'cs.lg cs.cr cs.cy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-18',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['chun-fu chen',\n",
       "   'shaohan hu',\n",
       "   'zhonghao shi',\n",
       "   'prateek gulati',\n",
       "   'bill moriarty',\n",
       "   'marco pistoia',\n",
       "   'vincenzo piuri',\n",
       "   'pierangela samarati'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.09904'},\n",
       " {'title': 'graph attention networks unveil determinants of intra- and inter-city   health disparity',\n",
       "  'id': '2210.10142',\n",
       "  'abstract': 'understanding the determinants underlying variations in urban health status is important for informing urban design and planning, as well as public health policies. multiple heterogeneous urban features could modulate the prevalence of diseases across different neighborhoods in cities and across different cities. this study examines heterogeneous features related to socio-demographics, population activity, mobility, and the built environment and their non-linear interactions to examine intra- and inter-city disparity in prevalence of four disease types: obesity, diabetes, cancer, and heart disease. features related to population activity, mobility, and facility density are obtained from large-scale anonymized mobility data. these features are used in training and testing graph attention network (gat) models to capture non-linear feature interactions as well as spatial interdependence among neighborhoods. we tested the models in five u.s. cities across the four disease types. the results show that the gat model can predict the health status of people in neighborhoods based on the top five determinant features. the findings unveil that population activity and built-environment features along with socio-demographic features differentiate the health status of neighborhoods to such a great extent that a gat model could predict the health status using these features with high accuracy. the results also show that the model trained on one city can predict health status in another city with high accuracy, allowing us to quantify the inter-city similarity and discrepancy in health status. the model and findings provide novel approaches and insights for urban designers, planners, and public health officials to better understand and improve health disparities in cities by considering the significant determinant features and their interactions.',\n",
       "  'categories': 'cs.lg cs.cy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-18',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['chenyue liu', 'chao fan', 'ali mostafavi'],\n",
       "  'affiliation': ['urban resilience.ai lab, zachry department of civil and environmental engineering, texas a&m university, college station, united states',\n",
       "   'glenn department of civil engineering, clemson university, clemson, south carolina, united states',\n",
       "   'urban resilience.ai lab, zachry department of civil and environmental engineering, texas a&m university, college station, united states'],\n",
       "  'url': 'https://arxiv.org/abs/2210.10142'},\n",
       " {'title': 'language model decomposition: quantifying the dependency and correlation   of language models',\n",
       "  'id': '2210.10289',\n",
       "  'abstract': 'pre-trained language models (lms), such as bert (devlin et al., 2018) and its variants, have led to significant improvements on various nlp tasks in past years. however, a theoretical framework for studying their relationships is still missing. in this paper, we fill this gap by investigating the linear dependency between pre-trained lms. the linear dependency of lms is defined analogously to the linear dependency of vectors. we propose language model decomposition (lmd) to represent a lm using a linear combination of other lms as basis, and derive the closed-form solution. a goodness-of-fit metric for lmd similar to the coefficient of determination is defined and used to measure the linear dependency of a set of lms. in experiments, we find that bert and eleven (11) bert-like lms are 91% linearly dependent. this observation suggests that current state-of-the-art (sota) lms are highly \"correlated\". to further advance sota we need more diverse and novel lms that are less dependent on existing lms.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['hao zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.10289'},\n",
       " {'title': 'attribution and obfuscation of neural text authorship: a data mining   perspective',\n",
       "  'id': '2210.10488',\n",
       "  'abstract': 'two interlocking research questions of growing interest and importance in privacy research are authorship attribution (aa) and authorship obfuscation (ao). given an artifact, especially a text t in question, an aa solution aims to accurately attribute t to its true author out of many candidate authors while an ao solution aims to modify t to hide its true authorship. traditionally, the notion of authorship and its accompanying privacy concern is only toward human authors. however, in recent years, due to the explosive advancements in neural text generation (ntg) techniques in nlp, capable of synthesizing human-quality open-ended texts (so-called \"neural texts\"), one has to now consider authorships by humans, machines, or their combination. due to the implications and potential threats of neural texts when used maliciously, it has become critical to understand the limitations of traditional aa/ao solutions and develop novel aa/ao solutions in dealing with neural texts. in this survey, therefore, we make a comprehensive review of recent literature on the attribution and obfuscation of neural text authorship from a data mining perspective, and share our view on their limitations and promising research directions.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '2022-10-23',\n",
       "  'authors': ['adaku uchendu', 'thai le', 'dongwon lee'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.10488'},\n",
       " {'title': 'a baseline revisited: pushing the limits of multi-segment models for   context-aware translation',\n",
       "  'id': '2210.10906',\n",
       "  'abstract': 'this paper addresses the task of contextual translation using multi-segment models. specifically we show that increasing model capacity further pushes the limits of this approach and that deeper models are more suited to capture context dependencies. furthermore, improvements observed with larger models can be transferred to smaller models using knowledge distillation. our experiments show that this approach achieves competitive performance across several languages and benchmarks, without additional language-specific tuning and task specific architectures.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['suvodeep majumder',\n",
       "   'stanislas lauly',\n",
       "   'maria nadejde',\n",
       "   'marcello federico',\n",
       "   'georgiana dinu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.10906'},\n",
       " {'title': 'palm up: playing in the latent manifold for unsupervised pretraining',\n",
       "  'id': '2210.10913',\n",
       "  'abstract': 'large and diverse datasets have been the cornerstones of many impressive advancements in artificial intelligence. intelligent creatures, however, learn by interacting with the environment, which changes the input sensory signals and the state of the environment. in this work, we aim to bring the best of both worlds and propose an algorithm that exhibits an exploratory behavior whilst it utilizes large diverse datasets. our key idea is to leverage deep generative models that are pretrained on static datasets and introduce a dynamic model in the latent space. the transition dynamics simply mixes an action and a random sampled latent. it then applies an exponential moving average for temporal persistency, the resulting latent is decoded to image using pretrained generator. we then employ an unsupervised reinforcement learning algorithm to explore in this environment and perform unsupervised representation learning on the collected data. we further leverage the temporal information of this data to pair data points as a natural supervision for representation learning. our experiments suggest that the learned representations can be successfully transferred to downstream tasks in both vision and reinforcement learning domains.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['hao liu', 'tom zahavy', 'volodymyr mnih', 'satinder singh'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.10913'},\n",
       " {'title': 'dot-vae: disentangling one factor at a time',\n",
       "  'id': '2210.10920',\n",
       "  'abstract': 'as we enter the era of machine learning characterized by an overabundance of data, discovery, organization, and interpretation of the data in an unsupervised manner becomes a critical need. one promising approach to this endeavour is the problem of disentanglement, which aims at learning the underlying generative latent factors, called the factors of variation, of the data and encoding them in disjoint latent representations. recent advances have made efforts to solve this problem for synthetic datasets generated by a fixed set of independent factors of variation. here, we propose to extend this to real-world datasets with a countable number of factors of variations. we propose a novel framework which augments the latent space of a variational autoencoders with a disentangled space and is trained using a wake-sleep-inspired two-step algorithm for unsupervised disentanglement. our network learns to disentangle interpretable, independent factors from the data ``one at a time\", and encode it in different dimensions of the disentangled latent space, while making no prior assumptions about the number of factors or their joint distribution. we demonstrate its quantitative and qualitative effectiveness by evaluating the latent representations learned on two synthetic benchmark datasets; dsprites and 3dshapes and on a real datasets celeba.',\n",
       "  'categories': 'cs.lg cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['vaishnavi patil', 'matthew evanusa', 'joseph jaja'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.10920'},\n",
       " {'title': 'discovering many diverse solutions with bayesian optimization',\n",
       "  'id': '2210.10953',\n",
       "  'abstract': 'bayesian optimization (bo) is a popular approach for sample-efficient optimization of black-box objective functions. while bo has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective bo only seek to find a single best solution. this can be a significant limitation in situations where solutions may later turn out to be intractable. for example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. to address this issue, we propose rank-ordered bayesian optimization with trust-regions (robot) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. we evaluate robot on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['natalie maus', 'kaiwen wu', 'david eriksson', 'jacob gardner'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.10953'},\n",
       " {'title': 'removing grid structure in angle-resolved photoemission spectra via deep   learning method',\n",
       "  'id': '2210.11200',\n",
       "  'abstract': 'spectroscopic data may often contain unwanted extrinsic signals. for example, in arpes experiment, a wire mesh is typically placed in front of the ccd to block stray photo-electrons, but could cause a grid-like structure in the spectra during quick measurement mode. in the past, this structure was often removed using the mathematical fourier filtering method by erasing the periodic structure. however, this method may lead to information loss and vacancies in the spectra because the grid structure is not strictly linearly superimposed. here, we propose a deep learning method to effectively overcome this problem. our method takes advantage of the self-correlation information within the spectra themselves and can greatly optimize the quality of the spectra while removing the grid structure and noise simultaneously. it has the potential to be extended to all spectroscopic measurements to eliminate other extrinsic signals and enhance the spectral quality based on the self-correlation of the spectra solely.',\n",
       "  'categories': 'cond-mat.mtrl-sci cs.lg physics.data-an',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['junde liu', 'dongchen huang', 'yi-feng yang', 'tian qian'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11200'},\n",
       " {'title': 'interpretable machine learning for detection and classification of   ransomware families based on api calls',\n",
       "  'id': '2210.11235',\n",
       "  'abstract': \"ransomware has appeared as one of the major global threats in recent days. the alarming increasing rate of ransomware attacks and new ransomware variants intrigue the researchers to constantly examine the distinguishing traits of ransomware and refine their detection strategies. application programming interface (api) is a way for one program to collaborate with another; api calls are the medium by which they communicate. ransomware uses this strategy to interact with the os and makes a significantly higher number of calls in different sequences to ask for taking action. this research work utilizes the frequencies of different api calls to detect and classify ransomware families. first, a web-crawler is developed to automate collecting the windows portable executable (pe) files of 15 different ransomware families. by extracting different frequencies of 68 api calls, we develop our dataset in the first phase of the two-phase feature engineering process. after selecting the most significant features in the second phase of the feature engineering process, we deploy six supervised machine learning models: naive bayes, logistic regression, random forest, stochastic gradient descent, k-nearest neighbor, and support vector machine. then, the performances of all the classifiers are compared to select the best model. the results reveal that logistic regression can efficiently classify ransomware into their corresponding families securing 99.15% accuracy. finally, instead of relying on the 'black box' characteristic of the machine learning models, we present the interpretability of our best-performing model using shap values to ascertain the transparency and trustworthiness of the model's prediction.\",\n",
       "  'categories': 'cs.cr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-16',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['rawshan ara mowri', 'madhuri siddula', 'kaushik roy'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11235'},\n",
       " {'title': 'accurate extrinsic prediction of physical systems using transformers',\n",
       "  'id': '2210.11269',\n",
       "  'abstract': 'accurate high-altitude wind forecasting is important for air traffic control. and the large volume of data available for this task makes deep neural network-based models a possibility. however, special methods are required because the data is measured only sparsely: along the main aircraft trajectories and arranged sparsely in space, namely along the main air corridors. several deep learning approaches have been proposed, and in this work, we show that transformers can fit this data efficiently and are able to extrapolate coherently from a context set.   we show this by an extensive comparison of transformers to numerous existing deep learning-based baselines in the literature. besides high-altitude wind forecasting, we compare competing models on other dynamical physical systems, namely those modelled by partial differential equations, in particular the poisson equation and darcy flow equation. for these experiments, in the case where the data is arranged non-regularly in space, transformers outperform all the other evaluated methods. we also compared them in a more standard setup where the data is arranged on a grid and show that the transformers are competitive with state-of-the-art methods, even though it does not require regular spacing. the code and datasets of the different experiments will be made publicly available at publication time.',\n",
       "  'categories': 'cs.lg physics.ao-ph physics.flu-dyn',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['arnaud pannatier', 'kyle matoba', 'françois fleuret'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11269'},\n",
       " {'title': 'dynamic selection of p-norm in linear adaptive filtering via online   kernel-based reinforcement learning',\n",
       "  'id': '2210.11317',\n",
       "  'abstract': \"this study addresses the problem of selecting dynamically, at each time instance, the ``optimal'' p-norm to combat outliers in linear adaptive filtering without any knowledge on the potentially time-varying probability distribution function of the outliers. to this end, an online and data-driven framework is designed via kernel-based reinforcement learning (kbrl). novel bellman mappings on reproducing kernel hilbert spaces (rkhss) are introduced that need no knowledge on transition probabilities of markov decision processes, and are nonexpansive with respect to the underlying hilbertian norm. an approximate policy-iteration framework is finally offered via the introduction of a finite-dimensional affine superset of the fixed-point set of the proposed bellman mappings. the well-known ``curse of dimensionality'' in rkhss is addressed by building a basis of vectors via an approximate linear dependency criterion. numerical tests on synthetic data demonstrate that the proposed framework selects always the ``optimal'' p-norm for the outlier scenario at hand, outperforming at the same time several non-rl and kbrl schemes.\",\n",
       "  'categories': 'eess.sp cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '2022-10-20',\n",
       "  'authors': ['minh vu', 'yuki akiyama', 'konstantinos slavakis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11317'},\n",
       " {'title': 'deep conditional transformation models for survival analysis',\n",
       "  'id': '2210.11366',\n",
       "  'abstract': 'an every increasing number of clinical trials features a time-to-event outcome and records non-tabular patient data, such as magnetic resonance imaging or text data in the form of electronic health records. recently, several neural-network based solutions have been proposed, some of which are binary classifiers. parametric, distribution-free approaches which make full use of survival time and censoring status have not received much attention. we present deep conditional transformation models (dctms) for survival outcomes as a unifying approach to parametric and semiparametric survival analysis. dctms allow the specification of non-linear and non-proportional hazards for both tabular and non-tabular data and extend to all types of censoring and truncation. on real and semi-synthetic data, we show that dctms compete with state-of-the-art dl approaches to survival analysis.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['gabriele campanella',\n",
       "   'lucas kook',\n",
       "   'ida häggström',\n",
       "   'torsten hothorn',\n",
       "   'thomas j. fuchs'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11366'},\n",
       " {'title': 'scaling instruction-finetuned language models',\n",
       "  'id': '2210.11416',\n",
       "  'abstract': 'finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. in this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. we find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (palm, t5, u-palm), prompting setups (zero-shot, few-shot, cot), and evaluation benchmarks (mmlu, bbh, tydiqa, mgsm, open-ended generation). for instance, flan-palm 540b instruction-finetuned on 1.8k tasks outperforms palm 540b by a large margin (+9.4% on average). flan-palm 540b achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot mmlu. we also publicly release flan-t5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as palm 62b. overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.',\n",
       "  'categories': 'cs.lg cs.cl',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '2022-10-21',\n",
       "  'authors': ['hyung won chung',\n",
       "   'le hou',\n",
       "   'shayne longpre',\n",
       "   'barret zoph',\n",
       "   'yi tay',\n",
       "   'william fedus',\n",
       "   'eric li',\n",
       "   'xuezhi wang',\n",
       "   'mostafa dehghani',\n",
       "   'siddhartha brahma',\n",
       "   'albert webson',\n",
       "   'shixiang shane gu',\n",
       "   'zhuyun dai',\n",
       "   'mirac suzgun',\n",
       "   'xinyun chen',\n",
       "   'aakanksha chowdhery',\n",
       "   'sharan narang',\n",
       "   'gaurav mishra',\n",
       "   'adams yu',\n",
       "   'vincent zhao',\n",
       "   'yanping huang',\n",
       "   'andrew dai',\n",
       "   'hongkun yu',\n",
       "   'slav petrov',\n",
       "   'ed h. chi',\n",
       "   'jeff dean',\n",
       "   'jacob devlin',\n",
       "   'adam roberts',\n",
       "   'denny zhou',\n",
       "   'quoc v. le',\n",
       "   'jason wei'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11416'},\n",
       " {'title': 'improving aircraft performance using machine learning: a review',\n",
       "  'id': '2210.11481',\n",
       "  'abstract': 'this review covers the new developments in machine learning (ml) that are impacting the multi-disciplinary area of aerospace engineering, including fundamental fluid dynamics (experimental and numerical), aerodynamics, acoustics, combustion and structural health monitoring. we review the state of the art, gathering the advantages and challenges of ml methods across different aerospace disciplines and provide our view on future opportunities. the basic concepts and the most relevant strategies for ml are presented together with the most relevant applications in aerospace engineering, revealing that ml is improving aircraft performance and that these techniques will have a large impact in the near future.',\n",
       "  'categories': 'cs.lg physics.data-an physics.flu-dyn',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['soledad le clainche',\n",
       "   'esteban ferrer',\n",
       "   'sam gibson',\n",
       "   'elisabeth cross',\n",
       "   'alessandro parente',\n",
       "   'ricardo vinuesa'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11481'},\n",
       " {'title': 'a methodology for the prediction of drug target interaction using cdk   descriptors',\n",
       "  'id': '2210.11482',\n",
       "  'abstract': 'detecting probable drug target interaction (dti) is a critical task in drug discovery. conventional dti studies are expensive, labor-intensive, and take a lot of time, hence there are significant reasons to construct useful computational techniques that may successfully anticipate possible dtis. although certain methods have been developed for this cause, numerous interactions are yet to be discovered, and prediction accuracy is still low. to meet these challenges, we propose a dti prediction model built on molecular structure of drugs and sequence of target proteins. in the proposed model, we use simplified molecular input line entry system (smiles) to create cdk descriptors, molecular access system (maccs) fingerprints, electrotopological state (estate) fingerprints and amino acid sequences of targets to get pseudo amino acid composition (pseaac). we target to evaluate performance of dti prediction models using cdk descriptors. for comparison, we use benchmark data and evaluate models performance on two widely used fingerprints, maccs fingerprints and estate fingerprints. the evaluation of performances shows that cdk descriptors are superior at predicting dtis. the proposed method also outperforms other previously published techniques significantly.',\n",
       "  'categories': 'q-bio.qm cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['tanya liyaqat', 'tanvir ahmad', 'chandni saxena'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11482'},\n",
       " {'title': 'machine-learning compression for particle physics discoveries',\n",
       "  'id': '2210.11489',\n",
       "  'abstract': 'in collider-based particle and nuclear physics experiments, data are produced at such extreme rates that only a subset can be recorded for later analysis. typically, algorithms select individual collision events for preservation and store the complete experimental response. a relatively new alternative strategy is to additionally save a partial record for a larger subset of events, allowing for later specific analysis of a larger fraction of events. we propose a strategy that bridges these paradigms by compressing entire events for generic offline analysis but at a lower fidelity. an optimal-transport-based $\\\\beta$ variational autoencoder (vae) is used to automate the compression and the hyperparameter $\\\\beta$ controls the compression fidelity. we introduce a new approach for multi-objective learning functions by simultaneously learning a vae appropriate for all values of $\\\\beta$ through parameterization. we present an example use case, a di-muon resonance search at the large hadron collider (lhc), where we show that simulated data compressed by our $\\\\beta$-vae has enough fidelity to distinguish distinct signal morphologies.',\n",
       "  'categories': 'hep-ph cs.lg hep-ex physics.data-an',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['jack h. collins',\n",
       "   'yifeng huang',\n",
       "   'simon knapen',\n",
       "   'benjamin nachman',\n",
       "   'daniel whiteson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11489'},\n",
       " {'title': 'multimodal neural network for demand forecasting',\n",
       "  'id': '2210.11502',\n",
       "  'abstract': 'demand forecasting applications have immensely benefited from the state-of-the-art deep learning methods used for time series forecasting. traditional uni-modal models are predominantly seasonality driven which attempt to model the demand as a function of historic sales along with information on holidays and promotional events. however, accurate and robust sales forecasting calls for accommodating multiple other factors, such as natural calamities, pandemics, elections, etc., impacting the demand for products and product categories in general. we propose a multi-modal sales forecasting network that combines real-life events from news articles with traditional data such as historical sales and holiday information. further, we fuse information from general product trends published by google trends. empirical results show statistically significant improvements in the smape error metric with an average improvement of 7.37% against the existing state-of-the-art sales forecasting techniques on a real-world supermarket dataset.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['nitesh kumar',\n",
       "   'kumar dheenadayalan',\n",
       "   'suprabath reddy',\n",
       "   'sumant kulkarni'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11502'},\n",
       " {'title': 'overexposure mask fusion: generalizable reverse isp multi-step   refinement',\n",
       "  'id': '2210.11511',\n",
       "  'abstract': \"with the advent of deep learning methods replacing the isp in transforming sensor raw readings into rgb images, numerous methodologies solidified into real-life applications. equally potent is the task of inverting this process which will have applications in enhancing computational photography tasks that are conducted in the raw domain, addressing lack of available raw data while reaping from the benefits of performing tasks directly on sensor readings. this paper's proposed methodology is a state-of-the-art solution to the task of raw reconstruction, and the multi-step refinement process integrating an overexposure mask is novel in three ways: instead of from rgb to bayer, the pipeline trains from rgb to demosaiced raw allowing use of perceptual loss functions; the multi-step processes has greatly enhanced the performance of the baseline u-net from start to end; the pipeline is a generalizable process of refinement that can enhance other high performance methodologies that support end-to-end learning.\",\n",
       "  'categories': 'cs.cv cs.lg eess.iv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['jinha kim', 'jun jiang', 'jinwei gu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11511'},\n",
       " {'title': 'learning sample reweighting for accuracy and adversarial robustness',\n",
       "  'id': '2210.11513',\n",
       "  'abstract': \"there has been great interest in enhancing the robustness of neural network classifiers to defend against adversarial perturbations through adversarial training, while balancing the trade-off between robust accuracy and standard accuracy. we propose a novel adversarial training framework that learns to reweight the loss associated with individual training samples based on a notion of class-conditioned margin, with the goal of improving robust generalization. we formulate weighted adversarial training as a bilevel optimization problem with the upper-level problem corresponding to learning a robust classifier, and the lower-level problem corresponding to learning a parametric function that maps from a sample's \\\\textit{multi-class margin} to an importance weight. extensive experiments demonstrate that our approach consistently improves both clean and robust accuracy compared to related methods and state-of-the-art baselines.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['chester holtz', 'tsui-wei weng', 'gal mishne'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11513'},\n",
       " {'title': 'composing ensembles of pre-trained models via iterative consensus',\n",
       "  'id': '2210.11522',\n",
       "  'abstract': 'large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. language models such as gpt-3 are capable of textual reasoning but cannot understand visual information, while vision models such as dall-e can generate photorealistic photos but fail to understand complex language descriptions. in this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. we use pre-trained models as \"generators\" or \"scorers\" and compose them via closed-loop iterative consensus optimization. the generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. we demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. project page: https://energy-based-model.github.io/composing-pretrained-models.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['shuang li',\n",
       "   'yilun du',\n",
       "   'joshua b. tenenbaum',\n",
       "   'antonio torralba',\n",
       "   'igor mordatch'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11522'},\n",
       " {'title': 'theoretical analysis of deep neural networks for temporally dependent   observations',\n",
       "  'id': '2210.11530',\n",
       "  'abstract': 'deep neural networks are powerful tools to model observations over time with non-linear patterns. despite the widespread use of neural networks in such settings, most theoretical developments of deep neural networks are under the assumption of independent observations, and theoretical results for temporally dependent observations are scarce. to bridge this gap, we study theoretical properties of deep neural networks on modeling non-linear time series data. specifically, non-asymptotic bounds for prediction error of (sparse) feed-forward neural network with relu activation function is established under mixing-type assumptions. these assumptions are mild such that they include a wide range of time series models including auto-regressive models. compared to independent observations, established convergence rates have additional logarithmic factors to compensate for additional complexity due to dependence among data points. the theoretical results are supported via various numerical simulation settings as well as an application to a macroeconomic data set.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['mingliang ma', 'abolfazl safikhani'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11530'},\n",
       " {'title': 'dnn-forwardtesting: a new trading strategy validation using statistical   timeseries analysis and deep neural networks',\n",
       "  'id': '2210.11532',\n",
       "  'abstract': 'in general, traders test their trading strategies by applying them on the historical market data (backtesting), and then apply to the future trades the strategy that achieved the maximum profit on such past data.   in this paper, we propose a new trading strategy, called dnn-forwardtesting, that determines the strategy to apply by testing it on the possible future predicted by a deep neural network that has been designed to perform stock price forecasts and trained with the market historical data.   in order to generate such an historical dataset, we first perform an exploratory data analysis on a set of ten securities and, in particular, analize their volatility through a novel k-means-based procedure. then, we restrict the dataset to a small number of assets with the same volatility coefficient and use such data to train a deep feed-forward neural network that forecasts the prices for the next 30 days of open stocks market. finally, our trading system calculates the most effective technical indicator by applying it to the dnns predictions and uses such indicator to guide its trades.   the results confirm that neural networks outperform classical statistical techniques when performing such forecasts, and their predictions allow to select a trading strategy that, when applied to the real future, increases expectancy, sharpe, sortino, and calmar ratios with respect to the strategy selected through traditional backtesting.',\n",
       "  'categories': 'q-fin.tr cs.ce cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['ivan letteri',\n",
       "   'giuseppe della penna',\n",
       "   'giovanni de gasperis',\n",
       "   'abeer dyoub'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11532'},\n",
       " {'title': 'an improved algorithm for clustered federated learning',\n",
       "  'id': '2210.11538',\n",
       "  'abstract': 'in this paper, we address the dichotomy between heterogeneous models and simultaneous training in federated learning (fl) via a clustering framework. we define a new clustering model for fl based on the (optimal) local models of the users: two users belong to the same cluster if their local models are close; otherwise they belong to different clusters. a standard algorithm for clustered fl is proposed in \\\\cite{ghosh_efficient_2021}, called \\\\texttt{ifca}, which requires \\\\emph{suitable} initialization and the knowledge of hyper-parameters like the number of clusters (which is often quite difficult to obtain in practical applications) to converge. we propose an improved algorithm, \\\\emph{successive refine federated clustering algorithm} (\\\\texttt{sr-fca}), which removes such restrictive assumptions. \\\\texttt{sr-fca} treats each user as a singleton cluster as an initialization, and then successively refine the cluster estimation via exploiting similar users belonging to the same cluster. in any intermediate step, \\\\texttt{sr-fca} uses a robust federated learning algorithm within each cluster to exploit simultaneous training and to correct clustering errors. furthermore, \\\\texttt{sr-fca} does not require any \\\\emph{good} initialization (warm start), both in theory and practice. we show that with proper choice of learning rate, \\\\texttt{sr-fca} incurs arbitrarily small clustering error. additionally, we validate the performance of our algorithm on standard fl datasets in non-convex problems like neural nets, and we show the benefits of \\\\texttt{sr-fca} over baselines.',\n",
       "  'categories': 'stat.ml cs.dc cs.it cs.lg math.it',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['n/a harshvardhan', 'avishek ghosh', 'arya mazumdar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11538'},\n",
       " {'title': 'transferring learned patterns from ground-based field imagery to predict   uav-based imagery for crop and weed semantic segmentation in precision crop   farming',\n",
       "  'id': '2210.11545',\n",
       "  'abstract': 'weed and crop segmentation is becoming an increasingly integral part of precision farming that leverages the current computer vision and deep learning technologies. research has been extensively carried out based on images captured with a camera from various platforms. unmanned aerial vehicles (uavs) and ground-based vehicles including agricultural robots are the two popular platforms for data collection in fields. they all contribute to site-specific weed management (sswm) to maintain crop yield. currently, the data from these two platforms is processed separately, though sharing the same semantic objects (weed and crop). in our paper, we have developed a deep convolutional network that enables to predict both field and aerial images from uavs for weed segmentation and mapping with only field images provided in the training phase. the network learning process is visualized by feature maps at shallow and deep layers. the results show that the mean intersection of union (iou) values of the segmentation for the crop (maize), weeds, and soil background in the developed model for the field dataset are 0.744, 0.577, 0.979, respectively, and the performance of aerial images from an uav with the same model, the iou values of the segmentation for the crop (maize), weeds and soil background are 0.596, 0.407, and 0.875, respectively. to estimate the effect on the use of plant protection agents, we quantify the relationship between herbicide spraying saving rate and grid size (spraying resolution) based on the predicted weed map. the spraying saving rate is up to 90% when the spraying resolution is at 1.78 x 1.78 cm2. the study shows that the developed deep convolutional neural network could be used to classify weeds from both field and aerial images and delivers satisfactory results.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['junfeng gao',\n",
       "   'wenzhi liao',\n",
       "   'david nuyttens',\n",
       "   'peter lootens',\n",
       "   'erik alexandersson',\n",
       "   'jan pieters'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11545'},\n",
       " {'title': 'low-rank representations towards classification problem of complex   networks',\n",
       "  'id': '2210.11561',\n",
       "  'abstract': 'complex networks representing social interactions, brain activities, molecular structures have been studied widely to be able to understand and predict their characteristics as graphs. models and algorithms for these networks are used in real-life applications, such as search engines, and recommender systems. in general, such networks are modelled by constructing a low-dimensional euclidean embedding of the vertices of the network, where proximity of the vertices in the euclidean space hints the likelihood of an edge (link). in this work, we study the performance of such low-rank representations of real-life networks on a network classification problem.',\n",
       "  'categories': 'cs.si cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['murat çelik', 'ali baran taşdemir', 'lale özkahya'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11561'},\n",
       " {'title': 'local sgd in overparameterized linear regression',\n",
       "  'id': '2210.11562',\n",
       "  'abstract': 'we consider distributed learning using constant stepsize sgd (dsgd) over several devices, each sending a final model update to a central server. in a final step, the local estimates are aggregated. we prove in the setting of overparameterized linear regression general upper bounds with matching lower bounds and derive learning rates for specific data generating distributions. we show that the excess risk is of order of the variance provided the number of local nodes grows not too large with the global sample size. we further compare the sample complexity of dsgd with the sample complexity of distributed ridge regression (drr) and show that the excess sgd-risk is smaller than the excess rr-risk, where both sample complexities are of the same order.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['mike nguyen', 'charly kirst', 'nicole mücke'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11562'},\n",
       " {'title': 'rethinking learning approaches for long-term action anticipation',\n",
       "  'id': '2210.11566',\n",
       "  'abstract': 'action anticipation involves predicting future actions having observed the initial portion of a video. typically, the observed video is processed as a whole to obtain a video-level representation of the ongoing activity in the video, which is then used for future prediction. we introduce anticipatr which performs long-term action anticipation leveraging segment-level representations learned using individual segments from different activities, in addition to a video-level representation. we propose a two-stage learning approach to train a novel transformer-based model that uses these two types of representations to directly predict a set of future action instances over any given anticipation duration. results on breakfast, 50salads, epic-kitchens-55, and egtea gaze+ datasets demonstrate the effectiveness of our approach.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['megha nawhal', 'akash abdu jyothi', 'greg mori'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11566'},\n",
       " {'title': 'global convergence of direct policy search for state-feedback   $\\\\mathcal{h}_\\\\infty$ robust control: a revisit of nonsmooth synthesis with   goldstein subdifferential',\n",
       "  'id': '2210.11577',\n",
       "  'abstract': \"direct policy search has been widely applied in modern reinforcement learning and continuous control. however, the theoretical properties of direct policy search on nonsmooth robust control synthesis have not been fully understood. the optimal $\\\\mathcal{h}_\\\\infty$ control framework aims at designing a policy to minimize the closed-loop $\\\\mathcal{h}_\\\\infty$ norm, and is arguably the most fundamental robust control paradigm. in this work, we show that direct policy search is guaranteed to find the global solution of the robust $\\\\mathcal{h}_\\\\infty$ state-feedback control design problem. notice that policy search for optimal $\\\\mathcal{h}_\\\\infty$ control leads to a constrained nonconvex nonsmooth optimization problem, where the nonconvex feasible set consists of all the policies stabilizing the closed-loop dynamics. we show that for this nonsmooth optimization problem, all clarke stationary points are global minimum. next, we identify the coerciveness of the closed-loop $\\\\mathcal{h}_\\\\infty$ objective function, and prove that all the sublevel sets of the resultant policy search problem are compact. based on these properties, we show that goldstein's subgradient method and its implementable variants can be guaranteed to stay in the nonconvex feasible set and eventually find the global optimal solution of the $\\\\mathcal{h}_\\\\infty$ state-feedback synthesis problem. our work builds a new connection between nonconvex nonsmooth optimization theory and robust control, leading to an interesting global convergence result for direct policy search on optimal $\\\\mathcal{h}_\\\\infty$ synthesis.\",\n",
       "  'categories': 'math.oc cs.lg cs.sy eess.sy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['xingang guo', 'bin hu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11577'},\n",
       " {'title': 'model-based lifelong reinforcement learning with bayesian exploration',\n",
       "  'id': '2210.11579',\n",
       "  'abstract': 'we propose a model-based lifelong reinforcement-learning approach that estimates a hierarchical bayesian posterior distilling the common structure shared across different tasks. the learned posterior combined with a sample-based bayesian exploration procedure increases the sample efficiency of learning across a family of related tasks. we first derive an analysis of the relationship between the sample complexity and the initialization quality of the posterior in the finite mdp setting. we next scale the approach to continuous-state domains by introducing a variational bayesian lifelong reinforcement learning algorithm that can be combined with recent model-based deep rl methods, and that exhibits backward transfer. experimental results on several challenging domains show that our algorithms achieve both better forward and backward transfer performance than state-of-the-art lifelong rl methods.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['haotian fu',\n",
       "   'shangqun yu',\n",
       "   'michael littman',\n",
       "   'george konidaris'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11579'},\n",
       " {'title': 'monotonic risk relationships under distribution shifts for regularized   risk minimization',\n",
       "  'id': '2210.11589',\n",
       "  'abstract': 'machine learning systems are often applied to data that is drawn from a different distribution than the training distribution. recent work has shown that for a variety of classification and signal reconstruction problems, the out-of-distribution performance is strongly linearly correlated with the in-distribution performance. if this relationship or more generally a monotonic one holds, it has important consequences. for example, it allows to optimize performance on one distribution as a proxy for performance on the other. in this paper, we study conditions under which a monotonic relationship between the performances of a model on two distributions is expected. we prove an exact asymptotic linear relation for squared error and a monotonic relation for misclassification error for ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['daniel lejeune', 'jiayu liu', 'reinhard heckel'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11589'},\n",
       " {'title': 'xc: exploring quantitative use cases for explanations in 3d object   detection',\n",
       "  'id': '2210.11590',\n",
       "  'abstract': \"explainable ai (xai) methods are frequently applied to obtain qualitative insights about deep models' predictions. however, such insights need to be interpreted by a human observer to be useful. in this paper, we aim to use explanations directly to make decisions without human observers. we adopt two gradient-based explanation methods, integrated gradients (ig) and backprop, for the task of 3d object detection. then, we propose a set of quantitative measures, named explanation concentration (xc) scores, that can be used for downstream tasks. these scores quantify the concentration of attributions within the boundaries of detected objects. we evaluate the effectiveness of xc scores via the task of distinguishing true positive (tp) and false positive (fp) detected objects in the kitti and waymo datasets. the results demonstrate an improvement of more than 100\\\\% on both datasets compared to other heuristics such as random guesses and the number of lidar points in the bounding box, raising confidence in xc's potential for application in more use cases. our results also indicate that computationally expensive xai methods like ig may not be more valuable when used quantitatively compare to simpler methods.\",\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['sunsheng gu', 'vahdat abdelzad', 'krzysztof czarnecki'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11590'},\n",
       " {'title': 'gsuite: a flexible and framework independent benchmark suite for graph   neural network inference on gpus',\n",
       "  'id': '2210.11601',\n",
       "  'abstract': \"as the interest to graph neural networks (gnns) is growing, the importance of benchmarking and performance characterization studies of gnns is increasing. so far, we have seen many studies that investigate and present the performance and computational efficiency of gnns. however, the work done so far has been carried out using a few high-level gnn frameworks. although these frameworks provide ease of use, they contain too many dependencies to other existing libraries. the layers of implementation details and the dependencies complicate the performance analysis of gnn models that are built on top of these frameworks, especially while using architectural simulators. furthermore, different approaches on gnn computation are generally overlooked in prior characterization studies, and merely one of the common computational models is evaluated. based on these shortcomings and needs that we observed, we developed a benchmark suite that is framework independent, supporting versatile computational models, easily configurable and can be used with architectural simulators without additional effort.   our benchmark suite, which we call gsuite, makes use of only hardware vendor's libraries and therefore it is independent of any other frameworks. gsuite enables performing detailed performance characterization studies on gnn inference using both contemporary gpu profilers and architectural gpu simulators. to illustrate the benefits of our new benchmark suite, we perform a detailed characterization study with a set of well-known gnn models with various datasets; running gsuite both on a real gpu card and a timing-detailed gpu simulator. we also implicate the effect of computational models on performance. we use several evaluation metrics to rigorously measure the performance of gnn computation.\",\n",
       "  'categories': 'cs.lg cs.pf',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['taha tekdoğan', 'serkan göktaş', 'ayse yilmazer-metin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11601'},\n",
       " {'title': '3dall-e: integrating text-to-image ai in 3d design workflows',\n",
       "  'id': '2210.11603',\n",
       "  'abstract': 'text-to-image ai systems are capable of generating novel images for inspiration, but their applications for 3d design workflows and how designers can build 3d models using ai-provided inspiration is less understood. to investigate this, we integrated dall-e, gpt-3, and clip within a cad software in 3dall-e, a plugin that allows users to construct text and image prompts based on what they are modelling. in a study with 13 designers, we found that designers saw great potential to incorporate 3dall-e into their workflows and to use text-to-image ai for reference images, renders, materials, and design considerations. additionally, we elaborate on prompting patterns and provide measures of prompt complexity observed across participants. we conclude on a discussion of how 3dall-e can merge with existing generative design workflows and propose prompt bibliographies as a form of human-ai design history.',\n",
       "  'categories': 'cs.hc cs.ai cs.cy cs.lg cs.mm',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['vivian liu',\n",
       "   'jo vermeulen',\n",
       "   'george fitzmaurice',\n",
       "   'justin matejka'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11603'},\n",
       " {'title': 'horizon-free reinforcement learning for latent markov decision processes',\n",
       "  'id': '2210.11604',\n",
       "  'abstract': 'we study regret minimization for reinforcement learning (rl) in latent markov decision processes (lmdps) with context in hindsight. we design a novel model-based algorithmic framework which can be instantiated with both a model-optimistic and a value-optimistic solver. we prove an $\\\\widetilde{o}\\\\left(\\\\sqrt{m \\\\gamma s a k}\\\\right)$ regret bound where $m$ is the number of contexts, $s$ is the number of states, $a$ is the number of actions, $k$ is the number of episodes, and $\\\\gamma \\\\le s$ is the maximum transition degree of any state-action pair. the regret bound only scales logarithmically with the planning horizon, thus yielding the first (nearly) horizon-free regret bound for lmdp. key in our proof is an analysis of the total variance of alpha vectors, which is carefully bounded by a recursion-based technique. we complement our positive result with a novel $\\\\omega\\\\left(\\\\sqrt{m s a k}\\\\right)$ regret lower bound with $\\\\gamma = 2$, which shows our upper bound minimax optimal when $\\\\gamma$ is a constant. our lower bound relies on new constructions of hard instances and an argument based on the symmetrization technique from theoretical computer science, both of which are technically different from existing lower bound proof for mdps, and thus can be of independent interest.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['runlong zhou', 'ruosong wang', 'simon s. du'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11604'},\n",
       " {'title': 'generalized reciprocal perspective',\n",
       "  'id': '2210.11616',\n",
       "  'abstract': 'across many domains, real-world problems can be represented as a network. nodes represent domain-specific elements and edges capture the relationship between elements. leveraging high-performance computing and optimized link prediction algorithms, it is increasingly possible to evaluate every possible combination of nodal pairs enabling the generation of a comprehensive prediction matrix (cpm) that places an individual link prediction score in the context of all possible links involving either node (providing data-driven context). historically, this contextual information has been ignored given exponentially growing problem sizes resulting in computational intractability; however, we demonstrate that expending high-performance compute resources to generate cpms is a worthwhile investment given the improvement in predictive performance. in this work, we generalize for all pairwise link-prediction tasks our novel semi-supervised machine learning method, denoted reciprocal perspective (rp). we demonstrate that rp significantly improves link prediction accuracy by leveraging the wealth of information in a cpm. context-based features are extracted from the cpm for use in a stacked classifier and we demonstrate that the application of rp in a cascade almost always results in significantly (p < 0.05) improved predictions. these results on rs-type problems suggest that rp is applicable to a broad range of link prediction problems.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['kevin dick', 'daniel g. kyrollos', 'james r. green'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11616'},\n",
       " {'title': 'boosting natural language generation from instructions with   meta-learning',\n",
       "  'id': '2210.11617',\n",
       "  'abstract': 'recent work has shown that language models (lms) trained with multi-task \\\\textit{instructional learning} (mtil) can solve diverse nlp tasks in zero- and few-shot settings with improved performance compared to prompt tuning. mtil illustrates that lms can extract and use information about the task from instructions beyond the surface patterns of the inputs and outputs. this suggests that meta-learning may further enhance the utilization of instructions for effective task transfer. in this paper we investigate whether meta-learning applied to mtil can further improve generalization to unseen tasks in a zero-shot setting. specifically, we propose to adapt meta-learning to mtil in three directions: 1) model agnostic meta learning (maml), 2) hyper-network (hnet) based adaptation to generate task specific parameters conditioned on instructions, and 3) an approach combining hnet and maml. through extensive experiments on the large scale natural instructions v2 dataset, we show that our proposed approaches significantly improve over strong baselines in zero-shot settings. in particular, meta-learning improves the effectiveness of instructions and is most impactful when the test tasks are strictly zero-shot (i.e. no similar tasks in the training set) and are \"hard\" for lms, illustrating the potential of meta-learning for mtil for out-of-distribution tasks.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['budhaditya deb', 'guoqing zheng', 'ahmed hassan awadallah'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11617'},\n",
       " {'title': 'multitasking models are robust to structural failure: a neural model for   bilingual cognitive reserve',\n",
       "  'id': '2210.11618',\n",
       "  'abstract': 'we find a surprising connection between multitask learning and robustness to neuron failures. our experiments show that bilingual language models retain higher performance under various neuron perturbations, such as random deletions, magnitude pruning and weight noise compared to equivalent monolingual ones. we provide a theoretical justification for this robustness by mathematically analyzing linear representation learning and showing that multitasking creates more robust representations. our analysis connects robustness to spectral properties of the learned representation and proves that multitasking leads to higher robustness for diverse task vectors. we open-source our code and models: https://github.com/giannisdaras/multilingual_robustness',\n",
       "  'categories': 'cs.lg cs.ai cs.cl',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['giannis daras',\n",
       "   'negin raoof',\n",
       "   'zoi gkalitsiou',\n",
       "   'alexandros g. dimakis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11618'},\n",
       " {'title': 'lot: layer-wise orthogonal training on improving l2 certified robustness',\n",
       "  'id': '2210.11620',\n",
       "  'abstract': 'recent studies show that training deep neural networks (dnns) with lipschitz constraints are able to enhance adversarial robustness and other model properties such as stability. in this paper, we propose a layer-wise orthogonal training method (lot) to effectively train 1-lipschitz convolution layers via parametrizing an orthogonal matrix with an unconstrained matrix. we then efficiently compute the inverse square root of a convolution kernel by transforming the input domain to the fourier frequency domain. on the other hand, as existing works show that semi-supervised training helps improve empirical robustness, we aim to bridge the gap and prove that semi-supervised learning also improves the certified robustness of lipschitz-bounded models. we conduct comprehensive evaluations for lot under different settings. we show that lot significantly outperforms baselines regarding deterministic l2 certified robustness, and scales to deeper neural networks. under the supervised scenario, we improve the state-of-the-art certified robustness for all architectures (e.g. from 59.04% to 63.50% on cifar-10 and from 32.57% to 34.59% on cifar-100 at radius rho = 36/255 for 40-layer networks). with semi-supervised learning over unlabelled data, we are able to improve state-of-the-art certified robustness on cifar-10 at rho = 108/255 from 36.04% to 42.39%. in addition, lot consistently outperforms baselines on different model architectures with only 1/3 evaluation time.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['xiaojun xu', 'linyi li', 'bo li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11620'},\n",
       " {'title': 'small-100: introducing shallow multilingual machine translation model   for low-resource languages',\n",
       "  'id': '2210.11621',\n",
       "  'abstract': 'in recent years, multilingual machine translation models have achieved promising performance on low-resource language pairs by sharing information between similar languages, thus enabling zero-shot translation. to overcome the \"curse of multilinguality\", these models often opt for scaling up the number of parameters, which makes their use in resource-constrained environments challenging. we introduce small-100, a distilled version of the m2m-100 (12b) model, a massively multilingual machine translation model covering 100 languages. we train small-100 with uniform sampling across all language pairs and therefore focus on preserving the performance of low-resource languages. we evaluate small-100 on different low-resource benchmarks: flores-101, tatoeba, and tico-19 and demonstrate that it outperforms previous massively multilingual models of comparable sizes (200-600m) while improving inference latency and memory usage. additionally, our model achieves comparable results to m2m-100 (1.2b), while being 3.6x smaller and 4.3x faster at inference. code and pre-trained models: https://github.com/alirezamshi/small100',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['alireza mohammadshahi',\n",
       "   'vassilina nikoulina',\n",
       "   'alexandre berard',\n",
       "   'caroline brun',\n",
       "   'james henderson',\n",
       "   'laurent besacier'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11621'},\n",
       " {'title': \"sparse dynamical features generation, application to parkinson's disease   diagnosis\",\n",
       "  'id': '2210.11624',\n",
       "  'abstract': \"in this study we focus on the diagnosis of parkinson's disease (pd) based on electroencephalogram (eeg) signals. we propose a new approach inspired by the functioning of the brain that uses the dynamics, frequency and temporal content of eegs to extract new demarcating features of the disease. the method was evaluated on a publicly available dataset containing eeg signals recorded during a 3-oddball auditory task involving n = 50 subjects, of whom 25 suffer from pd. by extracting two features, and separating them with a straight line using a linear discriminant analysis (lda) classifier, we can separate the healthy from the unhealthy subjects with an accuracy of 90% (p < 1.8$\\\\times$10-5) using a single channel. by aggregating the information from three channels and making them vote, we obtain an accuracy of 94 %, a sensitivity of 96 % and a specificity of 92 %. the evaluation was carried out using a nested leave-one-out cross-validation procedure, thus preventing data leakage problems and giving a less biased evaluation. several tests were carried out to assess the validity and robustness of our approach, including the test where we use only half the available data for training. under this constraint, the model achieves an accuracy of 89.4 %.\",\n",
       "  'categories': 'eess.sy cs.lg cs.sy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['houssem meghnoudj', 'bogdan robu', 'mazen alamir'],\n",
       "  'affiliation': ['univ. grenoble alpes, cnrs, grenoble inp, gipsa-lab, 38000 grenoble, france',\n",
       "   'univ. grenoble alpes, cnrs, grenoble inp, gipsa-lab, 38000 grenoble, france',\n",
       "   'univ. grenoble alpes, cnrs, grenoble inp, gipsa-lab, 38000 grenoble, france'],\n",
       "  'url': 'https://arxiv.org/abs/2210.11624'},\n",
       " {'title': 'graphically structured diffusion models',\n",
       "  'id': '2210.11633',\n",
       "  'abstract': \"we introduce a framework for automatically defining and learning deep generative models with problem-specific structure. we tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for sudoku, and matrix factorization. concretely, we train diffusion models with an architecture tailored to the problem specification. this problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. permutation invariances can also be exploited. across a diverse set of experiments we improve the scaling relationship between problem dimension and our model's performance, in terms of both training time and final accuracy.\",\n",
       "  'categories': 'cs.lg cs.ne cs.pl',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['christian weilbach', 'william harvey', 'frank wood'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11633'},\n",
       " {'title': 'hesscale: scalable computation of hessian diagonals',\n",
       "  'id': '2210.11639',\n",
       "  'abstract': 'second-order optimization uses curvature information about the objective function, which can help in faster convergence. however, such methods typically require expensive computation of the hessian matrix, preventing their usage in a scalable way. the absence of efficient ways of computation drove the most widely used methods to focus on first-order approximations that do not capture the curvature information. in this paper, we develop hesscale, a scalable approach to approximating the diagonal of the hessian matrix, to incorporate second-order information in a computationally efficient manner. we show that hesscale has the same computational complexity as backpropagation. our results on supervised classification show that hesscale achieves high approximation accuracy, allowing for scalable and efficient second-order optimization.',\n",
       "  'categories': 'cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['mohamed elsayed', 'a. rupam mahmood'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11639'},\n",
       " {'title': 'paco: parameter-compositional multi-task reinforcement learning',\n",
       "  'id': '2210.11653',\n",
       "  'abstract': 'the purpose of multi-task reinforcement learning (mtrl) is to train a single policy that can be applied to a set of different tasks. sharing parameters allows us to take advantage of the similarities among tasks. however, the gaps between contents and difficulties of different tasks bring us challenges on both which tasks should share the parameters and what parameters should be shared, as well as the optimization challenges due to parameter sharing. in this work, we introduce a parameter-compositional approach (paco) as an attempt to address these challenges. in this framework, a policy subspace represented by a set of parameters is learned. policies for all the single tasks lie in this subspace and can be composed by interpolating with the learned set. it allows not only flexible parameter sharing but also a natural way to improve training. we demonstrate the state-of-the-art performance on meta-world benchmarks, verifying the effectiveness of the proposed approach.',\n",
       "  'categories': 'cs.lg cs.ai cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['lingfeng sun', 'haichao zhang', 'wei xu', 'masayoshi tomizuka'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11653'},\n",
       " {'title': 'mnedgenet -- accurate decomposition of mixed oxidation states for mn xas   and eels l2,3 edges without reference and calibration',\n",
       "  'id': '2210.11657',\n",
       "  'abstract': 'accurate decomposition of the mixed mn oxidation states is highly important for characterizing the electronic structures, charge transfer, and redox centers for electronic, electrocatalytic, and energy storage materials that contain mn. electron energy loss spectroscopy (eels) and soft x-ray absorption spectroscopy (xas) measurements of the mn l2,3 edges are widely used for this purpose. to date, although the measurement of the mn l2,3 edges is straightforward given the sample is prepared properly, an accurate decomposition of the mix valence states of mn remains non-trivial. for both eels and xas, 2+, 3+, 4+ reference spectra need to be taken on the same instrument/beamline and preferably in the same experimental session because the instrumental resolution and the energy axis offset could vary from one session to another. to circumvent this hurdle, in this study, we adopted a deep learning approach and developed a calibration-free and reference-free method to decompose the oxidation state of mn l2,3 edges for both eels and xas. to synthesize physics-informed and ground-truth labeled training datasets, we created a forward model that takes into account plural scattering, instrumentation broadening, noise, and energy axis offset. with that, we created a 1.2 million-spectrum database with a three-element oxidation state composition label. the library includes a sufficient variety of data including both eels and xas spectra. by training on this large database, our convolutional neural network achieves 85% accuracy on the validation dataset. we tested the model and found it is robust against noise (down to psnr of 10) and plural scattering (up to t/{\\\\lambda} = 1). we further validated the model against spectral data that were not used in training.',\n",
       "  'categories': 'cond-mat.mtrl-sci cs.ai cs.lg physics.chem-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['huolin l. xin', 'mike hu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11657'},\n",
       " {'title': 'local bayesian optimization via maximizing probability of descent',\n",
       "  'id': '2210.11662',\n",
       "  'abstract': 'local optimization presents a promising approach to expensive, high-dimensional black-box optimization by sidestepping the need to globally explore the search space. for objective functions whose gradient cannot be evaluated directly, bayesian optimization offers one solution -- we construct a probabilistic model of the objective, design a policy to learn about the gradient at the current location, and use the resulting information to navigate the objective landscape. previous work has realized this scheme by minimizing the variance in the estimate of the gradient, then moving in the direction of the expected gradient. in this paper, we re-examine and refine this approach. we demonstrate that, surprisingly, the expected value of the gradient is not always the direction maximizing the probability of descent, and in fact, these directions may be nearly orthogonal. this observation then inspires an elegant optimization scheme seeking to maximize the probability of descent while moving in the direction of most-probable descent. experiments on both synthetic and real-world objectives show that our method outperforms previous realizations of this optimization scheme and is competitive against other, significantly more complicated baselines.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['quan nguyen', 'kaiwen wu', 'jacob r. gardner', 'roman garnett'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11662'},\n",
       " {'title': 'doctors handwritten prescription recognition system in multi language   using deep learning',\n",
       "  'id': '2210.11666',\n",
       "  'abstract': \"doctors typically write in incomprehensible handwriting, making it difficult for both the general public and some pharmacists to understand the medications they have prescribed. it is not ideal for them to write the prescription quietly and methodically because they will be dealing with dozens of patients every day and will be swamped with work.as a result, their handwriting is illegible. this may result in reports or prescriptions consisting of short forms and cursive writing that a typical person or pharmacist won't be able to read properly, which will cause prescribed medications to be misspelled. however, some individuals are accustomed to writing prescriptions in regional languages because we all live in an area with a diversity of regional languages. it makes analyzing the content much more challenging. so, in this project, we'll use a recognition system to build a tool that can translate the handwriting of physicians in any language. this system will be made into an application which is fully autonomous in functioning. as the user uploads the prescription image the program will pre-process the image by performing image pre-processing, and word segmentations initially before processing the image for training. and it will be done for every language we require the model to detect. and as of the deduction model will be made using deep learning techniques including cnn, rnn, and lstm, which are utilized to train the model. to match words from various languages that will be written in the system, unicode will be used. furthermore, fuzzy search and market basket analysis are employed to offer an end result that will be optimized from the pharmaceutical database and displayed to the user as a structured output.\",\n",
       "  'categories': 'cs.cv cs.lg cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['pavithiran g',\n",
       "   'sharan padmanabhan',\n",
       "   'nuvvuru divya',\n",
       "   'aswathy v',\n",
       "   'irene jerusha p',\n",
       "   'chandar b'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11666'},\n",
       " {'title': 'stochastic adaptive activation function',\n",
       "  'id': '2210.11672',\n",
       "  'abstract': 'the simulation of human neurons and neurotransmission mechanisms has been realized in deep neural networks based on the theoretical implementations of activation functions. however, recent studies have reported that the threshold potential of neurons exhibits different values according to the locations and types of individual neurons, and that the activation functions have limitations in terms of representing this variability. therefore, this study proposes a simple yet effective activation function that facilitates different thresholds and adaptive activations according to the positions of units and the contexts of inputs. furthermore, the proposed activation function mathematically exhibits a more generalized form of swish activation function, and thus we denoted it as adaptive swish (ash). ash highlights informative features that exhibit large values in the top percentiles in an input, whereas it rectifies low values. most importantly, ash exhibits trainable, adaptive, and context-aware properties compared to other activation functions. furthermore, ash represents general formula of the previously studied activation function and provides a reasonable mathematical background for the superior performance. to validate the effectiveness and robustness of ash, we implemented ash into many deep learning models for various tasks, including classification, detection, segmentation, and image generation. experimental analysis demonstrates that our activation function can provide the benefits of more accurate prediction and earlier convergence in many deep learning applications.',\n",
       "  'categories': 'cs.lg cs.ai cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['kyungsu lee', 'jaeseung yang', 'haeyun lee', 'jae youn hwang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11672'},\n",
       " {'title': 'fuzzy granular-ball computing framework and its implementation in svm',\n",
       "  'id': '2210.11675',\n",
       "  'abstract': 'most existing fuzzy computing methods use points as input, which is the finest granularity from the perspective of granular computing. consequently, these classifiers are neither efficient nor robust to label noise. therefore, we propose a framework for a fuzzy granular-ball computational classifier by introducing granular-ball computing into fuzzy set. the computational framework is based on the granular-balls input rather than points; therefore, it is more efficient and robust than traditional fuzzy methods. furthermore, the framework is extended to the fuzzy support vector machine (fsvm), and granular ball fuzzy svm (gbfsvm) is derived. the experimental results demonstrate the effectiveness and efficiency of gbfsvm.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['shuyin xia', 'xiaoyu lian', 'yabin shao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11675'},\n",
       " {'title': 'twin contrastive learning for online clustering',\n",
       "  'id': '2210.11680',\n",
       "  'abstract': 'this paper proposes to perform online clustering by conducting twin contrastive learning (tcl) at the instance and cluster level. specifically, we find that when the data is projected into a feature space with a dimensionality of the target cluster number, the rows and columns of its feature matrix correspond to the instance and cluster representation, respectively. based on the observation, for a given dataset, the proposed tcl first constructs positive and negative pairs through data augmentations. thereafter, in the row and column space of the feature matrix, instance- and cluster-level contrastive learning are respectively conducted by pulling together positive pairs while pushing apart the negatives. to alleviate the influence of intrinsic false-negative pairs and rectify cluster assignments, we adopt a confidence-based criterion to select pseudo-labels for boosting both the instance- and cluster-level contrastive learning. as a result, the clustering performance is further improved. besides the elegant idea of twin contrastive learning, another advantage of tcl is that it could independently predict the cluster assignment for each instance, thus effortlessly fitting online scenarios. extensive experiments on six widely-used image and text benchmarks demonstrate the effectiveness of tcl. the code will be released on github.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '10.1007/s11263-022-01639-z',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['yunfan li',\n",
       "   'mouxing yang',\n",
       "   'dezhong peng',\n",
       "   'taihao li',\n",
       "   'jiantao huang',\n",
       "   'xi peng'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11680'},\n",
       " {'title': 'competing bandits in time varying matching markets',\n",
       "  'id': '2210.11692',\n",
       "  'abstract': 'we study the problem of online learning in two-sided non-stationary matching markets, where the objective is to converge to a stable match. in particular, we consider the setting where one side of the market, the arms, has fixed known set of preferences over the other side, the players. while this problem has been studied when the players have fixed but unknown preferences, in this work we study the problem of how to learn when the preferences of the players are time varying. we propose the {\\\\it restart competing bandits (rcb)} algorithm, which combines a simple {\\\\it restart strategy} to handle the non-stationarity with the {\\\\it competing bandits} algorithm \\\\citep{liu2020competing} designed for the stationary case. we show that, with the proposed algorithm, each player receives a uniform sub-linear regret of {$\\\\widetilde{\\\\mathcal{o}}(l^{1/2}_tt^{1/2})$} up to the number of changes in the underlying preference of agents, $l_t$. we also discuss extensions of this algorithm to the case where the number of changes need not be known a priori.',\n",
       "  'categories': 'cs.lg cs.gt cs.ma',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['deepan muthirayan',\n",
       "   'chinmay maheshwari',\n",
       "   'pramod p. khargonekar',\n",
       "   'shankar sastry'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11692'},\n",
       " {'title': 'amos: an adam-style optimizer with adaptive weight decay towards   model-oriented scale',\n",
       "  'id': '2210.11693',\n",
       "  'abstract': 'we present amos, a stochastic gradient-based optimizer designed for training deep neural networks. it can be viewed as an adam optimizer with theoretically supported, adaptive learning-rate decay and weight decay. a key insight behind amos is that it leverages model-specific information to determine the initial learning-rate and decaying schedules. when used for pre-training bert variants and t5, amos consistently converges faster than the state-of-the-art settings of adamw, achieving better validation loss within <=70% training steps and time, while requiring <=51% memory for slot variables. our code is open-sourced at: https://github.com/google-research/jestimator',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['ran tian', 'ankur p. parikh'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11693'},\n",
       " {'title': 'multi-view reasoning: consistent contrastive learning for math word   problem',\n",
       "  'id': '2210.11694',\n",
       "  'abstract': 'math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and diverse equations. however, human solving naturally involves two consistent reasoning views: top-down and bottom-up, just as math equations also can be expressed in multiple equivalent forms: pre-order and post-order. we propose a multi-view consistent contrastive learning for a more complete semantics-to-equation mapping. the entire process is decoupled into two independent but consistent views: top-down decomposition and bottom-up construction, and the two reasoning views are aligned in multi-granularity for consistency, enhancing global generation and precise reasoning. experiments on multiple datasets across two languages show our approach significantly outperforms the existing baselines, especially on complex problems. we also show after consistent alignment, multi-view can absorb the merits of both views and generate more diverse results consistent with the mathematical laws.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['wenqi zhang',\n",
       "   'yongliang shen',\n",
       "   'yanna ma',\n",
       "   'xiaoxia cheng',\n",
       "   'zeqi tan',\n",
       "   'qingpeng nong',\n",
       "   'weiming lu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11694'},\n",
       " {'title': 'global counterfactual explainer for graph neural networks',\n",
       "  'id': '2210.11695',\n",
       "  'abstract': 'graph neural networks (gnns) find applications in various domains such as computational biology, natural language processing, and computer security. owing to their popularity, there is an increasing need to explain gnn predictions since gnns are black-box machine learning models. one way to address this is counterfactual reasoning where the objective is to change the gnn prediction by minimal changes in the input graph. existing methods for counterfactual explanation of gnns are limited to instance-specific local reasoning. this approach has two major limitations of not being able to offer global recourse policies and overloading human cognitive ability with too much information. in this work, we study the global explainability of gnns through global counterfactual reasoning. specifically, we want to find a small set of representative counterfactual graphs that explains all input graphs. towards this goal, we propose gcfexplainer, a novel algorithm powered by vertex-reinforced random walks on an edit map of graphs with a greedy summary. extensive experiments on real graph datasets show that the global explanation from gcfexplainer provides important high-level insights of the model behavior and achieves a 46.9% gain in recourse coverage and a 9.5% reduction in recourse cost compared to the state-of-the-art local counterfactual explainers.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['mert kosan',\n",
       "   'zexi huang',\n",
       "   'sourav medya',\n",
       "   'sayan ranu',\n",
       "   'ambuj singh'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11695'},\n",
       " {'title': 'learning robust dynamics through variational sparse gating',\n",
       "  'id': '2210.11698',\n",
       "  'abstract': 'learning world models from their sensory inputs enables agents to plan for actions by imagining their future outcomes. world models have previously been shown to improve sample-efficiency in simulated environments with few objects, but have not yet been applied successfully to environments with many objects. in environments with many objects, often only a small number of them are moving or interacting at the same time. in this paper, we investigate integrating this inductive bias of sparse interactions into the latent dynamics of world models trained from pixels. first, we introduce variational sparse gating (vsg), a latent dynamics model that updates its feature dimensions sparsely through stochastic binary gates. moreover, we propose a simplified architecture simple variational sparse gating (svsg) that removes the deterministic pathway of previous models, resulting in a fully stochastic transition function that leverages the vsg mechanism. we evaluate the two model architectures in the bringbackshapes (bbs) environment that features a large number of moving objects and partial observability, demonstrating clear improvements over prior models.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['arnav kumar jain',\n",
       "   'shivakanth sujit',\n",
       "   'shruti joshi',\n",
       "   'vincent michalski',\n",
       "   'danijar hafner',\n",
       "   'samira ebrahimi-kahou'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11698'},\n",
       " {'title': 'efficiently tuned parameters are task embeddings',\n",
       "  'id': '2210.11705',\n",
       "  'abstract': 'intermediate-task transfer can benefit a wide range of nlp tasks with properly selected source datasets. however, it is computationally infeasible to experiment with all intermediate transfer combinations, making choosing a useful source task a challenging problem. in this paper, we anticipate that task-specific parameters updated in parameter-efficient tuning methods are likely to encode task-specific information. therefore, such parameters can be predictive for inter-task transferability. thus, we propose to exploit these efficiently tuned parameters as off-the-shelf task embeddings for the efficient selection of source datasets for intermediate-task transfer. we experiment with 11 text classification tasks and 11 question answering tasks. experimental results show that our approach can consistently outperform existing inter-task transferability prediction methods while being conceptually simple and computationally efficient. our analysis also reveals that the ability of efficiently tuned parameters on transferability prediction is disentangled with their in-task performance. this allows us to use parameters from early checkpoints as task embeddings to further improve efficiency.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['wangchunshu zhou', 'canwen xu', 'julian mcauley'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11705'},\n",
       " {'title': 'crt-6d: fast 6d object pose estimation with cascaded refinement   transformers',\n",
       "  'id': '2210.11718',\n",
       "  'abstract': 'learning based 6d object pose estimation methods rely on computing large intermediate pose representations and/or iteratively refining an initial estimation with a slow render-compare pipeline. this paper introduces a novel method we call cascaded pose refinement transformers, or crt-6d. we replace the commonly used dense intermediate representation with a sparse set of features sampled from the feature pyramid we call oskfs(object surface keypoint features) where each element corresponds to an object keypoint. we employ lightweight deformable transformers and chain them together to iteratively refine proposed poses over the sampled oskfs. we achieve inference runtimes 2x faster than the closest real-time state of the art methods while supporting up to 21 objects on a single model. we demonstrate the effectiveness of crt-6d by performing extensive experiments on the lm-o and ycbv datasets. compared to real-time methods, we achieve state of the art on lm-o and ycb-v, falling slightly behind methods with inference runtimes one order of magnitude higher. the source code is available at: https://github.com/pedrocastro/crt-6d',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['pedro castro', 'tae-kyun kim'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11718'},\n",
       " {'title': 'privacy-preserved neural graph similarity learning',\n",
       "  'id': '2210.11730',\n",
       "  'abstract': 'to develop effective and efficient graph similarity learning (gsl) models, a series of data-driven neural algorithms have been proposed in recent years. although gsl models are frequently deployed in privacy-sensitive scenarios, the user privacy protection of neural gsl models has not drawn much attention. to comprehensively understand the privacy protection issues, we first introduce the concept of attackable representation to systematically characterize the privacy attacks that each model can face. inspired by the qualitative results, we propose a novel privacy-preserving neural graph matching network model, named ppgm, for graph similarity learning. to prevent reconstruction attacks, the proposed model does not communicate node-level representations between devices. instead, we learn multi-perspective graph representations based on learnable context vectors. to alleviate the attacks to graph properties, the obfuscated features that contain information from both graphs are communicated. in this way, the private properties of each graph can be difficult to infer. based on the node-graph matching techniques while calculating the obfuscated features, ppgm can also be effective in similarity measuring. to quantitatively evaluate the privacy-preserving ability of neural gsl models, we further propose an evaluation protocol via training supervised black-box attack models. extensive experiments on widely-used benchmarks show the effectiveness and strong privacy-protection ability of the proposed model ppgm. the code is available at: https://github.com/rucaibox/ppgm.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yupeng hou', 'wayne xin zhao', 'yaliang li', 'ji-rong wen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11730'},\n",
       " {'title': 'bayesian deep learning framework for uncertainty quantification in high   dimensions',\n",
       "  'id': '2210.11737',\n",
       "  'abstract': 'we develop a novel deep learning method for uncertainty quantification in stochastic partial differential equations based on bayesian neural network (bnn) and hamiltonian monte carlo (hmc). a bnn efficiently learns the posterior distribution of the parameters in deep neural networks by performing bayesian inference on the network parameters. the posterior distribution is efficiently sampled using hmc to quantify uncertainties in the system. several numerical examples are shown for both forward and inverse problems in high dimension to demonstrate the effectiveness of the proposed method for uncertainty quantification. these also show promising results that the computational cost is almost independent of the dimension of the problem demonstrating the potential of the method for tackling the so-called curse of dimensionality.',\n",
       "  'categories': 'stat.ml cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['jeahan jung', 'minseok choi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11737'},\n",
       " {'title': 'online and lightweight kernel-based approximated policy iteration for   dynamic p-norm linear adaptive filtering',\n",
       "  'id': '2210.11755',\n",
       "  'abstract': \"this paper introduces a solution to the problem of selecting dynamically (online) the ``optimal'' p-norm to combat outliers in linear adaptive filtering without any knowledge on the probability density function of the outliers. the proposed online and data-driven framework is built on kernel-based reinforcement learning (kbrl). to this end, novel bellman mappings on reproducing kernel hilbert spaces (rkhss) are introduced. these mappings do not require any knowledge on transition probabilities of markov decision processes, and are nonexpansive with respect to the underlying hilbertian norm. the fixed-point sets of the proposed bellman mappings are utilized to build an approximate policy-iteration (api) framework for the problem at hand. to address the ``curse of dimensionality'' in rkhss, random fourier features are utilized to bound the computational complexity of the api. numerical tests on synthetic data for several outlier scenarios demonstrate the superior performance of the proposed api framework over several non-rl and kbrl schemes.\",\n",
       "  'categories': 'cs.lg eess.sp',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yuki akiyama', 'minh vu', 'konstantinos slavakis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11755'},\n",
       " {'title': 'reaching through latent space: from joint statistics to path planning in   manipulation',\n",
       "  'id': '2210.11779',\n",
       "  'abstract': 'we present a novel approach to path planning for robotic manipulators, in which paths are produced via iterative optimisation in the latent space of a generative model of robot poses. constraints are incorporated through the use of constraint satisfaction classifiers operating on the same space. optimisation leverages gradients through our learned models that provide a simple way to combine goal reaching objectives with constraint satisfaction, even in the presence of otherwise non-differentiable constraints. our models are trained in a task-agnostic manner on randomly sampled robot poses. in baseline comparisons against a number of widely used planners, we achieve commensurate performance in terms of task success, planning time and path length, performing successful path planning with obstacle avoidance on a real 7-dof robot arm.',\n",
       "  'categories': 'cs.ro cs.lg',\n",
       "  'doi': '10.1109/lra.2022.3152697',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['chia-man hung',\n",
       "   'shaohong zhong',\n",
       "   'walter goodwin',\n",
       "   'oiwi parker jones',\n",
       "   'martin engelcke',\n",
       "   'ioannis havoutis',\n",
       "   'ingmar posner'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11779'},\n",
       " {'title': 'correlating sparse sensing for network-wide traffic speed estimation: an   integrated graph tensor-based kriging approach',\n",
       "  'id': '2210.11780',\n",
       "  'abstract': 'traffic speed is central to characterizing the fluidity of the road network. many transportation applications rely on it, such as real-time navigation, dynamic route planning, and congestion management. rapid advances in sensing and communication techniques make traffic speed detection easier than ever. however, due to sparse deployment of static sensors or low penetration of mobile sensors, speeds detected are incomplete and far from network-wide use. in addition, sensors are prone to error or missing data due to various kinds of reasons, speeds from these sensors can become highly noisy. these drawbacks call for effective techniques to recover credible estimates from the incomplete data. in this work, we first identify the problem as a spatiotemporal kriging problem and propose a unified graph embedded tensor (sget) learning framework featuring both low-rankness and multi-dimensional correlations for network-wide traffic speed kriging under limited observations. to be specific, three types of speed correlation including temporal continuity, temporal periodicity, and spatial proximity are carefully chosen. we then design an efficient solution algorithm via several effective numeric techniques to scale up the proposed model to network-wide kriging. by performing experiments on two public million-level traffic speed datasets, we finally draw the conclusion and find our proposed sget achieves the state-of-the-art kriging performance even under low observation rates, while at the same time saving more than half computing time compared with baseline methods. some insights into spatiotemporal traffic data kriging at the network level are provided as well.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['tong nie', 'guoyang qin', 'yunpeng wang', 'jian sun'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11780'},\n",
       " {'title': 'fosr: first-order spectral rewiring for addressing oversquashing in gnns',\n",
       "  'id': '2210.11790',\n",
       "  'abstract': 'graph neural networks (gnns) are able to leverage the structure of graph data by passing messages along the edges of the graph. while this allows gnns to learn features depending on the graph structure, for certain graph topologies it leads to inefficient information propagation and a problem known as oversquashing. this has recently been linked with the curvature and spectral gap of the graph. on the other hand, adding edges to the message-passing graph can lead to increasingly similar node representations and a problem known as oversmoothing. we propose a computationally efficient algorithm that prevents oversquashing by systematically adding edges to the graph based on spectral expansion. we combine this with a relational architecture, which lets the gnn preserve the original graph structure and provably prevents oversmoothing. we find experimentally that our algorithm outperforms existing graph rewiring methods in several graph classification tasks.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['kedar karhadkar', 'pradeep kr. banerjee', 'guido montúfar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11790'},\n",
       " {'title': 'diffuser: efficient transformers with multi-hop attention diffusion for   long sequences',\n",
       "  'id': '2210.11794',\n",
       "  'abstract': 'efficient transformers have been developed for long sequence modeling, due to their subquadratic memory and time complexity. sparse transformer is a popular approach to improving the efficiency of transformers by restricting self-attention to locations specified by the predefined sparse patterns. however, leveraging sparsity may sacrifice expressiveness compared to full-attention, when important token correlations are multiple hops away. to combine advantages of both the efficiency of sparse transformer and the expressiveness of full-attention transformer, we propose \\\\textit{diffuser}, a new state-of-the-art efficient transformer. diffuser incorporates all token interactions within one attention layer while maintaining low computation and memory costs. the key idea is to expand the receptive field of sparse attention using attention diffusion, which computes multi-hop token correlations based on all paths between corresponding disconnected tokens, besides attention among neighboring tokens. theoretically, we show the expressiveness of diffuser as a universal sequence approximator for sequence-to-sequence modeling, and investigate its ability to approximate full-attention by analyzing the graph expander property from the spectral perspective. experimentally, we investigate the effectiveness of diffuser with extensive evaluations, including language modeling, image modeling, and long range arena (lra). evaluation results show that diffuser achieves improvements by an average of 0.94% on text classification tasks and 2.30% on lra, with 1.67$\\\\times$ memory savings compared to state-of-the-art benchmarks, which demonstrates superior performance of diffuser in both expressiveness and efficiency aspects.',\n",
       "  'categories': 'cs.lg cs.cl',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['aosong feng', 'irene li', 'yuang jiang', 'rex ying'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11794'},\n",
       " {'title': 'differentiable constrained imitation learning for robot motion planning   and control',\n",
       "  'id': '2210.11796',\n",
       "  'abstract': \"motion planning and control are crucial components of robotics applications. here, spatio-temporal hard constraints like system dynamics and safety boundaries (e.g., obstacles in automated driving) restrict the robot's motions. direct methods from optimal control solve a constrained optimization problem. however, in many applications finding a proper cost function is inherently difficult because of the weighting of partially conflicting objectives. on the other hand, imitation learning (il) methods such as behavior cloning (bc) provide a intuitive framework for learning decision-making from offline demonstrations and constitute a promising avenue for planning and control in complex robot applications. prior work primarily relied on soft-constraint approaches, which use additional auxiliary loss terms describing the constraints. however, catastrophic safety-critical failures might occur in out-of-distribution (ood) scenarios. this work integrates the flexibility of il with hard constraint handling in optimal control. our approach constitutes a general framework for constraint robotic motion planning and control using offline il. hard constraints are integrated into the learning problem in a differentiable manner, via explicit completion and gradient-based correction. simulated experiments of mobile robot navigation and automated driving provide evidence for the performance of the proposed method.\",\n",
       "  'categories': 'cs.ro cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['christopher diehl',\n",
       "   'janis adamek',\n",
       "   'martin krüger',\n",
       "   'frank hoffmann',\n",
       "   'torsten bertram'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11796'},\n",
       " {'title': 'random actions vs random policies: bootstrapping model-based direct   policy search',\n",
       "  'id': '2210.11801',\n",
       "  'abstract': 'this paper studies the impact of the initial data gathering method on the subsequent learning of a dynamics model. dynamics models approximate the true transition function of a given task, in order to perform policy search directly on the model rather than on the costly real system. this study aims to determine how to bootstrap a model as efficiently as possible, by comparing initialization methods employed in two different policy search frameworks in the literature. the study focuses on the model performance under the episode-based framework of evolutionary methods using probabilistic ensembles. experimental results show that various task-dependant factors can be detrimental to each method, suggesting to explore hybrid approaches.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['elias hanna', 'alex coninx', 'stéphane doncieux'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11801'},\n",
       " {'title': 'revisiting checkpoint averaging for neural machine translation',\n",
       "  'id': '2210.11803',\n",
       "  'abstract': 'checkpoint averaging is a simple and effective method to boost the performance of converged neural machine translation models. the calculation is cheap to perform and the fact that the translation improvement almost comes for free, makes it widely adopted in neural machine translation research. despite the popularity, the method itself simply takes the mean of the model parameters from several checkpoints, the selection of which is mostly based on empirical recipes without many justifications. in this work, we revisit the concept of checkpoint averaging and consider several extensions. specifically, we experiment with ideas such as using different checkpoint selection strategies, calculating weighted average instead of simple mean, making use of gradient information and fine-tuning the interpolation weights on development data. our results confirm the necessity of applying checkpoint averaging for optimal performance, but also suggest that the landscape between the converged checkpoints is rather flat and not much further improvement compared to simple averaging is to be obtained.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yingbo gao', 'christian herold', 'zijian yang', 'hermann ney'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11803'},\n",
       " {'title': 'is encoder-decoder redundant for neural machine translation?',\n",
       "  'id': '2210.11807',\n",
       "  'abstract': 'encoder-decoder architecture is widely adopted for sequence-to-sequence modeling tasks. for machine translation, despite the evolution from long short-term memory networks to transformer networks, plus the introduction and development of attention mechanism, encoder-decoder is still the de facto neural network architecture for state-of-the-art models. while the motivation for decoding information from some hidden space is straightforward, the strict separation of the encoding and decoding steps into an encoder and a decoder in the model architecture is not necessarily a must. compared to the task of autoregressive language modeling in the target language, machine translation simply has an additional source sentence as context. given the fact that neural language models nowadays can already handle rather long contexts in the target language, it is natural to ask whether simply concatenating the source and target sentences and training a language model to do translation would work. in this work, we investigate the aforementioned concept for machine translation. specifically, we experiment with bilingual translation, translation with additional target monolingual data, and multilingual translation. in all cases, this alternative approach performs on par with the baseline encoder-decoder transformer, suggesting that an encoder-decoder architecture might be redundant for neural machine translation.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yingbo gao', 'christian herold', 'zijian yang', 'hermann ney'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11807'},\n",
       " {'title': 'self-supervised pretraining on satellite imagery: a case study on   label-efficient vehicle detection',\n",
       "  'id': '2210.11815',\n",
       "  'abstract': \"in defense-related remote sensing applications, such as vehicle detection on satellite imagery, supervised learning requires a huge number of labeled examples to reach operational performances. such data are challenging to obtain as it requires military experts, and some observables are intrinsically rare. this limited labeling capability, as well as the large number of unlabeled images available due to the growing number of sensors, make object detection on remote sensing imagery highly relevant for self-supervised learning. we study in-domain self-supervised representation learning for object detection on very high resolution optical satellite imagery, that is yet poorly explored. for the first time to our knowledge, we study the problem of label efficiency on this task. we use the large land use classification dataset functional map of the world to pretrain representations with an extension of the momentum contrast framework. we then investigate this model's transferability on a real-world task of fine-grained vehicle detection and classification on preligens proprietary data, which is designed to be representative of an operational use case of strategic site surveillance. we show that our in-domain self-supervised learning model is competitive with imagenet pretraining, and outperforms it in the low-label regime.\",\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['jules bourcier',\n",
       "   'thomas floquet',\n",
       "   'gohar dashyan',\n",
       "   'tugdual ceillier',\n",
       "   'karteek alahari',\n",
       "   'jocelyn chanussot'],\n",
       "  'affiliation': ['thoth', 'thoth', 'thoth', 'thoth', 'thoth', 'thoth'],\n",
       "  'url': 'https://arxiv.org/abs/2210.11815'},\n",
       " {'title': 'valuing vicinity: memory attention framework for context-based semantic   segmentation in histopathology',\n",
       "  'id': '2210.11822',\n",
       "  'abstract': \"the segmentation of histopathological whole slide images into tumourous and non-tumourous types of tissue is a challenging task that requires the consideration of both local and global spatial contexts to classify tumourous regions precisely. the identification of subtypes of tumour tissue complicates the issue as the sharpness of separation decreases and the pathologist's reasoning is even more guided by spatial context. however, the identification of detailed types of tissue is crucial for providing personalized cancer therapies. due to the high resolution of whole slide images, existing semantic segmentation methods, restricted to isolated image sections, are incapable of processing context information beyond. to take a step towards better context comprehension, we propose a patch neighbour attention mechanism to query the neighbouring tissue context from a patch embedding memory bank and infuse context embeddings into bottleneck hidden feature maps. our memory attention framework (maf) mimics a pathologist's annotation procedure -- zooming out and considering surrounding tissue context. the framework can be integrated into any encoder-decoder segmentation method. we evaluate the maf on a public breast cancer and an internal kidney cancer data set using famous segmentation models (u-net, deeplabv3) and demonstrate the superiority over other context-integrating algorithms -- achieving a substantial improvement of up to $17\\\\%$ on dice score. the code is publicly available at: https://github.com/tio-ikim/valuing-vicinity\",\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['oliver ester',\n",
       "   'fabian hörst',\n",
       "   'constantin seibold',\n",
       "   'julius keyl',\n",
       "   'saskia ting',\n",
       "   'nikolaos vasileiadis',\n",
       "   'jessica schmitz',\n",
       "   'philipp ivanyi',\n",
       "   'viktor grünwald',\n",
       "   'jan hinrich bräsen',\n",
       "   'jan egger',\n",
       "   'jens kleesiek'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11822'},\n",
       " {'title': 'integrating policy summaries with reward decomposition for explaining   reinforcement learning agents',\n",
       "  'id': '2210.11825',\n",
       "  'abstract': \"explaining the behavior of reinforcement learning agents operating in sequential decision-making settings is challenging, as their behavior is affected by a dynamic environment and delayed rewards. methods that help users understand the behavior of such agents can roughly be divided into local explanations that analyze specific decisions of the agents and global explanations that convey the general strategy of the agents. in this work, we study a novel combination of local and global explanations for reinforcement learning agents. specifically, we combine reward decomposition, a local explanation method that exposes which components of the reward function influenced a specific decision, and highlights, a global explanation method that shows a summary of the agent's behavior in decisive states. we conducted two user studies to evaluate the integration of these explanation methods and their respective benefits. our results show significant benefits for both methods. in general, we found that the local reward decomposition was more useful for identifying the agents' priorities. however, when there was only a minor difference between the agents' preferences, then the global information provided by highlights additionally improved participants' understanding.\",\n",
       "  'categories': 'cs.lg cs.ai cs.hc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yael septon', 'tobias huber', 'elisabeth andré', 'ofra amir'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11825'},\n",
       " {'title': 'management of machine learning lifecycle artifacts: a survey',\n",
       "  'id': '2210.11831',\n",
       "  'abstract': 'the explorative and iterative nature of developing and operating machine learning (ml) applications leads to a variety of artifacts, such as datasets, features, models, hyperparameters, metrics, software, configurations, and logs. in order to enable comparability, reproducibility, and traceability of these artifacts across the ml lifecycle steps and iterations, systems and tools have been developed to support their collection, storage, and management. it is often not obvious what precise functional scope such systems offer so that the comparison and the estimation of synergy effects between candidates are quite challenging. in this paper, we aim to give an overview of systems and platforms which support the management of ml lifecycle artifacts. based on a systematic literature review, we derive assessment criteria and apply them to a representative selection of more than 60 systems and platforms.',\n",
       "  'categories': 'cs.db cs.lg cs.se',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['marius schlegel', 'kai-uwe sattler'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11831'},\n",
       " {'title': 'optimal contextual bandits with knapsacks under realizibility via   regression oracles',\n",
       "  'id': '2210.11834',\n",
       "  'abstract': 'we study the stochastic contextual bandit with knapsacks (cbwk) problem, where each action, taken upon a context, not only leads to a random reward but also costs a random resource consumption in a vector form. the challenge is to maximize the total reward without violating the budget for each resource. we study this problem under a general realizability setting where the expected reward and expected cost are functions of contexts and actions in some given general function classes $\\\\mathcal{f}$ and $\\\\mathcal{g}$, respectively. existing works on cbwk are restricted to the linear function class since they use ucb-type algorithms, which heavily rely on the linear form and thus are difficult to extend to general function classes. motivated by online regression oracles that have been successfully applied to contextual bandits, we propose the first universal and optimal algorithmic framework for cbwk by reducing it to online regression. we also establish the lower regret bound to show the optimality of our algorithm for a variety of function classes.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yuxuan han',\n",
       "   'jialin zeng',\n",
       "   'yang wang',\n",
       "   'yang xiang',\n",
       "   'jiheng zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11834'},\n",
       " {'title': 'structural kernel search via bayesian optimization and symbolical   optimal transport',\n",
       "  'id': '2210.11836',\n",
       "  'abstract': \"despite recent advances in automated machine learning, model selection is still a complex and computationally intensive process. for gaussian processes (gps), selecting the kernel is a crucial task, often done manually by the expert. additionally, evaluating the model selection criteria for gaussian processes typically scales cubically in the sample size, rendering kernel search particularly computationally expensive. we propose a novel, efficient search method through a general, structured kernel space. previous methods solved this task via bayesian optimization and relied on measuring the distance between gp's directly in function space to construct a kernel-kernel. we present an alternative approach by defining a kernel-kernel over the symbolic representation of the statistical hypothesis that is associated with a kernel. we empirically show that this leads to a computationally more efficient way of searching through a discrete kernel space.\",\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['matthias bitzer', 'mona meister', 'christoph zimmer'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11836'},\n",
       " {'title': 'diffusion visual counterfactual explanations',\n",
       "  'id': '2210.11841',\n",
       "  'abstract': \"visual counterfactual explanations (vces) are an important tool to understand the decisions of an image classifier. they are 'small' but 'realistic' semantic changes of the image changing the classifier decision. current approaches for the generation of vces are restricted to adversarially robust models and often contain non-realistic artefacts, or are limited to image classification problems with few classes. in this paper, we overcome this by generating diffusion visual counterfactual explanations (dvces) for arbitrary imagenet classifiers via a diffusion process. two modifications to the diffusion process are key for our dvces: first, an adaptive parameterization, whose hyperparameters generalize across images and models, together with distance regularization and late start of the diffusion process, allow us to generate images with minimal semantic changes to the original ones but different classification. second, our cone regularization via an adversarially robust model ensures that the diffusion process does not converge to trivial non-semantic changes, but instead produces realistic images of the target class which achieve high confidence by the classifier.\",\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['maximilian augustin',\n",
       "   'valentyn boreiko',\n",
       "   'francesco croce',\n",
       "   'matthias hein'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11841'},\n",
       " {'title': 'cox-hawkes: doubly stochastic spatiotemporal poisson processes',\n",
       "  'id': '2210.11844',\n",
       "  'abstract': 'hawkes processes are point process models that have been used to capture self-excitatory behavior in social interactions, neural activity, earthquakes and viral epidemics. they can model the occurrence of the times and locations of events. here we develop a new class of spatiotemporal hawkes processes that can capture both triggering and clustering behavior and we provide an efficient method for performing inference. we use a log-gaussian cox process (lgcp) as prior for the background rate of the hawkes process which gives arbitrary flexibility to capture a wide range of underlying background effects (for infectious diseases these are called endemic effects). the hawkes process and lgcp are computationally expensive due to the former having a likelihood with quadratic complexity in the number of observations and the latter involving inversion of the precision matrix which is cubic in observations. here we propose a novel approach to perform mcmc sampling for our hawkes process with lgcp background, using pre-trained gaussian process generators which provide direct and cheap access to samples during inference. we show the efficacy and flexibility of our approach in experiments on simulated data and use our methods to uncover the trends in a dataset of reported crimes in the us.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['xenia miscouridou',\n",
       "   'samir bhatt',\n",
       "   'george mohler',\n",
       "   'seth flaxman',\n",
       "   'swapnil mishra'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11844'},\n",
       " {'title': 'learning in rkhm: a $c^*$-algebraic twist for kernel machines',\n",
       "  'id': '2210.11855',\n",
       "  'abstract': 'supervised learning in reproducing kernel hilbert space (rkhs) and vector-valued rkhs (vvrkhs) has been investigated for more than 30 years. in this paper, we provide a new twist to this rich literature by generalizing supervised learning in rkhs and vvrkhs to reproducing kernel hilbert $c^*$-module (rkhm), and show how to construct effective positive-definite kernels by considering the perspective of $c^*$-algebra. unlike the cases of rkhs and vvrkhs, we can use $c^*$-algebras to enlarge representation spaces. this enables us to construct rkhms whose representation power goes beyond rkhss, vvrkhss, and existing methods such as convolutional neural networks. our framework is suitable, for example, for effectively analyzing image data by allowing the interaction of fourier components.',\n",
       "  'categories': 'stat.ml cs.lg math.oa',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yuka hashimoto', 'masahiro ikeda', 'hachem kadri'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11855'},\n",
       " {'title': 'blind polynomial regression',\n",
       "  'id': '2210.11874',\n",
       "  'abstract': 'fitting a polynomial to observed data is an ubiquitous task in many signal processing and machine learning tasks, such as interpolation and prediction. in that context, input and output pairs are available and the goal is to find the coefficients of the polynomial. however, in many applications, the input may be partially known or not known at all, rendering conventional regression approaches not applicable. in this paper, we formally state the (potentially partial) blind regression problem, illustrate some of its theoretical properties, and propose algorithmic approaches to solve it. as a case-study, we apply our methods to a jitter-correction problem and corroborate its performance.',\n",
       "  'categories': 'eess.sp cs.lg math.st stat.ml stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['alberto natali', 'geert leus'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11874'},\n",
       " {'title': 'glcc: a general framework for graph-level clustering',\n",
       "  'id': '2210.11879',\n",
       "  'abstract': 'this paper studies the problem of graph-level clustering, which is a novel yet challenging task. this problem is critical in a variety of real-world applications such as protein clustering and genome analysis in bioinformatics. recent years have witnessed the success of deep clustering coupled with graph neural networks (gnns). however, existing methods focus on clustering among nodes given a single graph, while exploring clustering on multiple graphs is still under-explored. in this paper, we propose a general graph-level clustering framework named graph-level contrastive clustering (glcc) given multiple graphs. specifically, glcc first constructs an adaptive affinity graph to explore instance- and cluster-level contrastive learning (cl). instance-level cl leverages graph laplacian based contrastive loss to learn clustering-friendly representations while cluster-level cl captures discriminative cluster representations incorporating neighbor information of each sample. moreover, we utilize neighbor-aware pseudo-labels to reward the optimization of representation learning. the two steps can be alternatively trained to collaborate and benefit each other. experiments on a range of well-known datasets demonstrate the superiority of our proposed glcc over competitive baselines.',\n",
       "  'categories': 'cs.lg cs.ai cs.ir cs.si',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['wei ju',\n",
       "   'yiyang gu',\n",
       "   'binqi chen',\n",
       "   'gongbo sun',\n",
       "   'yifang qin',\n",
       "   'xingyuming liu',\n",
       "   'xiao luo',\n",
       "   'ming zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11879'},\n",
       " {'title': 'boosting vision transformers for image retrieval',\n",
       "  'id': '2210.11909',\n",
       "  'abstract': 'vision transformers have achieved remarkable progress in vision tasks such as image classification and detection. however, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. we propose a number of improvements that make transformers outperform the state of the art for the first time. (1) we show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) we introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image representation. (3) in each branch, we collect multi-layer features from the transformer encoder, corresponding to skip connections across distant layers. (4) we enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. we train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. in all cases, we outperform previous models based on global representation. public code is available at https://github.com/dealicious-inc/dtop.',\n",
       "  'categories': 'cs.cv cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['chull hwan song',\n",
       "   'jooyoung yoon',\n",
       "   'shunghyun choi',\n",
       "   'yannis avrithis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11909'},\n",
       " {'title': 'nerel-bio: a dataset of biomedical abstracts annotated with nested named   entities',\n",
       "  'id': '2210.11913',\n",
       "  'abstract': 'this paper describes nerel-bio -- an annotation scheme and corpus of pubmed abstracts in russian and smaller number of abstracts in english. nerel-bio extends the general domain dataset nerel by introducing domain-specific entity types. nerel-bio annotation scheme covers both general and biomedical domains making it suitable for domain transfer experiments. nerel-bio provides annotation for nested named entities as an extension of the scheme employed for nerel. nested named entities may cross entity boundaries to connect to shorter entities nested within longer entities, making them harder to detect.   nerel-bio contains annotations for 700+ russian and 100+ english abstracts. all english pubmed annotations have corresponding russian counterparts. thus, nerel-bio comprises the following specific features: annotation of nested named entities, it can be used as a benchmark for cross-domain (nerel -> nerel-bio) and cross-language (english -> russian) transfer. we experiment with both transformer-based sequence models and machine reading comprehension (mrc) models and report their results.   the dataset is freely available at https://github.com/nerel-ds/nerel-bio.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['natalia loukachevitch',\n",
       "   'suresh manandhar',\n",
       "   'elina baral',\n",
       "   'igor rozhkov',\n",
       "   'pavel braslavski',\n",
       "   'vladimir ivanov',\n",
       "   'tatiana batura',\n",
       "   'elena tutubalina'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11913'},\n",
       " {'title': 'efficient identification of informative features in simulation-based   inference',\n",
       "  'id': '2210.11915',\n",
       "  'abstract': 'simulation-based bayesian inference (sbi) can be used to estimate the parameters of complex mechanistic models given observed model outputs without requiring access to explicit likelihood evaluations. a prime example for the application of sbi in neuroscience involves estimating the parameters governing the response dynamics of hodgkin-huxley (hh) models from electrophysiological measurements, by inferring a posterior over the parameters that is consistent with a set of observations. to this end, many sbi methods employ a set of summary statistics or scientifically interpretable features to estimate a surrogate likelihood or posterior. however, currently, there is no way to identify how much each summary statistic or feature contributes to reducing posterior uncertainty. to address this challenge, one could simply compare the posteriors with and without a given feature included in the inference process. however, for large or nested feature sets, this would necessitate repeatedly estimating the posterior, which is computationally expensive or even prohibitive. here, we provide a more efficient approach based on the sbi method neural likelihood estimation (nle): we show that one can marginalize the trained surrogate likelihood post-hoc before inferring the posterior to assess the contribution of a feature. we demonstrate the usefulness of our method by identifying the most important features for inferring parameters of an example hh neuron model. beyond neuroscience, our method is generally applicable to sbi workflows that rely on data features for inference used in other scientific fields.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['jonas beck',\n",
       "   'michael deistler',\n",
       "   'yves bernaerts',\n",
       "   'jakob macke',\n",
       "   'philipp berens'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11915'},\n",
       " {'title': 'men also do laundry: multi-attribute bias amplification',\n",
       "  'id': '2210.11924',\n",
       "  'abstract': 'as computer vision systems become more widely deployed, there is increasing concern from both the research community and the public that these systems are not only reproducing but amplifying harmful social biases. the phenomenon of bias amplification, which is the focus of this work, refers to models amplifying inherent training set biases at test time. existing metrics measure bias amplification with respect to single annotated attributes (e.g., $\\\\texttt{computer}$). however, several visual datasets consist of images with multiple attribute annotations. we show models can learn to exploit correlations with respect to multiple attributes (e.g., {$\\\\texttt{computer}$, $\\\\texttt{keyboard}$}), which are not accounted for by current metrics. in addition, we show current metrics can give the erroneous impression that minimal or no bias amplification has occurred as they involve aggregating over positive and negative values. further, these metrics lack a clear desired value, making them difficult to interpret. to address these shortcomings, we propose a new metric: multi-attribute bias amplification. we validate our proposed metric through an analysis of gender bias amplification on the coco and imsitu datasets. finally, we benchmark bias mitigation methods using our proposed metric, suggesting possible avenues for future bias mitigation',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['dora zhao', 'jerone t. a. andrews', 'alice xiang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11924'},\n",
       " {'title': 'barrier hamiltonian monte carlo',\n",
       "  'id': '2210.11925',\n",
       "  'abstract': 'in this paper, we propose barrier hamiltonian monte carlo (bhmc), a version of hmc which aims at sampling from a gibbs distribution $\\\\pi$ on a manifold $\\\\mathsf{m}$, endowed with a hessian metric $\\\\mathfrak{g}$ derived from a self-concordant barrier. like riemannian manifold hmc, our method relies on hamiltonian dynamics which comprise $\\\\mathfrak{g}$. it incorporates the constraints defining $\\\\mathsf{m}$ and is therefore able to exploit its underlying geometry. we first introduce c-bhmc (continuous bhmc), for which we assume that the hamiltonian dynamics can be integrated exactly, and show that it generates a markov chain for which $\\\\pi$ is invariant. secondly, we design n-bhmc (numerical bhmc), a metropolis-hastings algorithm which combines an acceptance filter including a \"reverse integration check\" and numerical integrators of the hamiltonian dynamics. our main results establish that n-bhmc generates a reversible markov chain with respect to $\\\\pi$. this is in contrast to existing algorithms which extend the hmc method to riemannian manifolds, as they do not deal with asymptotic bias. our conclusions are supported by numerical experiments where we consider target distributions defined on polytopes.',\n",
       "  'categories': 'stat.ml cs.lg math.pr',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['maxence noble', 'valentin de bortoli', 'alain durmus'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11925'},\n",
       " {'title': 'deep reinforcement learning for inverse inorganic materials design',\n",
       "  'id': '2210.11931',\n",
       "  'abstract': 'a major obstacle to the realization of novel inorganic materials with desirable properties is the inability to perform efficient optimization across both materials properties and synthesis of those materials. in this work, we propose a reinforcement learning (rl) approach to inverse inorganic materials design, which can identify promising compounds with specified properties and synthesizability constraints. our model learns chemical guidelines such as charge and electronegativity neutrality while maintaining chemical diversity and uniqueness. we demonstrate a multi-objective rl approach, which can generate novel compounds with targeted materials properties including formation energy and bulk/shear modulus alongside a lower sintering temperature synthesis objectives. using this approach, the model can predict promising compounds of interest, while suggesting an optimized chemical design space for inorganic materials discovery.',\n",
       "  'categories': 'cond-mat.mtrl-sci cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['elton pan', 'christopher karpovich', 'elsa olivetti'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11931'},\n",
       " {'title': 'diican: dual time-scale state-coupled co-estimation of soc, soh and rul   for lithium-ion batteries',\n",
       "  'id': '2210.11941',\n",
       "  'abstract': \"accurate co-estimations of battery states, such as state-of-charge (soc), state-of-health (soh,) and remaining useful life (rul), are crucial to the battery management systems to assure safe and reliable management. although the external properties of the battery charge with the aging degree, batteries' degradation mechanism shares similar evolving patterns. since batteries are complicated chemical systems, these states are highly coupled with intricate electrochemical processes. a state-coupled co-estimation method named deep inter and intra-cycle attention network (diican) is proposed in this paper to estimate soc, soh, and rul, which organizes battery measurement data into the intra-cycle and inter-cycle time scales. and to extract degradation-related features automatically and adapt to practical working conditions, the convolutional neural network is applied. the state degradation attention unit is utilized to extract the battery state evolution pattern and evaluate the battery degradation degree. to account for the influence of battery aging on the soc estimation, the battery degradation-related state is incorporated in the soc estimation for capacity calibration. the diican method is validated on the oxford battery dataset. the experimental results show that the proposed method can achieve soh and rul co-estimation with high accuracy and effectively improve soc estimation accuracy for the whole lifespan.\",\n",
       "  'categories': 'eess.sy cs.lg cs.sy physics.data-an',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['ningbo cai', 'yuwen qin', 'xin chen', 'kai wu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11941'},\n",
       " {'title': 'oracles & followers: stackelberg equilibria in deep multi-agent   reinforcement learning',\n",
       "  'id': '2210.11942',\n",
       "  'abstract': 'stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or automated mechanism design, and have received increasing attention in the reinforcement learning literature recently. we present a general framework for implementing stackelberg equilibria search as a multi-agent rl problem, allowing a wide range of design choices. we discuss how previous approaches can be seen as specific instantiations of this framework. as a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-rl techniques for follower convergence. we evaluate examples of novel approaches predicted by our framework experimentally on standard benchmark domains. finally, we discuss directions for future work implied by our work.',\n",
       "  'categories': 'cs.gt cs.ai cs.lg cs.ma',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '',\n",
       "  'authors': ['matthias gerstgrasser', 'david c. parkes'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11942'},\n",
       " {'title': 'computer-aided cancer diagnosis via machine learning and deep learning:   a comparative review',\n",
       "  'id': '2210.11943',\n",
       "  'abstract': 'the past years have seen a considerable increase in cancer cases. however, a cancer diagnosis is often complex and depends on the types of images provided for analysis. it requires highly skilled practitioners but is often time-consuming and error-prone. if machine learning and deep learning algorithms have been widely used, a comprehensive review of the techniques used from the pre-processing steps to the final prediction is lacking. with this review, we aim to provide a comprehensive overview of the current steps required in building efficient and accurate machine learning algorithm for cancer prediction, detection and classification. to do so, we compile the results of cancer related study using ai over the past years. we include various cancers that encompass different types of images, and therefore different related techniques. we show that tremendous improvements have been made in the early detection of cancerous tumors and tissues. the techniques used are various and often problem-tailored and our findings is confirmed through the study of a large number of research. moreover, we investigate the approaches best suited for different types of images such as histology, dermoscopic, mri, etc. with this work, we summarize the main finding over the past years in cancer detection using deep learning techniques. we discuss the challenges of cancer research related to the large discrepancies in the images, and we provide some notable results in the field for lung, breast, and skin cancers.',\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '',\n",
       "  'authors': ['solene bechelli'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11943'},\n",
       " {'title': 'lo-fi: distributed fine-tuning without communication',\n",
       "  'id': '2210.11948',\n",
       "  'abstract': \"when fine-tuning large neural networks, it is common to use multiple nodes and to communicate gradients at each optimization step. by contrast, we investigate completely local fine-tuning, which we refer to as lo-fi. during lo-fi, each node is fine-tuned independently without any communication. then, the weights are averaged across nodes at the conclusion of fine-tuning. when fine-tuning deit-base and deit-large on imagenet, this procedure matches accuracy in-distribution and improves accuracy under distribution shift compared to the baseline, which observes the same amount of data but communicates gradients at each step. we also observe that lo-fi matches the baseline's performance when fine-tuning opt language models (up to 1.3b parameters) on common crawl. by removing the communication requirement, lo-fi reduces resource barriers for fine-tuning large models and enables fine-tuning in settings with prohibitive communication cost.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '',\n",
       "  'authors': ['mitchell wortsman',\n",
       "   'suchin gururangan',\n",
       "   'shen li',\n",
       "   'ali farhadi',\n",
       "   'ludwig schmidt',\n",
       "   'michael rabbat',\n",
       "   'ari s. morcos'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11948'},\n",
       " {'title': 'learning graphical factor models with riemannian optimization',\n",
       "  'id': '2210.11950',\n",
       "  'abstract': 'graphical models and factor analysis are well-established tools in multivariate statistics. while these models can be both linked to structures exhibited by covariance and precision matrices, they are generally not jointly leveraged within graph learning processes. this paper therefore addresses this issue by proposing a flexible algorithmic framework for graph learning under low-rank structural constraints on the covariance matrix. the problem is expressed as penalized maximum likelihood estimation of an elliptical distribution (a generalization of gaussian graphical models to possibly heavy-tailed distributions), where the covariance matrix is optionally constrained to be structured as low-rank plus diagonal (low-rank factor model). the resolution of this class of problems is then tackled with riemannian optimization, where we leverage geometries of positive definite matrices and positive semi-definite matrices of fixed rank that are well suited to elliptical models. numerical experiments on real-world data sets illustrate the effectiveness of the proposed approach.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['alexandre hippert-ferrer',\n",
       "   'florent bouchard',\n",
       "   'ammar mian',\n",
       "   'titouan vayer',\n",
       "   'arnaud breloy'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11950'},\n",
       " {'title': 'a ga-like dynamic probability method with mutual information for feature   selection',\n",
       "  'id': '2210.11954',\n",
       "  'abstract': \"feature selection plays a vital role in promoting the classifier's performance. however, current methods ineffectively distinguish the complex interaction in the selected features. to further remove these hidden negative interactions, we propose a ga-like dynamic probability (gadp) method with mutual information which has a two-layer structure. the first layer applies the mutual information method to obtain a primary feature subset. the ga-like dynamic probability algorithm, as the second layer, mines more supportive features based on the former candidate features. essentially, the ga-like method is one of the population-based algorithms so its work mechanism is similar to the ga. different from the popular works which frequently focus on improving ga's operators for enhancing the search ability and lowering the converge time, we boldly abandon ga's operators and employ the dynamic probability that relies on the performance of each chromosome to determine feature selection in the new generation. the dynamic probability mechanism significantly reduces the parameter number in ga that making it easy to use. as each gene's probability is independent, the chromosome variety in gadp is more notable than in traditional ga, which ensures gadp has a wider search space and selects relevant features more effectively and accurately. to verify our method's superiority, we evaluate our method under multiple conditions on 15 datasets. the results demonstrate the outperformance of the proposed method. generally, it has the best accuracy. further, we also compare the proposed model to the popular heuristic methods like pos, fpa, and woa. our model still owns advantages over them.\",\n",
       "  'categories': 'cs.it cs.lg cs.ne math.it',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['gaoshuai wang', 'fabrice lauri', 'amir hajjam el hassani'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.11954'},\n",
       " {'title': 'when expressivity meets trainability: fewer than $n$ neurons can work',\n",
       "  'id': '2210.12001',\n",
       "  'abstract': 'modern neural networks are often quite wide, causing large memory and computation costs. it is thus of great interest to train a narrower network. however, training narrow neural nets remains a challenging task. we ask two theoretical questions: can narrow networks have as strong expressivity as wide ones? if so, does the loss function exhibit a benign optimization landscape? in this work, we provide partially affirmative answers to both questions for 1-hidden-layer networks with fewer than $n$ (sample size) neurons when the activation is smooth. first, we prove that as long as the width $m \\\\geq 2n/d$ (where $d$ is the input dimension), its expressivity is strong, i.e., there exists at least one global minimizer with zero training loss. second, we identify a nice local region with no local-min or saddle points. nevertheless, it is not clear whether gradient descent can stay in this nice region. third, we consider a constrained optimization formulation where the feasible region is the nice local region, and prove that every kkt point is a nearly global minimizer. it is expected that projected gradient methods converge to kkt points under mild technical conditions, but we leave the rigorous convergence analysis to future work. thorough numerical results show that projected gradient methods on this constrained formulation significantly outperform sgd for training narrow neural nets.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['jiawei zhang',\n",
       "   'yushun zhang',\n",
       "   'mingyi hong',\n",
       "   'ruoyu sun',\n",
       "   'zhi-quan luo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12001'},\n",
       " {'title': 'integrated brier score based survival cobra -- a regression based   approach',\n",
       "  'id': '2210.12006',\n",
       "  'abstract': 'in this paper, we provide two novel regression-based integrations of combined regression strategy (cobra) ensemble using integrated brier score to predict conditional survival function. our proposition includes a weighted version of all predictions based on integrated brier score score made by all weak learners to predict the final survival function apart from the straight implementation. two different norms (frobenius and sup norm) used to figure out the proximity points in the algorithm. our implementations consider right-censored data too. we illustrate the proposed algorithms through few real-life data analysis.',\n",
       "  'categories': 'cs.lg cs.ai stat.ap stat.co',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['rahul goswami', 'arabin kumar dey'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12006'},\n",
       " {'title': 'hcl: improving graph representation with hierarchical contrastive   learning',\n",
       "  'id': '2210.12020',\n",
       "  'abstract': 'contrastive learning has emerged as a powerful tool for graph representation learning. however, most contrastive learning methods learn features of graphs with fixed coarse-grained scale, which might underestimate either local or global information. to capture more hierarchical and richer representation, we propose a novel hierarchical contrastive learning (hcl) framework that explicitly learns graph representation in a hierarchical manner. specifically, hcl includes two key components: a novel adaptive learning to pool (l2pool) method to construct more reasonable multi-scale graph topology for more comprehensive contrastive objective, a novel multi-channel pseudo-siamese network to further enable more expressive learning of mutual information within each scale. comprehensive experimental results show hcl achieves competitive performance on 12 datasets involving node classification, node clustering and graph classification. in addition, the visualization of learned representation reveals that hcl successfully captures meaningful characteristics of graphs.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['jun wang',\n",
       "   'weixun li',\n",
       "   'changyu hou',\n",
       "   'xin tang',\n",
       "   'yixuan qiao',\n",
       "   'rui fang',\n",
       "   'pengyong li',\n",
       "   'peng gao',\n",
       "   'guotong xie'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12020'},\n",
       " {'title': 'a causal framework to quantify the robustness of mathematical reasoning   with language models',\n",
       "  'id': '2210.12023',\n",
       "  'abstract': 'we have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. at the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when predicting a solution. building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands and math operators on the output solution. by grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. we apply our framework on a test bed of bivariate math word problems. our analysis shows that robustness does not appear to continuously improve as a function of scale, but that the recent llm, gpt-3-instruct (175b), achieves a dramatic improvement in both robustness and sensitivity, compared to all other gpt variants.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '2022-10-24',\n",
       "  'authors': ['alessandro stolfo',\n",
       "   'zhijing jin',\n",
       "   'kumar shridhar',\n",
       "   'bernhard schölkopf',\n",
       "   'mrinmaya sachan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12023'},\n",
       " {'title': 'evolution of neural tangent kernels under benign and adversarial   training',\n",
       "  'id': '2210.12030',\n",
       "  'abstract': \"two key challenges facing modern deep learning are mitigating deep networks' vulnerability to adversarial attacks and understanding deep learning's generalization capabilities. towards the first issue, many defense strategies have been developed, with the most common being adversarial training (at). towards the second challenge, one of the dominant theories that has emerged is the neural tangent kernel (ntk) -- a characterization of neural network behavior in the infinite-width limit. in this limit, the kernel is frozen, and the underlying feature map is fixed. in finite widths, however, there is evidence that feature learning happens at the earlier stages of the training (kernel learning) before a second phase where the kernel remains fixed (lazy training). while prior work has aimed at studying adversarial vulnerability through the lens of the frozen infinite-width ntk, there is no work that studies the adversarial robustness of the empirical/finite ntk during training. in this work, we perform an empirical study of the evolution of the empirical ntk under standard and adversarial training, aiming to disambiguate the effect of adversarial training on kernel learning and lazy training. we find under adversarial training, the empirical ntk rapidly converges to a different kernel (and feature map) than standard training. this new kernel provides adversarial robustness, even when non-robust training is performed on top of it. furthermore, we find that adversarial training on top of a fixed kernel can yield a classifier with $76.1\\\\%$ robust accuracy under pgd attacks with $\\\\varepsilon = 4/255$ on cifar-10.\",\n",
       "  'categories': 'cs.lg cs.ai cs.ne stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['noel loo', 'ramin hasani', 'alexander amini', 'daniela rus'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12030'},\n",
       " {'title': 'neuro-symbolic causal reasoning meets signaling game for emergent   semantic communications',\n",
       "  'id': '2210.12040',\n",
       "  'abstract': \"semantic communication (sc) aims to communicate reliably with minimal data transfer while simultaneously providing seamless connectivity to heterogeneous services and users. in this paper, a novel emergent sc (esc) system framework is proposed and is composed of a signaling game for emergent language design and a neuro-symbolic (nesy) artificial intelligence (ai) approach for causal reasoning. in order to design the language, the signaling game is solved using an alternating maximization between the communicating node's utilities. the emergent language helps create a context-aware transmit vocabulary (minimal semantic representation) and aids the reasoning process (enabling generalization to unseen scenarios) by splitting complex messages into simpler reasoning tasks for the receiver. the causal description at the transmitter is then modeled (a neural component) as a posterior distribution of the relevant attributes present in the data. using the reconstructed causal state, the receiver evaluates a set of logical formulas (symbolic part) to execute its task. the nodes nesy reasoning components are implemented by the recently proposed ai tool called generative flow networks, and they are optimized for higher semantic reliability. the esc system is designed to enhance the novel metrics of semantic information, reliability, distortion and similarity that are designed using rigorous algebraic properties from category theory thereby generalizing the metrics beyond shannon's notion of uncertainty. simulation results validate the ability of esc to communicate efficiently (with reduced bits) and achieve better semantic reliability than conventional wireless and state-of-the-art systems that do not exploit causal reasoning capabilities.\",\n",
       "  'categories': 'cs.lg cs.cl cs.it math.it',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['christo kurisummoottil thomas', 'walid saad'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12040'},\n",
       " {'title': 'ollivier-ricci curvature for hypergraphs: a unified framework',\n",
       "  'id': '2210.12048',\n",
       "  'abstract': 'bridging geometry and topology, curvature is a powerful and expressive invariant. while the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. on graphs, ollivier-ricci curvature measures differences between random walks via wasserstein distances, thus grounding a geometric concept in ideas from probability and optimal transport. we develop orchid, a flexible framework generalizing ollivier-ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that orchid curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice.',\n",
       "  'categories': 'cs.lg cs.si stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['corinna coupette', 'sebastian dalleiger', 'bastian rieck'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12048'},\n",
       " {'title': 'the privacy issue of counterfactual explanations: explanation linkage   attacks',\n",
       "  'id': '2210.12051',\n",
       "  'abstract': 'black-box machine learning models are being used in more and more high-stakes domains, which creates a growing need for explainable ai (xai). unfortunately, the use of xai in machine learning introduces new privacy risks, which currently remain largely unnoticed. we introduce the explanation linkage attack, which can occur when deploying instance-based strategies to find counterfactual explanations. to counter such an attack, we propose k-anonymous counterfactual explanations and introduce pureness as a new metric to evaluate the validity of these k-anonymous counterfactual explanations. our results show that making the explanations, rather than the whole dataset, k- anonymous, is beneficial for the quality of the explanations.',\n",
       "  'categories': 'cs.lg cs.cr cs.cy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['sofie goethals', 'kenneth sörensen', 'david martens'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12051'},\n",
       " {'title': 'towards global neural network abstractions with locally-exact   reconstruction',\n",
       "  'id': '2210.12054',\n",
       "  'abstract': 'neural networks are a powerful class of non-linear functions. however, their black-box nature makes it difficult to explain their behaviour and certify their safety. abstraction techniques address this challenge by transforming the neural network into a simpler, over-approximated function. unfortunately, existing abstraction techniques are slack, which limits their applicability to small local regions of the input domain. in this paper, we propose global interval neural network abstractions with center-exact reconstruction (ginnacer). our novel abstraction technique produces sound over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input. our experiments show that ginnacer is several orders of magnitude tighter than state-of-the-art global abstraction techniques, while being competitive with local ones.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['edoardo manino', 'iury bessa', 'lucas cordeiro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12054'},\n",
       " {'title': 'efficient global planning in large mdps via stochastic primal-dual   optimization',\n",
       "  'id': '2210.12057',\n",
       "  'abstract': 'we propose a new stochastic primal-dual optimization algorithm for planning in a large discounted markov decision process with a generative model and linear function approximation. assuming that the feature map approximately satisfies standard realizability and bellman-closedness conditions and also that the feature vectors of all state-action pairs are representable as convex combinations of a small core set of state-action pairs, we show that our method outputs a near-optimal policy after a polynomial number of queries to the generative model. our method is computationally efficient and comes with the major advantage that it outputs a single softmax policy that is compactly represented by a low-dimensional parameter vector, and does not need to execute computationally expensive local planning subroutines in runtime.',\n",
       "  'categories': 'cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['gergely neu', 'nneka okolo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12057'},\n",
       " {'title': 'validation of composite systems by discrepancy propagation',\n",
       "  'id': '2210.12061',\n",
       "  'abstract': 'assessing the validity of a real-world system with respect to given quality criteria is a common yet costly task in industrial applications due to the vast number of required real-world tests. validating such systems by means of simulation offers a promising and less expensive alternative, but requires an assessment of the simulation accuracy and therefore end-to-end measurements. additionally, covariate shifts between simulations and actual usage can cause difficulties for estimating the reliability of such systems. in this work, we present a validation method that propagates bounds on distributional discrepancy measures through a composite system, thereby allowing us to derive an upper bound on the failure probability of the real system from potentially inaccurate simulations. each propagation step entails an optimization problem, where -- for measures such as maximum mean discrepancy (mmd) -- we develop tight convex relaxations based on semidefinite programs. we demonstrate that our propagation method yields valid and useful bounds for composite systems exhibiting a variety of realistic effects. in particular, we show that the proposed method can successfully account for data shifts within the experimental design as well as model inaccuracies within the used simulation.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['david reeb',\n",
       "   'kanil patel',\n",
       "   'karim barsim',\n",
       "   'martin schiegg',\n",
       "   'sebastian gerwinn'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12061'},\n",
       " {'title': 'efficient dataset distillation using random feature approximation',\n",
       "  'id': '2210.12067',\n",
       "  'abstract': \"dataset distillation compresses large datasets into smaller synthetic coresets which retain performance with the aim of reducing the storage and computational burden of processing the entire dataset. today's best-performing algorithm, \\\\textit{kernel inducing points} (kip), which makes use of the correspondence between infinite-width neural networks and kernel-ridge regression, is prohibitively slow due to the exact computation of the neural tangent kernel matrix, scaling $o(|s|^2)$, with $|s|$ being the coreset size. to improve this, we propose a novel algorithm that uses a random feature approximation (rfa) of the neural network gaussian process (nngp) kernel, which reduces the kernel matrix computation to $o(|s|)$. our algorithm provides at least a 100-fold speedup over kip and can run on a single gpu. our new method, termed an rfa distillation (rfad), performs competitively with kip and other dataset condensation algorithms in accuracy over a range of large-scale datasets, both in kernel regression and finite-width network training. we demonstrate the effectiveness of our approach on tasks involving model interpretability and privacy preservation.\",\n",
       "  'categories': 'cs.lg cs.ai cs.ne stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['noel loo', 'ramin hasani', 'alexander amini', 'daniela rus'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12067'},\n",
       " {'title': 'neural networks for local search and crossover in vehicle routing: a   possible overkill?',\n",
       "  'id': '2210.12075',\n",
       "  'abstract': 'extensive research has been conducted, over recent years, on various ways of enhancing heuristic search for combinatorial optimization problems with machine learning algorithms. in this study, we investigate the use of predictions from graph neural networks (gnns) in the form of heatmaps to improve the hybrid genetic search (hgs), a state-of-the-art algorithm for the capacitated vehicle routing problem (cvrp). the crossover and local-search components of hgs are instrumental in finding improved solutions, yet these components essentially rely on simple greedy or random choices. it seems intuitive to attempt to incorporate additional knowledge at these levels. throughout a vast experimental campaign on more than 10,000 problem instances, we show that exploiting more sophisticated strategies using measures of node relatedness (heatmaps, or simply distance) within these algorithmic components can significantly enhance performance. however, contrary to initial expectations, we also observed that heatmaps did not present significant advantages over simpler distance measures for these purposes. therefore, we faced a common -- though rarely documented -- situation of overkill: gnns can indeed improve performance on an important optimization task, but an ablation analysis demonstrated that simpler alternatives perform equally well.',\n",
       "  'categories': 'cs.ne cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-09-09',\n",
       "  'updated': '',\n",
       "  'authors': ['ítalo santana', 'andrea lodi', 'thibaut vidal'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12075'},\n",
       " {'title': 'a non-asymptotic moreau envelope theory for high-dimensional generalized   linear models',\n",
       "  'id': '2210.12082',\n",
       "  'abstract': 'we prove a new generalization bound that shows for any class of linear predictors in gaussian space, the rademacher complexity of the class and the training error under any continuous loss $\\\\ell$ can control the test error under all moreau envelopes of the loss $\\\\ell$. we use our finite-sample bound to directly recover the \"optimistic rate\" of zhou et al. (2021) for linear regression with the square loss, which is known to be tight for minimal $\\\\ell_2$-norm interpolation, but we also handle more general settings where the label is generated by a potentially misspecified multi-index model. the same argument can analyze noisy interpolation of max-margin classifiers through the squared hinge loss, and establishes consistency results in spiked-covariance settings. more generally, when the loss is only assumed to be lipschitz, our bound effectively improves talagrand\\'s well-known contraction lemma by a factor of two, and we prove uniform convergence of interpolators (koehler et al. 2021) for all smooth, non-negative losses. finally, we show that application of our generalization bound using localized gaussian width will generally be sharp for empirical risk minimizers, establishing a non-asymptotic moreau envelope theory for generalization that applies outside of proportional scaling regimes, handles model misspecification, and complements existing asymptotic moreau envelope theories for m-estimation.',\n",
       "  'categories': 'stat.ml cs.lg math.st stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['lijia zhou',\n",
       "   'frederic koehler',\n",
       "   'pragya sur',\n",
       "   'danica j. sutherland',\n",
       "   'nathan srebro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12082'},\n",
       " {'title': \"decoding a neural retriever's latent space for query suggestion\",\n",
       "  'id': '2210.12084',\n",
       "  'abstract': 'neural retrieval models have superseded classic bag-of-words methods such as bm25 as the retrieval framework of choice. however, neural systems lack the interpretability of bag-of-words models; it is not trivial to connect a query change to a change in the latent space that ultimately determines the retrieval results. to shed light on this embedding space, we learn a \"query decoder\" that, given a latent representation of a neural search engine, generates the corresponding query. we show that it is possible to decode a meaningful query from its latent representation and, when moving in the right direction in latent space, to decode a query that retrieves the relevant paragraph. in particular, the query decoder can be useful to understand \"what should have been asked\" to retrieve a particular paragraph from the collection. we employ the query decoder to generate a large synthetic dataset of query reformulations for msmarco, leading to improved retrieval performance. on this data, we train a pseudo-relevance feedback (prf) t5 model for the application of query suggestion that outperforms both query reformulation and prf information retrieval baselines.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['leonard adolphs',\n",
       "   'michelle chen huebscher',\n",
       "   'christian buck',\n",
       "   'sertan girgin',\n",
       "   'olivier bachem',\n",
       "   'massimiliano ciaramita',\n",
       "   'thomas hofmann'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12084'},\n",
       " {'title': 'a survey on graph counterfactual explanations: definitions, methods,   evaluation',\n",
       "  'id': '2210.12089',\n",
       "  'abstract': \"in recent years, graph neural networks have reported outstanding performance in tasks like community detection, molecule classification and link prediction. however, the black-box nature of these models prevents their application in domains like health and finance, where understanding the models' decisions is essential. counterfactual explanations (ce) provide these understandings through examples. moreover, the literature on ce is flourishing with novel explanation methods which are tailored to graph learning.   in this survey, we analyse the existing graph counterfactual explanation methods, by providing the reader with an organisation of the literature according to a uniform formal notation for definitions, datasets, and metrics, thus, simplifying potential comparisons w.r.t to the method advantages and disadvantages. we discussed seven methods and sixteen synthetic and real datasets providing details on the possible generation strategies. we highlight the most common evaluation strategies and formalise nine of the metrics used in the literature. we first introduce the evaluation framework gretel and how it is possible to extend and use it while providing a further dimension of comparison encompassing reproducibility aspects. finally, we provide a discussion on how counterfactual explanation interplays with privacy and fairness, before delving into open challenges and future works.\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['mario alfonso prado-romero',\n",
       "   'bardh prenkaj',\n",
       "   'giovanni stilo',\n",
       "   'fosca giannotti'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12089'},\n",
       " {'title': 'autoprognosis 2.0: democratizing diagnostic and prognostic modeling in   healthcare with automated machine learning',\n",
       "  'id': '2210.12090',\n",
       "  'abstract': 'diagnostic and prognostic models are increasingly important in medicine and inform many clinical decisions. recently, machine learning approaches have shown improvement over conventional modeling techniques by better capturing complex interactions between patient covariates in a data-driven manner. however, the use of machine learning introduces a number of technical and practical challenges that have thus far restricted widespread adoption of such techniques in clinical settings. to address these challenges and empower healthcare professionals, we present a machine learning framework, autoprognosis 2.0, to develop diagnostic and prognostic models. autoprognosis leverages state-of-the-art advances in automated machine learning to develop optimized machine learning pipelines, incorporates model explainability tools, and enables deployment of clinical demonstrators, without requiring significant technical expertise. our framework eliminates the major technical obstacles to predictive modeling with machine learning that currently impede clinical adoption. to demonstrate autoprognosis 2.0, we provide an illustrative application where we construct a prognostic risk score for diabetes using the uk biobank, a prospective study of 502,467 individuals. the models produced by our automated framework achieve greater discrimination for diabetes than expert clinical risk scores. our risk score has been implemented as a web-based decision support tool and can be publicly accessed by patients and clinicians worldwide. in addition, autoprognosis 2.0 is provided as an open-source python package. by open-sourcing our framework as a tool for the community, clinicians and other medical practitioners will be able to readily develop new risk scores, personalized diagnostics, and prognostics using modern machine learning techniques.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['fergus imrie',\n",
       "   'bogdan cebere',\n",
       "   'eoin f. mckinney',\n",
       "   'mihaela van der schaar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12090'},\n",
       " {'title': 'robust singular values based on l1-norm pca',\n",
       "  'id': '2210.12097',\n",
       "  'abstract': 'singular-value decomposition (svd) is a ubiquitous data analysis method in engineering, science, and statistics. singular-value estimation, in particular, is of critical importance in an array of engineering applications, such as channel estimation in communication systems, electromyography signal analysis, and image compression, to name just a few. conventional svd of a data matrix coincides with standard principal-component analysis (pca). the l2-norm (sum of squared values) formulation of pca promotes peripheral data points and, thus, makes pca sensitive against outliers. naturally, svd inherits this outlier sensitivity. in this work, we present a novel robust non-parametric method for svd and singular-value estimation based on a l1-norm (sum of absolute values) formulation, which we name l1-csvd. accordingly, the proposed method demonstrates sturdy resistance against outliers and can facilitate more reliable data analysis and processing in a wide range of engineering applications.',\n",
       "  'categories': 'eess.sp cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['duc le', 'panos p. markopoulos'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12097'},\n",
       " {'title': 'triplet losses-based matrix factorization for robust recommendations',\n",
       "  'id': '2210.12098',\n",
       "  'abstract': 'much like other learning-based models, recommender systems can be affected by biases in the training data. while typical evaluation metrics (e.g. hit rate) are not concerned with them, some categories of final users are heavily affected by these biases. in this work, we propose using multiple triplet losses terms to extract meaningful and robust representations of users and items. we empirically evaluate the soundness of such representations through several \"bias-aware\" evaluation metrics, as well as in terms of stability to changes in the training set and agreement of the predictions variance w.r.t. that of each user.',\n",
       "  'categories': 'cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['flavio giobergia'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12098'},\n",
       " {'title': 'boomerang: local sampling on image manifolds using diffusion models',\n",
       "  'id': '2210.12100',\n",
       "  'abstract': 'diffusion models can be viewed as mapping points in a high-dimensional latent space onto a low-dimensional learned manifold, typically an image manifold. the intermediate values between the latent space and image manifold can be interpreted as noisy images which are determined by the noise scheduling scheme employed during pre-training. we exploit this interpretation to introduce boomerang, a local image manifold sampling approach using the dynamics of diffusion models. we call it boomerang because we first add noise to an input image, moving it closer to the latent space, then bring it back to the image space through diffusion dynamics. we use this method to generate images which are similar, but nonidentical, to the original input images on the image manifold. we are able to set how close the generated image is to the original based on how much noise we add. additionally, the generated images have a degree of stochasticity, allowing us to locally sample as many times as we want without repetition. we show three applications for which boomerang can be used. first, we provide a framework for constructing privacy-preserving datasets having controllable degrees of anonymity. second, we show how to use boomerang for data augmentation while staying on the image manifold. third, we introduce a framework for image super-resolution with 8x upsampling. boomerang does not require any modification to the training of diffusion models and can be used with pretrained models on a single, inexpensive gpu.',\n",
       "  'categories': 'cs.cv cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['lorenzo luzi',\n",
       "   'ali siahkoohi',\n",
       "   'paul m mayer',\n",
       "   'josue casco-rodriguez',\n",
       "   'richard baraniuk'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12100'},\n",
       " {'title': 'neural network approximations of pdes beyond linearity: representational   perspective',\n",
       "  'id': '2210.12101',\n",
       "  'abstract': 'a burgeoning line of research has developed deep neural networks capable of approximating the solutions to high dimensional pdes, opening related lines of theoretical inquiry focused on explaining how it is that these models appear to evade the curse of dimensionality. however, most theoretical analyses thus far have been limited to linear pdes. in this work, we take a step towards studying the representational power of neural networks for approximating solutions to nonlinear pdes. we focus on a class of pdes known as \\\\emph{nonlinear elliptic variational pdes}, whose solutions minimize an \\\\emph{euler-lagrange} energy functional $\\\\mathcal{e}(u) = \\\\int_\\\\omega l(\\\\nabla u) dx$. we show that if composing a function with barron norm $b$ with $l$ produces a function of barron norm at most $b_l b^p$, the solution to the pde can be $\\\\epsilon$-approximated in the $l^2$ sense by a function with barron norm $o\\\\left(\\\\left(db_l\\\\right)^{p^{\\\\log(1/\\\\epsilon)}}\\\\right)$. by a classical result due to barron [1993], this correspondingly bounds the size of a 2-layer neural network needed to approximate the solution. treating $p, \\\\epsilon, b_l$ as constants, this quantity is polynomial in dimension, thus showing neural networks can evade the curse of dimensionality. our proof technique involves neurally simulating (preconditioned) gradient in an appropriate hilbert space, which converges exponentially fast to the solution of the pde, and such that we can bound the increase of the barron norm at each iterate. our results subsume and substantially generalize analogous prior results for linear elliptic pdes.',\n",
       "  'categories': 'cs.lg cs.na math.na',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['tanya marwah',\n",
       "   'zachary c. lipton',\n",
       "   'jianfeng lu',\n",
       "   'andrej risteski'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12101'},\n",
       " {'title': 'towards transparent ann wind turbine power curve models',\n",
       "  'id': '2210.12104',\n",
       "  'abstract': 'accurate wind turbine power curve models, which translate ambient conditions into turbine power output, are crucial for wind energy to scale and fulfil its proposed role in the global energy transition. machine learning methods, in particular deep neural networks (dnns), have shown significant advantages over parametric, physics-informed power curve modelling approaches. nevertheless, they are often criticised as opaque black boxes with no physical understanding of the system they model, which hinders their application in practice. we apply shapley values, a popular explainable artificial intelligence (xai) method, to, for the first time, uncover and validate the strategies learned by dnns from operational wind turbine data. our findings show that the trend towards ever larger model architectures, driven by the focus on test-set performance, can result in physically implausible model strategies, similar to the clever hans effect observed in classification. we, therefore, call for a more prominent role of xai methods in model selection and additionally offer a practical strategy to use model explanations for wind turbine condition monitoring.',\n",
       "  'categories': 'cs.lg eess.sp',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['simon letzgus'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12104'},\n",
       " {'title': 'adversarial permutation invariant training for universal sound   separation',\n",
       "  'id': '2210.12108',\n",
       "  'abstract': 'universal sound separation consists of separating mixes with arbitrary sounds of different types, and permutation invariant training (pit) is used to train source agnostic models that do so. in this work, we complement pit with adversarial losses but find it challenging with the standard formulation used in speech source separation. we overcome this challenge with a novel i-replacement context-based adversarial loss, and by training with multiple discriminators. our experiments show that by simply improving the loss (keeping the same model and dataset) we obtain a non-negligible improvement of 1.4 db si-snri in the reverberant fuss dataset. we also find adversarial pit to be effective at reducing spectral holes, ubiquitous in mask-based separation models, which highlights the potential relevance of adversarial losses for source separation.',\n",
       "  'categories': 'cs.sd cs.lg eess.as',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['emilian postolache',\n",
       "   'jordi pons',\n",
       "   'santiago pascual',\n",
       "   'joan serrà'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12108'},\n",
       " {'title': 'multitask brain tumor inpainting with diffusion models: a methodological   report',\n",
       "  'id': '2210.12113',\n",
       "  'abstract': 'despite the ever-increasing interest in applying deep learning (dl) models to medical imaging, the typical scarcity and imbalance of medical datasets can severely impact the performance of dl models. the generation of synthetic data that might be freely shared without compromising patient privacy is a well-known technique for addressing these difficulties. inpainting algorithms are a subset of dl generative models that can alter one or more regions of an input image while matching its surrounding context and, in certain cases, non-imaging input conditions. although the majority of inpainting techniques for medical imaging data use generative adversarial networks (gans), the performance of these algorithms is frequently suboptimal due to their limited output variety, a problem that is already well-known for gans. denoising diffusion probabilistic models (ddpms) are a recently introduced family of generative networks that can generate results of comparable quality to gans, but with diverse outputs. in this paper, we describe a ddpm to execute multiple inpainting tasks on 2d axial slices of brain mri with various sequences, and present proof-of-concept examples of its performance in a variety of evaluation scenarios. our model and a public online interface to try our tool are available at: https://github.com/mayo-radiology-informatics-lab/mbti',\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['pouria rouzrokh',\n",
       "   'bardia khosravi',\n",
       "   'shahriar faghani',\n",
       "   'mana moassefi',\n",
       "   'sanaz vahdati',\n",
       "   'bradley j. erickson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12113'},\n",
       " {'title': 'targeted active learning for probabilistic models',\n",
       "  'id': '2210.12122',\n",
       "  'abstract': 'a fundamental task in science is to design experiments that yield valuable insights about the system under study. mathematically, these insights can be represented as a utility or risk function that shapes the value of conducting each experiment. we present pdbal, a targeted active learning method that adaptively designs experiments to maximize scientific utility. pdbal takes a user-specified risk function and combines it with a probabilistic model of the experimental outcomes to choose designs that rapidly converge on a high-utility model. we prove theoretical bounds on the label complexity of pdbal and provide fast closed-form solutions for designing experiments with common exponential family likelihoods. in simulation studies, pdbal consistently outperforms standard untargeted approaches that focus on maximizing expected information gain over the design space. finally, we demonstrate the scientific potential of pdbal through a study on a large cancer drug screen dataset where pdbal quickly recovers the most efficacious drugs with a small fraction of the total number of experiments.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['christopher tosh', 'mauricio tec', 'wesley tansey'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12122'},\n",
       " {'title': 'equivariant networks for zero-shot coordination',\n",
       "  'id': '2210.12124',\n",
       "  'abstract': 'successful coordination in dec-pomdps requires agents to adopt robust strategies and interpretable styles of play for their partner. a common failure mode is symmetry breaking, when agents arbitrarily converge on one out of many equivalent but mutually incompatible policies. commonly these examples include partial observability, e.g. waving your right hand vs. left hand to convey a covert message. in this paper, we present a novel equivariant network architecture for use in dec-pomdps that prevents the agent from learning policies which break symmetries, doing so more effectively than prior methods. our method also acts as a \"coordination-improvement operator\" for generic, pre-trained policies, and thus may be applied at test-time in conjunction with any self-play algorithm. we provide theoretical guarantees of our work and test on the ai benchmark task of hanabi, where we demonstrate our methods outperforming other symmetry-aware baselines in zero-shot coordination, as well as able to improve the coordination ability of a variety of pre-trained policies. in particular, we show our method can be used to improve on the state of the art for zero-shot coordination on the hanabi benchmark.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['darius muglich',\n",
       "   'christian schroeder de witt',\n",
       "   'elise van der pol',\n",
       "   'shimon whiteson',\n",
       "   'jakob foerster'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12124'},\n",
       " {'title': 'neural fields for robotic object manipulation from a single image',\n",
       "  'id': '2210.12126',\n",
       "  'abstract': 'we present a unified and compact representation for object rendering, 3d reconstruction, and grasp pose prediction that can be inferred from a single image within a few seconds. we achieve this by leveraging recent advances in the neural radiance field (nerf) literature that learn category-level priors and fine-tune on novel objects with minimal data and time. our insight is that we can learn a compact shape representation and extract meaningful additional information from it, such as grasping poses. we believe this to be the first work to retrieve grasping poses directly from a nerf-based representation using a single viewpoint (rgb-only), rather than going through a secondary network and/or representation. when compared to prior art, our method is two to three orders of magnitude smaller while achieving comparable performance at view reconstruction and grasping. accompanying our method, we also propose a new dataset of rendered shoes for training a sim-2-real nerf method with grasping poses for different widths of grippers.',\n",
       "  'categories': 'cs.ro cs.ai cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['valts blukis',\n",
       "   'taeyeop lee',\n",
       "   'jonathan tremblay',\n",
       "   'bowen wen',\n",
       "   'in so kweon',\n",
       "   'kuk-jin yoon',\n",
       "   'dieter fox',\n",
       "   'stan birchfield'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12126'},\n",
       " {'title': 'graph few-shot learning with task-specific structures',\n",
       "  'id': '2210.12130',\n",
       "  'abstract': 'graph few-shot learning is of great importance among various graph learning tasks. under the few-shot scenario, models are often required to conduct classification given limited labeled samples. existing graph few-shot learning methods typically leverage graph neural networks (gnns) and perform classification across a series of meta-tasks. nevertheless, these methods generally rely on the original graph (i.e., the graph that the meta-task is sampled from) to learn node representations. consequently, the graph structure used in each meta-task is identical. since the class sets are different across meta-tasks, node representations should be learned in a task-specific manner to promote classification performance. therefore, to adaptively learn node representations across meta-tasks, we propose a novel framework that learns a task-specific structure for each meta-task. to handle the variety of nodes across meta-tasks, we extract relevant nodes and learn task-specific structures based on node influence and mutual information. in this way, we can learn node representations with the task-specific structure tailored for each meta-task. we further conduct extensive experiments on five node classification datasets under both single- and multiple-graph settings to validate the superiority of our framework over the state-of-the-art baselines. our code is provided at https://github.com/songw-sw/glitter.',\n",
       "  'categories': 'cs.lg cs.si',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['song wang', 'chen chen', 'jundong li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12130'},\n",
       " {'title': 'geometric sparse coding in wasserstein space',\n",
       "  'id': '2210.12135',\n",
       "  'abstract': 'wasserstein dictionary learning is an unsupervised approach to learning a collection of probability distributions that generate observed distributions as wasserstein barycentric combinations. existing methods for wasserstein dictionary learning optimize an objective that seeks a dictionary with sufficient representation capacity via barycentric interpolation to approximate the observed training data, but without imposing additional structural properties on the coefficients associated to the dictionary. this leads to dictionaries that densely represent the observed data, which makes interpretation of the coefficients challenging and may also lead to poor empirical performance when using the learned coefficients in downstream tasks. in contrast and motivated by sparse dictionary learning in euclidean spaces, we propose a geometrically sparse regularizer for wasserstein space that promotes representations of a data point using only nearby dictionary elements. we show this approach leads to sparse representations in wasserstein space and addresses the problem of non-uniqueness of barycentric representation. moreover, when data is generated as wasserstein barycenters of fixed distributions, this regularizer facilitates the recovery of the generating distributions in cases that are ill-posed for unregularized wasserstein dictionary learning. through experimentation on synthetic and real data, we show that our geometrically regularized approach yields sparser and more interpretable dictionaries in wasserstein space, which perform better in downstream applications.',\n",
       "  'categories': 'cs.lg cs.cv eess.sp math.oc math.pr math.st stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['marshall mueller',\n",
       "   'shuchin aeron',\n",
       "   'james m. murphy',\n",
       "   'abiy tasissa'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12135'},\n",
       " {'title': 'a multi-scale deep learning framework for projecting weather extremes',\n",
       "  'id': '2210.12137',\n",
       "  'abstract': \"weather extremes are a major societal and economic hazard, claiming thousands of lives and causing billions of dollars in damage every year. under climate change, their impact and intensity are expected to worsen significantly. unfortunately, general circulation models (gcms), which are currently the primary tool for climate projections, cannot characterize weather extremes accurately. to address this, we present a multi-resolution deep-learning framework that, firstly, corrects a gcm's biases by matching low-order and tail statistics of its output with observations at coarse scales; and secondly, increases the level of detail of the debiased gcm output by reconstructing the finer scales as a function of the coarse scales. we use the proposed framework to generate statistically realistic realizations of the climate over western europe from a simple gcm corrected using observational atmospheric reanalysis. we also discuss implications for probabilistic risk assessment of natural disasters in a changing climate.\",\n",
       "  'categories': 'cs.lg physics.ao-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['antoine blanchard',\n",
       "   'nishant parashar',\n",
       "   'boyko dodov',\n",
       "   'christian lessig',\n",
       "   'themistoklis sapsis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12137'},\n",
       " {'title': 'unsupervised multi-object segmentation by predicting probable motion   patterns',\n",
       "  'id': '2210.12148',\n",
       "  'abstract': 'we propose a new approach to learn to segment multiple image objects without manual supervision. the method can extract objects form still images, but uses videos for supervision. while prior works have considered motion for segmentation, a key insight is that, while motion can be used to identify objects, not all objects are necessarily in motion: the absence of motion does not imply the absence of objects. hence, our model learns to predict image regions that are likely to contain motion patterns characteristic of objects moving rigidly. it does not predict specific motion, which cannot be done unambiguously from a still image, but a distribution of possible motions, which includes the possibility that an object does not move at all. we demonstrate the advantage of this approach over its deterministic counterpart and show state-of-the-art unsupervised object segmentation performance on simulated and real-world benchmarks, surpassing methods that use motion even at test time. as our approach is applicable to variety of network architectures that segment the scenes, we also apply it to existing image reconstruction-based models showing drastic improvement. project page and code: https://www.robots.ox.ac.uk/~vgg/research/ppmp .',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['laurynas karazija',\n",
       "   'subhabrata choudhury',\n",
       "   'iro laina',\n",
       "   'christian rupprecht',\n",
       "   'andrea vedaldi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12148'},\n",
       " {'title': 'on amortizing convex conjugates for optimal transport',\n",
       "  'id': '2210.12153',\n",
       "  'abstract': 'this paper focuses on computing the convex conjugate operation that arises when solving euclidean wasserstein-2 optimal transport problems. this conjugation, which is also referred to as the legendre-fenchel conjugate or $c$-transform, is considered difficult to compute and in practice, wasserstein-2 methods are limited by not being able to exactly conjugate the dual potentials in continuous space. i show that combining amortized approximations to the conjugate with a solver for fine-tuning is computationally easy. this combination significantly improves the quality of transport maps learned for the wasserstein-2 benchmark by korotin et al. (2021) and is able to model many 2-dimensional couplings and flows considered in the literature. all of the baselines, methods, and solvers in this paper are available at http://github.com/facebookresearch/w2ot',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['brandon amos'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12153'},\n",
       " {'title': 'improving medical predictions by irregular multimodal electronic health   records modeling',\n",
       "  'id': '2210.12156',\n",
       "  'abstract': 'health conditions among patients in intensive care units (icus) are monitored via electronic health records (ehrs), composed of numerical time series and lengthy clinical note sequences, both taken at irregular time intervals. dealing with such irregularity in every modality, and integrating irregularity into multimodal representations to improve medical predictions, is a challenging problem. in this paper, we address this problem by (1) modeling irregular time series by incorporating hand-crafted imputation embeddings into learned interpolation embeddings via a gating mechanism; (2) casting a series of clinical note representations as multivariate irregular time series and tackling irregularity via a time attention mechanism; and (3) fusing multimodalities with an interleaved attention mechanism across temporal steps to integrate irregularity into multimodal representations. to the best of our knowledge, this is the first work to thoroughly model irregularity in multimodalities and to take into account temporal knowledge in multimodal fusion, for improving medical predictions. the results on two medical prediction tasks show that our proposed methods outperform the state-of-the-art (sota) methods in both every single modality and multimodal fusion scenarios, illustrating the effectiveness of our methods and the value of modeling irregularity in multimodal fusion.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-18',\n",
       "  'updated': '',\n",
       "  'authors': ['xinlu zhang',\n",
       "   'shiyang li',\n",
       "   'zhiyu chen',\n",
       "   'xifeng yan',\n",
       "   'linda petzold'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12156'},\n",
       " {'title': 'graph coloring via neural networks for haplotype assembly and viral   quasispecies reconstruction',\n",
       "  'id': '2210.12158',\n",
       "  'abstract': 'understanding genetic variation, e.g., through mutations, in organisms is crucial to unravel their effects on the environment and human health. a fundamental characterization can be obtained by solving the haplotype assembly problem, which yields the variation across multiple copies of chromosomes. variations among fast evolving viruses that lead to different strains (called quasispecies) are also deciphered with similar approaches. in both these cases, high-throughput sequencing technologies that provide oversampled mixtures of large noisy fragments (reads) of genomes, are used to infer constituent components (haplotypes or quasispecies). the problem is harder for polyploid species where there are more than two copies of chromosomes. state-of-the-art neural approaches to solve this np-hard problem do not adequately model relations among the reads that are important for deconvolving the input signal. we address this problem by developing a new method, called neurhap, that combines graph representation learning with combinatorial optimization. our experiments demonstrate substantially better performance of neurhap in real and synthetic datasets compared to competing approaches.',\n",
       "  'categories': 'q-bio.gn cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['hansheng xue', 'vaibhav rajan', 'yu lin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12158'},\n",
       " {'title': 'task-based assessment for neural networks: evaluating undersampled mri   reconstructions based on human observer signal detection',\n",
       "  'id': '2210.12161',\n",
       "  'abstract': 'recent research has explored using neural networks to reconstruct undersampled magnetic resonance imaging (mri) data. because of the complexity of the artifacts in the reconstructed images, there is a need to develop task-based approaches of image quality. common metrics for evaluating image quality like the normalized root mean squared error (nrmse) and structural similarity (ssim) are global metrics which average out impact of subtle features in the images. using measures of image quality which incorporate a subtle signal for a specific task allow for image quality assessment which locally evaluates the effect of undersampling on a signal. we used a u-net to reconstruct under-sampled images with 2x, 3x, 4x and 5x fold 1-d undersampling rates. cross validation was performed for a 500 and a 4000 image training set with both structural similarity (ssim) and mean squared error (mse) losses. a two alternative forced choice (2-afc) observer study was carried out for detecting a subtle signal (small blurred disk) from images with the 4000 image training set. we found that for both loss functions and training set sizes, the human observer performance on the 2-afc studies led to a choice of a 2x undersampling but the ssim and nrmse led to a choice of a 3x undersampling. for this task, ssim and nrmse led to an overestimate of the achievable undersampling using a u-net before a steep loss of image quality when compared to the performance of human observers in the detection of a subtle lesion.',\n",
       "  'categories': 'eess.iv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['joshua d. herman',\n",
       "   'rachel e. roca',\n",
       "   \"alexandra g. o'neill\",\n",
       "   'marcus l. wong',\n",
       "   'sajan g. lingala',\n",
       "   'angel r. pineda'],\n",
       "  'affiliation': ['mathematics department, manhattan college, ny',\n",
       "   'mathematics department, manhattan college, ny',\n",
       "   'mathematics department, manhattan college, ny',\n",
       "   'mathematics department, manhattan college, ny',\n",
       "   'roy j. carver department of biomedical engineering, university of iowa, iowa city',\n",
       "   'mathematics department, manhattan college, ny'],\n",
       "  'url': 'https://arxiv.org/abs/2210.12161'},\n",
       " {'title': 'joint coreference resolution for zeros and non-zeros in arabic',\n",
       "  'id': '2210.12169',\n",
       "  'abstract': 'most existing proposals about anaphoric zero pronoun (azp) resolution regard full mention coreference and azp resolution as two independent tasks, even though the two tasks are clearly related. the main issues that need tackling to develop a joint model for zero and non-zero mentions are the difference between the two types of arguments (zero pronouns, being null, provide no nominal information) and the lack of annotated datasets of a suitable size in which both types of arguments are annotated for languages other than chinese and japanese. in this paper, we introduce two architectures for jointly resolving azps and non-azps, and evaluate them on arabic, a language for which, as far as we know, there has been no prior work on joint resolution. doing this also required creating a new version of the arabic subset of the standard coreference resolution dataset used for the conll-2012 shared task (pradhan et al.,2012) in which both zeros and non-zeros are included in a single dataset.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['abdulrahman aloraini', 'sameer pradhan', 'massimo poesio'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12169'},\n",
       " {'title': 'an unsupervised latent/output physics-informed convolutional-lstm   network for solving partial differential equations using peridynamic   differential operator',\n",
       "  'id': '2210.12177',\n",
       "  'abstract': 'this study presents a novel unsupervised convolutional neural network (nn) architecture with nonlocal interactions for solving partial differential equations (pdes). the nonlocal peridynamic differential operator (pddo) is employed as a convolutional filter for evaluating derivatives the field variable. the nn captures the time-dynamics in smaller latent space through encoder-decoder layers with a convolutional long-short term memory (convlstm) layer between them. the convlstm architecture is modified by employing a novel activation function to improve the predictive capability of the learning architecture for physics with periodic behavior. the physics is invoked in the form of governing equations at the output of the nn and in the latent (reduced) space. by considering a few benchmark pdes, we demonstrate the training performance and extrapolation capability of this novel nn architecture by comparing against physics informed neural networks (pinn) type solvers. it is more capable of extrapolating the solution for future timesteps than the other existing architectures.',\n",
       "  'categories': 'cs.lg cs.na math.na',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['a. mavi', 'a. c. bekar', 'e. haghighat', 'e. madenci'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12177'},\n",
       " {'title': 'the dark side of automl: towards architectural backdoor search',\n",
       "  'id': '2210.12179',\n",
       "  'abstract': \"this paper asks the intriguing question: is it possible to exploit neural architecture search (nas) as a new attack vector to launch previously improbable attacks? specifically, we present evas, a new attack that leverages nas to find neural architectures with inherent backdoors and exploits such vulnerability using input-aware triggers. compared with existing attacks, evas demonstrates many interesting properties: (i) it does not require polluting training data or perturbing model parameters; (ii) it is agnostic to downstream fine-tuning or even re-training from scratch; (iii) it naturally evades defenses that rely on inspecting model parameters or training data. with extensive evaluation on benchmark datasets, we show that evas features high evasiveness, transferability, and robustness, thereby expanding the adversary's design spectrum. we further characterize the mechanisms underlying evas, which are possibly explainable by architecture-level ``shortcuts'' that recognize trigger patterns. this work raises concerns about the current practice of nas and points to potential directions to develop effective countermeasures.\",\n",
       "  'categories': 'cs.cr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['ren pang',\n",
       "   'changjiang li',\n",
       "   'zhaohan xi',\n",
       "   'shouling ji',\n",
       "   'ting wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12179'},\n",
       " {'title': 'a new perspective for understanding generalization gap of deep neural   networks trained with large batch sizes',\n",
       "  'id': '2210.12184',\n",
       "  'abstract': \"deep neural networks (dnns) are typically optimized using various forms of mini-batch gradient descent algorithm. a major motivation for mini-batch gradient descent is that with a suitably chosen batch size, available computing resources can be optimally utilized (including parallelization) for fast model training. however, many works report the progressive loss of model generalization when the training batch size is increased beyond some limits. this is a scenario commonly referred to as generalization gap. although several works have proposed different methods for alleviating the generalization gap problem, a unanimous account for understanding generalization gap is still lacking in the literature. this is especially important given that recent works have observed that several proposed solutions for generalization gap problem such learning rate scaling and increased training budget do not indeed resolve it. as such, our main exposition in this paper is to investigate and provide new perspectives for the source of generalization loss for dnns trained with a large batch size. our analysis suggests that large training batch size results in increased near-rank loss of units' activation (i.e. output) tensors, which consequently impacts model optimization and generalization. extensive experiments are performed for validation on popular dnn models such as vgg-16, residual network (resnet-56) and lenet-5 using cifar-10, cifar-100, fashion-mnist and mnist datasets.\",\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['oyebade k. oyedotun',\n",
       "   'konstantinos papadopoulos',\n",
       "   'djamila aouada'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12184'},\n",
       " {'title': 'attention-based scattering network for satellite imagery',\n",
       "  'id': '2210.12185',\n",
       "  'abstract': 'multi-channel satellite imagery, from stacked spectral bands or spatiotemporal data, have meaningful representations for various atmospheric properties. combining these features in an effective manner to create a performant and trustworthy model is of utmost importance to forecasters. neural networks show promise, yet suffer from unintuitive computations, fusion of high-level features, and may be limited by the quantity of available data. in this work, we leverage the scattering transform to extract high-level features without additional trainable parameters and introduce a separation scheme to bring attention to independent input channels. experiments show promising results on estimating tropical cyclone intensity and predicting the occurrence of lightning from satellite imagery.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['jason stock', 'chuck anderson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12185'},\n",
       " {'title': 'conditional diffusion with less explicit guidance via model predictive   control',\n",
       "  'id': '2210.12192',\n",
       "  'abstract': 'how much explicit guidance is necessary for conditional diffusion? we consider the problem of conditional sampling using an unconditional diffusion model and limited explicit guidance (e.g., a noised classifier, or a conditional diffusion model) that is restricted to a small number of time steps. we explore a model predictive control (mpc)-like approach to approximate guidance by simulating unconditional diffusion forward, and backpropagating explicit guidance feedback. mpc-approximated guides have high cosine similarity to real guides, even over large simulation distances. adding mpc steps improves generative quality when explicit guidance is limited to five time steps.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['max w. shen',\n",
       "   'ehsan hajiramezanali',\n",
       "   'gabriele scalia',\n",
       "   'alex tseng',\n",
       "   'nathaniel diamant',\n",
       "   'tommaso biancalani',\n",
       "   'andreas loukas'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12192'},\n",
       " {'title': 'graphnet: graph neural networks for neutrino telescope event   reconstruction',\n",
       "  'id': '2210.12194',\n",
       "  'abstract': 'graphnet is an open-source python framework aimed at providing high quality, user friendly, end-to-end functionality to perform reconstruction tasks at neutrino telescopes using graph neural networks (gnns). graphnet makes it fast and easy to train complex models that can provide event reconstruction with state-of-the-art performance, for arbitrary detector configurations, with inference times that are orders of magnitude faster than traditional reconstruction techniques. gnns from graphnet are flexible enough to be applied to data from all neutrino telescopes, including future projects such as icecube extensions or p-one. this means that gnn-based reconstruction can be used to provide state-of-the-art performance on most reconstruction tasks in neutrino telescopes, at real-time event rates, across experiments and physics analyses, with vast potential impact for neutrino and astro-particle physics.',\n",
       "  'categories': 'astro-ph.im cs.lg hep-ex physics.data-an',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['andreas søgaard',\n",
       "   'rasmus f. ørsøe',\n",
       "   'leon bozianu',\n",
       "   'morten holm',\n",
       "   'kaare endrup iversen',\n",
       "   'tim guggenmos',\n",
       "   'martin ha minh',\n",
       "   'philipp eller',\n",
       "   'troels c. petersen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12194'},\n",
       " {'title': 'just mix once: worst-group generalization by group interpolation',\n",
       "  'id': '2210.12195',\n",
       "  'abstract': 'advances in deep learning theory have revealed how average generalization relies on superficial patterns in data. the consequences are brittle models with poor performance with shift in group distribution at test time. when group annotation is available, we can use robust optimization tools to tackle the problem. however, identification and annotation are time-consuming, especially on large datasets. a recent line of work leverages self-supervision and oversampling to improve generalization on minority groups without group annotation. we propose to unify and generalize these approaches using a class-conditional variant of mixup tailored for worst-group generalization. our approach, just mix once (jm1), interpolates samples during learning, augmenting the training distribution with a continuous mixture of groups. jm1 is domain agnostic and computationally efficient, can be used with any level of group annotation, and performs on par or better than the state-of-the-art on worst-group generalization. additionally, we provide a simple explanation of why jm1 works.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['giorgio giannone',\n",
       "   'serhii havrylov',\n",
       "   'jordan massiah',\n",
       "   'emine yilmaz',\n",
       "   'yunlong jiao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12195'},\n",
       " {'title': 'augmentation by counterfactual explanation -- fixing an overconfident   classifier',\n",
       "  'id': '2210.12196',\n",
       "  'abstract': 'a highly accurate but overconfident model is ill-suited for deployment in critical applications such as healthcare and autonomous driving. the classification outcome should reflect a high uncertainty on ambiguous in-distribution samples that lie close to the decision boundary. the model should also refrain from making overconfident decisions on samples that lie far outside its training distribution, far-out-of-distribution (far-ood), or on unseen samples from novel classes that lie near its training distribution (near-ood). this paper proposes an application of counterfactual explanations in fixing an over-confident classifier. specifically, we propose to fine-tune a given pre-trained classifier using augmentations from a counterfactual explainer (ace) to fix its uncertainty characteristics while retaining its predictive performance. we perform extensive experiments with detecting far-ood, near-ood, and ambiguous samples. our empirical results show that the revised model have improved uncertainty measures, and its performance is competitive to the state-of-the-art methods.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['sumedha singla',\n",
       "   'nihal murali',\n",
       "   'forough arabshahi',\n",
       "   'sofia triantafyllou',\n",
       "   'kayhan batmanghelich'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12196'},\n",
       " {'title': 'anonymous bandits for multi-user systems',\n",
       "  'id': '2210.12198',\n",
       "  'abstract': \"in this work, we present and study a new framework for online learning in systems with multiple users that provide user anonymity. specifically, we extend the notion of bandits to obey the standard $k$-anonymity constraint by requiring each observation to be an aggregation of rewards for at least $k$ users. this provides a simple yet effective framework where one can learn a clustering of users in an online fashion without observing any user's individual decision. we initiate the study of anonymous bandits and provide the first sublinear regret algorithms and lower bounds for this setting.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['hossein esfandiari', 'vahab mirrokni', 'jon schneider'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12198'},\n",
       " {'title': 'probing with noise: unpicking the warp and weft of embeddings',\n",
       "  'id': '2210.12206',\n",
       "  'abstract': 'improving our understanding of how information is encoded in vector space can yield valuable interpretability insights. alongside vector dimensions, we argue that it is possible for the vector norm to also carry linguistic information. we develop a method to test this: an extension of the probing framework which allows for relative intrinsic interpretations of probing results. it relies on introducing noise that ablates information encoded in embeddings, grounded in random baselines and confidence intervals. we apply the method to well-established probing tasks and find evidence that confirms the existence of separate information containers in english glove and bert embeddings. our correlation analysis aligns with the experimental findings that different encoders use the norm to encode different kinds of information: glove stores syntactic and sentence length information in the vector norm, while bert uses it to encode contextual incongruity.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['filip klubička', 'john d. kelleher'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12206'},\n",
       " {'title': 'feature engineering and classification models for partial discharge in   power transformers',\n",
       "  'id': '2210.12216',\n",
       "  'abstract': 'to ensure reliability, power transformers are monitored for partial discharge (pd) events, which are symptoms of transformer failure. since failures can have catastrophic cascading consequences, it is critical to preempt them as early as possible. our goal is to classify pds as corona, floating, particle, or void, to gain an understanding of the failure location. using phase resolved pd signal data, we create a small set of features, which can be used to classify pds with high accuracy. this set of features consists of the total magnitude, the maximum magnitude, and the length of the longest empty band. these features represent the entire signal and not just a single phase, so the feature set has a fixed size and is easily comprehensible. with both random forest and svm classification methods, we attain a 99% classification accuracy, which is significantly higher than classification using phase based feature sets such as phase magnitude. furthermore, we develop a stacking ensemble to combine several classification models, resulting in a superior model that outperforms existing methods in both accuracy and variance.',\n",
       "  'categories': 'cs.lg cs.ai eess.sp',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['jonathan wang', 'kesheng wu', 'alex sim', 'seongwook hwangbo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12216'},\n",
       " {'title': 'considerations for visualizing uncertainty in clinical machine learning   models',\n",
       "  'id': '2210.12220',\n",
       "  'abstract': 'clinician-facing predictive models are increasingly present in the healthcare setting. regardless of their success with respect to performance metrics, all models have uncertainty. we investigate how to visually communicate uncertainty in this setting in an actionable, trustworthy way. to this end, we conduct a qualitative study with cardiac critical care clinicians. our results reveal that clinician trust may be impacted most not by the degree of uncertainty, but rather by how transparent the visualization of what the sources of uncertainty are. our results show a clear connection between feature interpretability and clinical actionability.',\n",
       "  'categories': 'cs.hc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['caitlin f. harrigan',\n",
       "   'gabriela morgenshtern',\n",
       "   'anna goldenberg',\n",
       "   'fanny chevalier'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12220'},\n",
       " {'title': 'edukg: a heterogeneous sustainable k-12 educational knowledge graph',\n",
       "  'id': '2210.12228',\n",
       "  'abstract': \"web and artificial intelligence technologies, especially semantic web and knowledge graph (kg), have recently raised significant attention in educational scenarios. nevertheless, subject-specific kgs for k-12 education still lack sufficiency and sustainability from knowledge and data perspectives. to tackle these issues, we propose edukg, a heterogeneous sustainable k-12 educational knowledge graph. we first design an interdisciplinary and fine-grained ontology for uniformly modeling knowledge and resource in k-12 education, where we define 635 classes, 445 object properties, and 1314 datatype properties in total. guided by this ontology, we propose a flexible methodology for interactively extracting factual knowledge from textbooks. furthermore, we establish a general mechanism based on our proposed generalized entity linking system for edukg's sustainable maintenance, which can dynamically index numerous heterogeneous resources and data with knowledge topics in edukg. we further evaluate edukg to illustrate its sufficiency, richness, and variability. we publish edukg with more than 252 million entities and 3.86 billion triplets. our code and data repository is now available at https://github.com/thu-keg/edukg.\",\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['bowen zhao',\n",
       "   'jiuding sun',\n",
       "   'bin xu',\n",
       "   'xingyu lu',\n",
       "   'yuchen li',\n",
       "   'jifan yu',\n",
       "   'minghui liu',\n",
       "   'tingjian zhang',\n",
       "   'qiuyang chen',\n",
       "   'hanming li',\n",
       "   'lei hou',\n",
       "   'juanzi li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12228'},\n",
       " {'title': 'reducing training sample memorization in gans by training with   memorization rejection',\n",
       "  'id': '2210.12231',\n",
       "  'abstract': 'generative adversarial network (gan) continues to be a popular research direction due to its high generation quality. it is observed that many state-of-the-art gans generate samples that are more similar to the training set than a holdout testing set from the same distribution, hinting some training samples are implicitly memorized in these models. this memorization behavior is unfavorable in many applications that demand the generated samples to be sufficiently distinct from known samples. nevertheless, it is unclear whether it is possible to reduce memorization without compromising the generation quality. in this paper, we propose memorization rejection, a training scheme that rejects generated samples that are near-duplicates of training samples during training. our scheme is simple, generic and can be directly applied to any gan architecture. experiments on multiple datasets and gan models validate that memorization rejection effectively reduces training sample memorization, and in many cases does not sacrifice the generation quality. code to reproduce the experiment results can be found at $\\\\texttt{https://github.com/jybai/mrgan}$.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['andrew bai', 'cho-jui hsieh', 'wendy kan', 'hsuan-tien lin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12231'},\n",
       " {'title': 'tcab: a large-scale text classification attack benchmark',\n",
       "  'id': '2210.12233',\n",
       "  'abstract': 'we introduce the text classification attack benchmark (tcab), a dataset for analyzing, understanding, detecting, and labeling adversarial attacks against text classifiers. tcab includes 1.5 million attack instances, generated by twelve adversarial attacks targeting three classifiers trained on six source datasets for sentiment analysis and abuse detection in english. unlike standard text classification, text attacks must be understood in the context of the target classifier that is being attacked, and thus features of the target classifier are important as well. tcab includes all attack instances that are successful in flipping the predicted label; a subset of the attacks are also labeled by human annotators to determine how frequently the primary semantics are preserved. the process of generating attacks is automated, so that tcab can easily be extended to incorporate new text attacks and better classifiers as they are developed. in addition to the primary tasks of detecting and labeling attacks, tcab can also be used for attack localization, attack target labeling, and attack characterization. tcab code and dataset are available at https://react-nlp.github.io/tcab/.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['kalyani asthana',\n",
       "   'zhouhang xie',\n",
       "   'wencong you',\n",
       "   'adam noack',\n",
       "   'jonathan brophy',\n",
       "   'sameer singh',\n",
       "   'daniel lowd'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12233'},\n",
       " {'title': 'imbalanced classification in medical imaging',\n",
       "  'id': '2210.12234',\n",
       "  'abstract': 'we propose performing imbalanced classification by regrouping majority classes into small classes so that we turn the problem into balanced multiclass classification. this new idea is dramatically different from popular loss reweighting and class resampling methods. our preliminary result on imbalanced medical image classification shows that this natural idea can substantially boost the classification performance as measured by average precision (approximately area-under-the-precision-recall-curve, or auprc), which is more appropriate for evaluating imbalanced classification than other metrics such as balanced accuracy.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['le peng', 'yash travadi', 'rui zhang', 'ying cui', 'ju sun'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12234'},\n",
       " {'title': \"sequential gradient descent and quasi-newton's method for change-point   analysis\",\n",
       "  'id': '2210.12235',\n",
       "  'abstract': \"one common approach to detecting change-points is minimizing a cost function over possible numbers and locations of change-points. the framework includes several well-established procedures, such as the penalized likelihood and minimum description length. such an approach requires finding the cost value repeatedly over different segments of the data set, which can be time-consuming when (i) the data sequence is long and (ii) obtaining the cost value involves solving a non-trivial optimization problem. this paper introduces a new sequential method (se) that can be coupled with gradient descent (segd) and quasi-newton's method (sen) to find the cost value effectively. the core idea is to update the cost value using the information from previous steps without re-optimizing the objective function. the new method is applied to change-point detection in generalized linear models and penalized regression. numerical studies show that the new approach can be orders of magnitude faster than the pruned exact linear time (pelt) method without sacrificing estimation accuracy.\",\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['xianyang zhang', 'trisha dawn'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12235'},\n",
       " {'title': 'uncertain evidence in probabilistic models and stochastic simulators',\n",
       "  'id': '2210.12236',\n",
       "  'abstract': \"we consider the problem of performing bayesian inference in probabilistic models where observations are accompanied by uncertainty, referred to as `uncertain evidence'. in many real-world scenarios, such uncertainty stems from measurement errors associated with observable quantities in probabilistic models. we explore how to interpret uncertain evidence, and by extension the importance of proper interpretation as it pertains to inference about latent variables. we consider a recently-proposed method `stochastic evidence' as well as revisit two older methods: jeffrey's rule and virtual evidence. we devise concrete guidelines on how to account for uncertain evidence and we provide new insights, particularly regarding consistency. to showcase the impact of different interpretations of the same uncertain evidence, we carry out experiments in which we compare inference results associated with each interpretation.\",\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['andreas munk', 'alexander mead', 'frank wood'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12236'},\n",
       " {'title': 'auto-encoder neural network incorporating x-ray fluorescence fundamental   parameters with machine learning',\n",
       "  'id': '2210.12239',\n",
       "  'abstract': 'we consider energy-dispersive x-ray fluorescence (edxrf) applications where the fundamental parameters method is impractical such as when instrument parameters are unavailable. for example, on a mining shovel or conveyor belt, rocks are constantly moving (leading to varying angles of incidence and distances) and there may be other factors not accounted for (like dust). neural networks do not require instrument and fundamental parameters but training neural networks requires xrf spectra labelled with elemental composition, which is often limited because of its expense. we develop a neural network model that learns from limited labelled data and learns to invert a forward model. the forward model uses transition energies and probabilities of all elements and parameterized distributions to approximate other fundamental and instrument parameters. we evaluate the model and baseline models on a rock dataset from a lithium mine and identify which elements are appropriate for this method. this model demonstrates the potential to calibrate a neural network in a noisy environment where labelled data is limited.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['matthew dirks', 'david poole'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12239'},\n",
       " {'title': 'benchmarking gpu and tpu performance with graph neural networks',\n",
       "  'id': '2210.12247',\n",
       "  'abstract': 'many artificial intelligence (ai) devices have been developed to accelerate the training and inference of neural networks models. the most common ones are the graphics processing unit (gpu) and tensor processing unit (tpu). they are highly optimized for dense data representations. however, sparse representations such as graphs are prevalent in many domains, including science. it is therefore important to characterize the performance of available ai accelerators on sparse data. this work analyzes and compares the gpu and tpu performance training a graph neural network (gnn) developed to solve a real-life pattern recognition problem. characterizing the new class of models acting on sparse data may prove helpful in optimizing the design of deep learning libraries and future ai accelerators.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['xiangyang ju',\n",
       "   'yunsong wang',\n",
       "   'daniel murnane',\n",
       "   'nicholas choma',\n",
       "   'steven farrell',\n",
       "   'paolo calafiura'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12247'},\n",
       " {'title': 'score-based denoising diffusion with non-isotropic gaussian noise models',\n",
       "  'id': '2210.12254',\n",
       "  'abstract': 'generative models based on denoising diffusion techniques have led to an unprecedented increase in the quality and diversity of imagery that is now possible to create with neural generative models. however, most contemporary state-of-the-art methods are derived from a standard isotropic gaussian formulation. in this work we examine the situation where non-isotropic gaussian distributions are used. we present the key mathematical derivations for creating denoising diffusion models using an underlying non-isotropic gaussian noise model. we also provide initial experiments to help verify empirically that this more general modelling approach can also yield high-quality samples.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['vikram voleti', 'christopher pal', 'adam oberman'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12254'},\n",
       " {'title': 'uncertainty estimates of predictions via a general bias-variance   decomposition',\n",
       "  'id': '2210.12256',\n",
       "  'abstract': 'reliably estimating the uncertainty of a prediction throughout the model lifecycle is crucial in many safety-critical applications.   the most common way to measure this uncertainty is via the predicted confidence. while this tends to work well for in-domain samples, these estimates are unreliable under domain drift.   alternatively, a bias-variance decomposition allows to directly measure the predictive uncertainty across the entire input space.   but, such a decomposition for proper scores does not exist in current literature, and for exponential families it is convoluted.   in this work, we introduce a general bias-variance decomposition for proper scores and reformulate the exponential family case, giving rise to the bregman information as the variance term in both cases.   this allows us to prove that the bregman information for classification measures the uncertainty in the logit space. we showcase the practical relevance of this decomposition on two downstream tasks.   first, we show how to construct confidence intervals for predictions on the instance-level based on the bregman information.   second, we demonstrate how different approximations of the instance-level bregman information allow reliable out-of-distribution detection for all degrees of domain drift.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['sebastian gruber', 'florian buettner'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12256'},\n",
       " {'title': 'efficient automatic machine learning via design graphs',\n",
       "  'id': '2210.12257',\n",
       "  'abstract': 'despite the success of automated machine learning (automl), which aims to find the best design, including the architecture of deep networks and hyper-parameters, conventional automl methods are computationally expensive and hardly provide insights into the relations of different model design choices. to tackle the challenges, we propose falcon, an efficient sample-based method to search for the optimal model design. our key insight is to model the design space of possible model designs as a design graph, where the nodes represent design choices, and the edges denote design similarities. falcon features 1) a task-agnostic module, which performs message passing on the design graph via a graph neural network (gnn), and 2) a task-specific module, which conducts label propagation of the known model performance information on the design graph. both modules are combined to predict the design performances in the design space, navigating the search direction. we conduct extensive experiments on 27 node and graph classification tasks from various application domains, and an image classification task on the cifar-10 dataset. we empirically show that falcon can efficiently obtain the well-performing designs for each task using only 30 explored nodes. specifically, falcon has a comparable time cost with the one-shot approaches while achieving an average improvement of 3.3% compared with the best baselines.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['shirley wu', 'jiaxuan you', 'jure leskovec', 'rex ying'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12257'},\n",
       " {'title': 'group distributionally robust reinforcement learning with hierarchical   latent variables',\n",
       "  'id': '2210.12262',\n",
       "  'abstract': \"one key challenge for multi-task reinforcement learning (rl) in practice is the absence of task indicators. robust rl has been applied to deal with task ambiguity, but may result in over-conservative policies. to balance the worst-case (robustness) and average performance, we propose group distributionally robust markov decision process (gdr-mdp), a flexible hierarchical mdp formulation that encodes task groups via a latent mixture model. gdr-mdp identifies the optimal policy that maximizes the expected return under the worst-possible qualified belief over task groups within an ambiguity set. we rigorously show that gdr-mdp's hierarchical structure improves distributional robustness by adding regularization to the worst possible outcomes. we then develop deep rl algorithms for gdr-mdp for both value-based and policy-based rl methods. extensive experiments on box2d control tasks, mujoco benchmarks, and google football platforms show that our algorithms outperform classic robust training algorithms across diverse environments in terms of robustness under belief uncertainties. demos are available on our project page (\\\\url{https://sites.google.com/view/gdr-rl/home}).\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['mengdi xu',\n",
       "   'peide huang',\n",
       "   'yaru niu',\n",
       "   'visak kumar',\n",
       "   'jielin qiu',\n",
       "   'chao fang',\n",
       "   'kuan-hui lee',\n",
       "   'xuewei qi',\n",
       "   'henry lam',\n",
       "   'bo li',\n",
       "   'ding zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12262'},\n",
       " {'title': 'implicit offline reinforcement learning via supervised learning',\n",
       "  'id': '2210.12272',\n",
       "  'abstract': 'offline reinforcement learning (rl) via supervised learning is a simple and effective way to learn robotic skills from a dataset collected by policies of different expertise levels. it is as simple as supervised learning and behavior cloning (bc), but takes advantage of return information. on datasets collected by policies of similar expertise, implicit bc has been shown to match or outperform explicit bc. despite the benefits of using implicit models to learn robotic skills via bc, offline rl via supervised learning algorithms have been limited to explicit models. we show how implicit models can leverage return information and match or outperform explicit algorithms to acquire robotic skills from fixed datasets. furthermore, we show the close relationship between our implicit methods and other popular rl via supervised learning algorithms to provide a unified framework. finally, we demonstrate the effectiveness of our method on high-dimension manipulation and locomotion tasks.',\n",
       "  'categories': 'stat.ml cs.lg cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['alexandre piche',\n",
       "   'rafael pardinas',\n",
       "   'david vazquez',\n",
       "   'igor mordatch',\n",
       "   'chris pal'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12272'},\n",
       " {'title': 'bridging the gap between target networks and functional regularization',\n",
       "  'id': '2210.12282',\n",
       "  'abstract': 'bootstrapping is behind much of the successes of deep reinforcement learning. however, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. target networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. despite the popularity of target networks, their effect on the optimization is still misunderstood. in this work, we show that they act as an implicit regularizer. this regularizer has disadvantages such as being inflexible and non convex. to overcome these issues, we propose an explicit functional regularization that is a convex regularizer in function space and can easily be tuned. we analyze the convergence of our method theoretically and empirically demonstrate that replacing target networks with the more theoretically grounded functional regularization approach leads to better sample efficiency and performance improvements.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['alexandre piche',\n",
       "   'valentin thomas',\n",
       "   'joseph marino',\n",
       "   'rafael pardinas',\n",
       "   'gian maria marconi',\n",
       "   'christopher pal',\n",
       "   'mohammad emtiyaz khan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12282'},\n",
       " {'title': 'draft, sketch, and prove: guiding formal theorem provers with informal   proofs',\n",
       "  'id': '2210.12283',\n",
       "  'abstract': 'the formalization of existing mathematical proofs is a notoriously difficult process. despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. while previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. in this work, we introduce draft, sketch, and prove (dsp), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. we investigate two relevant setups where informal proofs are either written by humans or generated by a language model. our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.',\n",
       "  'categories': 'cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['albert q. jiang',\n",
       "   'sean welleck',\n",
       "   'jin peng zhou',\n",
       "   'wenda li',\n",
       "   'jiacheng liu',\n",
       "   'mateja jamnik',\n",
       "   'timothée lacroix',\n",
       "   'yuhuai wu',\n",
       "   'guillaume lample'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12283'},\n",
       " {'title': 'exploring representation-level augmentation for code search',\n",
       "  'id': '2210.12285',\n",
       "  'abstract': 'code search, which aims at retrieving the most relevant code fragment for a given natural language query, is a common activity in software development practice. recently, contrastive learning is widely used in code search research, where many data augmentation approaches for source code (e.g., semantic-preserving program transformation) are proposed to learn better representations. however, these augmentations are at the raw-data level, which requires additional code analysis in the preprocessing stage and additional training costs in the training stage. in this paper, we explore augmentation methods that augment data (both code and query) at representation level which does not require additional data processing and training, and based on this we propose a general format of representation-level augmentation that unifies existing methods. then, we propose three new augmentation methods (linear extrapolation, binary interpolation, and gaussian scaling) based on the general format. furthermore, we theoretically analyze the advantages of the proposed augmentation methods over traditional contrastive learning methods on code search. we experimentally evaluate the proposed representation-level augmentation methods with state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. the experimental results show that our approach can consistently boost the performance of the studied code search models. our source code is available at https://github.com/alex-haochenli/racs.',\n",
       "  'categories': 'cs.se cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['haochen li',\n",
       "   'chunyan miao',\n",
       "   'cyril leung',\n",
       "   'yanxian huang',\n",
       "   'yuan huang',\n",
       "   'hongyu zhang',\n",
       "   'yanlin wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12285'},\n",
       " {'title': 'learning ultrametric trees for optimal transport regression',\n",
       "  'id': '2210.12288',\n",
       "  'abstract': 'optimal transport provides a metric which quantifies the dissimilarity between probability measures. for measures supported in discrete metric spaces, finding optimal transport distance has cubic time complexity in the size of the space. however, measures supported on trees admit a closed-form optimal transport which can be computed in linear time. in this paper, we aim to find an optimal tree structure for a given discrete metric space, so that the tree-wasserstein distance can best approximate the optimal transport distance in the original space. one of our key ideas is to cast the problem in ultrametric spaces. this helps define different tree structures and allows us to optimize the tree structure via projected gradient decent over space of ultrametric matrices. during optimization, we project the parameters to the ultrametric space via a hierarchical minimum spanning tree algorithm. experimental results on real datasets show that our approach outperforms previous approaches in approximating optimal transport distances. finally, experiments on synthetic data generated on ground truth trees show that our algorithm can accurately uncover the underlying tree metrics.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['samantha chen', 'puoya tabaghi', 'yusu wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12288'},\n",
       " {'title': 'dl-corrector-remapper: a grid-free bias-correction deep learning   methodology for data-driven high-resolution global weather forecasting',\n",
       "  'id': '2210.12293',\n",
       "  'abstract': \"data-driven models, such as fourcastnet (fcn), have shown exemplary performance in high-resolution global weather forecasting. this performance, however, is based on supervision on mesh-gridded weather data without the utilization of raw climate observational data, the gold standard ground truth. in this work we develop a methodology to correct, remap, and fine-tune gridded uniform forecasts of fcn so it can be directly compared against observational ground truth, which is sparse and non-uniform in space and time. this is akin to bias correction and post-processing of numerical weather prediction (nwp), a routine operation at meteorological and weather forecasting centers across the globe. the adaptive fourier neural operator (afno) architecture is used as the backbone to learn continuous representations of the atmosphere. the spatially and temporally non-uniform output is evaluated by the non-uniform discrete inverse fourier transform (nuidft) given the output query locations. we call this network the deep-learning-corrector-remapper (dlcr). the improvement in dlcr's performance against the gold standard ground truth over the baseline's performance shows its potential to correct, remap, and fine-tune the mesh-gridded forecasts under the supervision of observations.\",\n",
       "  'categories': 'physics.ao-ph cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['tao ge',\n",
       "   'jaideep pathak',\n",
       "   'akshay subramaniam',\n",
       "   'karthik kashinath'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12293'},\n",
       " {'title': 'continual reinforcement learning with group symmetries',\n",
       "  'id': '2210.12301',\n",
       "  'abstract': 'continual reinforcement learning (rl) aims to learn a sequence of tasks while retaining the capability to solve seen tasks and growing a new policy to solve novel tasks. existing continual rl methods ignore that some tasks are equivalent under simple group operations, such as rotations or translations. they thus extend a new policy for each equivalent task and train the policy from scratch, resulting in poor sample complexity and generalization capability. in this work, we propose a novel continual rl framework with group symmetries, which grows a policy for each group of equivalent tasks instead of a single task. we introduce a ppo-based rl algorithm with an invariant feature extractor and a novel task grouping mechanism based on invariant features. we test our algorithm in realistic autonomous driving scenarios, where each group is associated with a map configuration. we show that our algorithm assigns tasks to different groups with high accuracy and outperforms baselines in terms of generalization capability by a large margin.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['shiqi liu',\n",
       "   'mengdi xu',\n",
       "   'piede huang',\n",
       "   'yongkang liu',\n",
       "   'kentaro oguchi',\n",
       "   'ding zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12301'},\n",
       " {'title': 'pentatron: personalized context-aware transformer for retrieval-based   conversational understanding',\n",
       "  'id': '2210.12308',\n",
       "  'abstract': \"conversational understanding is an integral part of modern intelligent devices. in a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer's query due to factors including ambiguous mentions, mispronunciation, background noise and faulty on-device signal processing. such errors are compounded by two common deficiencies from intelligent devices namely, (1) the device not being tailored to individual customers, and (2) the device responses being unaware of the context in the conversation session. viewing this problem via the lens of retrieval-based search engines, we build and evaluate a scalable entity correction system, pentatron. the system leverages a parametric transformer-based language model to learn patterns from in-session customer-device interactions coupled with a non-parametric personalized entity index to compute the correct query, which aids downstream components in reasoning about the best response. in addition to establishing baselines and demonstrating the value of personalized and context-aware systems, we use multitasking to learn the domain of the correct entity. we also investigate the utility of language model prompts. through extensive experiments, we show a significant upward movement of the key metric (exact match) by up to 500.97% (relative to the baseline).\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['niranjan uma naresh',\n",
       "   'ziyan jiang',\n",
       "   'n/a ankit',\n",
       "   'sungjin lee',\n",
       "   'jie hao',\n",
       "   'xing fan',\n",
       "   'chenlei guo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12308'},\n",
       " {'title': 'learning vector-quantized item representation for transferable   sequential recommenders',\n",
       "  'id': '2210.12316',\n",
       "  'abstract': 'recently, the generality of natural language text has been leveraged to develop transferable recommender systems. the basic idea is to employ pre-trained language model (plm) to encode item text into item representations. despite the promising transferability, the binding between item text and item representations might be too tight, leading to potential problems such as over-emphasizing text similarity and exaggerating domain gaps. to address this issue, this paper proposes vq-rec, a novel approach to learning vector-quantized item representations for transferable sequential recommender. the major novelty of our approach lies in the new item representation scheme: it first maps item text into a vector of discrete indices (called item code), and then employs these indices to lookup the code embedding table for deriving item representations. such a scheme can be denoted as \"text -> code -> representation\". based on this representation scheme, we further propose an enhanced contrastive pre-training approach, using semi-synthetic and mixed-domain code representations as hard negatives. furthermore, we design a new cross-domain fine-tuning method based on a differentiable permutation-based network. extensive experiments conducted on six public benchmarks demonstrate the effectiveness of the proposed approach, in both cross-domain and cross-platform settings.',\n",
       "  'categories': 'cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['yupeng hou', 'zhankui he', 'julian mcauley', 'wayne xin zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12316'},\n",
       " {'title': 'transformer-based conditioned variational autoencoder for dialogue   generation',\n",
       "  'id': '2210.12326',\n",
       "  'abstract': 'in human dialogue, a single query may elicit numerous appropriate responses. the transformer-based dialogue model produces frequently occurring sentences in the corpus since it is a one-to-one mapping function. cvae is a technique for reducing generic replies. in this paper, we create a new dialogue model (cvae-t) based on the transformer with cvae structure. we use a pre-trained mlm model to rewrite some key n-grams in responses to obtain a series of negative examples, and introduce a regularization term during training to explicitly guide the latent variable in learning the semantic differences between each pair of positive and negative examples. experiments suggest that the method we design is capable of producing more informative replies.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['huihui yang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12326'},\n",
       " {'title': 'salience allocation as guidance for abstractive summarization',\n",
       "  'id': '2210.12330',\n",
       "  'abstract': 'abstractive summarization models typically learn to capture the salient information from scratch implicitly. recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance. however, extractive summaries as guidance could be over strict, leading to information loss or noisy signals. furthermore, it cannot easily adapt to documents with various abstractiveness. as the number and allocation of salience content pieces vary, it is hard to find a fixed threshold deciding which content should be included in the guidance. in this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely season (salience allocation as guidance for abstractive summarization). season utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness. automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable. empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['fei wang',\n",
       "   'kaiqiang song',\n",
       "   'hongming zhang',\n",
       "   'lifeng jin',\n",
       "   'sangwoo cho',\n",
       "   'wenlin yao',\n",
       "   'xiaoyang wang',\n",
       "   'muhao chen',\n",
       "   'dong yu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12330'},\n",
       " {'title': \"deep multi-branch cnn architecture for early alzheimer's detection from   brain mris\",\n",
       "  'id': '2210.12331',\n",
       "  'abstract': \"alzheimer's disease (ad) is a neuro-degenerative disease that can cause dementia and result severe reduction in brain function inhibiting simple tasks especially if no preventative care is taken. over 1 in 9 americans suffer from ad induced dementia and unpaid care for people with ad related dementia is valued at $271.6 billion. in this paper, we first review other approaches that could be used for early detection of ad. we then give an overview of our dataset that was from the alzheimer's disease neuroimaging initiative (adni) and propose a deep convolutional neural network (cnn) architecture consisting of 7,866,819 parameters. this model has three different length convolutional branches each comprised of different kernel sizes that can predict whether a patient is non-demented, mild-demented, or moderately-demented with a 99.05% three class accuracy.\",\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['paul k. mandal', 'rakesh mahto'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12331'},\n",
       " {'title': 'adaptive data fusion for multi-task non-smooth optimization',\n",
       "  'id': '2210.12334',\n",
       "  'abstract': 'we study the problem of multi-task non-smooth optimization that arises ubiquitously in statistical learning, decision-making and risk management. we develop a data fusion approach that adaptively leverages commonalities among a large number of objectives to improve sample efficiency while tackling their unknown heterogeneities. we provide sharp statistical guarantees for our approach. numerical experiments on both synthetic and real data demonstrate significant advantages of our approach over benchmarks.',\n",
       "  'categories': 'stat.ml cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['henry lam', 'kaizheng wang', 'yuhang wu', 'yichen zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12334'},\n",
       " {'title': 'detection of risk predictors of covid-19 mortality with classifier   machine learning models operated with routine laboratory biomarkers',\n",
       "  'id': '2210.12342',\n",
       "  'abstract': 'early evaluation of patients who require special care and high death expectancy in covid-19 and effective determination of relevant biomarkers on large sample groups are important to reduce mortality. this study aimed to reveal the routine blood value predictors of covid-19 mortality and to determine the lethal risk levels of these predictors during the disease process. the dataset of the study consists of 38 routine blood values of 2597 patients who died (n = 233) and recovered (n = 2364) from covid-19 in august-december, 2021. in this study, histogram-based gradient boosting (hgb) model was the most successful mashine learning classifier in detecting living and deceased covid-19 patients (with squared f1 metrics f1^2 = 1). the most efficient binary combinations with procalcitonin were obtained with d-dimer, esr, d.bil and ferritin. the hgb model operated with these couples correctly detected almost all of the patients who survived and died. (precision > 0.98, recall > 0.98, f1^2 > 0.98). furthermore, in the hgb model operated with a single feature, the most efficient features were procalcitonin (f1^2 = 0.96) and ferritin (f1^2 = 0.91). in addition, according to the two-threshold approach ferritin values between 376.2 mkg/l and 396.0 mkg/l (f1^2 = 0.91) and procalcitonin values between 0.2 mkg/l and 5.2 mkg/l (f1^2 = 0.95) were found to be fatal risk levels for covid-19. considering all the results, we suggest that many features combined with these features, especially procalcitonin and ferritin, operated with the hgb model, can be used to achieve very successful results in the classification of those who live and die from covid-19.moreover, we strongly recommend that clinicians consider the critical levels we have found for procalcitonin and ferritin properties to reduce the lethality of covid-19 disease.',\n",
       "  'categories': 'cs.lg physics.med-ph q-bio.qm',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['mehmet tahir huyut', 'andrei velichko', 'maksim belyaev'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12342'},\n",
       " {'title': 'ai-based arabic language and speech tutor',\n",
       "  'id': '2210.12346',\n",
       "  'abstract': 'in the past decade, we have observed a growing interest in using technologies such as artificial intelligence (ai), machine learning, and chatbots to provide assistance to language learners, especially in second language learning. by using ai and natural language processing (nlp) and chatbots, we can create an intelligent self-learning environment that goes beyond multiple-choice questions and/or fill in the blank exercises. in addition, nlp allows for learning to be adaptive in that it offers more than an indication that an error has occurred. it also provides a description of the error, uses linguistic analysis to isolate the source of the error, and then suggests additional drills to achieve optimal individualized learning outcomes. in this paper, we present our approach for developing an artificial intelligence-based arabic language and speech tutor (ai-alst) for teaching the moroccan arabic dialect. the ai-alst system is an intelligent tutor that provides analysis and assessment of students learning the moroccan dialect at university of arizona (ua). the ai-alst provides a self-learned environment to practice each lesson for pronunciation training. in this paper, we present our initial experimental evaluation of the ai-alst that is based on mfcc (mel frequency cepstrum coefficient) feature extraction, bidirectional lstm (long short-term memory), attention mechanism, and a cost-based strategy for dealing with class-imbalance learning. we evaluated our tutor on the word pronunciation of lesson 1 of the moroccan arabic dialect class. the experimental results show that the ai-alst can effectively and successfully detect pronunciation errors and evaluate its performance by using f_1-score, accuracy, precision, and recall.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['sicong shao',\n",
       "   'saleem alharir',\n",
       "   'salim hariri',\n",
       "   'pratik satam',\n",
       "   'sonia shiri',\n",
       "   'abdessamad mbarki'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12346'},\n",
       " {'title': 'quantifying complexity: an object-relations approach to complex systems',\n",
       "  'id': '2210.12347',\n",
       "  'abstract': 'the best way to model, understand, and quantify the information contained in complex systems is an open question in physics, mathematics, and computer science. the uncertain relationship between entropy and complexity further complicates this question. with ideas drawn from the object-relations theory of psychology, this paper develops an object-relations model of complex systems which generalizes to systems of all types, including mathematical operations, machines, biological organisms, and social structures. the resulting complex information entropy (cie) equation is a robust method to quantify complexity across various contexts. the paper also describes algorithms to iteratively update and improve approximate solutions to the cie equation, to recursively infer the composition of complex systems, and to discover the connections among objects across different lengthscales and timescales. applications are discussed in the fields of engineering design, atomic and molecular physics, chemistry, materials science, neuroscience, psychology, sociology, ecology, economics, and medicine.',\n",
       "  'categories': 'cs.lg cs.it math.it physics.data-an',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['stephen casey'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12347'},\n",
       " {'title': 'context-aware image completion',\n",
       "  'id': '2210.12350',\n",
       "  'abstract': 'image completion is a task that aims to fill in the missing region of a masked image with plausible contents.however, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. in this work, we propose a novel image completion model, dubbed refill, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. refill first adopts a transformer architecture that considers the types, locations of the visible instances, and the location of the missing region. then, refill completes the missing foreground and background semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance to generate missing contents with seamless boundaries. finally, we condition the image synthesis blocks of refill using the completed segmentation mask to generate photo-realistic contents to fill out the missing region. experimental results show the superiority of refill over state-of-the-art image completion approaches on various natural images.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['jinoh cho', 'minguk kang', 'vibhav vineet', 'jaesik park'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12350'},\n",
       " {'title': 'neuphysics: editable neural geometry and physics from monocular videos',\n",
       "  'id': '2210.12352',\n",
       "  'abstract': 'we present a method for learning 3d geometry and physics parameters of a dynamic scene from only a monocular rgb video input. to decouple the learning of underlying scene geometry from dynamic motion, we represent the scene as a time-invariant signed distance function (sdf) which serves as a reference frame, along with a time-conditioned deformation field. we further bridge this neural geometry representation with a differentiable physics simulator by designing a two-way conversion between the neural field and its corresponding hexahedral mesh, enabling us to estimate physics parameters from the source video by minimizing a cycle consistency loss. our method also allows a user to interactively edit 3d objects from the source video by modifying the recovered hexahedral mesh, and propagating the operation back to the neural field representation. experiments show that our method achieves superior mesh and video reconstruction of dynamic scenes compared to competing neural field approaches, and we provide extensive examples which demonstrate its ability to extract useful 3d representations from videos captured with consumer-grade cameras.',\n",
       "  'categories': 'cs.cv cs.gr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['yi-ling qiao', 'alexander gao', 'ming c. lin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12352'},\n",
       " {'title': 'leveraging large language models for multiple choice question answering',\n",
       "  'id': '2210.12353',\n",
       "  'abstract': 'while large language models (llms) like gpt-3 have achieved impressive results on multiple choice question answering (mcqa) tasks in the zero, one, and few-shot settings, they generally lag behind the mcqa state of the art (sota). mcqa tasks have traditionally been presented to llms like cloze tasks. an llm is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). a more natural prompting approach is to present the question and answer options to the llm jointly and have it output the symbol (e.g., \"a\") associated with its chosen answer option. this approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. for the natural approach to be effective the llm it is used with must be able to associate answer options with the symbols that represent them. the llm needs what we term multiple choice symbol binding (mcsb) ability. this ability varies greatly by model. we show that a model with high mcsb ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the sota, suggesting that the mcqa ability of llms has been previously underestimated.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['joshua robinson',\n",
       "   'christopher michael rytting',\n",
       "   'david wingate'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12353'},\n",
       " {'title': 'bayesian convolutional deep sets with task-dependent stationary prior',\n",
       "  'id': '2210.12363',\n",
       "  'abstract': 'convolutional deep sets are the architecture of a deep neural network (dnn) that can model stationary stochastic process. this architecture uses the kernel smoother and the dnn to construct the translation equivariant functional representations, and thus reflects the inductive bias of the stationarity into dnn. however, since this architecture employs the kernel smoother known as the non-parametric model, it may produce ambiguous representations when the number of data points is not given sufficiently. to remedy this issue, we introduce bayesian convolutional deep sets that construct the random translation equivariant functional representations with stationary prior. furthermore, we present how to impose the task-dependent prior for each dataset because a wrongly imposed prior forms an even worse representation than that of the kernel smoother. we validate the proposed architecture and its training on various experiments with time-series and image datasets.',\n",
       "  'categories': 'stat.ml cs.lg stat.me',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['yohan jung', 'jinkyoo park'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12363'},\n",
       " {'title': 'counterfactual generation under confounding',\n",
       "  'id': '2210.12368',\n",
       "  'abstract': 'a machine learning model, under the influence of observed or unobserved confounders in the training data, can learn spurious correlations and fail to generalize when deployed. for image classifiers, augmenting a training dataset using counterfactual examples has been empirically shown to break spurious correlations. however, the counterfactual generation task itself becomes more difficult as the level of confounding increases. existing methods for counterfactual generation under confounding consider a fixed set of interventions (e.g., texture, rotation) and are not flexible enough to capture diverse data-generating processes. given a causal generative process, we formally characterize the adverse effects of confounding on any downstream tasks and show that the correlation between generative factors (attributes) can be used to quantitatively measure confounding between generative factors. to minimize such correlation, we propose a counterfactual generation method that learns to modify the value of any attribute in an image and generate new images given a set of observed attributes, even when the dataset is highly confounded. these counterfactual images are then used to regularize the downstream classifier such that the learned representations are the same across various generative factors conditioned on the class label. our method is computationally efficient, simple to implement, and works well for any number of generative factors and confounding variables. our experimental results on both synthetic (mnist variants) and real-world (celeba) datasets show the usefulness of our approach.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['abbavaram gowtham reddy',\n",
       "   'saloni dash',\n",
       "   'amit sharma',\n",
       "   'vineeth n balasubramanian'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12368'},\n",
       " {'title': 'explanation shift: detecting distribution shifts on tabular data via the   explanation space',\n",
       "  'id': '2210.12369',\n",
       "  'abstract': 'as input data distributions evolve, the predictive performance of machine learning models tends to deteriorate. in the past, predictive performance was considered the key indicator to monitor. however, explanation aspects have come to attention within the last years. in this work, we investigate how model predictive performance and model explanation characteristics are affected under distribution shifts and how these key indicators are related to each other for tabular data. we find that the modeling of explanation shifts can be a better indicator for the detection of predictive performance changes than state-of-the-art techniques based on representations of distribution shifts. we provide a mathematical analysis of different types of distribution shifts as well as synthetic experimental examples.',\n",
       "  'categories': 'cs.lg cs.ai stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['carlos mougan',\n",
       "   'klaus broelemann',\n",
       "   'gjergji kasneci',\n",
       "   'thanassis tiropanis',\n",
       "   'steffen staab'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12369'},\n",
       " {'title': 'torchode: a parallel ode solver for pytorch',\n",
       "  'id': '2210.12375',\n",
       "  'abstract': \"we introduce an ode solver for the pytorch ecosystem that can solve multiple odes in parallel independently from each other while achieving significant performance gains. our implementation tracks each ode's progress separately and is carefully optimized for gpus and compatibility with pytorch's jit compiler. its design lets researchers easily augment any aspect of the solver and collect and analyze internal solver statistics. in our experiments, our implementation is up to 4.3 times faster per step than other ode solvers and it is robust against within-batch interactions that lead other solvers to take up to 4 times as many steps.\",\n",
       "  'categories': 'cs.lg cs.na math.na',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['marten lienen', 'stephan günnemann'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12375'},\n",
       " {'title': 'the devil is in the conflict: disentangled information graph neural   networks for fraud detection',\n",
       "  'id': '2210.12384',\n",
       "  'abstract': 'graph-based fraud detection has heretofore received considerable attention. owning to the great success of graph neural networks (gnns), many approaches adopting gnns for fraud detection has been gaining momentum. however, most existing methods are based on the strong inductive bias of homophily, which indicates that the context neighbors tend to have same labels or similar features. in real scenarios, fraudsters often engage in camouflage behaviors in order to avoid detection system. therefore, the homophilic assumption no longer holds, which is known as the inconsistency problem. in this paper, we argue that the performance degradation is mainly attributed to the inconsistency between topology and attribute. to address this problem, we propose to disentangle the fraud network into two views, each corresponding to topology and attribute respectively. then we propose a simple and effective method that uses the attention mechanism to adaptively fuse two views which captures data-specific preference. in addition, we further improve it by introducing mutual information constraints for topology and attribute. to this end, we propose a disentangled information graph neural network (dignn) model, which utilizes variational bounds to find an approximate solution to our proposed optimization objective function. extensive experiments demonstrate that our model can significantly outperform stateof-the-art baselines on real-world fraud detection datasets.',\n",
       "  'categories': 'cs.lg cs.ai cs.si',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['zhixun li', 'dingshuo chen', 'qiang liu', 'shu wu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12384'},\n",
       " {'title': 'diversity-promoting ensemble for medical image segmentation',\n",
       "  'id': '2210.12388',\n",
       "  'abstract': 'medical image segmentation is an actively studied task in medical imaging, where the precision of the annotations is of utter importance towards accurate diagnosis and treatment. in recent years, the task has been approached with various deep learning systems, among the most popular models being u-net. in this work, we propose a novel strategy to generate ensembles of different architectures for medical image segmentation, by leveraging the diversity (decorrelation) of the models forming the ensemble. more specifically, we utilize the dice score among model pairs to estimate the correlation between the outputs of the two models forming each pair. to promote diversity, we select models with low dice scores among each other. we carry out gastro-intestinal tract image segmentation experiments to compare our diversity-promoting ensemble (dipe) with another strategy to create ensembles based on selecting the top scoring u-net models. our empirical results show that dipe surpasses both individual models as well as the ensemble creation strategy based on selecting the top scoring models.',\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['mariana-iuliana georgescu',\n",
       "   'radu tudor ionescu',\n",
       "   'andreea-iuliana miron'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12388'},\n",
       " {'title': 'testing independence of exchangeable random variables',\n",
       "  'id': '2210.12392',\n",
       "  'abstract': 'given well-shuffled data, can we determine whether the data items are statistically (in)dependent? formally, we consider the problem of testing whether a set of exchangeable random variables are independent. we will show that this is possible and develop tests that can confidently reject the null hypothesis that data is independent and identically distributed and have high power for (some) exchangeable distributions. we will make no structural assumptions on the underlying sample space. one potential application is in deep learning, where data is often scraped from the whole internet, with duplications abound, which can render data non-iid and test-set evaluation prone to give wrong answers.',\n",
       "  'categories': 'math.st cs.lg stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['marcus hutter'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12392'},\n",
       " {'title': 'addmu: detection of far-boundary adversarial examples with data and   model uncertainty estimation',\n",
       "  'id': '2210.12396',\n",
       "  'abstract': \"adversarial examples detection (aed) is a crucial defense technique against adversarial attacks and has drawn increasing attention from the natural language processing (nlp) community. despite the surge of new aed methods, our studies show that existing methods heavily rely on a shortcut to achieve good performance. in other words, current search-based adversarial attacks in nlp stop once model predictions change, and thus most adversarial examples generated by those attacks are located near model decision boundaries. to surpass this shortcut and fairly evaluate aed methods, we propose to test aed methods with \\\\textbf{f}ar \\\\textbf{b}oundary (\\\\textbf{fb}) adversarial examples. existing methods show worse than random guess performance under this scenario. to overcome this limitation, we propose a new technique, \\\\textbf{addmu}, \\\\textbf{a}dversary \\\\textbf{d}etection with \\\\textbf{d}ata and \\\\textbf{m}odel \\\\textbf{u}ncertainty, which combines two types of uncertainty estimation for both regular and fb adversarial example detection. our new method outperforms previous methods by 3.6 and 6.0 \\\\emph{auc} points under each scenario. finally, our analysis shows that the two types of uncertainty provided by \\\\textbf{addmu} can be leveraged to characterize adversarial examples and identify the ones that contribute most to model's robustness in adversarial training.\",\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['fan yin', 'yao li', 'cho-jui hsieh', 'kai-wei chang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12396'},\n",
       " {'title': 'digmn: dynamic intent guided meta network for differentiated user   engagement forecasting in online professional social platforms',\n",
       "  'id': '2210.12402',\n",
       "  'abstract': \"user engagement prediction plays a critical role for designing interaction strategies to grow user engagement and increase revenue in online social platforms. through the in-depth analysis of the real-world data from the world's largest professional social platforms, i.e., linkedin, we find that users expose diverse engagement patterns, and a major reason for the differences in user engagement patterns is that users have different intents. that is, people have different intents when using linkedin, e.g., applying for jobs, building connections, or checking notifications, which shows quite different engagement patterns. meanwhile, user intents and the corresponding engagement patterns may change over time. although such pattern differences and dynamics are essential for user engagement prediction, differentiating user engagement patterns based on user dynamic intents for better user engagement forecasting has not received enough attention in previous works. in this paper, we proposed a dynamic intent guided meta network (digmn), which can explicitly model user intent varying with time and perform differentiated user engagement forecasting. specifically, we derive some interpretable basic user intents as prior knowledge from data mining and introduce prior intents in explicitly modeling dynamic user intent. furthermore, based on the dynamic user intent representations, we propose a meta predictor to perform differentiated user engagement forecasting. through a comprehensive evaluation on linkedin anonymous user data, our method outperforms state-of-the-art baselines significantly, i.e., 2.96% and 3.48% absolute error reduction, on coarse-grained and fine-grained user engagement prediction tasks, respectively, demonstrating the effectiveness of our method.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['feifan li',\n",
       "   'lun du',\n",
       "   'qiang fu',\n",
       "   'shi han',\n",
       "   'yushu du',\n",
       "   'guangming lu',\n",
       "   'zi li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12402'},\n",
       " {'title': 'mild: multimodal interactive latent dynamics for learning human-robot   interaction',\n",
       "  'id': '2210.12418',\n",
       "  'abstract': \"modeling interaction dynamics to generate robot trajectories that enable a robot to adapt and react to a human's actions and intentions is critical for efficient and effective collaborative human-robot interactions (hri). learning from demonstration (lfd) methods from human-human interactions (hhi) have shown promising results, especially when coupled with representation learning techniques. however, such methods for learning hri either do not scale well to high dimensional data or cannot accurately adapt to changing via-poses of the interacting partner. we propose multimodal interactive latent dynamics (mild), a method that couples deep representation learning and probabilistic machine learning to address the problem of two-party physical hris. we learn the interaction dynamics from demonstrations, using hidden semi-markov models (hsmms) to model the joint distribution of the interacting agents in the latent space of a variational autoencoder (vae). our experimental evaluations for learning hri from hhi demonstrations show that mild effectively captures the multimodality in the latent representations of hri tasks, allowing us to decode the varying dynamics occurring in such tasks. compared to related work, mild generates more accurate trajectories for the controlled agent (robot) when conditioned on the observed agent's (human) trajectory. notably, mild can learn directly from camera-based pose estimations to generate trajectories, which we then map to a humanoid robot without the need for any additional training.\",\n",
       "  'categories': 'cs.ro cs.ai cs.hc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['vignesh prasad',\n",
       "   'dorothea koert',\n",
       "   'ruth stock-homburg',\n",
       "   'jan peters',\n",
       "   'georgia chalvatzaki'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12418'},\n",
       " {'title': 'speech emotion recognition via an attentive time-frequency neural   network',\n",
       "  'id': '2210.12430',\n",
       "  'abstract': 'spectrogram is commonly used as the input feature of deep neural networks to learn the high(er)-level time-frequency pattern of speech signal for speech emotion recognition (ser). \\\\textcolor{black}{generally, different emotions correspond to specific energy activations both within frequency bands and time frames on spectrogram, which indicates the frequency and time domains are both essential to represent the emotion for ser. however, recent spectrogram-based works mainly focus on modeling the long-term dependency in time domain, leading to these methods encountering the following two issues: (1) neglecting to model the emotion-related correlations within frequency domain during the time-frequency joint learning; (2) ignoring to capture the specific frequency bands associated with emotions.} to cope with the issues, we propose an attentive time-frequency neural network (atfnn) for ser, including a time-frequency neural network (tfnn) and time-frequency attention. specifically, aiming at the first issue, we design a tfnn with a frequency-domain encoder (f-encoder) based on the transformer encoder and a time-domain encoder (t-encoder) based on the bidirectional long short-term memory (bi-lstm). the f-encoder and t-encoder model the correlations within frequency bands and time frames, respectively, and they are embedded into a time-frequency joint learning strategy to obtain the time-frequency patterns for speech emotions. moreover, to handle the second issue, we also adopt time-frequency attention with a frequency-attention network (f-attention) and a time-attention network (t-attention) to focus on the emotion-related frequency band ranges and time frame ranges, which can enhance the discriminability of speech emotion features.',\n",
       "  'categories': 'cs.sd cs.lg cs.mm eess.as',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['cheng lu',\n",
       "   'wenming zheng',\n",
       "   'hailun lian',\n",
       "   'yuan zong',\n",
       "   'chuangao tang',\n",
       "   'sunan li',\n",
       "   'yan zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12430'},\n",
       " {'title': 'algorithms with prediction portfolios',\n",
       "  'id': '2210.12438',\n",
       "  'abstract': \"the research area of algorithms with predictions has seen recent success showing how to incorporate machine learning into algorithm design to improve performance when the predictions are correct, while retaining worst-case guarantees when they are not. most previous work has assumed that the algorithm has access to a single predictor. however, in practice, there are many machine learning methods available, often with incomparable generalization guarantees, making it hard to pick a best method a priori. in this work we consider scenarios where multiple predictors are available to the algorithm and the question is how to best utilize them.   ideally, we would like the algorithm's performance to depend on the quality of the best predictor. however, utilizing more predictions comes with a cost, since we now have to identify which prediction is the best. we study the use of multiple predictors for a number of fundamental problems, including matching, load balancing, and non-clairvoyant scheduling, which have been well-studied in the single predictor setting. for each of these problems we introduce new algorithms that take advantage of multiple predictors, and prove bounds on the resulting performance.\",\n",
       "  'categories': 'cs.lg cs.ds',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['michael dinitz',\n",
       "   'sungjin im',\n",
       "   'thomas lavastida',\n",
       "   'benjamin moseley',\n",
       "   'sergei vassilvitskii'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12438'},\n",
       " {'title': 'spectrum-bert: pre-training of deep bidirectional transformers for   spectral classification of chinese liquors',\n",
       "  'id': '2210.12440',\n",
       "  'abstract': \"spectral detection technology, as a non-invasive method for rapid detection of substances, combined with deep learning algorithms, has been widely used in food detection. however, in real scenarios, acquiring and labeling spectral data is an extremely labor-intensive task, which makes it impossible to provide enough high-quality data for training efficient supervised deep learning models. to better leverage limited samples, we apply pre-training & fine-tuning paradigm to the field of spectral detection for the first time and propose a pre-training method of deep bidirectional transformers for spectral classification of chinese liquors, abbreviated as spectrum-bert. specifically, first, to retain the model's sensitivity to the characteristic peak position and local information of the spectral curve, we innovatively partition the curve into multiple blocks and obtain the embeddings of different blocks, as the feature input for the next calculation. second, in the pre-training stage, we elaborately design two pre-training tasks, next curve prediction (ncp) and masked curve model (mcm), so that the model can effectively utilize unlabeled samples to capture the potential knowledge of spectral data, breaking the restrictions of the insufficient labeled samples, and improving the applicability and performance of the model in practical scenarios. finally, we conduct a large number of experiments on the real liquor spectral dataset. in the comparative experiments, the proposed spectrum-bert significantly outperforms the baselines in multiple metrics and this advantage is more significant on the imbalanced dataset. moreover, in the parameter sensitivity experiment, we also analyze the model performance under different parameter settings, to provide a reference for subsequent research.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['yansong wang',\n",
       "   'yundong sun',\n",
       "   'yansheng fu',\n",
       "   'dongjie zhu',\n",
       "   'zhaoshuo tian'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12440'},\n",
       " {'title': 'learning classifiers for imbalanced and overlapping data',\n",
       "  'id': '2210.12446',\n",
       "  'abstract': 'this study is about inducing classifiers using data that is imbalanced, with a minority class being under-represented in relation to the majority classes. the first section of this research focuses on the main characteristics of data that generate this problem. following a study of previous, relevant research, a variety of artificial, imbalanced data sets influenced by important elements were created. these data sets were used to create decision trees and rule-based classifiers. the second section of this research looks into how to improve classifiers by pre-processing data with resampling approaches. the results of the following trials are compared to the performance of distinct pre-processing re-sampling methods: two variants of random over-sampling and focused under-sampling ncr. this paper further optimises class imbalance with a new method called sparsity. the data is made more sparse from its class centers, hence making it more homogenous.',\n",
       "  'categories': 'cs.lg cs.it math.it',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['shivaditya shivganesh',\n",
       "   'nitin narayanan n',\n",
       "   'pranav murali',\n",
       "   'ajaykumar m'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12446'},\n",
       " {'title': 'probing transfer in deep reinforcement learning without task engineering',\n",
       "  'id': '2210.12448',\n",
       "  'abstract': \"we evaluate the use of original game curricula supported by the atari 2600 console as a heterogeneous transfer benchmark for deep reinforcement learning agents. game designers created curricula using combinations of several discrete modifications to the basic versions of games such as space invaders, breakout and freeway, making them progressively more challenging for human players. by formally organising these modifications into several factors of variation, we are able to show that analyses of variance (anova) are a potent tool for studying the effects of human-relevant domain changes on the learning and transfer performance of a deep reinforcement learning agent. since no manual task engineering is needed on our part, leveraging the original multi-factorial design avoids the pitfalls of unintentionally biasing the experimental setup. we find that game design factors have a large and statistically significant impact on an agent's ability to learn, and so do their combinatorial interactions. furthermore, we show that zero-shot transfer from the basic games to their respective variations is possible, but the variance in performance is also largely explained by interactions between factors. as such, we argue that atari game curricula offer a challenging benchmark for transfer learning in rl, that can help the community better understand the generalisation capabilities of rl agents along dimensions which meaningfully impact human generalisation performance. as a start, we report that value-function finetuning of regularly trained agents achieves positive transfer in a majority of cases, but significant headroom for algorithmic innovation remains. we conclude with the observation that selective transfer from multiple variants could further improve performance.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['andrei a. rusu',\n",
       "   'sebastian flennerhag',\n",
       "   'dushyant rao',\n",
       "   'razvan pascanu',\n",
       "   'raia hadsell'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12448'},\n",
       " {'title': 'neuroprim: an attention-based model for solving np-hard spanning tree   problems',\n",
       "  'id': '2210.12453',\n",
       "  'abstract': 'spanning tree problems with special constraints are widely applied in real-life scenarios, such as water supply, transportation and telecommunications, which often require complex algorithm design and exponential time to solve. in recent years, there has been a surge of interest in end-to-end deep neural networks (dnns) to solve routing problems. however, as the output of such methods is a sequence of vertices, it is difficult to apply them to combinatorial optimization problems where the solution set consists of a edges sets, such as various spanning tree problems. in this paper, we propose neuroprim, a novel framework combining neural networks and the prim algorithm, which is trained by reinforce with the pomo baseline to learn metrics for selecting edges for different spanning tree problems. we apply it to three difficult problems on euclidean spaces, namely degree-constrained minimum spanning tree problem (dcmstp), minimum routing cost spanning tree problem (mrcstp) and steiner tree problem in graphs (stpg). experimental results show that our model is able to outperform some of the heuristics and obtain extremely small gaps of less than $0.1\\\\%$ for simple problems such as dcmst with degree constraint $3$ and special cases of stpg up to 100 vertices. in addition, we find no significant degradation on problem instances as large as 1000, which demonstrates its strong generalization ability.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['yuchen shi', 'congying han', 'tiande guo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12453'},\n",
       " {'title': 'abstract interpretation-based feature importance for svms',\n",
       "  'id': '2210.12456',\n",
       "  'abstract': 'we propose a symbolic representation for support vector machines (svms) by means of abstract interpretation, a well-known and successful technique for designing and implementing static program analyses. we leverage this abstraction in two ways: (1) to enhance the interpretability of svms by deriving a novel feature importance measure, called abstract feature importance (afi), that does not depend in any way on a given dataset of the accuracy of the svm and is very fast to compute, and (2) for verifying stability, notably individual fairness, of svms and producing concrete counterexamples when the verification fails. we implemented our approach and we empirically demonstrated its effectiveness on svms based on linear and non-linear (polynomial and radial basis function) kernels. our experimental results show that, independently of the accuracy of the svm, our afi measure correlates much more strongly with the stability of the svm to feature perturbations than feature importance measures widely available in machine learning software such as permutation feature importance. it thus gives better insight into the trustworthiness of svms.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['abhinandan pal',\n",
       "   'francesco ranzato',\n",
       "   'caterina urban',\n",
       "   'marco zanella'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12456'},\n",
       " {'title': 'faster and more diverse de novo molecular optimization with double-loop   reinforcement learning using augmented smiles',\n",
       "  'id': '2210.12458',\n",
       "  'abstract': 'molecular generation via deep learning models in combination with reinforcement learning is a powerful way of generating proposed molecules with desirable properties. by defining a multi-objective scoring function, it is possible to generate thousands of ideas for molecules that scores well, which makes the approach interesting for drug discovery or material science purposes. however, if the scoring function is expensive regarding resources, such as time or computation, the high number of function evaluations needed for feedback in the reinforcement learning loop becomes a bottleneck. here we propose to use double-loop reinforcement learning with simplified molecular line entry system (smiles) augmentation to use scoring calculations more efficiently and arrive at well scoring molecules faster. by adding an inner loop where the smiles strings generated are augmented to alternative non-canonical smiles and used for additional rounds of reinforcement learning, we can effectively reuse the scoring calculations that are done on the molecular level. this approach speeds up the learning process regarding scoring function calls, as well as it protects moderately against mode collapse. we find that augmentation repeats between 5-10x seem safe for most scoring functions and additionally increase the diversity of the generated compounds, as well as making the sampling runs of chemical space more reproducible',\n",
       "  'categories': 'physics.chem-ph cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['esben jannik bjerrum',\n",
       "   'christian margreitter',\n",
       "   'thomas blaschke',\n",
       "   'raquel lopez-rios de castro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12458'},\n",
       " {'title': 'factor investing with a deep multi-factor model',\n",
       "  'id': '2210.12462',\n",
       "  'abstract': 'modeling and characterizing multiple factors is perhaps the most important step in achieving excess returns over market benchmarks. both academia and industry are striving to find new factors that have good explanatory power for future stock returns and good stability of their predictive power. in practice, factor investing is still largely based on linear multi-factor models, although many deep learning methods show promising results compared to traditional methods in stock trend prediction and portfolio risk management. however, the existing non-linear methods have two drawbacks: 1) there is a lack of interpretation of the newly discovered factors, 2) the financial insights behind the mining process are unclear, making practitioners reluctant to apply the existing methods to factor investing. to address these two shortcomings, we develop a novel deep multi-factor model that adopts industry neutralization and market neutralization modules with clear financial insights, which help us easily build a dynamic and multi-relational stock graph in a hierarchical structure to learn the graph representation of stock relationships at different levels, e.g., industry level and universal level. subsequently, graph attention modules are adopted to estimate a series of deep factors that maximize the cumulative factor returns. and a factor-attention module is developed to approximately compose the estimated deep factors from the input factors, as a way to interpret the deep factors explicitly. extensive experiments on real-world stock market data demonstrate the effectiveness of our deep multi-factor model in the task of factor investing.',\n",
       "  'categories': 'q-fin.cp cs.ce cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['zikai wei', 'bo dai', 'dahua lin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12462'},\n",
       " {'title': 'volatility forecasting using deep learning and sentiment analysis',\n",
       "  'id': '2210.12464',\n",
       "  'abstract': \"several studies have shown that deep learning models can provide more accurate volatility forecasts than the traditional methods used within this domain. this paper presents a composite model that merges a deep learning approach with sentiment analysis for predicting market volatility. to classify public sentiment, we use a convolutional neural network, which obtained data from reddit global news headlines. we then describe a composite forecasting model, a long-short-term-memory neural network method, to use historical sentiment and the previous day's volatility to make forecasts. we employed this method on the past volatility of the s\\\\&p500 and the major brics indices to corroborate its effectiveness. our results demonstrate that including sentiment can improve deep learning volatility forecasting models. however, in contrast to return forecasting, the performance benefits of including sentiment appear for volatility forecasting appears to be market specific.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['v ncume', 't. l van zyl', 'a paskaramoorthy'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12464'},\n",
       " {'title': 'learning correlated stackelberg equilibrium in general-sum   multi-leader-single-follower games',\n",
       "  'id': '2210.12470',\n",
       "  'abstract': 'many real-world strategic games involve interactions between multiple players. we study a hierarchical multi-player game structure, where players with asymmetric roles can be separated into leaders and followers, a setting often referred to as stackelberg game or leader-follower game. in particular, we focus on a stackelberg game scenario where there are multiple leaders and a single follower, called the multi-leader-single-follower (mlsf) game. we propose a novel asymmetric equilibrium concept for the mlsf game called correlated stackelberg equilibrium (cse). we design online learning algorithms that enable the players to interact in a distributed manner, and prove that it can achieve no-external stackelberg-regret learning. this further translates to the convergence to approximate cse via a reduction from no-external regret to no-swap regret. at the core of our works, we solve the intricate problem of how to learn equilibrium in leader-follower games with noisy bandit feedback by balancing exploration and exploitation in different learning structures.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['yaolong yu', 'haifeng xu', 'haipeng chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12470'},\n",
       " {'title': 'estimating oil and gas recovery factors via machine learning:   database-dependent accuracy and reliability',\n",
       "  'id': '2210.12491',\n",
       "  'abstract': 'with recent advances in artificial intelligence, machine learning (ml) approaches have become an attractive tool in petroleum engineering, particularly for reservoir characterizations. a key reservoir property is hydrocarbon recovery factor (rf) whose accurate estimation would provide decisive insights to drilling and production strategies. therefore, this study aims to estimate the hydrocarbon rf for exploration from various reservoir characteristics, such as porosity, permeability, pressure, and water saturation via the ml. we applied three regression-based models including the extreme gradient boosting (xgboost), support vector machine (svm), and stepwise multiple linear regression (mlr) and various combinations of three databases to construct ml models and estimate the oil and/or gas rf. using two databases and the cross-validation method, we evaluated the performance of the ml models. in each iteration 90 and 10% of the data were respectively used to train and test the models. the third independent database was then used to further assess the constructed models. for both oil and gas rfs, we found that the xgboost model estimated the rf for the train and test datasets more accurately than the svm and mlr models. however, the performance of all the models were unsatisfactory for the independent databases. results demonstrated that the ml algorithms were highly dependent and sensitive to the databases based on which they were trained. statistical tests revealed that such unsatisfactory performances were because the distributions of input features and target variables in the train datasets were significantly different from those in the independent databases (p-value < 0.05).',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['alireza roustazadeh',\n",
       "   'behzad ghanbarian',\n",
       "   'mohammad b. shadmand',\n",
       "   'vahid taslimitehrani',\n",
       "   'larry w. lake'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12491'},\n",
       " {'title': 'neuromapper: in-browser visualizer for neural network training',\n",
       "  'id': '2210.12492',\n",
       "  'abstract': \"we present our ongoing work neuromapper, an in-browser visualization tool that helps machine learning (ml) developers interpret the evolution of a model during training, providing a new way to monitor the training process and visually discover reasons for suboptimal training. while most existing deep neural networks (dnns) interpretation tools are designed for already-trained model, neuromapper scalably visualizes the evolution of the embeddings of a model's blocks across training epochs, enabling real-time visualization of 40,000 embedded points. to promote the embedding visualizations' spatial coherence across epochs, neuromapper adapts alignedumap, a recent nonlinear dimensionality reduction technique to align the embeddings. with neuromapper, users can explore the training dynamics of a resnet-50 model, and adjust the embedding visualizations' parameters in real time. neuromapper is open-sourced at https://github.com/poloclub/neuromapper and runs in all modern web browsers. a demo of the tool in action is available at: https://poloclub.github.io/neuromapper/.\",\n",
       "  'categories': 'cs.hc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['zhiyan zhou',\n",
       "   'kevin li',\n",
       "   'haekyu park',\n",
       "   'megan dass',\n",
       "   'austin wright',\n",
       "   'nilaksh das',\n",
       "   'duen horng chau'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12492'},\n",
       " {'title': 'generalized likelihood ratio test with one-class classifiers',\n",
       "  'id': '2210.12494',\n",
       "  'abstract': 'one-class classification (occ) is the problem of deciding whether an observed sample belongs to a target class or not. we consider the problem of learning an occ model when the dataset available at the learning stage contains only samples from the target class. we aim at obtaining a classifier that performs as the generalized likelihood ratio test (glrt), which is a well-known and provably optimal (under specific assumptions) classifier when the statistic of the target class is available. to this end, we consider both the multilayer perceptron neural network (nn) and the support vector machine (svm) models. they are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. we prove that, under suitable assumptions, the models converge (with a large dataset) to the glrt. moreover, we show that the one-class least squares svm (oclssvm) at convergence performs as the glrt, with a suitable transformation function. lastly, we compare the obtained solutions with the autoencoder (ae) classifier, which does not in general provide the glrt',\n",
       "  'categories': 'cs.lg eess.sp stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['francesco ardizzon', 'stefano tomasin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12494'},\n",
       " {'title': 'deep linear networks for matrix completion -- an infinite depth limit',\n",
       "  'id': '2210.12497',\n",
       "  'abstract': 'the deep linear network (dln) is a model for implicit regularization in gradient based optimization of overparametrized learning architectures. training the dln corresponds to a riemannian gradient flow, where the riemannian metric is defined by the architecture of the network and the loss function is defined by the learning task. we extend this geometric framework, obtaining explicit expressions for the volume form, including the case when the network has infinite depth. we investigate the link between the riemannian geometry and the training asymptotics for matrix completion with rigorous analysis and numerics. we propose that implicit regularization is a result of bias towards high state space volume.',\n",
       "  'categories': 'math.ds cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['nadav cohen', 'govind menon', 'zsolt veraszto'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12497'},\n",
       " {'title': 'generative modeling of high-resolution global precipitation forecasts',\n",
       "  'id': '2210.12504',\n",
       "  'abstract': 'forecasting global precipitation patterns and, in particular, extreme precipitation events is of critical importance to preparing for and adapting to climate change. making accurate high-resolution precipitation forecasts using traditional physical models remains a major challenge in operational weather forecasting as they incur substantial computational costs and struggle to achieve sufficient forecast skill. recently, deep-learning-based models have shown great promise in closing the gap with numerical weather prediction (nwp) models in terms of precipitation forecast skill, opening up exciting new avenues for precipitation modeling. however, it is challenging for these deep learning models to fully resolve the fine-scale structures of precipitation phenomena and adequately characterize the extremes of the long-tailed precipitation distribution. in this work, we present several improvements to the architecture and training process of a current state-of-the art deep learning precipitation model (fourcastnet) using a novel generative adversarial network (gan) to better capture fine scales and extremes. our improvements achieve superior performance in capturing the extreme percentiles of global precipitation, while comparable to state-of-the-art nwp models in terms of forecast skill at 1--2 day lead times. together, these improvements set a new state-of-the-art in global precipitation forecasting.',\n",
       "  'categories': 'cs.lg cs.ai cs.cv physics.ao-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['james duncan', 'shashank subramanian', 'peter harrington'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12504'},\n",
       " {'title': 'self-supervised graph-based point-of-interest recommendation',\n",
       "  'id': '2210.12506',\n",
       "  'abstract': \"the exponential growth of location-based social networks (lbsns) has greatly stimulated the demand for precise location-based recommendation services. next point-of-interest (poi) recommendation, which aims to provide personalised poi suggestions for users based on their visiting histories, has become a prominent component in location-based e-commerce. recent poi recommenders mainly employ self-attention mechanism or graph neural networks to model complex high-order poi-wise interactions. however, most of them are merely trained on the historical check-in data in a standard supervised learning manner, which fail to fully explore each user's multi-faceted preferences, and suffer from data scarcity and long-tailed poi distribution, resulting in sub-optimal performance. to this end, we propose a self-s}upervised graph-enhanced poi recommender (s2grec) for next poi recommendation. in particular, we devise a novel graph-enhanced self-attentive layer to incorporate the collaborative signals from both global transition graph and local trajectory graphs to uncover the transitional dependencies among pois and capture a user's temporal interests. in order to counteract the scarcity and incompleteness of poi check-ins, we propose a novel self-supervised learning paradigm in \\\\ssgrec, where the trajectory representations are contrastively learned from two augmented views on geolocations and temporal transitions. extensive experiments are conducted on three real-world lbsn datasets, demonstrating the effectiveness of our model against state-of-the-art methods.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['yang li',\n",
       "   'tong chen',\n",
       "   'peng-fei zhang',\n",
       "   'zi huang',\n",
       "   'hongzhi yin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12506'},\n",
       " {'title': 'cut-and-approximate: 3d shape reconstruction from planar cross-sections   with deep reinforcement learning',\n",
       "  'id': '2210.12509',\n",
       "  'abstract': 'current methods for 3d object reconstruction from a set of planar cross-sections still struggle to capture detailed topology or require a considerable number of cross-sections. in this paper, we present, to the best of our knowledge the first 3d shape reconstruction network to solve this task which additionally uses orthographic projections of the shape. our method is based on applying a reinforcement learning algorithm to learn how to effectively parse the shape using a trial-and-error scheme relying on scalar rewards. this method cuts a part of a 3d shape in each step which is then approximated as a polygon mesh. the agent aims to maximize the reward that depends on the accuracy of surface reconstruction for the approximated parts. we also consider pre-training of the network for faster learning using demonstrations generated by a heuristic approach. experiments show that our training algorithm which benefits from both imitation learning and also self exploration, learns efficient policies faster, which results the agent to produce visually compelling results.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['azimkhon ostonov'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12509'},\n",
       " {'title': 'exploring the landscape of distributional robustness for question   answering models',\n",
       "  'id': '2210.12517',\n",
       "  'abstract': 'we conduct a large empirical evaluation to investigate the landscape of distributional robustness in question answering. our investigation spans over 350 models and 16 question answering datasets, including a diverse set of architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter tuning, in-context learning, etc.). we find that, in many cases, model variations do not affect robustness and in-distribution performance alone determines out-of-distribution performance. moreover, our findings indicate that i) zero-shot and in-context learning methods are more robust to distribution shifts than fully fine-tuned models; ii) few-shot prompt fine-tuned models exhibit better robustness than few-shot fine-tuned span prediction models; iii) parameter-efficient and robustness enhancing training methods provide no significant robustness improvements. in addition, we publicly release all evaluations to encourage researchers to further analyze robustness trends for question answering models.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['anas awadalla',\n",
       "   'mitchell wortsman',\n",
       "   'gabriel ilharco',\n",
       "   'sewon min',\n",
       "   'ian magnusson',\n",
       "   'hannaneh hajishirzi',\n",
       "   'ludwig schmidt'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12517'},\n",
       " {'title': 'federated calibration and evaluation of binary classifiers',\n",
       "  'id': '2210.12526',\n",
       "  'abstract': 'we address two major obstacles to practical use of supervised classifiers on distributed private data. whether a classifier was trained by a federation of cooperating clients or trained centrally out of distribution, (1) the output scores must be calibrated, and (2) performance metrics must be evaluated -- all without assembling labels in one place. in particular, we show how to perform calibration and compute precision, recall, accuracy and roc-auc in the federated setting under three privacy models (i) secure aggregation, (ii) distributed differential privacy, (iii) local differential privacy. our theorems and experiments clarify tradeoffs between privacy, accuracy, and data efficiency. they also help decide whether a given application has sufficient data to support federated calibration and evaluation.',\n",
       "  'categories': 'cs.cr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['graham cormode', 'igor markov'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12526'},\n",
       " {'title': 'baby physical safety monitoring in smart home using action recognition   system',\n",
       "  'id': '2210.12527',\n",
       "  'abstract': 'humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. this is because the brain operates on a bidirectional communication model, which has radically improved the accuracy of recognition and prediction based on features connected to previous experiences. during the past decade, deep learning models for action recognition have significantly improved. however, deep neural networks struggle with these tasks on a smaller dataset for specific action recognition (ar) tasks. as with most action recognition tasks, the ambiguity of accurately describing activities in spatial-temporal data is a drawback that can be overcome by curating suitable datasets, including careful annotations and preprocessing of video data for analyzing various recognition tasks. in this study, we present a novel lightweight framework combining transfer learning techniques with a conv2d lstm layer to extract features from the pre-trained i3d model on the kinetics dataset for a new ar task (smart baby care) that requires a smaller dataset and less computational resources. furthermore, we developed a benchmark dataset and an automated model that uses lstm convolution with i3d (convlstm-i3d) for recognizing and predicting baby activities in a smart baby room. finally, we implemented video augmentation to improve model performance on the smart baby care task. compared to other benchmark models, our experimental framework achieved better performance with less computational resources.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['victor adewopo', 'nelly elsayed', 'kelly anderson'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12527'},\n",
       " {'title': 'on-demand sampling: learning optimally from multiple distributions',\n",
       "  'id': '2210.12529',\n",
       "  'abstract': \"social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative, group distributionally robust, and fair federated learning. in each of these settings, a learner seeks to minimize its worst-case loss over a set of $n$ predefined distributions, while using as few samples as possible. in this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. importantly, our sample complexity bounds exceed that of the sample complexity of learning a single distribution only by an additive factor of $n \\\\log(n) / \\\\epsilon^2$. these improve upon the best known sample complexity of agnostic federated learning by mohri et al. by a multiplicative factor of $n$, the sample complexity of collaborative learning by nguyen and zakynthinou by a multiplicative factor $\\\\log n / \\\\epsilon^3$, and give the first sample complexity bounds for the group dro objective of sagawa et al. to achieve optimal sample complexity, our algorithms learn to sample and learn from distributions on demand. our algorithm design and analysis is enabled by our extensions of stochastic optimization techniques for solving stochastic zero-sum games. in particular, we contribute variants of stochastic mirror descent that can trade off between players' access to cheap one-off samples or more expensive reusable ones.\",\n",
       "  'categories': 'cs.lg cs.cy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['nika haghtalab', 'michael i. jordan', 'eric zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12529'},\n",
       " {'title': 'lmpriors: pre-trained language models as task-specific priors',\n",
       "  'id': '2210.12530',\n",
       "  'abstract': \"particularly in low-data regimes, an outstanding challenge in machine learning is developing principled techniques for augmenting our models with suitable priors. this is to encourage them to learn in ways that are compatible with our understanding of the world. but in contrast to generic priors such as shrinkage or sparsity, we draw inspiration from the recent successes of large-scale language models (lms) to construct task-specific priors distilled from the rich knowledge of lms. our method, language model priors (lmpriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the lm's common-sense reasoning based on the metadata. empirically, we demonstrate that lmpriors improve model performance in settings where such natural language descriptions are available, and perform well on several tasks that benefit from such prior knowledge, such as feature selection, causal inference, and safe reinforcement learning.\",\n",
       "  'categories': 'cs.lg cs.ai cs.cl',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['kristy choi',\n",
       "   'chris cundy',\n",
       "   'sanjari srivastava',\n",
       "   'stefano ermon'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12530'},\n",
       " {'title': 'compressing multidimensional weather and climate data into neural   networks',\n",
       "  'id': '2210.12538',\n",
       "  'abstract': 'weather and climate simulations produce petabytes of high-resolution data that are later analyzed by researchers in order to understand climate change or severe weather. we propose a new method of compressing this multidimensional weather and climate data: a coordinate-based neural network is trained to overfit the data, and the resulting parameters are taken as a compact representation of the original grid-based data. while compression ratios range from 300x to more than 3,000x, our method outperforms the state-of-the-art compressor sz3 in terms of weighted rmse, mae. it can faithfully preserve important large scale atmosphere structures and does not introduce artifacts. when using the resulting neural network as a 790x compressed dataloader to train the weatherbench forecasting model, its rmse increases by less than 2%. the three orders of magnitude compression democratizes access to high-resolution climate data and enables numerous new research directions.',\n",
       "  'categories': 'cs.lg cs.it math.it physics.ao-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['langwen huang', 'torsten hoefler'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12538'},\n",
       " {'title': 'policy optimization with advantage regularization for long-term fairness   in decision systems',\n",
       "  'id': '2210.12546',\n",
       "  'abstract': 'long-term fairness is an important factor of consideration in designing and deploying learning-based decision systems in high-stake decision-making contexts. recent work has proposed the use of markov decision processes (mdps) to formulate decision-making with long-term fairness requirements in dynamically changing environments, and demonstrated major challenges in directly deploying heuristic and rule-based policies that worked well in static environments. we show that policy optimization methods from deep reinforcement learning can be used to find strictly better decision policies that can often achieve both higher overall utility and less violation of the fairness requirements, compared to previously-known strategies. in particular, we propose new methods for imposing fairness requirements in policy optimization by regularizing the advantage evaluation of different actions. our proposed methods make it easy to impose fairness constraints without reward engineering or sacrificing training efficiency. we perform detailed analyses in three established case studies, including attention allocation in incident monitoring, bank loan approval, and vaccine distribution in population networks.',\n",
       "  'categories': 'cs.lg cs.ai cs.cy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['eric yang yu', 'zhizhen qin', 'min kyung lee', 'sicun gao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12546'},\n",
       " {'title': 'surco: learning linear surrogates for combinatorial nonlinear   optimization problems',\n",
       "  'id': '2210.12547',\n",
       "  'abstract': 'optimization problems with expensive nonlinear cost functions and combinatorial constraints appear in many real-world applications, but remain challenging to solve efficiently. existing combinatorial solvers like mixed integer linear programming can be fast in practice but cannot readily optimize nonlinear cost functions, while general nonlinear optimizers like gradient descent often do not handle complex combinatorial structures, may require many queries of the cost function, and are prone to local optima. to bridge this gap, we propose surco that learns linear surrogate costs which can be used by existing combinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. we learn these linear surrogates end-to-end with the nonlinear loss by differentiating through the linear surrogate solver. three variants of surco are proposed: surco-zero operates on individual nonlinear problems, surco-prior trains a linear surrogate predictor on distributions of problems, and surco-hybrid uses a model trained offline to warm start online solving for surco-zero. we analyze our method theoretically and empirically, showing smooth convergence and improved performance. experiments show that compared to state-of-the-art approaches and expert-designed heuristics, surco obtains lower cost solutions with comparable or faster solve time for two realworld industry-level applications: embedding table sharding and inverse photonic design.',\n",
       "  'categories': 'cs.lg cs.ai math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['aaron ferber',\n",
       "   'taoan huang',\n",
       "   'daochen zha',\n",
       "   'martin schubert',\n",
       "   'benoit steiner',\n",
       "   'bistra dilkina',\n",
       "   'yuandong tian'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12547'},\n",
       " {'title': 'understanding domain learning in language models through subpopulation   analysis',\n",
       "  'id': '2210.12553',\n",
       "  'abstract': 'we investigate how different domains are encoded in modern neural network architectures. we analyze the relationship between natural language domains, model size, and the amount of training data used. the primary analysis tool we develop is based on subpopulation analysis with singular vector canonical correlation analysis (svcca), which we apply to transformer-based language models (lms). we compare the latent representations of such a language model at its different layers from a pair of models: a model trained on multiple domains (an experimental model) and a model trained on a single domain (a control model). through our method, we find that increasing the model capacity impacts how domain information is stored in upper and lower layers differently. in addition, we show that larger experimental models simultaneously embed domain-specific information as if they were conjoined control models. these findings are confirmed qualitatively, demonstrating the validity of our method.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['zheng zhao', 'yftah ziser', 'shay b. cohen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12553'},\n",
       " {'title': 'greedy modality selection via approximate submodular maximization',\n",
       "  'id': '2210.12562',\n",
       "  'abstract': 'multimodal learning considers learning from multi-modality data, aiming to fuse heterogeneous sources of information. however, it is not always feasible to leverage all available modalities due to memory constraints. further, training on all the modalities may be inefficient when redundant information exists within data, such as different subsets of modalities providing similar performance. in light of these challenges, we study modality selection, intending to efficiently select the most informative and complementary modalities under certain computational constraints. we formulate a theoretical framework for optimizing modality selection in multimodal learning and introduce a utility measure to quantify the benefit of selecting a modality. for this optimization problem, we present efficient algorithms when the utility measure exhibits monotonicity and approximate submodularity. we also connect the utility measure with existing shapley-value-based feature importance scores. last, we demonstrate the efficacy of our algorithm on synthetic (patch-mnist) and two real-world (pems-sf, cmu-mosi) datasets.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['runxiang cheng',\n",
       "   'gargi balasubramaniam',\n",
       "   'yifei he',\n",
       "   'yao-hung hubert tsai',\n",
       "   'han zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12562'},\n",
       " {'title': 'a visual tour of current challenges in multimodal language models',\n",
       "  'id': '2210.12565',\n",
       "  'abstract': 'transformer models trained on massive text corpora have become the de facto models for a wide range of natural language processing tasks. however, learning effective word representations for function words remains challenging. multimodal learning, which visually grounds transformer models in imagery, can overcome the challenges to some extent; however, there is still much work to be done. in this study, we explore the extent to which visual grounding facilitates the acquisition of function words using stable diffusion models that employ multimodal models for text-to-image generation. out of seven categories of function words, along with numerous subcategories, we find that stable diffusion models effectively model only a small fraction of function words -- a few pronoun subcategories and relatives. we hope that our findings will stimulate the development of new datasets and approaches that enable multimodal models to learn better representations of function words.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['shashank sonkar', 'naiming liu', 'richard g. baraniuk'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12565'},\n",
       " {'title': 'solving continuous control via q-learning',\n",
       "  'id': '2210.12566',\n",
       "  'abstract': 'while there has been substantial success in applying actor-critic methods to continuous control, simpler critic-only methods such as q-learning often remain intractable in the associated high-dimensional action spaces. however, most actor-critic methods come at the cost of added complexity: heuristics for stabilization, compute requirements as well as wider hyperparameter search spaces. we show that these issues can be largely alleviated via q-learning by combining action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (marl). with bang-bang actions, performance of this critic-only approach matches state-of-the-art continuous actor-critic methods when learning from features or pixels. we extend classical bandit examples from cooperative marl to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a wide variety of continuous control tasks.',\n",
       "  'categories': 'cs.lg cs.ai cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['tim seyde',\n",
       "   'peter werner',\n",
       "   'wilko schwarting',\n",
       "   'igor gilitschenski',\n",
       "   'martin riedmiller',\n",
       "   'daniela rus',\n",
       "   'markus wulfmeier'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12566'},\n",
       " {'title': 'an efficient nonlinear acceleration method that exploits symmetry of the   hessian',\n",
       "  'id': '2210.12573',\n",
       "  'abstract': 'nonlinear acceleration methods are powerful techniques to speed up fixed-point iterations. however, many acceleration methods require storing a large number of previous iterates and this can become impractical if computational resources are limited. in this paper, we propose a nonlinear truncated generalized conjugate residual method (nltgcr) whose goal is to exploit the symmetry of the hessian to reduce memory usage. the proposed method can be interpreted as either an inexact newton or a quasi-newton method. we show that, with the help of global strategies like residual check techniques, nltgcr can converge globally for general nonlinear problems and that under mild conditions, nltgcr is able to achieve superlinear convergence. we further analyze the convergence of nltgcr in a stochastic setting. numerical results demonstrate the superiority of nltgcr when compared with several other competitive baseline approaches on a few problems. our code will be available in the future.',\n",
       "  'categories': 'cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['huan he',\n",
       "   'shifan zhao',\n",
       "   'ziyuan tang',\n",
       "   'joyce c ho',\n",
       "   'yousef saad',\n",
       "   'yuanzhe xi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12573'},\n",
       " {'title': 'the curious case of absolute position embeddings',\n",
       "  'id': '2210.12574',\n",
       "  'abstract': 'transformer language models encode the notion of word order using positional information. most commonly, this positional information is represented by absolute position embeddings (apes), that are learned from the pretraining data. however, in natural language, it is not absolute position that matters, but relative position, and the extent to which apes can capture this type of information has not been investigated. in this work, we observe that models trained with ape over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information. specifically, when models are subjected to sentences starting from a non-zero position (excluding the effect of priming), they exhibit noticeably degraded performance on zero to full-shot tasks, across a range of model families and model sizes. our findings raise questions about the efficacy of apes to model the relativity of position information, and invite further introspection on the sentence and word order processing strategies employed by these models.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['koustuv sinha',\n",
       "   'amirhossein kazemnejad',\n",
       "   'siva reddy',\n",
       "   'joelle pineau',\n",
       "   'dieuwke hupkes',\n",
       "   'adina williams'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12574'},\n",
       " {'title': 'outsourcing training without uploading data via efficient collaborative   open-source sampling',\n",
       "  'id': '2210.12575',\n",
       "  'abstract': 'as deep learning blooms with growing demand for computation and data resources, outsourcing model training to a powerful cloud server becomes an attractive alternative to training at a low-power and cost-effective end device. traditional outsourcing requires uploading device data to the cloud server, which can be infeasible in many real-world applications due to the often sensitive nature of the collected data and the limited communication bandwidth. to tackle these challenges, we propose to leverage widely available open-source data, which is a massive dataset collected from public and heterogeneous sources (e.g., internet images). we develop a novel strategy called efficient collaborative open-source sampling (ecos) to construct a proximal proxy dataset from open-source data for cloud training, in lieu of client data. ecos probes open-source data on the cloud server to sense the distribution of client data via a communication- and computation-efficient sampling process, which only communicates a few compressed public features and client scalar responses. extensive empirical studies show that the proposed ecos improves the quality of automated client labeling, model compression, and label outsourcing when applied in various learning scenarios.',\n",
       "  'categories': 'cs.lg cs.cv cs.dc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['junyuan hong',\n",
       "   'lingjuan lyu',\n",
       "   'jiayu zhou',\n",
       "   'michael spranger'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12575'},\n",
       " {'title': 'efficient nearest neighbor search for cross-encoder models using matrix   factorization',\n",
       "  'id': '2210.12579',\n",
       "  'abstract': \"efficient k-nearest neighbor search is a fundamental task, foundational for many problems in nlp. when the similarity is measured by dot-product between dual-encoder vectors or $\\\\ell_2$-distance, there already exist many scalable and efficient search methods. but not so when similarity is measured by more accurate and expensive black-box neural similarity models, such as cross-encoders, which jointly encode the query and candidate neighbor. the cross-encoders' high computational cost typically limits their use to reranking candidates retrieved by a cheaper model, such as dual encoder or tf-idf. however, the accuracy of such a two-stage approach is upper-bounded by the recall of the initial candidate set, and potentially requires additional training to align the auxiliary retrieval model with the cross-encoder model. in this paper, we present an approach that avoids the use of a dual-encoder for retrieval, relying solely on the cross-encoder. retrieval is made efficient with cur decomposition, a matrix decomposition approach that approximates all pairwise cross-encoder distances from a small subset of rows and columns of the distance matrix. indexing items using our approach is computationally cheaper than training an auxiliary dual-encoder model through distillation. empirically, for k > 10, our approach provides test-time recall-vs-computational cost trade-offs superior to the current widely-used methods that re-rank items retrieved using a dual-encoder or tf-idf.\",\n",
       "  'categories': 'cs.cl cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['nishant yadav',\n",
       "   'nicholas monath',\n",
       "   'rico angell',\n",
       "   'manzil zaheer',\n",
       "   'andrew mccallum'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12579'},\n",
       " {'title': 'active learning of discrete-time dynamics for uncertainty-aware model   predictive control',\n",
       "  'id': '2210.12583',\n",
       "  'abstract': 'model-based control requires an accurate model of the system dynamics for precisely and safely controlling the robot in complex and dynamic environments. moreover, in presence of variations in the operating conditions, the model should be continuously refined to compensate for dynamics changes. in this paper, we propose a self-supervised learning approach to actively model robot discrete-time dynamics. we combine offline learning from past experience and online learning from present robot interaction with the unknown environment. these two ingredients enable highly sample-efficient and adaptive learning for accurate inference of the model dynamics in real-time even in operating regimes significantly different from the training distribution. moreover, we design an uncertainty-aware model predictive controller that is conditioned to the aleatoric (data) uncertainty of the learned dynamics. the controller actively selects the optimal control actions that (i) optimize the control performance and (ii) boost the online learning sample efficiency. we apply the proposed method to a quadrotor system in multiple challenging real-world experiments. our approach exhibits high flexibility and generalization capabilities by consistently adapting to unseen flight conditions, while it significantly outperforms classical and adaptive control baselines.',\n",
       "  'categories': 'cs.ro cs.lg cs.sy eess.sy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['alessandro saviolo',\n",
       "   'jonathan frey',\n",
       "   'abhishek rathod',\n",
       "   'moritz diehl',\n",
       "   'giuseppe loianno'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12583'},\n",
       " {'title': 'mr-based electrical property reconstruction using physics-informed   neural networks',\n",
       "  'id': '2210.12584',\n",
       "  'abstract': \"electrical properties (ep), namely permittivity and electric conductivity, dictate the interactions between electromagnetic waves and biological tissue. ep can be potential biomarkers for pathology characterization, such as cancer, and improve therapeutic modalities, such radiofrequency hyperthermia and ablation. mr-based electrical properties tomography (mr-ept) uses mr measurements to reconstruct the ep maps. using the homogeneous helmholtz equation, ep can be directly computed through calculations of second order spatial derivatives of the measured magnetic transmit or receive fields $(b_{1}^{+}, b_{1}^{-})$. however, the numerical approximation of derivatives leads to noise amplifications in the measurements and thus erroneous reconstructions. recently, a noise-robust supervised learning-based method (dl-ept) was introduced for ep reconstruction. however, the pattern-matching nature of such network does not allow it to generalize for new samples since the network's training is done on a limited number of simulated data. in this work, we leverage recent developments on physics-informed deep learning to solve the helmholtz equation for the ep reconstruction. we develop deep neural network (nn) algorithms that are constrained by the helmholtz equation to effectively de-noise the $b_{1}^{+}$ measurements and reconstruct ep directly at an arbitrarily high spatial resolution without requiring any known $b_{1}^{+}$ and ep distribution pairs.\",\n",
       "  'categories': 'cs.lg cs.na math.na physics.bio-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['xinling yu',\n",
       "   'josé e. c. serrallés',\n",
       "   'ilias i. giannakopoulos',\n",
       "   'ziyue liu',\n",
       "   'luca daniel',\n",
       "   'riccardo lattanzi',\n",
       "   'zheng zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12584'},\n",
       " {'title': 'metaems: a meta reinforcement learning-based control framework for   building energy management system',\n",
       "  'id': '2210.12590',\n",
       "  'abstract': 'the building sector has been recognized as one of the primary sectors for worldwide energy consumption. improving the energy efficiency of the building sector can help reduce the operation cost and reduce the greenhouse gas emission. the energy management system (ems) can monitor and control the operations of built-in appliances in buildings, so an efficient ems is of crucial importance to improve the building operation efficiency and maintain safe operations. with the growing penetration of renewable energy and electrical appliances, increasing attention has been paid to the development of intelligent building ems. recently, reinforcement learning (rl) has been applied for building ems and has shown promising potential. however, most of the current rl-based ems solutions would need a large amount of data to learn a reliable control policy, which limits the applicability of these solutions in the real world. in this work, we propose metaems, which can help achieve better energy management performance with the benefits of rl and meta-learning. experiment results showcase that our proposed metaems can adapt faster to environment changes and perform better in most situations compared with other baselines.',\n",
       "  'categories': 'cs.ai cs.lg cs.sy eess.sy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['huiliang zhang', 'di wu', 'benoit boulet'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12590'},\n",
       " {'title': 'online probabilistic model identification using adaptive recursive mcmc',\n",
       "  'id': '2210.12595',\n",
       "  'abstract': 'the bayesian paradigm provides a rigorous framework for estimating the whole probability distribution over unknown parameters, but due to high computational costs, its online application can be difficult. we propose the adaptive recursive markov chain monte carlo (armcmc) method, which calculates the complete probability density function of model parameters while alleviating the drawbacks of traditional online methods. these flaws include being limited to gaussian noise, being solely applicable to linear in the parameters (lip) systems, and having persisting excitation requirements (pe). a variable jump distribution based on a temporal forgetting factor (tff) is proposed in armcmc. the tff can be utilized in many dynamical systems as an effective way to adaptively present the forgetting factor instead of a constant hyperparameter. the particular jump distribution has tailored towards hybrid/multi-modal systems that enables inferences among modes by providing a trade-off between exploitation and exploration. these trade-off are adjusted based on parameter evolution rate. in comparison to traditional mcmc techniques, we show that armcmc requires fewer samples to obtain the same accuracy and reliability. we show our method on two challenging benchmarks: parameter estimation in a soft bending actuator and the hunt-crossley dynamic model. we also compare our method with recursive least squares and the particle filter, and show that our technique has significantly more accurate point estimates as well as a decrease in tracking error of the value of interest.',\n",
       "  'categories': 'cs.lg cs.ro math.st stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['pedram agand', 'mo chen', 'hamid d. taghirad'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12595'},\n",
       " {'title': 'dmode: differential monocular object distance estimation module without   class specific information',\n",
       "  'id': '2210.12596',\n",
       "  'abstract': \"using a single camera to estimate the distances of objects reduces costs compared to stereo-vision and lidar. although monocular distance estimation has been studied in the literature, previous methods mostly rely on knowing an object's class in some way. this can result in deteriorated performance for dataset with multi-class objects and objects with an undefined class. in this paper, we aim to overcome the potential downsides of class-specific approaches, and provide an alternative technique called dmode that does not require any information relating to its class. using differential approaches, we combine the changes in an object's size over time together with the camera's motion to estimate the object's distance. since dmode is class agnostic method, it is easily adaptable to new environments. therefore, it is able to maintain performance across different object detectors, and be easily adapted to new object classes. we tested our model across different scenarios of training and testing on the kitti mots dataset's ground-truth bounding box annotations, and bounding box outputs of trackrcnn and eagermot. the instantaneous change of bounding box sizes and camera position are then used to obtain an object's position in 3d without measuring its detection source or class properties. our results show that we are able to outperform traditional alternatives methods e.g. ipm \\\\cite{tuohyipm}, svr \\\\cite{svr}, and \\\\cite{zhu2019learning} in test environments with multi-class object distance detections.\",\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['pedram agand', 'michael chang', 'mo chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12596'},\n",
       " {'title': 'gani: global attacks on graph neural networks via imperceptible node   injections',\n",
       "  'id': '2210.12598',\n",
       "  'abstract': 'graph neural networks (gnns) have found successful applications in various graph-related tasks. however, recent studies have shown that many gnns are vulnerable to adversarial attacks. in a vast majority of existing studies, adversarial attacks on gnns are launched via direct modification of the original graph such as adding/removing links, which may not be applicable in practice. in this paper, we focus on a realistic attack operation via injecting fake nodes. the proposed global attack strategy via node injection (gani) is designed under the comprehensive consideration of an unnoticeable perturbation setting from both structure and feature domains. specifically, to make the node injections as imperceptible and effective as possible, we propose a sampling operation to determine the degree of the newly injected nodes, and then generate features and select neighbors for these injected nodes based on the statistical information of features and evolutionary perturbations obtained from a genetic algorithm, respectively. in particular, the proposed feature generation mechanism is suitable for both binary and continuous node features. extensive experimental results on benchmark datasets against both general and defended gnns show strong attack performance of gani. moreover, the imperceptibility analyses also demonstrate that gani achieves a relatively unnoticeable injection on benchmark datasets.',\n",
       "  'categories': 'cs.lg cs.ai cs.cr',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['junyuan fang',\n",
       "   'haixian wen',\n",
       "   'jiajing wu',\n",
       "   'qi xuan',\n",
       "   'zibin zheng',\n",
       "   'chi k. tse'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12598'},\n",
       " {'title': 'learning to perform complex tasks through compositional fine-tuning of   language models',\n",
       "  'id': '2210.12607',\n",
       "  'abstract': 'how to usefully encode compositional task structure has long been a core challenge in ai. recent work in chain of thought prompting has shown that for very large neural language models (lms), explicitly demonstrating the inferential steps involved in a target task may improve performance over end-to-end learning that focuses on the target task alone. however, chain of thought prompting has significant limitations due to its dependency on huge pretrained lms. in this work, we present compositional fine-tuning (cft): an approach based on explicitly decomposing a target task into component tasks, and then fine-tuning smaller lms on a curriculum of such component tasks. we apply cft to recommendation tasks in two domains, world travel and local dining, as well as a previously studied inferential task (sports understanding). we show that cft outperforms end-to-end learning even with equal amounts of data, and gets consistently better as more component tasks are modeled via fine-tuning. compared with chain of thought prompting, cft performs at least as well using lms only 7.4% of the size, and is moreover applicable to task domains for which data are not available during pretraining.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-22',\n",
       "  'updated': '',\n",
       "  'authors': ['victor s. bursztyn',\n",
       "   'david demeter',\n",
       "   'doug downey',\n",
       "   'larry birnbaum'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12607'},\n",
       " {'title': 'fast beam alignment via pure exploration in multi-armed bandits',\n",
       "  'id': '2210.12625',\n",
       "  'abstract': 'the beam alignment (ba) problem consists in accurately aligning the transmitter and receiver beams to establish a reliable communication link in wireless communication systems. existing ba methods search the entire beam space to identify the optimal transmit-receive beam pair. this incurs a significant latency when the number of antennas is large. in this work, we develop a bandit-based fast ba algorithm to reduce ba latency for millimeter-wave (mmwave) communications. our algorithm is named two-phase heteroscedastic track-and-stop (2pht\\\\&s). we first formulate the ba problem as a pure exploration problem in multi-armed bandits in which the objective is to minimize the required number of time steps given a certain fixed confidence level. by taking advantage of the correlation structure among beams that the information from nearby beams is similar and the heteroscedastic property that the variance of the reward of an arm (beam) is related to its mean, the proposed algorithm groups all beams into several beam sets such that the optimal beam set is first selected and the optimal beam is identified in this set after that. theoretical analysis and simulation results on synthetic and semi-practical channel data demonstrate the clear superiority of the proposed algorithm vis-\\\\`a-vis other baseline competitors.',\n",
       "  'categories': 'cs.it cs.lg math.it',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yi wei', 'zixin zhong', 'vincent y. f. tan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12625'},\n",
       " {'title': 'spending thinking time wisely: accelerating mcts with virtual expansions',\n",
       "  'id': '2210.12628',\n",
       "  'abstract': 'one of the most important ai research questions is to trade off computation versus performance since ``perfect rationality\" exists in theory but is impossible to achieve in practice. recently, monte-carlo tree search (mcts) has attracted considerable attention due to the significant performance improvement in various challenging domains. however, the expensive time cost during search severely restricts its scope for applications. this paper proposes the virtual mcts (v-mcts), a variant of mcts that spends more search time on harder states and less search time on simpler states adaptively. we give theoretical bounds of the proposed method and evaluate the performance and computations on $9 \\\\times 9$ go board games and atari games. experiments show that our method can achieve comparable performances to the original search algorithm while requiring less than $50\\\\%$ search time on average. we believe that this approach is a viable alternative for tasks under limited time and resources. the code is available at \\\\url{https://github.com/yewr/v-mcts.git}.',\n",
       "  'categories': 'cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['weirui ye', 'pieter abbeel', 'yang gao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12628'},\n",
       " {'title': 'neural eigenfunctions are structured representation learners',\n",
       "  'id': '2210.12637',\n",
       "  'abstract': 'in this paper, we introduce a scalable method for learning structured, adaptive-length deep representations. our approach is to train neural networks such that they approximate the principal eigenfunctions of a kernel. we show that, when the kernel is derived from positive relations in a contrastive learning setup, our method outperforms a number of competitive baselines in visual representation learning and transfer learning benchmarks, and importantly, produces structured representations where the order of features indicates degrees of importance. we demonstrate using such representations as adaptive-length codes in image retrieval systems. by truncation according to feature importance, our method requires up to 16$\\\\times$ shorter representation length than leading self-supervised learning methods to achieve similar retrieval performance. we further apply our method to graph data and report strong results on a node representation learning benchmark with more than one million nodes.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['zhijie deng',\n",
       "   'jiaxin shi',\n",
       "   'hao zhang',\n",
       "   'peng cui',\n",
       "   'cewu lu',\n",
       "   'jun zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12637'},\n",
       " {'title': 'tucker-o-minus decomposition for multi-view tensor subspace clustering',\n",
       "  'id': '2210.12638',\n",
       "  'abstract': 'with powerful ability to exploit latent structure of self-representation information, different tensor decompositions have been employed into low rank multi-view clustering (lrmvc) models for achieving significant performance. however, current approaches suffer from a series of problems related to those tensor decomposition, such as the unbalanced matricization scheme, rotation sensitivity, deficient correlations capture and so forth. all these will lead to lrmvc having insufficient access to global information, which is contrary to the target of multi-view clustering. to alleviate these problems, we propose a new tensor decomposition called tucker-o-minus decomposition (tomd) for multi-view clustering. specifically, based on the tucker format, we additionally employ the o-minus structure, which consists of a circle with an efficient bridge linking two weekly correlated factors. in this way, the core tensor in tucker format is replaced by the o-minus architecture with a more balanced structure, and the enhanced capacity of capturing the global low rank information will be achieved. the proposed tomd also provides more compact and powerful representation abilities for the self-representation tensor, simultaneously. the alternating direction method of multipliers is used to solve the proposed model tomd-mvc. numerical experiments on six benchmark data sets demonstrate the superiority of our proposed method in terms of f-score, precision, recall, normalized mutual information, adjusted rand index, and accuracy.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yingcong lu',\n",
       "   'yipeng liu',\n",
       "   'zhen long',\n",
       "   'zhangxin chen',\n",
       "   'ce zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12638'},\n",
       " {'title': 'accelerated linearized laplace approximation for bayesian deep learning',\n",
       "  'id': '2210.12642',\n",
       "  'abstract': 'laplace approximation (la) and its linearized variant (lla) enable effortless adaptation of pretrained deep neural networks to bayesian neural networks. the generalized gauss-newton (ggn) approximation is typically introduced to improve their tractability. however, la and lla are still confronted with non-trivial inefficiency issues and should rely on kronecker-factored, diagonal, or even last-layer approximate ggn matrices in practical use. these approximations are likely to harm the fidelity of learning outcomes. to tackle this issue, inspired by the connections between lla and neural tangent kernels (ntks), we develop a nystrom approximation to ntks to accelerate lla. our method benefits from the capability of popular deep learning libraries for forward mode automatic differentiation, and enjoys reassuring theoretical guarantees. extensive studies reflect the merits of the proposed method in aspects of both scalability and performance. our method can even scale up to architectures like vision transformers. we also offer valuable ablation studies to diagnose our method. code is available at \\\\url{https://github.com/thudzj/ella}.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['zhijie deng', 'feng zhou', 'jun zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12642'},\n",
       " {'title': 'sat: improving semi-supervised text classification with simple   instance-adaptive self-training',\n",
       "  'id': '2210.12653',\n",
       "  'abstract': 'self-training methods have been explored in recent years and have exhibited great performance in improving semi-supervised learning. this work presents a simple instance-adaptive self-training method (sat) for semi-supervised text classification. sat first generates two augmented views for each unlabeled data and then trains a meta-learner to automatically identify the relative strength of augmentations based on the similarity between the original view and the augmented views. the weakly-augmented view is fed to the model to produce a pseudo-label and the strongly-augmented view is used to train the model to predict the same pseudo-label. we conducted extensive experiments and analyses on three text classification datasets and found that with varying sizes of labeled training data, sat consistently shows competitive performance compared to existing semi-supervised learning methods. our code can be found at \\\\url{https://github.com/declare-lab/sat.git}.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['hui chen', 'wei han', 'soujanya poria'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12653'},\n",
       " {'title': 'no-regret learning in two-echelon supply chain with unknown demand   distribution',\n",
       "  'id': '2210.12663',\n",
       "  'abstract': \"supply chain management (scm) has been recognized as an important discipline with applications to many industries, where the two-echelon stochastic inventory model, involving one downstream retailer and one upstream supplier, plays a fundamental role for developing firms' scm strategies. in this work, we aim at designing online learning algorithms for this problem with an unknown demand distribution, which brings distinct features as compared to classic online optimization problems. specifically, we consider the two-echelon supply chain model introduced in [cachon and zipkin, 1999] under two different settings: the centralized setting, where a planner decides both agents' strategy simultaneously, and the decentralized setting, where two agents decide their strategy independently and selfishly. we design algorithms that achieve favorable guarantees for both regret and convergence to the optimal inventory decision in both settings, and additionally for individual regret in the decentralized setting. our algorithms are based on online gradient descent and online newton step, together with several new ingredients specifically designed for our problem. we also implement our algorithms and show their empirical effectiveness.\",\n",
       "  'categories': 'cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['mengxiao zhang', 'shi chen', 'haipeng luo', 'yingfei wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12663'},\n",
       " {'title': 'meta learning of interface conditions for multi-domain physics-informed   neural networks',\n",
       "  'id': '2210.12669',\n",
       "  'abstract': 'physics-informed neural networks (pinns) are emerging as popular mesh-free solvers for partial differential equations (pdes). recent extensions decompose the domain, applying different pinns to solve the equation in each subdomain and aligning the solution at the interface of the subdomains. hence, they can further alleviate the problem complexity, reduce the computational cost, and allow parallelization. however, the performance of the multi-domain pinns is sensitive to the choice of the interface conditions for solution alignment. while quite a few conditions have been proposed, there is no suggestion about how to select the conditions according to specific problems. to address this gap, we propose meta learning of interface conditions (metalic), a simple, efficient yet powerful approach to dynamically determine the optimal interface conditions for solving a family of parametric pdes. specifically, we develop two contextual multi-arm bandit models. the first one applies to the entire training procedure, and online updates a gaussian process (gp) reward surrogate that given the pde parameters and interface conditions predicts the solution error. the second one partitions the training into two stages, one is the stochastic phase and the other deterministic phase; we update a gp surrogate for each phase to enable different condition selections at the two stages so as to further bolster the flexibility and performance. we have shown the advantage of metalic on four bench-mark pde families.',\n",
       "  'categories': 'cs.lg physics.comp-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['shibo li',\n",
       "   'michael penwarden',\n",
       "   'robert m. kirby',\n",
       "   'shandian zhe'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12669'},\n",
       " {'title': 'less emphasis on difficult layer regions: curriculum learning for   singularly perturbed convection-diffusion-reaction problems',\n",
       "  'id': '2210.12685',\n",
       "  'abstract': \"although physics-informed neural networks (pinns) have been successfully applied to various differential equations, accurately solving perturbed convection-diffusion-reaction problems is still extremely challenging for pinns. this paper investigates the source of the learning difficulties and finds that the rapid transition of potential solution in the layer region causes the failure of convergence. based on this finding, we present a curriculum learning method that encourages neural networks to ``prioritize the learning on easier non-layer regions''. the method helps pinns to dynamically adjust the training data weights, speed up the learning procedure, and ultimately significantly improve the accuracy of the network approximation. extensive evaluation on multiple typical model equations shows that the proposed approach accurately captures the resolution of the layer regions, and achieves multiple orders of magnitude lower root-mean-squared error than ordinary pinns. we provide our pytorch code at https://github.com/wyu-feng/clpinn\",\n",
       "  'categories': 'cs.lg cs.na math.na',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yufeng wang', 'cong xu', 'min yang', 'jin zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12685'},\n",
       " {'title': 'coupling user preference with external rewards to enable driver-centered   and resource-aware ev charging recommendation',\n",
       "  'id': '2210.12693',\n",
       "  'abstract': 'electric vehicle (ev) charging recommendation that both accommodates user preference and adapts to the ever-changing external environment arises as a cost-effective strategy to alleviate the range anxiety of private ev drivers. previous studies focus on centralized strategies to achieve optimized resource allocation, particularly useful for privacy-indifferent taxi fleets and fixed-route public transits. however, private ev driver seeks a more personalized and resource-aware charging recommendation that is tailor-made to accommodate the user preference (when and where to charge) yet sufficiently adaptive to the spatiotemporal mismatch between charging supply and demand. here we propose a novel regularized actor-critic (rac) charging recommendation approach that would allow each ev driver to strike an optimal balance between the user preference (historical charging pattern) and the external reward (driving distance and wait time). experimental results on two real-world datasets demonstrate the unique features and superior performance of our approach to the competing methods.',\n",
       "  'categories': 'cs.lg cs.ai cs.hc cs.ir',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['chengyin li', 'zheng dong', 'nathan fisher', 'dongxiao zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12693'},\n",
       " {'title': 'batch multi-fidelity active learning with budget constraints',\n",
       "  'id': '2210.12704',\n",
       "  'abstract': 'learning functions with high-dimensional outputs is critical in many applications, such as physical simulation and engineering design. however, collecting training examples for these applications is often costly, e.g. by running numerical solvers. the recent work (li et al., 2022) proposes the first multi-fidelity active learning approach for high-dimensional outputs, which can acquire examples at different fidelities to reduce the cost while improving the learning performance. however, this method only queries at one pair of fidelity and input at a time, and hence has a risk to bring in strongly correlated examples to reduce the learning efficiency. in this paper, we propose batch multi-fidelity active learning with budget constraints (bmfal-bc), which can promote the diversity of training examples to improve the benefit-cost ratio, while respecting a given budget constraint for batch queries. hence, our method can be more practically useful. specifically, we propose a novel batch acquisition function that measures the mutual information between a batch of multi-fidelity queries and the target function, so as to penalize highly correlated queries and encourages diversity. the optimization of the batch acquisition function is challenging in that it involves a combinatorial search over many fidelities while subject to the budget constraint. to address this challenge, we develop a weighted greedy algorithm that can sequentially identify each (fidelity, input) pair, while achieving a near $(1 - 1/e)$-approximation of the optimum. we show the advantage of our method in several computational physics and engineering applications.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['shibo li',\n",
       "   'jeff m. phillips',\n",
       "   'xin yu',\n",
       "   'robert m. kirby',\n",
       "   'shandian zhe'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12704'},\n",
       " {'title': 'accelerating the training of single-layer binary neural networks using   the hhl quantum algorithm',\n",
       "  'id': '2210.12707',\n",
       "  'abstract': 'binary neural networks are a promising technique for implementing efficient deep models with reduced storage and computational requirements. the training of these is however, still a compute-intensive problem that grows drastically with the layer size and data input. at the core of this calculation is the linear regression problem. the harrow-hassidim-lloyd (hhl) quantum algorithm has gained relevance thanks to its promise of providing a quantum state containing the solution of a linear system of equations. the solution is encoded in superposition at the output of a quantum circuit. although this seems to provide the answer to the linear regression problem for the training neural networks, it also comes with multiple, difficult-to-avoid hurdles. this paper shows, however, that useful information can be extracted from the quantum-mechanical implementation of hhl, and used to reduce the complexity of finding the solution on the classical side.',\n",
       "  'categories': 'quant-ph cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['sonia lopez alarcon',\n",
       "   'cory merkel',\n",
       "   'martin hoffnagle',\n",
       "   'sabrina ly',\n",
       "   'alejandro pozas-kerstjens'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12707'},\n",
       " {'title': 'generative knowledge graph construction: a review',\n",
       "  'id': '2210.12714',\n",
       "  'abstract': 'generative knowledge graph construction (kgc) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. in this study, we summarize the recent compelling progress in generative knowledge graph construction. we present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. based on the review, we suggest promising research directions for the future. our contributions are threefold: (1) we present a detailed, complete taxonomy for the generative kgc methods; (2) we provide a theoretical and empirical analysis of the generative kgc methods; (3) we propose several research directions that can be developed in the future.',\n",
       "  'categories': 'cs.cl cs.ai cs.db cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['hongbin ye', 'ningyu zhang', 'hui chen', 'huajun chen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12714'},\n",
       " {'title': 'learning general world models in a handful of reward-free deployments',\n",
       "  'id': '2210.12719',\n",
       "  'abstract': 'building generally capable agents is a grand challenge for deep reinforcement learning (rl). to approach this challenge practically, we outline two key desiderata: 1) to facilitate generalization, exploration should be task agnostic; 2) to facilitate scalability, exploration policies should collect large quantities of data without costly centralized retraining. combining these two properties, we introduce the reward-free deployment efficiency setting, a new paradigm for rl research. we then present cascade, a novel approach for self-supervised exploration in this new setting. cascade seeks to learn a world model by collecting data with a population of agents, using an information theoretic objective inspired by bayesian active learning. cascade achieves this by specifically maximizing the diversity of trajectories sampled by the population through a novel cascading objective. we provide theoretical intuition for cascade which we show in a tabular setting improves upon na\\\\\"ive approaches that do not account for population diversity. we then demonstrate that cascade collects diverse task-agnostic datasets and learns agents that generalize zero-shot to novel, unseen downstream tasks on atari, minigrid, crafter and the dm control suite. code and videos are available at https://ycxuyingchen.github.io/cascade/',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yingchen xu',\n",
       "   'jack parker-holder',\n",
       "   'aldo pacchiano',\n",
       "   'philip j. ball',\n",
       "   'oleh rybkin',\n",
       "   'stephen j. roberts',\n",
       "   'tim rocktäschel',\n",
       "   'edward grefenstette'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12719'},\n",
       " {'title': 'a faithful deep sensitivity estimation for accelerated magnetic   resonance imaging',\n",
       "  'id': '2210.12723',\n",
       "  'abstract': 'recent deep learning is superior in providing high-quality images and ultra-fast reconstructions in accelerated magnetic resonance imaging (mri). faithful coil sensitivity estimation is vital for mri reconstruction. however, most deep learning methods still rely on pre-estimated sensitivity maps and ignore their inaccuracy, resulting in the significant quality degradation of reconstructed images. in this work, we propose a joint deep sensitivity estimation and image reconstruction network, called jdsi. during the image artifacts removal, it gradually provides more faithful sensitivity maps, leading to greatly improved image reconstructions. to understand the behavior of the network, the mutual promotion of sensitivity estimation and image reconstruction is revealed through the visualization of network intermediate results. results on in vivo datasets and radiologist reader study demonstrate that, the proposed jdsi achieves the state-of-the-art performance visually and quantitatively, especially when the accelerated factor is high. additionally, jdsi owns nice robustness to abnormal subjects and different number of autocalibration signals.',\n",
       "  'categories': 'eess.iv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['zi wang',\n",
       "   'haoming fang',\n",
       "   'chen qian',\n",
       "   'boxuan shi',\n",
       "   'lijun bao',\n",
       "   'liuhong zhu',\n",
       "   'jianjun zhou',\n",
       "   'wenping wei',\n",
       "   'jianzhong lin',\n",
       "   'di guo',\n",
       "   'xiaobo qu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12723'},\n",
       " {'title': 'functional indirection neural estimator for better out-of-distribution   generalization',\n",
       "  'id': '2210.12739',\n",
       "  'abstract': 'the capacity to achieve out-of-distribution (ood) generalization is a hallmark of human intelligence and yet remains out of reach for machines. this remarkable capability has been attributed to our abilities to make conceptual abstraction and analogy, and to a mechanism known as indirection, which binds two representations and uses one representation to refer to the other. inspired by these mechanisms, we hypothesize that ood generalization may be achieved by performing analogy-making and indirection in the functional space instead of the data space as in current methods. to realize this, we design fine (functional indirection neural estimator), a neural framework that learns to compose functions that map data input to output on-the-fly. fine consists of a backbone network and a trainable semantic memory of basis weight matrices. upon seeing a new input-output data pair, fine dynamically constructs the backbone weights by mixing the basis weights. the mixing coefficients are indirectly computed through querying a separate corresponding semantic memory using the data pair. we demonstrate empirically that fine can strongly improve out-of-distribution generalization on iq tasks that involve geometric transformations. in particular, we train fine and competing models on iq tasks using images from the mnist, omniglot and cifar100 datasets and test on tasks with unseen image classes from one or different datasets and unseen transformation rules. fine not only achieves the best performance on all tasks but also is able to adapt to small-scale data scenarios.',\n",
       "  'categories': 'cs.lg cs.ai cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['kha pham', 'hung le', 'man ngo', 'truyen tran'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12739'},\n",
       " {'title': 'a study of uncertainty quantification in overparametrized   high-dimensional models',\n",
       "  'id': '2210.12760',\n",
       "  'abstract': 'uncertainty quantification is a central challenge in reliable and trustworthy machine learning. naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. several methods, ranging from temperature scaling to different bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. in this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. we discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. this is in contrast with the empirical bayes method, which we show to be well calibrated in our setting despite the higher generalization error and overparametrization.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['lucas clarté',\n",
       "   'bruno loureiro',\n",
       "   'florent krzakala',\n",
       "   'lenka zdeborová'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12760'},\n",
       " {'title': 'discriminative language model as semantic consistency scorer for   prompt-based few-shot text classification',\n",
       "  'id': '2210.12763',\n",
       "  'abstract': 'this paper proposes a novel prompt-based finetuning method (called dlm-scs) for few-shot text classification by utilizing the discriminative language model electra that is pretrained to distinguish whether a token is original or generated. the underlying idea is that the prompt instantiated with the true label should have higher semantic consistency score than other prompts with false labels. since a prompt usually consists of several components (or parts), its semantic consistency can be decomposed accordingly. the semantic consistency of each component is then computed by making use of the pretrained electra model, without introducing extra parameters. extensive experiments have shown that our model outperforms several state-of-the-art prompt-based few-shot methods.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['zhipeng xie', 'yahe li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12763'},\n",
       " {'title': 'multi-objective gflownets',\n",
       "  'id': '2210.12765',\n",
       "  'abstract': 'in many applications of machine learning, like drug discovery and material design, the goal is to generate candidates that simultaneously maximize a set of objectives. as these objectives are often conflicting, there is no single candidate that simultaneously maximizes all objectives, but rather a set of pareto-optimal candidates where one objective cannot be improved without worsening another. moreover, in practice, these objectives are often under-specified, making the diversity of candidates a key consideration. the existing multi-objective optimization methods focus predominantly on covering the pareto front, failing to capture diversity in the space of candidates. motivated by the success of gflownets for generation of diverse candidates in a single objective setting, in this paper we consider multi-objective gflownets (mogfns). mogfns consist of a novel conditional gflownet which models a family of single-objective sub-problems derived by decomposing the multi-objective optimization problem. our work is the first to empirically demonstrate conditional gflownets. through a series of experiments on synthetic and benchmark tasks, we empirically demonstrate that mogfns outperform existing methods in terms of hypervolume, r2-distance and candidate diversity. we also demonstrate the effectiveness of mogfns over existing methods in active learning settings. finally, we supplement our empirical results with a careful analysis of each component of mogfns.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['moksh jain',\n",
       "   'sharath chandra raparthy',\n",
       "   'alex hernandez-garcia',\n",
       "   'jarrid rector-brooks',\n",
       "   'yoshua bengio',\n",
       "   'santiago miret',\n",
       "   'emmanuel bengio'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12765'},\n",
       " {'title': 'falsehoods that ml researchers believe about ood detection',\n",
       "  'id': '2210.12767',\n",
       "  'abstract': \"modelling the density $p(x)$ by probabilistic generative models is an intuitive way to detect out-of-distribution (ood) data, but it fails in the deep learning context. in this paper, we list some falsehoods that machine learning researchers believe about density-based ood detection. many recent works have proposed likelihood-ratio-based methods to `fix' this issue. we propose a framework, the ood proxy framework, to unify these methods, and we argue that likelihood ratio is a principled method for ood detection and not a mere `fix'. finally, we discuss the relationship between domain detection and semantics.\",\n",
       "  'categories': 'stat.ml cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['andi zhang', 'damon wischik'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12767'},\n",
       " {'title': 'on cross-domain pre-trained language models for clinical text mining:   how do they perform on data-constrained fine-tuning?',\n",
       "  'id': '2210.12770',\n",
       "  'abstract': 'pre-trained language models (plms) have been deployed in many natural language processing (nlp) tasks and in various domains. language model pre-training from general or mixed domain rich data plus fine-tuning using small amounts of available data in a low resource domain demonstrated beneficial results by researchers. in this work, we question this statement and verify if bert-based plms from the biomedical domain can perform well in clinical text mining tasks via fine-tuning. we test the state-of-the-art models, i.e. bioformer which is pre-trained on a large amount of biomedical data from pubmed corpus. we use a historical n2c2 clinical nlp challenge dataset for fine-tuning its task-adapted version (bioformerapt), and show that their performances are actually very low. we also present our own end-to-end model, transformercrf, which is developed using transformer and conditional random fields (crfs) as encoder and decoder. we further create a new variation model by adding a crf layer on top of plm bioformer (bioformercrf). we investigate the performances of transformercrf on clinical text mining tasks by training from scratch using a limited amount of data, as well as the model bioformercrf. experimental evaluation shows that, in a \\\\textit{constrained setting}, all tested models are \\\\textit{far from ideal} regarding extreme low-frequency special token recognition, even though they can achieve relatively higher accuracy on overall text tagging. our models including source codes will be hosted at \\\\url{https://github.com/poethan/transformercrf}.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yuping wu', 'lifeng han', 'valerio antonini', 'goran nenadic'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12770'},\n",
       " {'title': 'manifold alignment with label information',\n",
       "  'id': '2210.12774',\n",
       "  'abstract': 'multi-domain data is becoming increasingly common and presents both challenges and opportunities in the data science community. the integration of distinct data-views can be used for exploratory data analysis, and benefit downstream analysis including machine learning related tasks. with this in mind, we present a novel manifold alignment method called mali (manifold alignment with label information) that learns a correspondence between two distinct domains. mali can be considered as belonging to a middle ground between the more commonly addressed semi-supervised manifold alignment problem with some known correspondences between the two domains, and the purely unsupervised case, where no known correspondences are provided. to do this, mali learns the manifold structure in both domains via a diffusion process and then leverages discrete class labels to guide the alignment. by aligning two distinct domains, mali recovers a pairing and a common representation that reveals related samples in both domains. additionally, mali can be used for the transfer learning problem known as domain adaptation. we show that mali outperforms the current state-of-the-art manifold alignment methods across multiple datasets.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['andres f. duque',\n",
       "   'myriam lizotte',\n",
       "   'guy wolf',\n",
       "   'kevin r. moon'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12774'},\n",
       " {'title': 'retrieve, reason, and refine: generating accurate and faithful patient   instructions',\n",
       "  'id': '2210.12777',\n",
       "  'abstract': 'the \"patient instruction\" (pi), which contains critical instructional information provided both to carers and to the patient at the time of discharge, is essential for the patient to manage their condition outside hospital. an accurate and easy-to-follow pi can improve the self-management of patients which can in turn reduce hospital readmission rates. however, writing an appropriate pi can be extremely time-consuming for physicians, and is subject to being incomplete or error-prone for (potentially overworked) physicians. therefore, we propose a new task that can provide an objective means of avoiding incompleteness, while reducing clinical workload: the automatic generation of the pi, which is imagined as being a document that the clinician can review, modify, and approve as necessary (rather than taking the human \"out of the loop\"). we build a benchmark clinical dataset and propose the re3writer, which imitates the working patterns of physicians to first retrieve related working experience from historical pis written by physicians, then reason related medical knowledge. finally, it refines the retrieved working experience and reasoned medical knowledge to extract useful information, which is used to generate the pi for previously-unseen patient according to their health records during hospitalization. our experiments show that, using our method, the performance of five different models can be substantially boosted across all metrics, with up to 20%, 11%, and 19% relative improvements in bleu-4, rouge-l, and meteor, respectively. meanwhile, we show results from human evaluations to measure the effectiveness in terms of its usefulness for clinical practice. the code is available at https://github.com/ai-in-hospitals/patient-instructions',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['fenglin liu',\n",
       "   'bang yang',\n",
       "   'chenyu you',\n",
       "   'xian wu',\n",
       "   'shen ge',\n",
       "   'zhangdaihong liu',\n",
       "   'xu sun',\n",
       "   'yang yang',\n",
       "   'david a. clifton'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12777'},\n",
       " {'title': 'local and global structure preservation based spectral clustering',\n",
       "  'id': '2210.12778',\n",
       "  'abstract': 'spectral clustering (sc) is widely used for clustering data on a nonlinear manifold. sc aims to cluster data by considering the preservation of the local neighborhood structure on the manifold data. this paper extends spectral clustering to local and global structure preservation based spectral clustering (lgpsc) that incorporates both global structure and local neighborhood structure simultaneously. for this extension, lgpsc proposes two models to extend local structures preservation to local and global structures preservation: spectral clustering guided principal component analysis model and multilevel model. finally, we compare the experimental results of the state-of-the-art methods with our two models of lgpsc on various data sets such that the experimental results confirm the effectiveness of our lgpsc models to cluster nonlinear data.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['kajal eybpoosh', 'mansoor rezghi', 'abbas heydari'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12778'},\n",
       " {'title': 'respecting transfer gap in knowledge distillation',\n",
       "  'id': '2210.12787',\n",
       "  'abstract': \"knowledge distillation (kd) is essentially a process of transferring a teacher model's behavior, e.g., network response, to a student model. the network response serves as additional supervision to formulate the machine domain, which uses the data collected from the human domain as a transfer set. traditional kd methods hold an underlying assumption that the data collected in both human domain and machine domain are both independent and identically distributed (iid). we point out that this naive assumption is unrealistic and there is indeed a transfer gap between the two domains. although the gap offers the student model external knowledge from the machine domain, the imbalanced teacher knowledge would make us incorrectly estimate how much to transfer from teacher to student per sample on the non-iid transfer set. to tackle this challenge, we propose inverse probability weighting distillation (ipwd) that estimates the propensity score of a training sample belonging to the machine domain, and assigns its inverse amount to compensate for under-represented samples. experiments on cifar-100 and imagenet demonstrate the effectiveness of ipwd for both two-stage distillation and one-stage self-distillation.\",\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yulei niu', 'long chen', 'chang zhou', 'hanwang zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12787'},\n",
       " {'title': 'clustering-based tile embedding (cte): a general representation for   level design with skewed tile distributions',\n",
       "  'id': '2210.12789',\n",
       "  'abstract': 'there has been significant research interest in procedural level generation via machine learning (plgml), applying ml techniques to automated level generation. one recent trend is in the direction of learning representations for level design via embeddings, such as tile embeddings. tile embeddings are continuous vector representations of game levels unifying their visual, contextual and behavioural information. however, the original tile embedding struggled to generate levels with skewed tile distributions. for instance, super mario bros. (smb) wherein a majority of tiles represent the background. to remedy this, we present a modified tile embedding representation referred to as clustering-based tile embedding (cte). further, we employ clustering to discretize the continuous cte representation and present a novel two-step level generation to leverage both these representations. we evaluate the performance of our approach in generating levels for seen and unseen games with skewed tile distributions and outperform the original tile embeddings.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['mrunal jadhav', 'matthew guzdial'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12789'},\n",
       " {'title': 'o-type stars stellar parameter estimation using recurrent neural   networks',\n",
       "  'id': '2210.12791',\n",
       "  'abstract': 'in this paper, we present a deep learning system approach to estimating luminosity, effective temperature, and surface gravity of o-type stars using the optical region of the stellar spectra. in previous work, we compare a set of machine learning and deep learning algorithms in order to establish a reliable way to fit a stellar model using two methods: the classification of the stellar spectra models and the estimation of the physical parameters in a regression-type task. here we present the process to estimate individual physical parameters from an artificial neural network perspective with the capacity to handle stellar spectra with a low signal-to-noise ratio (s/n), in the $<$20 s/n boundaries. the development of three different recurrent neural network systems, the training process using stellar spectra models, the test over nine different observed stellar spectra, and the comparison with estimations in previous works are presented. additionally, characterization methods for stellar spectra in order to reduce the dimensionality of the input data for the system and optimize the computational resources are discussed.',\n",
       "  'categories': 'astro-ph.im astro-ph.sr cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['miguel flores r.',\n",
       "   'luis j. corral',\n",
       "   'celia r. fierro-santillán',\n",
       "   'silvana g. navarro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12791'},\n",
       " {'title': 'mm-align: learning optimal transport-based alignment dynamics for fast   and accurate inference on missing modality sequences',\n",
       "  'id': '2210.12798',\n",
       "  'abstract': 'existing multimodal tasks mostly target at the complete input modality setting, i.e., each modality is either complete or completely missing in both training and test sets. however, the randomly missing situations have still been underexplored. in this paper, we present a novel approach named mm-align to address the missing-modality inference problem. concretely, we propose 1) an alignment dynamics learning module based on the theory of optimal transport (ot) for indirect missing data imputation; 2) a denoising training algorithm to simultaneously enhance the imputation results and backbone network performance. compared with previous methods which devote to reconstructing the missing inputs, mm-align learns to capture and imitate the alignment dynamics between modality sequences. results of comprehensive experiments on three datasets covering two multimodal tasks empirically demonstrate that our method can perform more accurate and faster inference and relieve overfitting under various missing conditions.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['wei han', 'hui chen', 'min-yen kan', 'soujanya poria'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12798'},\n",
       " {'title': 'active exploration for robotic manipulation',\n",
       "  'id': '2210.12806',\n",
       "  'abstract': 'robotic manipulation stands as a largely unsolved problem despite significant advances in robotics and machine learning in recent years. one of the key challenges in manipulation is the exploration of the dynamics of the environment when there is continuous contact between the objects being manipulated. this paper proposes a model-based active exploration approach that enables efficient learning in sparse-reward robotic manipulation tasks. the proposed method estimates an information gain objective using an ensemble of probabilistic models and deploys model predictive control (mpc) to plan actions online that maximize the expected reward while also performing directed exploration. we evaluate our proposed algorithm in simulation and on a real robot, trained from scratch with our method, on a challenging ball pushing task on tilted tables, where the target ball position is not known to the agent a-priori. our real-world robot experiment serves as a fundamental application of active exploration in model-based reinforcement learning of complex robotic manipulation tasks.',\n",
       "  'categories': 'cs.ro cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['tim schneider',\n",
       "   'boris belousov',\n",
       "   'georgia chalvatzaki',\n",
       "   'diego romeres',\n",
       "   'devesh k. jha',\n",
       "   'jan peters'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12806'},\n",
       " {'title': 'symmetric (optimistic) natural policy gradient for multi-agent learning   with parameter convergence',\n",
       "  'id': '2210.12812',\n",
       "  'abstract': 'multi-agent interactions are increasingly important in the context of reinforcement learning, and the theoretical foundations of policy gradient methods have attracted surging research interest. we investigate the global convergence of natural policy gradient (npg) algorithms in multi-agent learning. we first show that vanilla npg may not have parameter convergence, i.e., the convergence of the vector that parameterizes the policy, even when the costs are regularized (which enabled strong convergence guarantees in the policy space in the literature). this non-convergence of parameters leads to stability issues in learning, which becomes especially relevant in the function approximation setting, where we can only operate on low-dimensional parameters, instead of the high-dimensional policy. we then propose variants of the npg algorithm, for several standard multi-agent learning scenarios: two-player zero-sum matrix and markov games, and multi-player monotone games, with global last-iterate parameter convergence guarantees. we also generalize the results to certain function approximation settings. note that in our algorithms, the agents take symmetric roles. our results might also be of independent interest for solving nonconvex-nonconcave minimax optimization problems with certain structures. simulations are also provided to corroborate our theoretical findings.',\n",
       "  'categories': 'math.oc cs.lg cs.ma stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['sarath pattathil', 'kaiqing zhang', 'asuman ozdaglar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12812'},\n",
       " {'title': 'simple alternating minimization provably solves complete dictionary   learning',\n",
       "  'id': '2210.12816',\n",
       "  'abstract': 'this paper focuses on complete dictionary learning problem, where the goal is to reparametrize a set of given signals as linear combinations of atoms from a learned dictionary. there are two main challenges faced by theoretical and practical studies of dictionary learning: the lack of theoretical guarantees for practically-used heuristic algorithms, and their poor scalability when dealing with huge-scale datasets. towards addressing these issues, we show that when the dictionary to be learned is orthogonal, that an alternating minimization method directly applied to the nonconvex and discrete formulation of the problem exactly recovers the ground truth. for the huge-scale, potentially online setting, we propose a minibatch version of our algorithm, which can provably learn a complete dictionary from a huge-scale dataset with minimal sample complexity, linear sparsity level, and linear convergence rate, thereby negating the need for any convex relaxation for the problem. our numerical experiments showcase the superiority of our method compared with the existing techniques when applied to tasks on real data.',\n",
       "  'categories': 'cs.lg eess.sp math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['geyu liang',\n",
       "   'gavin zhang',\n",
       "   'salar fattahi',\n",
       "   'richard y. zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12816'},\n",
       " {'title': 'towards real-time text2video via clip-guided, pixel-level optimization',\n",
       "  'id': '2210.12826',\n",
       "  'abstract': 'we introduce an approach to generating videos based on a series of given language descriptions. frames of the video are generated sequentially and optimized by guidance from the clip image-text encoder; iterating through language descriptions, weighting the current description higher than others. as opposed to optimizing through an image generator model itself, which tends to be computationally heavy, the proposed approach computes the clip loss directly at the pixel level, achieving general content at a speed suitable for near real-time systems. the approach can generate videos in up to 720p resolution, variable frame-rates, and arbitrary aspect ratios at a rate of 1-2 frames per second. please visit our website to view videos and access our open-source code: https://pschaldenbrand.github.io/text2video/ .',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['peter schaldenbrand', 'zhixuan liu', 'jean oh'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12826'},\n",
       " {'title': 'decentralized stochastic bilevel optimization with improved   per-iteration complexity',\n",
       "  'id': '2210.12839',\n",
       "  'abstract': 'bilevel optimization recently has received tremendous attention due to its great success in solving important machine learning problems like meta learning, reinforcement learning, and hyperparameter optimization. extending single-agent training on bilevel problems to the decentralized setting is a natural generalization, and there has been a flurry of work studying decentralized bilevel optimization algorithms. however, it remains unknown how to design the distributed algorithm with sample complexity and convergence rate comparable to sgd for stochastic optimization, and at the same time without directly computing the exact hessian or jacobian matrices. in this paper we propose such an algorithm. more specifically, we propose a novel decentralized stochastic bilevel optimization (dsbo) algorithm that only requires first order stochastic oracle, hessian-vector product and jacobian-vector product oracle. the sample complexity of our algorithm matches the currently best known results for dsbo, and the advantage of our algorithm is that it does not require estimating the full hessian and jacobian matrices, thereby having improved per-iteration complexity.',\n",
       "  'categories': 'math.oc cs.dc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['xuxing chen',\n",
       "   'minhui huang',\n",
       "   'shiqian ma',\n",
       "   'krishnakumar balasubramanian'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12839'},\n",
       " {'title': 'a cooperative reinforcement learning environment for detecting and   penalizing betrayal',\n",
       "  'id': '2210.12841',\n",
       "  'abstract': 'in this paper we present a reinforcement learning environment that leverages agent cooperation and communication, aimed at detection, learning and ultimately penalizing betrayal patterns that emerge in the behavior of self-interested agents. we provide a description of game rules, along with interesting cases of betrayal and trade-offs that arise. preliminary experimental investigations illustrate a) betrayal emergence, b) deceptive agents outperforming honest baselines and b) betrayal detection based on classification of behavioral features, which surpasses probabilistic detection baselines. finally, we propose approaches for penalizing betrayal, list directions for future work and suggest interesting extensions of the environment towards capturing and exploring increasingly complex patterns of social interactions.',\n",
       "  'categories': 'cs.lg cs.gt cs.ma',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['nikiforos pittaras'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12841'},\n",
       " {'title': 'deep-learning-based precipitation nowcasting with ground weather station   data and radar data',\n",
       "  'id': '2210.12853',\n",
       "  'abstract': 'recently, many deep-learning techniques have been applied to various weather-related prediction tasks, including precipitation nowcasting (i.e., predicting precipitation levels and locations in the near future). most existing deep-learning-based approaches for precipitation nowcasting, however, consider only radar and/or satellite images as inputs, and meteorological observations collected from ground weather stations, which are sparsely located, are relatively unexplored. in this paper, we propose asoc, a novel attentive method for effectively exploiting ground-based meteorological observations from multiple weather stations. asoc is designed to capture temporal dynamics of the observations and also contextual relationships between them. asoc is easily combined with existing image-based precipitation nowcasting models without changing their architectures. we show that such a combination improves the average critical success index (csi) of predicting heavy (at least 10 mm/hr) and light (at least 1 mm/hr) rainfall events at 1-6 hr lead times by 5.7%, compared to the original image-based model, using the radar images and ground-based observations around south korea collected from 2014 to 2020.',\n",
       "  'categories': 'physics.ao-ph cs.ai cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['jihoon ko', 'kyuhan lee', 'hyunjin hwang', 'kijung shin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12853'},\n",
       " {'title': 'explicit second-order min-max optimization methods with optimal   convergence guarantee',\n",
       "  'id': '2210.12860',\n",
       "  'abstract': 'we propose and analyze exact and inexact regularized newton-type methods for finding a global saddle point of a \\\\textit{convex-concave} unconstrained min-max optimization problem. compared to their first-order counterparts, investigations of second-order methods for min-max optimization are relatively limited, as obtaining global rates of convergence with second-order information is much more involved. in this paper, we highlight how second-order information can be used to speed up the dynamics of dual extrapolation methods {despite inexactness}. specifically, we show that the proposed algorithms generate iterates that remain within a bounded set and the averaged iterates converge to an $\\\\epsilon$-saddle point within $o(\\\\epsilon^{-2/3})$ iterations in terms of a gap function. our algorithms match the theoretically established lower bound in this context and our analysis provides a simple and intuitive convergence analysis for second-order methods without requiring any compactness assumptions. finally, we present a series of numerical experiments on synthetic and real data that demonstrate the efficiency of the proposed algorithms.',\n",
       "  'categories': 'math.oc cs.cc cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['tianyi lin', 'panayotis mertikopoulos', 'michael i. jordan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12860'},\n",
       " {'title': 'tight relative estimation in the mean of bernoulli random variables',\n",
       "  'id': '2210.12861',\n",
       "  'abstract': 'given a stream of bernoulli random variables, consider the problem of estimating the mean of the random variable within a specified relative error with a specified probability of failure. until now, the gamma bernoulli approximation scheme (gbas) was the method that accomplished this goal using the smallest number of average samples. in this work, a new method is introduced that is faster when the mean is bounded away from zero. the process uses a two-stage process together with some simple inequalities to get rigorous bounds on the error probability.',\n",
       "  'categories': 'cs.lg math.pr math.st stat.co stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['mark huber'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12861'},\n",
       " {'title': 'k-sam: sharpness-aware minimization at the speed of sgd',\n",
       "  'id': '2210.12864',\n",
       "  'abstract': 'sharpness-aware minimization (sam) has recently emerged as a robust technique for improving the accuracy of deep neural networks. however, sam incurs a high computational cost in practice, requiring up to twice as much computation as vanilla sgd. the computational challenge posed by sam arises because each iteration requires both ascent and descent steps and thus double the gradient computations. to address this challenge, we propose to compute gradients in both stages of sam on only the top-k samples with highest loss. k-sam is simple and extremely easy-to-implement while providing significant generalization boosts over vanilla sgd at little to no additional cost.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['renkun ni',\n",
       "   'ping-yeh chiang',\n",
       "   'jonas geiping',\n",
       "   'micah goldblum',\n",
       "   'andrew gordon wilson',\n",
       "   'tom goldstein'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12864'},\n",
       " {'title': 'knowledge transfer from answer ranking to answer generation',\n",
       "  'id': '2210.12865',\n",
       "  'abstract': 'recent studies show that question answering (qa) based on answer sentence selection (as2) can be improved by generating an improved answer from the top-k ranked answer sentences (termed genqa). this allows for synthesizing the information from multiple candidates into a concise, natural-sounding answer. however, creating large-scale supervised training data for genqa models is very challenging. in this paper, we propose to train a genqa model by transferring knowledge from a trained as2 model, to overcome the aforementioned issue. first, we use an as2 model to produce a ranking over answer candidates for a set of questions. then, we use the top ranked candidate as the generation target, and the next k top ranked candidates as context for training a genqa model. we also propose to use the as2 model prediction scores for loss weighting and score-conditioned input/output shaping, to aid the knowledge transfer. our evaluation on three public and one large industrial datasets demonstrates the superiority of our approach over the as2 baseline, and genqa trained using supervised data.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['matteo gabburo',\n",
       "   'rik koncel-kedziorski',\n",
       "   'siddhant garg',\n",
       "   'luca soldaini',\n",
       "   'alessandro moschitti'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12865'},\n",
       " {'title': 'deep equilibrium approaches to diffusion models',\n",
       "  'id': '2210.12867',\n",
       "  'abstract': \"diffusion-based generative models are extremely effective in generating high-quality images, with generated samples often surpassing the quality of those produced by other models under several metrics. one distinguishing feature of these models, however, is that they typically require long sampling chains to produce high-fidelity images. this presents a challenge not only from the lenses of sampling time, but also from the inherent difficulty in backpropagating through these chains in order to accomplish tasks such as model inversion, i.e. approximately finding latent states that generate known images. in this paper, we look at diffusion models through a different perspective, that of a (deep) equilibrium (deq) fixed point model. specifically, we extend the recent denoising diffusion implicit model (ddim; song et al. 2020), and model the entire sampling chain as a joint, multivariate fixed point system. this setup provides an elegant unification of diffusion and equilibrium models, and shows benefits in 1) single image sampling, as it replaces the fully-serial typical sampling process with a parallel one; and 2) model inversion, where we can leverage fast gradients in the deq setting to much more quickly find the noise that generates a given image. the approach is also orthogonal and thus complementary to other methods used to reduce the sampling time, or improve model inversion. we demonstrate our method's strong performance across several datasets, including cifar10, celeba, and lsun bedrooms and churches.\",\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['ashwini pokle', 'zhengyang geng', 'zico kolter'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12867'},\n",
       " {'title': 'imbalanced class data performance evaluation and improvement using novel   generative adversarial network-based approach: ssg and gbo',\n",
       "  'id': '2210.12870',\n",
       "  'abstract': \"class imbalance in a dataset is one of the major challenges that can significantly impact the performance of machine learning models resulting in biased predictions. numerous techniques have been proposed to address class imbalanced problems, including, but not limited to, oversampling, undersampling, and cost-sensitive approaches. due to its ability to generate synthetic data, oversampling techniques such as the synthetic minority oversampling technique (smote) is among the most widely used methodology by researchers. however, one of smote's potential disadvantages is that newly created minor samples may overlap with major samples. as an effect, the probability of ml models' biased performance towards major classes increases. recently, generative adversarial network (gan) has garnered much attention due to its ability to create almost real samples. however, gan is hard to train even though it has much potential. this study proposes two novel techniques: gan-based oversampling (gbo) and support vector machine-smote-gan (ssg) to overcome the limitations of the existing oversampling approaches. the preliminary computational result shows that ssg and gbo performed better on the expanded imbalanced eight benchmark datasets than the original smote. the study also revealed that the minor sample generated by ssg demonstrates gaussian distributions, which is often difficult to achieve using original smote.\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['md manjurul ahsan', 'md shahin ali', 'zahed siddique'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12870'},\n",
       " {'title': 'tighter abstract queries in neural network verification',\n",
       "  'id': '2210.12871',\n",
       "  'abstract': 'neural networks have become critical components of reactive systems in various domains within computer science. despite their excellent performance, using neural networks entails numerous risks that stem from our lack of ability to understand and reason about their behavior. due to these risks, various formal methods have been proposed for verifying neural networks; but unfortunately, these typically struggle with scalability barriers. recent attempts have demonstrated that abstraction-refinement approaches could play a significant role in mitigating these limitations; but these approaches can often produce networks that are so abstract, that they become unsuitable for verification. to deal with this issue, we present cegarette, a novel verification mechanism where both the system and the property are abstracted and refined simultaneously. we observe that this approach allows us to produce abstract networks which are both small and sufficiently accurate, allowing for quick verification times while avoiding a large number of refinement steps. for evaluation purposes, we implemented cegarette as an extension to the recently proposed cegar-nn framework. our results are very promising, and demonstrate a significant improvement in performance over multiple benchmarks.',\n",
       "  'categories': 'cs.lg cs.lo cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['elazar cohen',\n",
       "   'yizhak yisrael elboher',\n",
       "   'clark barrett',\n",
       "   'guy katz'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12871'},\n",
       " {'title': 'flip: a provable defense framework for backdoor mitigation in federated   learning',\n",
       "  'id': '2210.12873',\n",
       "  'abstract': 'federated learning (fl) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. in this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). existing work on robust aggregation and certified fl robustness does not study how hardening benign clients can affect the global model (and the malicious clients). in this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. we conduct comprehensive experiments across different datasets and attack settings. our results on eight competing sota defense methods show the empirical superiority of our method on both single-shot and continuous fl backdoor attacks.',\n",
       "  'categories': 'cs.cr cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['kaiyuan zhang',\n",
       "   'guanhong tao',\n",
       "   'qiuling xu',\n",
       "   'siyuan cheng',\n",
       "   'shengwei an',\n",
       "   'yingqi liu',\n",
       "   'shiwei feng',\n",
       "   'guangyu shen',\n",
       "   'pin-yu chen',\n",
       "   'shiqing ma',\n",
       "   'xiangyu zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12873'},\n",
       " {'title': 'tail batch sampling: approximating global contrastive losses as   optimization over batch assignments',\n",
       "  'id': '2210.12874',\n",
       "  'abstract': 'contrastive learning has recently achieved state-of-the-art performance in a wide range of tasks. many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. in this work, we provide an alternative to hard negative mining in supervised contrastive learning, tail batch sampling (tbs), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\\\\mathcal{l}^{global} - \\\\mathcal{l}^{train}$. tbs \\\\textbf{improves state-of-the-art performance} in sentence embedding (+0.37 spearman) and code-search tasks (+2.2\\\\% mrr), is easy to implement - requiring only a few additional lines of code, does not maintain external data structures such as nearest neighbor indices, is more computationally efficient when compared to the most minimal hard negative mining approaches, and makes no changes to the model being trained.',\n",
       "  'categories': 'cs.lg cs.cl stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['vin sachidananda', 'ziyi yang', 'chenguang zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12874'},\n",
       " {'title': 'stochastic mirror descent for large-scale sparse recovery',\n",
       "  'id': '2210.12882',\n",
       "  'abstract': 'in this paper we discuss an application of stochastic approximation to statistical estimation of high-dimensional sparse parameters. the proposed solution reduces to resolving a penalized stochastic optimization problem on each stage of a multistage algorithm; each problem being solved to a prescribed accuracy by the non-euclidean composite stochastic mirror descent (csmd) algorithm. assuming that the problem objective is smooth and quadratically minorated and stochastic perturbations are sub-gaussian, our analysis prescribes the method parameters which ensure fast convergence of the estimation error (the radius of a confidence ball of a given norm around the approximate solution). this convergence is linear during the first \"preliminary\" phase of the routine and is sublinear during the second \"asymptotic\" phase. we consider an application of the proposed approach to sparse generalized linear regression problem. in this setting, we show that the proposed algorithm attains the optimal convergence of the estimation error under weak assumptions on the regressor distribution. we also present a numerical study illustrating the performance of the algorithm on high-dimensional simulation data.',\n",
       "  'categories': 'stat.ml cs.lg math.oc math.st stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['sasila ilandarideva',\n",
       "   'yannis bekri',\n",
       "   'anatoli juditsky',\n",
       "   'vianney perchet'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12882'},\n",
       " {'title': 'aacher: assorted actor-critic deep reinforcement learning with hindsight   experience replay',\n",
       "  'id': '2210.12892',\n",
       "  'abstract': \"actor learning and critic learning are two components of the outstanding and mostly used deep deterministic policy gradient (ddpg) reinforcement learning method. since actor and critic learning plays a significant role in the overall robot's learning, the performance of the ddpg approach is relatively sensitive and unstable as a result. we propose a multi-actor-critic ddpg for reliable actor-critic learning to further enhance the performance and stability of ddpg. this multi-actor-critic ddpg is then integrated with hindsight experience replay (her) to form our new deep learning framework called aacher. aacher uses the average value of multiple actors or critics to substitute the single actor or critic in ddpg to increase resistance in the case when one actor or critic performs poorly. numerous independent actors and critics can also gain knowledge from the environment more broadly. we implemented our proposed aacher on goal-based environments: auboreach, fetchreach-v1, fetchpush-v1, fetchslide-v1, and fetchpickandplace-v1. for our experiments, we used various instances of actor/critic combinations, among which a10c10 and a20c20 were the best-performing combinations. overall results show that aacher outperforms the traditional algorithm (ddpg+her) in all of the actor/critic number combinations that are used for evaluation. when used on fetchpickandplace-v1, the performance boost for a20c20 is as high as roughly 3.8 times the success rate in ddpg+her.\",\n",
       "  'categories': 'cs.ro cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['adarsh sehgal', 'muskan sehgal', 'hung manh la'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12892'},\n",
       " {'title': 'predicting the citation count and citescore of journals one year in   advance',\n",
       "  'id': '2210.12908',\n",
       "  'abstract': 'prediction of the future performance of academic journals is a task that can benefit a variety of stakeholders including editorial staff, publishers, indexing services, researchers, university administrators and granting agencies. using historical data on journal performance, this can be framed as a machine learning regression problem. in this work, we study two such regression tasks: 1) prediction of the number of citations a journal will receive during the next calendar year, and 2) prediction of the elsevier citescore a journal will be assigned for the next calendar year. to address these tasks, we first create a dataset of historical bibliometric data for journals indexed in scopus. we propose the use of neural network models trained on our dataset to predict the future performance of journals. to this end, we perform feature selection and model configuration for a multi-layer perceptron and a long short-term memory. through experimental comparisons to heuristic prediction baselines and classical machine learning models, we demonstrate superior performance in our proposed models for the prediction of future citation and citescore values.',\n",
       "  'categories': 'cs.dl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['william croft', 'jörg-rüdiger sack'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12908'},\n",
       " {'title': 'a novel adaptive causal sampling method for physics-informed neural   networks',\n",
       "  'id': '2210.12914',\n",
       "  'abstract': 'physics-informed neural networks (pinns) have become a kind of attractive machine learning method for obtaining solutions of partial differential equations (pdes). training pinns can be seen as a semi-supervised learning task, in which only exact values of initial and boundary points can be obtained in solving forward problems, and in the whole spatio-temporal domain collocation points are sampled without exact labels, which brings training difficulties. thus the selection of collocation points and sampling methods are quite crucial in training pinns. existing sampling methods include fixed and dynamic types, and in the more popular latter one, sampling is usually controlled by pde residual loss. we point out that it is not sufficient to only consider the residual loss in adaptive sampling and sampling should obey temporal causality. we further introduce temporal causality into adaptive sampling and propose a novel adaptive causal sampling method to improve the performance and efficiency of pinns. numerical experiments of several pdes with high-order derivatives and strong nonlinearity, including cahn hilliard and kdv equations, show that the proposed sampling method can improve the performance of pinns with few collocation points. we demonstrate that by utilizing such a relatively simple sampling method, prediction performance can be improved up to two orders of magnitude compared with state-of-the-art results with almost no extra computation cost, especially when points are limited.',\n",
       "  'categories': 'cs.lg cs.na math.na',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['jia guo', 'haifeng wang', 'chenping hou'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12914'},\n",
       " {'title': 'unsupervised object representation learning using translation and   rotation group equivariant vae',\n",
       "  'id': '2210.12918',\n",
       "  'abstract': \"in many imaging modalities, objects of interest can occur in a variety of locations and poses (i.e. are subject to translations and rotations in 2d or 3d), but the location and pose of an object does not change its semantics (i.e. the object's essence). that is, the specific location and rotation of an airplane in satellite imagery, or the 3d rotation of a chair in a natural image, or the rotation of a particle in a cryo-electron micrograph, do not change the intrinsic nature of those objects. here, we consider the problem of learning semantic representations of objects that are invariant to pose and location in a fully unsupervised manner. we address shortcomings in previous approaches to this problem by introducing target-vae, a translation and rotation group-equivariant variational autoencoder framework. target-vae combines three core innovations: 1) a rotation and translation group-equivariant encoder architecture, 2) a structurally disentangled distribution over latent rotation, translation, and a rotation-translation-invariant semantic object representation, which are jointly inferred by the approximate inference network, and 3) a spatially equivariant generator network. in comprehensive experiments, we show that target-vae learns disentangled representations without supervision that significantly improve upon, and avoid the pathologies of, previous methods. when trained on images highly corrupted by rotation and translation, the semantic representations learned by target-vae are similar to those learned on consistently posed objects, dramatically improving clustering in the semantic latent space. furthermore, target-vae is able to perform remarkably accurate unsupervised pose and location inference. we expect methods like target-vae will underpin future approaches for unsupervised object generation, pose prediction, and object detection.\",\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['alireza nasiri', 'tristan bepler'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12918'},\n",
       " {'title': 'olla: decreasing the memory usage of neural networks by optimizing the   lifetime and location of arrays',\n",
       "  'id': '2210.12924',\n",
       "  'abstract': 'the size of deep neural networks has grown exponentially in recent years. unfortunately, hardware devices have not kept pace with the rapidly increasing memory requirements. to cope with this, researchers have turned to techniques such as spilling and recomputation, which increase training time, or reduced precision and model pruning, which can affect model accuracy. we present olla, an algorithm that optimizes the lifetime and memory location of the tensors used to train neural networks. our method reduces the memory usage of existing neural networks, without needing any modification to the models or their training procedures. we formulate the problem as a joint integer linear program (ilp). we present several techniques to simplify the encoding of the problem, and enable our approach to scale to the size of state-of-the-art neural networks using an off-the-shelf ilp solver. we experimentally demonstrate that olla only takes minutes if not seconds to allow the training of neural networks using one-third less memory on average.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['benoit steiner',\n",
       "   'mostafa elhoushi',\n",
       "   'jacob kahn',\n",
       "   'james hegarty'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12924'},\n",
       " {'title': 'gflowout: dropout with generative flow networks',\n",
       "  'id': '2210.12928',\n",
       "  'abstract': 'bayesian inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. however, scaling bayesian inference to large architectures is challenging and requires restrictive approximations. monte carlo dropout has been widely used as a relatively cheap way for approximate inference and to estimate uncertainty with deep neural networks. traditionally, the dropout mask is sampled independently from a fixed distribution. recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. these methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. in this work, we propose gflowout to address these issues. gflowout leverages the recently proposed probabilistic framework of generative flow networks (gflownets) to learn the posterior distribution over dropout masks. we empirically demonstrate that gflowout results in predictive distributions that generalize better to out-of-distribution data, and provide uncertainty estimates which lead to better performance in downstream tasks.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['dianbo liu',\n",
       "   'moksh jain',\n",
       "   'bonaventure dossou',\n",
       "   'qianli shen',\n",
       "   'salem lahlou',\n",
       "   'anirudh goyal',\n",
       "   'nikolay malkin',\n",
       "   'chris emezue',\n",
       "   'dinghuai zhang',\n",
       "   'nadhir hassen',\n",
       "   'xu ji',\n",
       "   'kenji kawaguchi',\n",
       "   'yoshua bengio'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12928'},\n",
       " {'title': 'finding memo: extractive memorization in constrained sequence generation   tasks',\n",
       "  'id': '2210.12929',\n",
       "  'abstract': 'memorization presents a challenge for several constrained natural language generation (nlg) tasks such as neural machine translation (nmt), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. however, previous studies of memorization in constrained nlg tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. in this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in nmt. we demonstrate that extractive memorization poses a serious threat to nmt reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. we have released the code to reproduce our results at https://github.com/vyraun/finding-memo.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['vikas raunak', 'arul menezes'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12929'},\n",
       " {'title': 'multi-agent path finding via tree lstm',\n",
       "  'id': '2210.12933',\n",
       "  'abstract': 'in recent years, multi-agent path finding (mapf) has attracted attention from the fields of both operations research (or) and reinforcement learning (rl). however, in the 2021 flatland3 challenge, a competition on mapf, the best rl method scored only 27.9, far less than the best or method. this paper proposes a new rl solution to flatland3 challenge, which scores 125.3, several times higher than the best rl solution before. we creatively apply a novel network architecture, treelstm, to mapf in our solution. together with several other rl techniques, including reward shaping, multiple-phase training, and centralized control, our solution is comparable to the top 2-3 or methods.',\n",
       "  'categories': 'cs.ai cs.lg cs.ma',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yuhao jiang',\n",
       "   'kunjie zhang',\n",
       "   'qimai li',\n",
       "   'jiaxin chen',\n",
       "   'xiaolong zhu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12933'},\n",
       " {'title': 'selecting and composing learning rate policies for deep neural networks',\n",
       "  'id': '2210.12936',\n",
       "  'abstract': 'the choice of learning rate (lr) functions and policies has evolved from a simple fixed lr to the decaying lr and the cyclic lr, aiming to improve the accuracy and reduce the training time of deep neural networks (dnns). this paper presents a systematic approach to selecting and composing an lr policy for effective dnn training to meet desired target accuracy and reduce training time within the pre-defined training iterations. it makes three original contributions. first, we develop an lr tuning mechanism for auto-verification of a given lr policy with respect to the desired accuracy goal under the pre-defined training time constraint. second, we develop an lr policy recommendation system (lrbench) to select and compose good lr policies from the same and/or different lr functions through dynamic tuning, and avoid bad choices, for a given learning task, dnn model and dataset. third, we extend lrbench by supporting different dnn optimizers and show the significant mutual impact of different lr policies and different optimizers. evaluated using popular benchmark datasets and different dnn models (lenet, cnn3, resnet), we show that our approach can effectively deliver high dnn test accuracy, outperform the existing recommended default lr policies, and reduce the dnn training time by 1.6$\\\\sim$6.7$\\\\times$ to meet a targeted model accuracy.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-23',\n",
       "  'updated': '',\n",
       "  'authors': ['yanzhao wu', 'ling liu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12936'},\n",
       " {'title': 'heterogeneous information crossing on graphs for session-based   recommender systems',\n",
       "  'id': '2210.12940',\n",
       "  'abstract': \"recommender systems are fundamental information filtering techniques to recommend content or items that meet users' personalities and potential needs. as a crucial solution to address the difficulty of user identification and unavailability of historical information, session-based recommender systems provide recommendation services that only rely on users' behaviors in the current session. however, most existing studies are not well-designed for modeling heterogeneous user behaviors and capturing the relationships between them in practical scenarios. to fill this gap, in this paper, we propose a novel graph-based method, namely heterogeneous information crossing on graphs (hicg). hicg utilizes multiple types of user behaviors in the sessions to construct heterogeneous graphs, and captures users' current interests with their long-term preferences by effectively crossing the heterogeneous information on the graphs. in addition, we also propose an enhanced version, named hicg-cl, which incorporates contrastive learning (cl) technique to enhance item representation ability. by utilizing the item co-occurrence relationships across different sessions, hicg-cl improves the recommendation performance of hicg. we conduct extensive experiments on three real-world recommendation datasets, and the results verify that (i) hicg achieves the state-of-the-art performance by utilizing multiple types of behaviors on the heterogeneous graph. (ii) hicg-cl further significantly improves the recommendation performance of hicg by the proposed contrastive learning module.\",\n",
       "  'categories': 'cs.ir cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['xiaolin zheng',\n",
       "   'rui wu',\n",
       "   'zhongxuan han',\n",
       "   'chaochao chen',\n",
       "   'linxun chen',\n",
       "   'bing han'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12940'},\n",
       " {'title': 'are we really making much progress in unsupervised graph outlier   detection? revisiting the problem with new insight and superior method',\n",
       "  'id': '2210.12941',\n",
       "  'abstract': 'a large number of studies on graph outlier detection (god) have emerged in recent years due to its wide applications, in which unsupervised node outlier detection (unod) on attributed networks is an important area. unod focuses on detecting two kinds of typical outliers in graphs: the structural outlier and the contextual outlier. most existing works conduct the experiments based on the datasets with injected outliers. however, we find that the most widely-used outlier injection approach has a serious data leakage issue. by only utilizing such data leakage, a simple approach can achieve the state-of-the-art performance in detecting outliers. in addition, we observe that most existing algorithms have performance drops with varied injection settings. the other major issue is on balanced detection performance between the two types of outliers, which has not been considered by existing studies. in this paper, we analyze the cause of the data leakage issue in depth since the injection approach is a building block to advance unod. moreover, we devise a novel variance-based model to detect structural outliers, which is more robust to different injection settings. on top of this, we propose a new framework, variance-based graph outlier detection (vgod), which combines our variance-based model and attribute reconstruction model to detect outliers in a balanced way. finally, we conduct extensive experiments to demonstrate the effectiveness and the efficiency of vgod. the results on 5 real-world datasets validate that vgod achieves not only the best performance in detecting outliers but also a balanced detection performance between structural and contextual outliers.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['yihong huang', 'liping wang', 'fan zhang', 'xuemin lin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12941'},\n",
       " {'title': 'it-ruda: information theory assisted robust unsupervised domain   adaptation',\n",
       "  'id': '2210.12947',\n",
       "  'abstract': 'distribution shift between train (source) and test (target) datasets is a common problem encountered in machine learning applications. one approach to resolve this issue is to use the unsupervised domain adaptation (uda) technique that carries out knowledge transfer from a label-rich source domain to an unlabeled target domain. outliers that exist in either source or target datasets can introduce additional challenges when using uda in practice. in this paper, $\\\\alpha$-divergence is used as a measure to minimize the discrepancy between the source and target distributions while inheriting robustness, adjustable with a single parameter $\\\\alpha$, as the prominent feature of this measure. here, it is shown that the other well-known divergence-based uda techniques can be derived as special cases of the proposed method. furthermore, a theoretical upper bound is derived for the loss in the target domain in terms of the source loss and the initial $\\\\alpha$-divergence between the two domains. the robustness of the proposed method is validated through testing on several benchmarked datasets in open-set and partial uda setups where extra classes existing in target and source datasets are considered as outliers.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['shima rashidi',\n",
       "   'ruwan tennakoon',\n",
       "   'aref miri rekavandi',\n",
       "   'papangkorn jessadatavornwong',\n",
       "   'amanda freis',\n",
       "   'garret huff',\n",
       "   'mark easton',\n",
       "   'adrian mouritz',\n",
       "   'reza hoseinnezhad',\n",
       "   'alireza bab-hadiashar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12947'},\n",
       " {'title': 'enhancing label consistency on document-level named entity recognition',\n",
       "  'id': '2210.12949',\n",
       "  'abstract': 'named entity recognition (ner) is a fundamental part of extracting information from documents in biomedical applications. a notable advantage of ner is its consistency in extracting biomedical entities in a document context. although existing document ner models show consistent predictions, they still do not meet our expectations. we investigated whether the adjectives and prepositions within an entity cause a low label consistency, which results in inconsistent predictions. in this paper, we present our method, conner, which enhances the label dependency of modifiers (e.g., adjectives and prepositions) to achieve higher label agreement. conner refines the draft labels of the modifiers to improve the output representations of biomedical entities. the effectiveness of our method is demonstrated on four popular biomedical ner datasets; in particular, its efficacy is proved on two datasets with 7.5-8.6% absolute improvements in the f1 score. we interpret that our conner method is effective on datasets that have intrinsically low label consistency. in the qualitative analysis, we demonstrate how our approach makes the ner model generate consistent predictions. our code and resources are available at https://github.com/dmis-lab/conner/.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['minbyul jeong', 'jaewoo kang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12949'},\n",
       " {'title': 'ares: a system-oriented wargame framework for adversarial ml',\n",
       "  'id': '2210.12952',\n",
       "  'abstract': 'since the discovery of adversarial attacks against machine learning models nearly a decade ago, research on adversarial machine learning has rapidly evolved into an eternal war between defenders, who seek to increase the robustness of ml models against adversarial attacks, and adversaries, who seek to develop better attacks capable of weakening or defeating these defenses. this domain, however, has found little buy-in from ml practitioners, who are neither overtly concerned about these attacks affecting their systems in the real world nor are willing to trade off the accuracy of their models in pursuit of robustness against these attacks.   in this paper, we motivate the design and implementation of ares, an evaluation framework for adversarial ml that allows researchers to explore attacks and defenses in a realistic wargame-like environment. ares frames the conflict between the attacker and defender as two agents in a reinforcement learning environment with opposing objectives. this allows the introduction of system-level evaluation metrics such as time to failure and evaluation of complex strategies such as moving target defenses. we provide the results of our initial exploration involving a white-box attacker against an adversarially trained defender.',\n",
       "  'categories': 'cs.lg cs.ai cs.cr',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['farhan ahmed',\n",
       "   'pratik vaishnavi',\n",
       "   'kevin eykholt',\n",
       "   'amir rahmati'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12952'},\n",
       " {'title': 'implementation of trained factorization machine recommendation system on   quantum annealer',\n",
       "  'id': '2210.12953',\n",
       "  'abstract': 'factorization machine (fm) is the most commonly used model to build a recommendation system since it can incorporate side information to improve performance. however, producing item suggestions for a given user with a trained fm is time-consuming. it requires a run-time of $o((n_m \\\\log n_m)^2)$, where $n_m$ is the number of items in the dataset. to address this problem, we propose a quadratic unconstrained binary optimization (qubo) scheme to combine with fm and apply quantum annealing (qa) computation. compared to classical methods, this hybrid algorithm provides a faster than quadratic speedup in finding good user suggestions. we then demonstrate the aforementioned computational advantage on current nisq hardware by experimenting with a real example on a d-wave annealer.',\n",
       "  'categories': 'quant-ph cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['chen-yu liu',\n",
       "   'hsin-yu wang',\n",
       "   'pei-yen liao',\n",
       "   'ching-jui lai',\n",
       "   'min-hsiu hsieh'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12953'},\n",
       " {'title': 'on the optimization and pruning for bayesian deep learning',\n",
       "  'id': '2210.12957',\n",
       "  'abstract': 'the goal of bayesian deep learning is to provide uncertainty quantification via the posterior distribution. however, exact inference over the weight space is computationally intractable due to the ultra-high dimensions of the neural network. variational inference (vi) is a promising approach, but naive application on weight space does not scale well and often underperform on predictive accuracy. in this paper, we propose a new adaptive variational bayesian algorithm to train neural networks on weight space that achieves high predictive accuracy. by showing that there is an equivalence to stochastic gradient hamiltonian monte carlo(sghmc) with preconditioning matrix, we then propose an mcmc within em algorithm, which incorporates the spike-and-slab prior to capture the sparsity of the neural network. the em-mcmc algorithm allows us to perform optimization and model pruning within one-shot. we evaluate our methods on cifar-10, cifar-100 and imagenet datasets, and demonstrate that our dense model can reach the state-of-the-art performance and our sparse model perform very well compared to previously proposed pruning schemes.',\n",
       "  'categories': 'cs.lg stat.co',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['xiongwen ke', 'yanan fan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12957'},\n",
       " {'title': 'non-contrastive learning-based behavioural biometrics for smart iot   devices',\n",
       "  'id': '2210.12964',\n",
       "  'abstract': 'behaviour biometrics are being explored as a viable alternative to overcome the limitations of traditional authentication methods such as passwords and static biometrics. also, they are being considered as a viable authentication method for iot devices such as smart headsets with ar/vr capabilities, wearables, and erables, that do not have a large form factor or the ability to seamlessly interact with the user. recent behavioural biometric solutions use deep learning models that require large amounts of annotated training data. collecting such volumes of behaviour biometrics data raises privacy and usability concerns. to this end, we propose using simsiam-based non-contrastive self-supervised learning to improve the label efficiency of behavioural biometric systems. the key idea is to use large volumes of unlabelled (and anonymised) data to build good feature extractors that can be subsequently used in supervised settings. using two eeg datasets, we show that at lower amounts of labelled data, non-contrastive learning performs 4%-11% more than conventional methods such as supervised learning and data augmentation. we also show that, in general, self-supervised learning methods perform better than other baselines. finally, through careful experimentation, we show various modifications that can be incorporated into the non-contrastive learning process to archive high performance.',\n",
       "  'categories': 'cs.cr cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['oshan jayawardana', 'fariza rashid', 'suranga seneviratne'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12964'},\n",
       " {'title': 'high-resolution image editing via multi-stage blended diffusion',\n",
       "  'id': '2210.12965',\n",
       "  'abstract': 'diffusion models have shown great results in image generation and in image editing. however, current approaches are limited to low resolutions due to the computational cost of training diffusion models for high-resolution generation. we propose an approach that uses a pre-trained low-resolution diffusion model to edit images in the megapixel range. we first use blended diffusion to edit the image at a low resolution, and then upscale it in multiple stages, using a super-resolution model and blended diffusion. using our approach, we achieve higher visual fidelity than by only applying off the shelf super-resolution methods to the output of the diffusion model. we also obtain better global consistency than directly using the diffusion model at a higher resolution.',\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['johannes ackermann', 'minjun li'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12965'},\n",
       " {'title': 'investigating neuron disturbing in fusing heterogeneous neural networks',\n",
       "  'id': '2210.12974',\n",
       "  'abstract': 'fusing deep learning models trained on separately located clients into a global model in a one-shot communication round is a straightforward implementation of federated learning. although current model fusion methods are shown experimentally valid in fusing neural networks with almost identical architectures, they are rarely theoretically analyzed. in this paper, we reveal the phenomenon of neuron disturbing, where neurons from heterogeneous local models interfere with each other mutually. we give detailed explanations from a bayesian viewpoint combining the data heterogeneity among clients and properties of neural networks. furthermore, to validate our findings, we propose an experimental method that excludes neuron disturbing and fuses neural networks via adaptively selecting a local model, called ams, to execute the prediction according to the input. the experiments demonstrate that ams is more robust in data heterogeneity than general model fusion and ensemble methods. this implies the necessity of considering neural disturbing in model fusion. besides, ams is available for fusing models with varying architectures as an experimental algorithm, and we also list several possible extensions of ams for future work.',\n",
       "  'categories': 'cs.lg cs.ai cs.dc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['biao zhang', 'peng xiao', 'shuqin zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12974'},\n",
       " {'title': 'optimal activity and battery scheduling algorithm using load and solar   generation forecasts',\n",
       "  'id': '2210.12990',\n",
       "  'abstract': 'energy usage optimal scheduling has attracted great attention in the power system community, where various methodologies have been proposed. however, in real-world applications, the optimal scheduling problems require reliable energy forecasting, which is scarcely discussed as a joint solution to the scheduling problem. the 5\\\\textsuperscript{th} ieee computational intelligence society (ieee-cis) competition raised a practical problem of decreasing the electricity bill by scheduling building activities, where forecasting the solar energy generation and building consumption is a necessity. to solve this problem, we propose a technical sequence for tackling the solar pv and demand forecast and optimal scheduling problems, where solar generation prediction methods and an optimal university lectures scheduling algorithm are proposed.',\n",
       "  'categories': 'cs.lg math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['yogesh pipada sunil kumar',\n",
       "   'rui yuan',\n",
       "   'nam trong dinh',\n",
       "   's. ali pourmousavi'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.12990'},\n",
       " {'title': 'modeling information change in science communication with semantically   matched paraphrases',\n",
       "  'id': '2210.13001',\n",
       "  'abstract': 'whether the media faithfully communicate scientific information has long been a core issue to the science community. automatically identifying paraphrased scientific findings could enable large-scale tracking and analysis of information changes in the science communication process, but this requires systems to understand the similarity between scientific information across multiple domains. to this end, we present the scientific paraphrase and information change dataset (spiced), the first paraphrase dataset of scientific findings annotated for degree of information change. spiced contains 6,000 scientific finding pairs extracted from news stories, social media discussions, and full texts of original papers. we demonstrate that spiced poses a challenging task and that models trained on spiced improve downstream performance on evidence retrieval for fact checking of real-world scientific claims. finally, we show that models trained on spiced can reveal large-scale trends in the degrees to which people and organizations faithfully communicate new scientific findings. data, code, and pre-trained models are available at http://www.copenlu.com/publication/2022_emnlp_wright/.',\n",
       "  'categories': 'cs.cl cs.cy cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['dustin wright',\n",
       "   'jiaxin pei',\n",
       "   'david jurgens',\n",
       "   'isabelle augenstein'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13001'},\n",
       " {'title': 'on representation of natural image patches',\n",
       "  'id': '2210.13004',\n",
       "  'abstract': 'starting from the first principle i derive an unsupervised learning method named even code to model local statistics of natural images. the first version uses orthogonal bases with independent states to model simple probability distribution of a few pixels. the second version uses a microscopic loss function to learn a nonlinear sparse binary representation of image patches. the distance in the binary representation space reflects image patch similarity. the learned model also has local edge detecting and orientation selective units like early visual systems.',\n",
       "  'categories': 'cs.cv cs.lg eess.iv q-bio.nc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['cheng guo'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13004'},\n",
       " {'title': 'towards out-of-distribution sequential event prediction: a causal   treatment',\n",
       "  'id': '2210.13005',\n",
       "  'abstract': 'the goal of sequential event prediction is to estimate the next event based on a sequence of historical events, with applications to sequential recommendation, user behavior analysis and clinical treatment. in practice, the next-event prediction models are trained with sequential data collected at one time and need to generalize to newly arrived sequences in remote future, which requires models to handle temporal distribution shift from training to testing. in this paper, we first take a data-generating perspective to reveal a negative result that existing approaches with maximum likelihood estimation would fail for distribution shift due to the latent context confounder, i.e., the common cause for the historical events and the next event. then we devise a new learning objective based on backdoor adjustment and further harness variational inference to make it tractable for sequence learning problems. on top of that, we propose a framework with hierarchical branching structures for learning context-specific representations. comprehensive experiments on diverse tasks (e.g., sequential recommendation) demonstrate the effectiveness, applicability and scalability of our method with various off-the-shelf models as backbones.',\n",
       "  'categories': 'cs.lg cs.ir',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['chenxiao yang',\n",
       "   'qitian wu',\n",
       "   'qingsong wen',\n",
       "   'zhiqiang zhou',\n",
       "   'liang sun',\n",
       "   'junchi yan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13005'},\n",
       " {'title': 'iterative patch selection for high-resolution image recognition',\n",
       "  'id': '2210.13007',\n",
       "  'abstract': 'high-resolution images are prevalent in various applications, such as autonomous driving and computer-aided diagnosis. however, training neural networks on such images is computationally challenging and easily leads to out-of-memory errors even on modern gpus. we propose a simple method, iterative patch selection (ips), which decouples the memory usage from the input size and thus enables the processing of arbitrarily large images under tight hardware constraints. ips achieves this by selecting only the most salient patches, which are then aggregated into a global representation for image recognition. for both patch selection and aggregation, a cross-attention based transformer is introduced, which exhibits a close connection to multiple instance learning. our method demonstrates strong performance and has wide applicability across different domains, training regimes and image sizes while using minimal accelerator memory. for example, we are able to finetune our model on whole-slide images consisting of up to 250k patches (>16 gigapixels) with only 5 gb of gpu vram at a batch size of 16.',\n",
       "  'categories': 'cs.cv cs.lg eess.iv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['benjamin bergner', 'christoph lippert', 'aravindh mahendran'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13007'},\n",
       " {'title': 'on all-action policy gradients',\n",
       "  'id': '2210.13011',\n",
       "  'abstract': 'in this paper, we analyze the variance of stochastic policy gradient with many action samples per state (all-action spg). we decompose the variance of spg and derive an optimality condition for all-action spg. the optimality condition shows when all-action spg should be preferred over single-action counterpart and allows to determine a variance-minimizing sampling scheme in spg estimation. furthermore, we propose dynamics-all-action (daa) module, an augmentation that allows for all-action sampling without manipulation of the environment. daa addresses the problems associated with using a q-network for all-action sampling and can be readily applied to any on-policy spg algorithm. we find that using daa with a canonical on-policy algorithm (ppo) yields better sample efficiency and higher policy returns on a variety of challenging continuous action environments.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['michal nauman', 'marek cygan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13011'},\n",
       " {'title': 'geometric knowledge distillation: topology compression for graph neural   networks',\n",
       "  'id': '2210.13014',\n",
       "  'abstract': 'we study a new paradigm of knowledge transfer that aims at encoding graph topological information into graph neural networks (gnns) by distilling knowledge from a teacher gnn model trained on a complete graph to a student gnn model operating on a smaller or sparser graph. to this end, we revisit the connection between thermodynamics and the behavior of gnn, based on which we propose neural heat kernel (nhk) to encapsulate the geometric property of the underlying manifold concerning the architecture of gnns. a fundamental and principled solution is derived by aligning nhks on teacher and student models, dubbed as geometric knowledge distillation. we develop non- and parametric instantiations and demonstrate their efficacy in various experimental settings for knowledge distillation regarding different types of privileged topological information and teacher-student schemes.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['chenxiao yang', 'qitian wu', 'junchi yan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13014'},\n",
       " {'title': 'cards against ai: predicting humor in a fill-in-the-blank party game',\n",
       "  'id': '2210.13016',\n",
       "  'abstract': \"humor is an inherently social phenomenon, with humorous utterances shaped by what is socially and culturally accepted. understanding humor is an important nlp challenge, with many applications to human-computer interactions. in this work we explore humor in the context of cards against humanity -- a party game where players complete fill-in-the-blank statements using cards that can be offensive or politically incorrect. we introduce a novel dataset of 300,000 online games of cards against humanity, including 785k unique jokes, analyze it and provide insights. we trained machine learning models to predict the winning joke per game, achieving performance twice as good (20\\\\%) as random, even without any user information. on the more difficult task of judging novel cards, we see the models' ability to generalize is moderate. interestingly, we find that our models are primarily focused on punchline card, with the context having little impact. analyzing feature importance, we observe that short, crude, juvenile punchlines tend to win.\",\n",
       "  'categories': 'cs.lg cs.ai cs.cl cs.cy cs.gl',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['dan ofer', 'dafna shahaf'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13016'},\n",
       " {'title': 'fairgen: fair synthetic data generation',\n",
       "  'id': '2210.13023',\n",
       "  'abstract': 'with the rising adoption of machine learning across the domains like banking, pharmaceutical, ed-tech, etc, it has become utmost important to adopt responsible ai methods to ensure models are not unfairly discriminating against any group. given the lack of clean training data, generative adversarial techniques are preferred to generate synthetic data with several state-of-the-art architectures readily available across various domains from unstructured data such as text, images to structured datasets modelling fraud detection and many more. these techniques overcome several challenges such as class imbalance, limited training data, restricted access to data due to privacy issues. existing work focusing on generating fair data either works for a certain gan architecture or is very difficult to tune across the gans. in this paper, we propose a pipeline to generate fairer synthetic data independent of the gan architecture. the proposed paper utilizes a pre-processing algorithm to identify and remove bias inducing samples. in particular, we claim that while generating synthetic data most gans amplify bias present in the training data but by removing these bias inducing samples, gans essentially focuses more on real informative samples. our experimental evaluation on two open-source datasets demonstrates how the proposed pipeline is generating fair data along with improved performance in some cases.',\n",
       "  'categories': 'cs.lg cs.ai cs.cy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['bhushan chaudhari',\n",
       "   'himanshu choudhary',\n",
       "   'aakash agarwal',\n",
       "   'kamna meena',\n",
       "   'tanmoy bhowmik'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13023'},\n",
       " {'title': 'e-valuating classifier two-sample tests',\n",
       "  'id': '2210.13027',\n",
       "  'abstract': \"we propose e-c2st, a classifier two-sample test for high-dimensional data based on e-values. compared to $p$-values-based tests, tests with e-values have finite sample guarantees for the type i error. e-c2st combines ideas from existing work on split likelihood ratio tests and predictive independence testing. the resulting e-values incorporate information about the alternative hypothesis. we demonstrate the utility of e-c2st on simulated and real-life data. in all experiments, we observe that when going from small to large sample sizes, as expected, e-c2st starts with lower power compared to other methods but eventually converges towards one. simultaneously, e-c2st's type i error stays substantially below the chosen significance level, which is not always the case for the baseline methods. finally, we use an mri dataset to demonstrate that multiplying e-values from multiple independently conducted studies leads to a combined e-value that retains the finite sample type i error guarantees while increasing the power.\",\n",
       "  'categories': 'stat.me cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['teodora pandeva',\n",
       "   'tim bakker',\n",
       "   'christian a. naesseth',\n",
       "   'patrick forré'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13027'},\n",
       " {'title': 'subspace-based set operations on a pre-trained word embedding space',\n",
       "  'id': '2210.13034',\n",
       "  'abstract': 'word embedding is a fundamental technology in natural language processing. it is often exploited for tasks using sets of words, although standard methods for representing word sets and set operations remain limited. if we can leverage the advantage of word embedding for such set operations, we can calculate sentence similarity and find words that effectively share a concept with a given word set in a straightforward way. in this study, we formulate representations of sets and set operations in a pre-trained word embedding space. inspired by \\\\textit{quantum logic}, we propose a novel formulation of set operations using subspaces in a pre-trained word embedding space. based on our definitions, we propose two metrics based on the degree to which a word belongs to a set and the similarity between embedding two sets. our experiments with text concept set retrieval and semantic textual similarity tasks demonstrated the effectiveness of our proposed method.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['yoichi ishibashi',\n",
       "   'sho yokoi',\n",
       "   'katsuhito sudoh',\n",
       "   'satoshi nakamura'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13034'},\n",
       " {'title': 'data-iq: characterizing subgroups with heterogeneous outcomes in tabular   data',\n",
       "  'id': '2210.13043',\n",
       "  'abstract': \"high model performance, on average, can hide that models may systematically underperform on subgroups of the data. we consider the tabular setting, which surfaces the unique issue of outcome heterogeneity - this is prevalent in areas such as healthcare, where patients with similar features can have different outcomes, thus making reliable predictions challenging. to tackle this, we propose data-iq, a framework to systematically stratify examples into subgroups with respect to their outcomes. we do this by analyzing the behavior of individual examples during training, based on their predictive confidence and, importantly, the aleatoric (data) uncertainty. capturing the aleatoric uncertainty permits a principled characterization and then subsequent stratification of data examples into three distinct subgroups (easy, ambiguous, hard). we experimentally demonstrate the benefits of data-iq on four real-world medical datasets. we show that data-iq's characterization of examples is most robust to variation across similarly performant (yet different) models, compared to baselines. since data-iq can be used with any ml model (including neural networks, gradient boosting etc.), this property ensures consistency of data characterization, while allowing flexible model selection. taking this a step further, we demonstrate that the subgroups enable us to construct new approaches to both feature acquisition and dataset selection. furthermore, we highlight how the subgroups can inform reliable model usage, noting the significant impact of the ambiguous subgroup on model generalization.\",\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['nabeel seedat',\n",
       "   'jonathan crabbé',\n",
       "   'ioana bica',\n",
       "   'mihaela van der schaar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13043'},\n",
       " {'title': 'parafac2-based coupled matrix and tensor factorizations',\n",
       "  'id': '2210.13054',\n",
       "  'abstract': 'coupled matrix and tensor factorizations (cmtf) have emerged as an effective data fusion tool to jointly analyze data sets in the form of matrices and higher-order tensors. the parafac2 model has shown to be a promising alternative to the candecomp/parafac (cp) tensor model due to its flexibility and capability to handle irregular/ragged tensors. while fusion models based on a parafac2 model coupled with matrix/tensor decompositions have been recently studied, they are limited in terms of possible regularizations and/or types of coupling between data sets. in this paper, we propose an algorithmic framework for fitting parafac2-based cmtf models with the possibility of imposing various constraints on all modes and linear couplings, using alternating optimization (ao) and the alternating direction method of multipliers (admm). through numerical experiments, we demonstrate that the proposed algorithmic approach accurately recovers the underlying patterns using various constraints and linear couplings.',\n",
       "  'categories': 'cs.lg math.oc stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['carla schenker', 'xiulin wang', 'evrim acar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13054'},\n",
       " {'title': 'hardness in markov decision processes: theory and practice',\n",
       "  'id': '2210.13075',\n",
       "  'abstract': 'meticulously analysing the empirical strengths and weaknesses of reinforcement learning methods in hard (challenging) environments is essential to inspire innovations and assess progress in the field. in tabular reinforcement learning, there is no well-established standard selection of environments to conduct such analysis, which is partially due to the lack of a widespread understanding of the rich theory of hardness of environments. the goal of this paper is to unlock the practical usefulness of this theory through four main contributions. first, we present a systematic survey of the theory of hardness, which also identifies promising research directions. second, we introduce colosseum, a pioneering package that enables empirical hardness analysis and implements a principled benchmark composed of environments that are diverse with respect to different measures of hardness. third, we present an empirical analysis that provides new insights into computable measures. finally, we benchmark five tabular agents in our newly proposed benchmark. while advancing the theoretical understanding of hardness in non-tabular reinforcement learning remains essential, our contributions in the tabular setting are intended as solid steps towards a principled non-tabular benchmark. accordingly, we benchmark four agents in non-tabular versions of colosseum environments, obtaining results that demonstrate the generality of tabular hardness measures.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['michelangelo conserva', 'paulo rauber'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13075'},\n",
       " {'title': 'are deep sequence classifiers good at non-trivial generalization?',\n",
       "  'id': '2210.13082',\n",
       "  'abstract': 'recent advances in deep learning models for sequence classification have greatly improved their classification accuracy, specially when large training sets are available. however, several works have suggested that under some settings the predictions made by these models are poorly calibrated. in this work we study binary sequence classification problems and we look at model calibration from a different perspective by asking the question: are deep learning models capable of learning the underlying target class distribution? we focus on sparse sequence classification, that is problems in which the target class is rare and compare three deep learning sequence classification models. we develop an evaluation that measures how well a classifier is learning the target class distribution. in addition, our evaluation disentangles good performance achieved by mere compression of the training sequences versus performance achieved by proper model generalization. our results suggest that in this binary setting the deep-learning models are indeed able to learn the underlying class distribution in a non-trivial manner, i.e. by proper generalization beyond data compression.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['francesco cazzaro', 'ariadna quattoni', 'xavier carreras'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13082'},\n",
       " {'title': 'scalable representation learning in linear contextual bandits with   constant regret guarantees',\n",
       "  'id': '2210.13083',\n",
       "  'abstract': 'we study the problem of representation learning in stochastic contextual linear bandits. while the primary concern in this domain is usually to find realizable representations (i.e., those that allow predicting the reward function at any context-action pair exactly), it has been recently shown that representations with certain spectral properties (called hls) may be more effective for the exploration-exploitation task, enabling linucb to achieve constant (i.e., horizon-independent) regret. in this paper, we propose banditsrl, a representation learning algorithm that combines a novel constrained optimization problem to learn a realizable representation with good spectral properties with a generalized likelihood ratio test to exploit the recovered representation and avoid excessive exploration. we prove that banditsrl can be paired with any no-regret algorithm and achieve constant regret whenever an hls representation is available. furthermore, banditsrl can be easily combined with deep neural networks and we show how regularizing towards hls representations is beneficial in standard benchmarks.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['andrea tirinzoni',\n",
       "   'matteo papini',\n",
       "   'ahmed touati',\n",
       "   'alessandro lazaric',\n",
       "   'matteo pirotta'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13083'},\n",
       " {'title': 'deep grey-box modeling with adaptive data-driven models toward   trustworthy estimation of theory-driven models',\n",
       "  'id': '2210.13103',\n",
       "  'abstract': \"the combination of deep neural nets and theory-driven models, which we call deep grey-box modeling, can be inherently interpretable to some extent thanks to the theory backbone. deep grey-box models are usually learned with a regularized risk minimization to prevent a theory-driven part from being overwritten and ignored by a deep neural net. however, an estimation of the theory-driven part obtained by uncritically optimizing a regularizer can hardly be trustworthy when we are not sure what regularizer is suitable for the given data, which may harm the interpretability. toward a trustworthy estimation of the theory-driven part, we should analyze regularizers' behavior to compare different candidates and to justify a specific choice. in this paper, we present a framework that enables us to analyze a regularizer's behavior empirically with a slight change in the neural net's architecture and the training objective.\",\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['naoya takeishi', 'alexandros kalousis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13103'},\n",
       " {'title': 'heat demand forecasting with multi-resolutional representation of   heterogeneous temporal ensemble',\n",
       "  'id': '2210.13108',\n",
       "  'abstract': 'one of the primal challenges faced by utility companies is ensuring efficient supply with minimal greenhouse gas emissions. the advent of smart meters and smart grids provide an unprecedented advantage in realizing an optimised supply of thermal energies through proactive techniques such as load forecasting. in this paper, we propose a forecasting framework for heat demand based on neural networks where the time series are encoded as scalograms equipped with the capacity of embedding exogenous variables such as weather, and holiday/non-holiday. subsequently, cnns are utilized to predict the heat load multi-step ahead. finally, the proposed framework is compared with other state-of-the-art methods, such as sarimax and lstm. the quantitative results from retrospective experiments show that the proposed framework consistently outperforms the state-of-the-art baseline method with real-world data acquired from denmark. a minimal mean error of 7.54% for mape and 417kw for rmse is achieved with the proposed framework in comparison to all other methods.',\n",
       "  'categories': 'cs.lg cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['adithya ramachandran',\n",
       "   'satyaki chatterjee',\n",
       "   'siming bayer',\n",
       "   'andreas maier',\n",
       "   'thorkil flensmark'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13108'},\n",
       " {'title': 'federated and meta learning over non-wireless and wireless networks: a   tutorial',\n",
       "  'id': '2210.13111',\n",
       "  'abstract': 'in recent years, various machine learning (ml) solutions have been developed to solve resource management, interference management, autonomy, and decision-making problems in non-wireless and wireless networks. standard ml approaches require collecting data at a central server for training, which cannot preserve the data privacy of devices. to address this issue, federated learning (fl) is an effective method to allow edge devices to collaboratively train ml models without sharing local datasets for data privacy. typically, fl focuses on learning a global model for a given task and all devices and hence cannot adapt the model to devices with different data distributions. in such cases, meta learning can be employed to adapt learning models to different data distributions using a few data samples. in this tutorial, we conduct a comprehensive review on fl, meta learning, and federated meta learning (fedmeta). compared to other tutorial papers, our objective is to leverage how fl/meta-learning/fedmeta can be designed, optimized, and evolved over non-wireless and wireless networks. furthermore, we analyze not only the relationship among these learning algorithms but also their advantages and disadvantages in real-world applications.',\n",
       "  'categories': 'cs.lg cs.ai cs.ni',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['xiaonan liu',\n",
       "   'yansha deng',\n",
       "   'arumugam nallanathan',\n",
       "   'mehdi bennis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13111'},\n",
       " {'title': 'unsupervised term extraction for highly technical domains',\n",
       "  'id': '2210.13118',\n",
       "  'abstract': 'term extraction is an information extraction task at the root of knowledge discovery platforms. developing term extractors that are able to generalize across very diverse and potentially highly technical domains is challenging, as annotations for domains requiring in-depth expertise are scarce and expensive to obtain. in this paper, we describe the term extraction subsystem of a commercial knowledge discovery platform that targets highly technical fields such as pharma, medical, and material science. to be able to generalize across domains, we introduce a fully unsupervised annotator (ua). it extracts terms by combining novel morphological signals from sub-word tokenization with term-to-topic and intra-term similarity metrics, computed using general-domain pre-trained sentence-encoders. the annotator is used to implement a weakly-supervised setup, where transformer-models are fine-tuned (or pre-trained) over the training data generated by running the ua over large unlabeled corpora. our experiments demonstrate that our setup can improve the predictive performance while decreasing the inference latency on both cpus and gpus. our annotators provide a very competitive baseline for all the cases where annotations are not available.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['francesco fusco', 'peter staar', 'diego antognini'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13118'},\n",
       " {'title': 'pac-bayesian offline contextual bandits with guarantees',\n",
       "  'id': '2210.13132',\n",
       "  'abstract': 'this paper introduces a new principled approach for offline policy optimisation in contextual bandits. for two well-established risk estimators, we propose novel generalisation bounds able to confidently improve upon the logging policy offline. unlike previous work, our approach does not require tuning hyperparameters on held-out sets, and enables deployment with no prior a/b testing. this is achieved by analysing the problem through the pac-bayesian lens; mainly, we let go of traditional policy parametrisation (e.g. softmax) and instead interpret the policies as mixtures of deterministic strategies. we demonstrate through extensive experiments evidence of our bounds tightness and the effectiveness of our approach in practical scenarios.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['otmane sakhi', 'nicolas chopin', 'pierre alquier'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13132'},\n",
       " {'title': 'binary graph convolutional network with capacity exploration',\n",
       "  'id': '2210.13149',\n",
       "  'abstract': 'the current success of graph neural networks (gnns) usually relies on loading the entire attributed graph for processing, which may not be satisfied with limited memory resources, especially when the attributed graph is large. this paper pioneers to propose a binary graph convolutional network (bi-gcn), which binarizes both the network parameters and input node attributes and exploits binary operations instead of floating-point matrix multiplications for network compression and acceleration. meanwhile, we also propose a new gradient approximation based back-propagation method to properly train our bi-gcn. according to the theoretical analysis, our bi-gcn can reduce the memory consumption by an average of ~31x for both the network parameters and input data, and accelerate the inference speed by an average of ~51x, on three citation networks, i.e., cora, pubmed, and citeseer. besides, we introduce a general approach to generalize our binarization method to other variants of gnns, and achieve similar efficiencies. although the proposed bi-gcn and bi-gnns are simple yet efficient, these compressed networks may also possess a potential capacity problem, i.e., they may not have enough storage capacity to learn adequate representations for specific tasks. to tackle this capacity problem, an entropy cover hypothesis is proposed to predict the lower bound of the width of bi-gnn hidden layers. extensive experiments have demonstrated that our bi-gcn and bi-gnns can give comparable performances to the corresponding full-precision baselines on seven node classification datasets and verified the effectiveness of our entropy cover hypothesis for solving the capacity problem.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['junfu wang', 'yuanfang guo', 'liang yang', 'yunhong wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13149'},\n",
       " {'title': 'a pac-bayesian generalization bound for equivariant networks',\n",
       "  'id': '2210.13150',\n",
       "  'abstract': 'equivariant networks capture the inductive bias about the symmetry of the learning task by building those symmetries into the model. in this paper, we study how equivariance relates to generalization error utilizing pac bayesian analysis for equivariant networks, where the transformation laws of feature spaces are determined by group representations. by using perturbation analysis of equivariant networks in fourier domain for each layer, we derive norm-based pac-bayesian generalization bounds. the bound characterizes the impact of group size, and multiplicity and degree of irreducible representations on the generalization error and thereby provide a guideline for selecting them. in general, the bound indicates that using larger group size in the model improves the generalization error substantiated by extensive numerical experiments.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['arash behboodi', 'gabriele cesa', 'taco cohen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13150'},\n",
       " {'title': 'reachability-aware laplacian representation in reinforcement learning',\n",
       "  'id': '2210.13153',\n",
       "  'abstract': 'in reinforcement learning (rl), laplacian representation (laprep) is a task-agnostic state representation that encodes the geometry of the environment. a desirable property of laprep stated in prior works is that the euclidean distance in the laprep space roughly reflects the reachability between states, which motivates the usage of this distance for reward shaping. however, we find that laprep does not necessarily have this property in general: two states having small distance under laprep can actually be far away in the environment. such mismatch would impede the learning process in reward shaping. to fix this issue, we introduce a reachability-aware laplacian representation (ra-laprep), by properly scaling each dimension of laprep. despite the simplicity, we demonstrate that ra-laprep can better capture the inter-state reachability as compared to laprep, through both theoretical explanations and experimental results. additionally, we show that this improvement yields a significant boost in reward shaping performance and also benefits bottleneck state discovery.',\n",
       "  'categories': 'cs.lg cs.ai',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['kaixin wang',\n",
       "   'kuangqi zhou',\n",
       "   'jiashi feng',\n",
       "   'bryan hooi',\n",
       "   'xinchao wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13153'},\n",
       " {'title': 'occam learning',\n",
       "  'id': '2210.13179',\n",
       "  'abstract': 'we discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. we argue that learning machines with this architecture enjoy a number of desirable properties. for example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense.   when hidden units are binary variables, these models have a natural interpretation in terms of features. we show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-gaussian statistical properties of the data. we suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. we introduce the hierarchical feature model as an example of a model that satisfies this principle, and that encodes an a priori organisation of the feature space.   we present extensive numerical experiments in order i) to test that the internal representation of learning machines can indeed be independent of the data with which they are trained and ii) that only a finite number of features are needed to describe a datasets.',\n",
       "  'categories': 'cond-mat.dis-nn cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['rongrong xie', 'matteo marsili'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13179'},\n",
       " {'title': 'simultaneous improvement of ml model fairness and performance by   identifying bias in data',\n",
       "  'id': '2210.13182',\n",
       "  'abstract': \"machine learning models built on datasets containing discriminative instances attributed to various underlying factors result in biased and unfair outcomes. it's a well founded and intuitive fact that existing bias mitigation strategies often sacrifice accuracy in order to ensure fairness. but when ai engine's prediction is used for decision making which reflects on revenue or operational efficiency such as credit risk modelling, it would be desirable by the business if accuracy can be somehow reasonably preserved. this conflicting requirement of maintaining accuracy and fairness in ai motivates our research. in this paper, we propose a fresh approach for simultaneous improvement of fairness and accuracy of ml models within a realistic paradigm. the essence of our work is a data preprocessing technique that can detect instances ascribing a specific kind of bias that should be removed from the dataset before training and we further show that such instance removal will have no adverse impact on model accuracy. in particular, we claim that in the problem settings where instances exist with similar feature but different labels caused by variation in protected attributes , an inherent bias gets induced in the dataset, which can be identified and mitigated through our novel scheme. our experimental evaluation on two open-source datasets demonstrates how the proposed method can mitigate bias along with improving rather than degrading accuracy, while offering certain set of control for end user.\",\n",
       "  'categories': 'cs.lg cs.cy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['bhushan chaudhari', 'akash agarwal', 'tanmoy bhowmik'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13182'},\n",
       " {'title': 'meta input: how to leverage off-the-shelf deep neural networks',\n",
       "  'id': '2210.13186',\n",
       "  'abstract': \"these days, although deep neural networks (dnns) have achieved a noticeable progress in a wide range of research area, it lacks the adaptability to be employed in the real-world applications because of the environment discrepancy problem. such a problem originates from the difference between training and testing environments, and it is widely known that it causes serious performance degradation, when a pretrained dnn model is applied to a new testing environment. therefore, in this paper, we introduce a novel approach that allows end-users to exploit pretrained dnn models in their own testing environment without modifying the models. to this end, we present a \\\\textit{meta input} which is an additional input transforming the distribution of testing data to be aligned with that of training data. the proposed meta input can be optimized with a small number of testing data only by considering the relation between testing input data and its output prediction. also, it does not require any knowledge of the network's internal architecture and modification of its weight parameters. then, the obtained meta input is added to testing data in order to shift the distribution of testing data to that of originally used training data. as a result, end-users can exploit well-trained models in their own testing environment which can differ from the training environment. we validate the effectiveness and versatility of the proposed meta input by showing the robustness against the environment discrepancy through the comprehensive experiments with various tasks.\",\n",
       "  'categories': 'cs.lg cs.ai cs.cv',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['minsu kim', 'youngjoon yu', 'sungjune park', 'yong man ro'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13186'},\n",
       " {'title': 'can visual context improve automatic speech recognition for an embodied   agent?',\n",
       "  'id': '2210.13189',\n",
       "  'abstract': \"the usage of automatic speech recognition (asr) systems are becoming omnipresent ranging from personal assistant to chatbots, home, and industrial automation systems, etc. modern robots are also equipped with asr capabilities for interacting with humans as speech is the most natural interaction modality. however, asr in robots faces additional challenges as compared to a personal assistant. being an embodied agent, a robot must recognize the physical entities around it and therefore reliably recognize the speech containing the description of such entities. however, current asr systems are often unable to do so due to limitations in asr training, such as generic datasets and open-vocabulary modeling. also, adverse conditions during inference, such as noise, accented, and far-field speech makes the transcription inaccurate. in this work, we present a method to incorporate a robot's visual information into an asr system and improve the recognition of a spoken utterance containing a visible entity. specifically, we propose a new decoder biasing technique to incorporate the visual context while ensuring the asr output does not degrade for incorrect context. we achieve a 59% relative reduction in wer from an unmodified asr system.\",\n",
       "  'categories': 'eess.as cs.ai cs.cl cs.cv cs.lg cs.ro',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['pradip pramanick', 'chayan sarkar'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13189'},\n",
       " {'title': 'navigating the challenges in creating complex data systems: a   development philosophy',\n",
       "  'id': '2210.13191',\n",
       "  'abstract': \"in this perspective, we argue that despite the democratization of powerful tools for data science and machine learning over the last decade, developing the code for a trustworthy and effective data science system (dss) is getting harder. perverse incentives and a lack of widespread software engineering (se) skills are among many root causes we identify that naturally give rise to the current systemic crisis in reproducibility of dsss. we analyze why se and building large complex systems is, in general, hard. based on these insights, we identify how se addresses those difficulties and how we can apply and generalize se methods to construct dsss that are fit for purpose. we advocate two key development philosophies, namely that one should incrementally grow -- not biphasically plan and build -- dsss, and one should always employ two types of feedback loops during development: one which tests the code's correctness and another that evaluates the code's efficacy.\",\n",
       "  'categories': 'cs.se cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['sören dittmer',\n",
       "   'michael roberts',\n",
       "   'julian gilbey',\n",
       "   'ander biguri',\n",
       "   'aix-covnet collaboration',\n",
       "   'jacobus preller',\n",
       "   'james h. f. rudd',\n",
       "   'john a. d. aston',\n",
       "   'carola-bibiane schönlieb'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13191'},\n",
       " {'title': 'langevin dynamics based algorithm e-th$\\\\varepsilon$o poula for   stochastic optimization problems with discontinuous stochastic gradient',\n",
       "  'id': '2210.13193',\n",
       "  'abstract': 'we introduce a new langevin dynamics based algorithm, called e-th$\\\\varepsilon$o poula, to solve optimization problems with discontinuous stochastic gradients which naturally appear in real-world applications such as quantile estimation, vector quantization, cvar minimization, and regularized optimization problems involving relu neural networks. we demonstrate both theoretically and numerically the applicability of the e-th$\\\\varepsilon$o poula algorithm. more precisely, under the conditions that the stochastic gradient is locally lipschitz in average and satisfies a certain convexity at infinity condition, we establish non-asymptotic error bounds for e-th$\\\\varepsilon$o poula in wasserstein distances and provide a non-asymptotic estimate for the expected excess risk, which can be controlled to be arbitrarily small. three key applications in finance and insurance are provided, namely, multi-period portfolio optimization, transfer learning in multi-period portfolio optimization, and insurance claim prediction, which involve neural networks with (leaky)-relu activation functions. numerical experiments conducted using real-world datasets illustrate the superior empirical performance of e-th$\\\\varepsilon$o poula compared to sgld, adam, and amsgrad in terms of model accuracy.',\n",
       "  'categories': 'math.oc cs.lg cs.na math.na math.pr stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['dong-young lim',\n",
       "   'ariel neufeld',\n",
       "   'sotirios sabanis',\n",
       "   'ying zhang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13193'},\n",
       " {'title': 'multiplicity-adjusted bootstrap tilting lower confidence bounds for   conditional prediction performance measures',\n",
       "  'id': '2210.13206',\n",
       "  'abstract': 'in machine learning, the selection of a promising model from a potentially large number of competing models and the assessment of its generalization performance are critical tasks that need careful consideration. typically, model selection and evaluation are strictly separated endeavors, splitting the sample at hand into a training, validation, and evaluation set, and only compute a single confidence interval for the prediction performance of the final selected model. we however propose an algorithm how to compute valid lower confidence bounds for multiple models that have been selected based on their prediction performances in the evaluation set by interpreting the selection problem as a simultaneous inference problem. we use bootstrap tilting and a maxt-type multiplicity correction. the approach is universally applicable for any combination of prediction models, any model selection strategy, and any prediction performance measure that accepts weights. we conducted various simulation experiments which show that our proposed approach yields lower confidence bounds that are at least comparably good as bounds from standard approaches, and that reliably reach the nominal coverage probability. in addition, especially when sample size is small, our proposed approach yields better performing prediction models than the default selection of only one model for evaluation does.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['pascal rink', 'werner brannath'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13206'},\n",
       " {'title': 'a dimension-augmented physics-informed neural network (dapinn) with high   level accuracy and efficiency',\n",
       "  'id': '2210.13212',\n",
       "  'abstract': 'physics-informed neural networks (pinns) have been widely applied in different fields due to their effectiveness in solving partial differential equations (pdes). however, the accuracy and efficiency of pinns need to be considerably improved for scientific and commercial use. to address this issue, we systematically propose a novel dimension-augmented physics-informed neural network (dapinn), which simultaneously and significantly improves the accuracy and efficiency of the pinn. in the dapinn model, we introduce inductive bias in the neural network to enhance network generalizability by adding a special regularization term to the loss function. furthermore, we manipulate the network input dimension by inserting additional sample features and incorporating the expanded dimensionality in the loss function. moreover, we verify the effectiveness of power series augmentation, fourier series augmentation and replica augmentation, in both forward and backward problems. in most experiments, the error of dapinn is 1$\\\\sim$2 orders of magnitude lower than that of pinn. the results show that the dapinn outperforms the original pinn in terms of both accuracy and efficiency with a reduced dependence on the number of sample points. we also discuss the complexity of the dapinn and its compatibility with other methods.',\n",
       "  'categories': 'cs.lg cs.ne',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '',\n",
       "  'authors': ['weilong guan', 'kaihan yang', 'yinsheng chen', 'zhong guan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13212'},\n",
       " {'title': 'biologically plausible variational policy gradient with spiking   recurrent winner-take-all networks',\n",
       "  'id': '2210.13225',\n",
       "  'abstract': 'one stream of reinforcement learning research is exploring biologically plausible models and algorithms to simulate biological intelligence and fit neuromorphic hardware. among them, reward-modulated spike-timing-dependent plasticity (r-stdp) is a recent branch with good potential in energy efficiency. however, current r-stdp methods rely on heuristic designs of local learning rules, thus requiring task-specific expert knowledge. in this paper, we consider a spiking recurrent winner-take-all network, and propose a new r-stdp method, spiking variational policy gradient (svpg), whose local learning rules are derived from the global policy gradient and thus eliminate the need for heuristic designs. in experiments of mnist classification and gym invertedpendulum, our svpg achieves good training performance, and also presents better robustness to various kinds of noises than conventional methods.',\n",
       "  'categories': 'cs.ne cs.lg q-bio.nc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['zhile yang', 'shangqi guo', 'ying fang', 'jian k. liu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13225'},\n",
       " {'title': 'comparing exploratory graphical analyses and unique variable analysis to   other dimension reduction methods on machine learning algorithms',\n",
       "  'id': '2210.13230',\n",
       "  'abstract': 'developing interpretable machine learning models has become an increasingly important issue. one way in which data scientists have been able to develop interpretable models has been to use dimension reduction techniques. in this paper, we examine several dimension reduction techniques including two recent approaches developed in the network psychometrics literature called exploratory graph analysis (ega) and unique variable analysis (uva). we compared ega and uva with two other dimension reduction techniques common in the machine learning literature (principal component analysis and independent component analysis) as well as no reduction to the variables real data. we show that ega and uva perform as well as the other reduction techniques or no reduction. consistent with previous literature, we show that dimension reduction can decrease, increase, or provide the same accuracy as no reduction of variables. our tentative results find that dimension reduction tends to lead to better performance when used for classification tasks.',\n",
       "  'categories': 'cs.lg stat.ap',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '',\n",
       "  'authors': ['sean h. merritt', 'alexander p. christensen'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13230'},\n",
       " {'title': 'analysing training-data leakage from gradients through linear systems   and gradient matching',\n",
       "  'id': '2210.13231',\n",
       "  'abstract': 'recent works have demonstrated that it is possible to reconstruct training images and their labels from gradients of an image-classification model when its architecture is known. unfortunately, there is still an incomplete theoretical understanding of the efficacy and failure of these gradient-leakage attacks. in this paper, we propose a novel framework to analyse training-data leakage from gradients that draws insights from both analytic and optimisation-based gradient-leakage attacks. we formulate the reconstruction problem as solving a linear system from each layer iteratively, accompanied by corrections using gradient matching. under this framework, we claim that the solubility of the reconstruction problem is primarily determined by that of the linear system at each layer. as a result, we are able to partially attribute the leakage of the training data in a deep network to its architecture. we also propose a metric to measure the level of security of a deep learning model against gradient-based attacks on the training data.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-20',\n",
       "  'updated': '',\n",
       "  'authors': ['cangxiong chen', 'neill d. f. campbell'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13231'},\n",
       " {'title': 'a complete recipe for bayesian knowledge transfer: object tracking',\n",
       "  'id': '2210.13232',\n",
       "  'abstract': 'the problem of sequentially transferring from a source object track and a model to another bayesian filter has become ubiquitous. due to the lack of a structural model that can capture the dependence among different models, the transfer may not be fully specified. in this paper, we introduce a novel bayesian model that accounts for the model-jump from which the object can choose a model and follow. we aim to track the trajectory of the object while sequentially transferring from the source object to the target object. the main idea is to impute the dynamical model while tracking the object and estimating the state parameters of the moving object according to discretized dynamic systems. we demonstrate this procedure can handle the model mismatch as it sequentially corrects the predictive model. particularly, for a fixed number of motion models, the object can learn what motion to follow at each time step. we employ a prior model for each model and then adaptively correct for changing one model to another to robustly estimate object trajectory under various motions. more concretely, we propose a robust bayesian recipe to handle the model-jump and then integrate it with a markov chain monte carlo (mcmc) approach to sample from the posterior distribution. we demonstrate through experiments the advantage of accounting for model-jump in our proposed method for knowledge transfer between learning tasks in bayesian transfer learning.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '',\n",
       "  'authors': ['bahman moraffah', 'antonia papandreou-suppappola'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13232'},\n",
       " {'title': 'chaos theory and adversarial robustness',\n",
       "  'id': '2210.13235',\n",
       "  'abstract': 'neural networks, being susceptible to adversarial attacks, should face a strict level of scrutiny before being deployed in critical or adversarial applications. this paper uses ideas from chaos theory to explain, analyze, and quantify the degree to which neural networks are susceptible to or robust against adversarial attacks. our results show that susceptibility to attack grows significantly with the depth of the model, which has significant safety implications for the design of neural networks for production environments. we also demonstrate how to quickly and easily approximate the certified robustness radii for extremely large models, which until now has been computationally infeasible to calculate directly, as well as show a clear relationship between our new susceptibility metric and post-attack accuracy.',\n",
       "  'categories': 'cs.lg cs.cr math.ds',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-19',\n",
       "  'updated': '',\n",
       "  'authors': ['jonathan s. kent'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13235'},\n",
       " {'title': 'multimodal model with text and drug embeddings for adverse drug reaction   classification',\n",
       "  'id': '2210.13238',\n",
       "  'abstract': 'in this paper, we focus on the classification of tweets as sources of potential signals for adverse drug effects (ades) or drug reactions (adrs). following the intuition that text and drug structure representations are complementary, we introduce a multimodal model with two components. these components are state-of-the-art bert-based models for language understanding and molecular property prediction. experiments were carried out on multilingual benchmarks of the social media mining for health research and applications (#smm4h) initiative. our models obtained state-of-the-art results of 0.61 f1 and 0.57 f1 on #smm4h 2021 shared tasks 1a and 2 in english and russian, respectively. on the classification of french tweets from smm4h 2020 task 1, our approach pushes the state of the art by an absolute gain of 8% f1. our experiments show that the molecular information obtained from neural networks is more beneficial for ade classification than traditional molecular descriptors. the source code for our models is freely available at https://github.com/andoree/smm4h_2021_classification.',\n",
       "  'categories': 'q-bio.qm cs.cl cs.lg',\n",
       "  'doi': '10.1016/j.jbi.2022.104182',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['andrey sakhovskiy', 'elena tutubalina'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13238'},\n",
       " {'title': 'iquaflow: a new framework to measure image quality',\n",
       "  'id': '2210.13269',\n",
       "  'abstract': 'iquaflow is a new image quality framework that provides a set of tools to assess image quality. the user can add custom metrics that can be easily integrated. furthermore, iquaflow allows to measure quality by using the performance of ai models trained on the images as a proxy. this also helps to easily make studies of performance degradation of several modifications of the original dataset, for instance, with images reconstructed after different levels of lossy compression; satellite images would be a use case example, since they are commonly compressed before downloading to the ground. in this situation, the optimization problem consists in finding the smallest images that provide yet sufficient quality to meet the required performance of the deep learning algorithms. thus, a study with iquaflow is suitable for such case. all this development is wrapped in mlflow: an interactive tool used to visualize and summarize the results. this document describes different use cases and provides links to their respective repositories. to ease the creation of new studies, we include a cookie-cutter repository. the source code, issue tracker and aforementioned repositories are all hosted on github https://github.com/satellogic/iquaflow.',\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['p. gallés',\n",
       "   'k. takats',\n",
       "   'm. hernández-cabronero',\n",
       "   'd. berga',\n",
       "   'l. pega',\n",
       "   'l. riordan-chen',\n",
       "   'c. garcia',\n",
       "   'g. becker',\n",
       "   'a. garriga',\n",
       "   'a. bukva',\n",
       "   'j. serra-sagristà',\n",
       "   'd. vilaseca',\n",
       "   'j. marín'],\n",
       "  'affiliation': ['satellogic inc',\n",
       "   'satellogic inc',\n",
       "   'universitat autònoma de barcelona - uab-deic-gici',\n",
       "   'eurecat - multimedia technologies unit',\n",
       "   'satellogic inc',\n",
       "   'satellogic inc',\n",
       "   'satellogic inc',\n",
       "   'satellogic inc',\n",
       "   'eurecat - multimedia technologies unit',\n",
       "   'eurecat - multimedia technologies unit',\n",
       "   'universitat autònoma de barcelona - uab-deic-gici',\n",
       "   'satellogic inc',\n",
       "   'satellogic inc'],\n",
       "  'url': 'https://arxiv.org/abs/2210.13269'},\n",
       " {'title': 'generating hierarchical explanations on text classification without   connecting rules',\n",
       "  'id': '2210.13270',\n",
       "  'abstract': 'the opaqueness of deep nlp models has motivated the development of methods for interpreting how deep models predict. recently, work has introduced hierarchical attribution, which produces a hierarchical clustering of words, along with an attribution score for each cluster. however, existing work on hierarchical attribution all follows the connecting rule, limiting the cluster to a continuous span in the input text. we argue that the connecting rule as an additional prior may undermine the ability to reflect the model decision process faithfully. to this end, we propose to generate hierarchical explanations without the connecting rule and introduce a framework for generating hierarchical clusters. experimental results and further analysis show the effectiveness of the proposed method in providing high-quality explanations for reflecting model predicting process.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['yiming ju', 'yuanzhe zhang', 'kang liu', 'jun zhao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13270'},\n",
       " {'title': 'ecg artifact removal from single-channel surface emg using fully   convolutional networks',\n",
       "  'id': '2210.13271',\n",
       "  'abstract': 'electrocardiogram (ecg) artifact contamination often occurs in surface electromyography (semg) applications when the measured muscles are in proximity to the heart. previous studies have developed and proposed various methods, such as high-pass filtering, template subtraction and so forth. however, these methods remain limited by the requirement of reference signals and distortion of original semg. this study proposed a novel denoising method to eliminate ecg artifacts from the single-channel semg signals using fully convolutional networks (fcn). the proposed method adopts a denoise autoencoder structure and powerful nonlinear mapping capability of neural networks for semg denoising. we compared the proposed approach with conventional approaches, including high-pass filters and template subtraction, on open datasets called the non-invasive adaptive prosthetics database and mit-bih normal sinus rhythm database. the experimental results demonstrate that the fcn outperforms conventional methods in semg reconstruction quality under a wide range of signal-to-noise ratio inputs.',\n",
       "  'categories': 'eess.sp cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['kuan-chen wang', 'kai-chun liu', 'sheng-yu peng', 'yu tsao'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13271'},\n",
       " {'title': 'provably doubly accelerated federated learning: the first theoretically   successful combination of local training and compressed communication',\n",
       "  'id': '2210.13277',\n",
       "  'abstract': 'in the modern paradigm of federated learning, a large number of users are involved in a global learning task, in a collaborative way. they alternate local computations and two-way communication with a distant orchestrating server. communication, which can be slow and costly, is the main bottleneck in this setting. to reduce the communication load and therefore accelerate distributed gradient descent, two strategies are popular: 1) communicate less frequently; that is, perform several iterations of local computations between the communication rounds; and 2) communicate compressed information instead of full-dimensional vectors. in this paper, we propose the first algorithm for distributed optimization and federated learning, which harnesses these two strategies jointly and converges linearly to an exact solution, with a doubly accelerated rate: our algorithm benefits from the two acceleration mechanisms provided by local training and compression, namely a better dependency on the condition number of the functions and on the dimension of the model, respectively.',\n",
       "  'categories': 'cs.lg cs.dc math.oc',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['laurent condat', 'ivan agarsky', 'peter richtárik'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13277'},\n",
       " {'title': 'nvidia flare: federated learning from simulation to real-world',\n",
       "  'id': '2210.13291',\n",
       "  'abstract': 'federated learning (fl) enables building robust and generalizable ai models by leveraging diverse datasets from multiple collaborators without centralizing the data. we created nvidia flare as an open-source software development kit (sdk) to make it easier for data scientists to use fl in their research and real-world applications. the sdk includes solutions for state-of-the-art fl algorithms and federated machine learning approaches, which facilitate building workflows for distributed learning across enterprises and enable platform developers to create a secure, privacy-preserving offering for multiparty collaboration utilizing homomorphic encryption or differential privacy. the sdk is a lightweight, flexible, and scalable python package, and allows researchers to bring their data science workflows implemented in any training libraries (pytorch, tensorflow, xgboost, or even numpy) and apply them in real-world fl settings. this paper introduces the key design principles of flare and illustrates some use cases (e.g., covid analysis) with customizable fl workflows that implement different privacy-preserving algorithms.   code is available at https://github.com/nvidia/nvflare.',\n",
       "  'categories': 'cs.lg cs.ai cs.cv cs.ni cs.se',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['holger r. roth',\n",
       "   'yan cheng',\n",
       "   'yuhong wen',\n",
       "   'isaac yang',\n",
       "   'ziyue xu',\n",
       "   'yuan-ting hsieh',\n",
       "   'kristopher kersten',\n",
       "   'ahmed harouni',\n",
       "   'can zhao',\n",
       "   'kevin lu',\n",
       "   'zhihong zhang',\n",
       "   'wenqi li',\n",
       "   'andriy myronenko',\n",
       "   'dong yang',\n",
       "   'sean yang',\n",
       "   'nicola rieke',\n",
       "   'abood quraini',\n",
       "   'chester chen',\n",
       "   'daguang xu',\n",
       "   'nic ma',\n",
       "   'prerna dogra',\n",
       "   'mona flores',\n",
       "   'andrew feng'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13291'},\n",
       " {'title': 'designing universal causal deep learning models: the case of   infinite-dimensional dynamical systems from stochastic analysis',\n",
       "  'id': '2210.13300',\n",
       "  'abstract': 'deep learning (dl) is becoming indispensable to contemporary stochastic analysis and finance; nevertheless, it is still unclear how to design a principled dl framework for approximating infinite-dimensional causal operators. this paper proposes a \"geometry-aware\" solution to this open problem by introducing a dl model-design framework that takes a suitable infinite-dimensional linear metric spaces as inputs and returns a universal sequential dl models adapted to these linear geometries: we call these models causal neural operators (cno). our main result states that the models produced by our framework can uniformly approximate on compact sets and across arbitrarily finite-time horizons h\\\\\"older or smooth trace class operators which causally map sequences between given linear metric spaces. consequentially, we deduce that a single cno can efficiently approximate the solution operator to a broad range of sdes, thus allowing us to simultaneously approximate predictions from families of sde models, which is vital to computational robust finance. we deduce that the cno can approximate the solution operator to most stochastic filtering problems, implying that a single cno can simultaneously filter a family of partially observed stochastic volatility models.',\n",
       "  'categories': 'math.ds cs.lg q-fin.cp',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['luca galimberti', 'giulia livieri', 'anastasis kratsios'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13300'},\n",
       " {'title': 'learning and covering sums of independent random variables with   unbounded support',\n",
       "  'id': '2210.13313',\n",
       "  'abstract': \"we study the problem of covering and learning sums $x = x_1 + \\\\cdots + x_n$ of independent integer-valued random variables $x_i$ (siirvs) with unbounded, or even infinite, support. de et al. at focs 2018, showed that the maximum value of the collective support of $x_i$'s necessarily appears in the sample complexity of learning $x$. in this work, we address two questions: (i) are there general families of siirvs with unbounded support that can be learned with sample complexity independent of both $n$ and the maximal element of the support? (ii) are there general families of siirvs with unbounded support that admit proper sparse covers in total variation distance? as for question (i), we provide a set of simple conditions that allow the unbounded siirv to be learned with complexity $\\\\text{poly}(1/\\\\epsilon)$ bypassing the aforementioned lower bound. we further address question (ii) in the general setting where each variable $x_i$ has unimodal probability mass function and is a different member of some, possibly multi-parameter, exponential family $\\\\mathcal{e}$ that satisfies some structural properties. these properties allow $\\\\mathcal{e}$ to contain heavy tailed and non log-concave distributions. moreover, we show that for every $\\\\epsilon > 0$, and every $k$-parameter family $\\\\mathcal{e}$ that satisfies some structural assumptions, there exists an algorithm with $\\\\tilde{o}(k) \\\\cdot \\\\text{poly}(1/\\\\epsilon)$ samples that learns a sum of $n$ arbitrary members of $\\\\mathcal{e}$ within $\\\\epsilon$ in tv distance. the output of the learning algorithm is also a sum of random variables whose distribution lies in the family $\\\\mathcal{e}$. en route, we prove that any discrete unimodal exponential family with bounded constant-degree central moments can be approximated by the family corresponding to a bounded subset of the initial (unbounded) parameter space.\",\n",
       "  'categories': 'cs.lg cs.ds math.st stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['alkis kalavasis',\n",
       "   'konstantinos stavropoulos',\n",
       "   'manolis zampetakis'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13313'},\n",
       " {'title': 'mars: meta-learning as score matching in the function space',\n",
       "  'id': '2210.13319',\n",
       "  'abstract': 'meta-learning aims to extract useful inductive biases from a set of related datasets. in bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. however, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. as a result, existing approaches resort to meta-learning restrictive diagonal gaussian priors, severely limiting their expressiveness and performance. to circumvent these issues, we approach meta-learning through the lens of functional bayesian neural network inference, which views the prior as a stochastic process and performs inference in the function space. specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score function of the data-generating process marginals instead of parameter space priors. in a comprehensive benchmark, we demonstrate that our method achieves state-of-the-art performance in terms of predictive accuracy and substantial improvements in the quality of uncertainty estimates.',\n",
       "  'categories': 'cs.lg stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['krunoslav lehman pavasovic',\n",
       "   'jonas rothfuss',\n",
       "   'andreas krause'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13319'},\n",
       " {'title': 'deep kronecker network',\n",
       "  'id': '2210.13327',\n",
       "  'abstract': \"we propose deep kronecker network (dkn), a novel framework designed for analyzing medical imaging data, such as mri, fmri, ct, etc. medical imaging data is different from general images in at least two aspects: i) sample size is usually much more limited, ii) model interpretation is more of a concern compared to outcome prediction. due to its unique nature, general methods, such as convolutional neural network (cnn), are difficult to be directly applied. as such, we propose dkn, that is able to i) adapt to low sample size limitation, ii) provide desired model interpretation, and iii) achieve the prediction power as cnn. the dkn is general in the sense that it not only works for both matrix and (high-order) tensor represented image data, but also could be applied to both discrete and continuous outcomes. the dkn is built on a kronecker product structure and implicitly imposes a piecewise smooth property on coefficients. moreover, the kronecker structure can be written into a convolutional form, so dkn also resembles a cnn, particularly, a fully convolutional network (fcn). furthermore, we prove that with an alternating minimization algorithm, the solutions of dkn are guaranteed to converge to the truth geometrically even if the objective function is highly nonconvex. interestingly, the dkn is also highly connected to the tensor regression framework proposed by zhou et al. (2010), where a candecomp/parafac (cp) low-rank structure is imposed on tensor coefficients. finally, we conduct both classification and regression analyses using real mri data from the alzheimer's disease neuroimaging initiative (adni) to demonstrate the effectiveness of dkn.\",\n",
       "  'categories': 'stat.ml cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['long feng', 'guang yang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13327'},\n",
       " {'title': 'theoretical guarantees for domain adaptation with hierarchical optimal   transport',\n",
       "  'id': '2210.13331',\n",
       "  'abstract': 'domain adaptation arises as an important problem in statistical learning theory when the data-generating processes differ between training and test samples, respectively called source and target domains. recent theoretical advances show that the success of domain adaptation algorithms heavily relies on their ability to minimize the divergence between the probability distributions of the source and target domains. however, minimizing this divergence cannot be done independently of the minimization of other key ingredients such as the source risk or the combined error of the ideal joint hypothesis. the trade-off between these terms is often ensured by algorithmic solutions that remain implicit and not directly reflected by the theoretical guarantees. to get to the bottom of this issue, we propose in this paper a new theoretical framework for domain adaptation through hierarchical optimal transport. this framework provides more explicit generalization bounds and allows us to consider the natural hierarchical organization of samples in both domains into classes or clusters. additionally, we provide a new divergence measure between the source and target domains called hierarchical wasserstein distance that indicates under mild assumptions, which structures have to be aligned to lead to a successful adaptation.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['mourad el hamri', 'younès bennani', 'issam falih'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13331'},\n",
       " {'title': 'real-time speech interruption analysis: from cloud to client deployment',\n",
       "  'id': '2210.13334',\n",
       "  'abstract': 'meetings are an essential form of communication for all types of organizations, and remote collaboration systems have been much more widely used since the covid-19 pandemic. one major issue with remote meetings is that it is challenging for remote participants to interrupt and speak. we have recently developed the first speech interruption analysis model, which detects failed speech interruptions, shows very promising performance, and is being deployed in the cloud. to deliver this feature in a more cost-efficient and environment-friendly way, we reduced the model complexity and size to ship the wavlm_si model in client devices. in this paper, we first describe how we successfully improved the true positive rate (tpr) at a 1% false positive rate (fpr) from 50.9% to 68.3% for the failed speech interruption detection model by training on a larger dataset and fine-tuning. we then shrank the model size from 222.7 mb to 9.3 mb with an acceptable loss in accuracy and reduced the complexity from 31.2 gmacs (giga multiply-accumulate operations per second) to 4.3 gmacs. we also estimated the environmental impact of the complexity reduction, which can be used as a general guideline for large transformer-based models, and thus make those models more accessible with less computation overhead.',\n",
       "  'categories': 'cs.cl cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['quchen fu',\n",
       "   'szu-wei fu',\n",
       "   'yaran fan',\n",
       "   'yu wu',\n",
       "   'zhuo chen',\n",
       "   'jayant gupchup',\n",
       "   'ross cutler'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13334'},\n",
       " {'title': 'brain tumor segmentation using enhanced u-net model with empirical   analysis',\n",
       "  'id': '2210.13336',\n",
       "  'abstract': \"cancer of the brain is deadly and requires careful surgical segmentation. the brain tumors were segmented using u-net using a convolutional neural network (cnn). when looking for overlaps of necrotic, edematous, growing, and healthy tissue, it might be hard to get relevant information from the images. the 2d u-net network was improved and trained with the brats datasets to find these four areas. u-net can set up many encoder and decoder routes that can be used to get information from images that can be used in different ways. to reduce computational time, we use image segmentation to exclude insignificant background details. experiments on the brats datasets show that our proposed model for segmenting brain tumors from mri (mri) works well. in this study, we demonstrate that the brats datasets for 2017, 2018, 2019, and 2020 do not significantly differ from the brats 2019 dataset's attained dice scores of 0.8717 (necrotic), 0.9506 (edema), and 0.9427 (enhancing).\",\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['md abdullah al nasim',\n",
       "   'abdullah al munem',\n",
       "   'maksuda islam',\n",
       "   'md aminul haque palash',\n",
       "   'md. mahim anjum haque',\n",
       "   'faisal muhammad shah'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13336'},\n",
       " {'title': '(la)yer-neigh(bor) sampling: defusing neighborhood explosion in gnns',\n",
       "  'id': '2210.13339',\n",
       "  'abstract': 'graph neural networks have recently received a significant attention, however, training them at a large scale still remains a challenge. minibatch training coupled with sampling is used to alleviate this challenge. even so existing approaches either suffer from the neighborhood explosion phenomenon or do not have good performance. to deal with these issues, we propose a new sampling algorithm called layer-neighbor sampling (labor). it is designed to be a direct replacement for neighborhood sampling with the same fanout hyperparameter while sampling much fewer vertices, without sacrificing quality. by design, the variance of the estimator of each vertex matches neighbor sampling from the point of view of a single vertex. in our experiments, we demonstrate the superiority of our approach when it comes to model convergence behaviour against neighbor sampling and also the other layer sampling approaches under the same limited vertex sampling budget constraints.',\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['muhammed fatih balın', 'ümit v. çatalyürek'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13339'},\n",
       " {'title': 'augmenting task-oriented dialogue systems with relation extraction',\n",
       "  'id': '2210.13344',\n",
       "  'abstract': \"the standard task-oriented dialogue pipeline uses intent classification and slot-filling to interpret user utterances. while this approach can handle a wide range of queries, it does not extract the information needed to handle more complex queries that contain relationships between slots. we propose integration of relation extraction into this pipeline as an effective way to expand the capabilities of dialogue systems. we evaluate our approach by using an internal dataset with slot and relation annotations spanning three domains. finally, we show how slot-filling annotation schemes can be simplified once the expressive power of relation annotations is available, reducing the number of slots while still capturing the user's intended meaning.\",\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['andrew lee',\n",
       "   'zhenguo chen',\n",
       "   'kevin leach',\n",
       "   'jonathan k. kummerfeld'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13344'},\n",
       " {'title': 'matching map recovery with an unknown number of outliers',\n",
       "  'id': '2210.13354',\n",
       "  'abstract': 'we consider the problem of finding the matching map between two sets of $d$-dimensional noisy feature-vectors. the distinctive feature of our setting is that we do not assume that all the vectors of the first set have their corresponding vector in the second set. if $n$ and $m$ are the sizes of these two sets, we assume that the matching map that should be recovered is defined on a subset of unknown cardinality $k^*\\\\le \\\\min(n,m)$. we show that, in the high-dimensional setting, if the signal-to-noise ratio is larger than $5(d\\\\log(4nm/\\\\alpha))^{1/4}$, then the true matching map can be recovered with probability $1-\\\\alpha$. interestingly, this threshold does not depend on $k^*$ and is the same as the one obtained in prior work in the case of $k = \\\\min(n,m)$. the procedure for which the aforementioned property is proved is obtained by a data-driven selection among candidate mappings $\\\\{\\\\hat\\\\pi_k:k\\\\in[\\\\min(n,m)]\\\\}$. each $\\\\hat\\\\pi_k$ minimizes the sum of squares of distances between two sets of size $k$. the resulting optimization problem can be formulated as a minimum-cost flow problem, and thus solved efficiently. finally, we report the results of numerical experiments on both synthetic and real-world data that illustrate our theoretical results and provide further insight into the properties of the algorithms studied in this work.',\n",
       "  'categories': 'math.st cs.lg stat.th',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['arshak minasyan',\n",
       "   'tigran galstyan',\n",
       "   'sona hunanyan',\n",
       "   'arnak dalalyan'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13354'},\n",
       " {'title': 'calibration tests beyond classification',\n",
       "  'id': '2210.13355',\n",
       "  'abstract': 'most supervised machine learning tasks are subject to irreducible prediction errors. probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. in the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. for regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. in this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. it applies to any such model, including classification and regression models of arbitrary dimension. furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. in particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression problems.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-21',\n",
       "  'updated': '',\n",
       "  'authors': ['david widmann', 'fredrik lindsten', 'dave zachariah'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13355'},\n",
       " {'title': 'robust self-supervised learning with lie groups',\n",
       "  'id': '2210.13356',\n",
       "  'abstract': \"deep learning has led to remarkable advances in computer vision. even so, today's best models are brittle when presented with variations that differ even slightly from those seen during training. minor shifts in the pose, color, or illumination of an object can lead to catastrophic misclassifications. state-of-the art models struggle to understand how a set of variations can affect different objects. we propose a framework for instilling a notion of how objects vary in more realistic settings. our approach applies the formalism of lie groups to capture continuous transformations to improve models' robustness to distributional shifts. we apply our framework on top of state-of-the-art self-supervised learning (ssl) models, finding that explicitly modeling transformations with lie groups leads to substantial performance gains of greater than 10% for mae on both known instances seen in typical poses now presented in new poses, and on unknown instances in any pose. we also apply our approach to imagenet, finding that the lie operator improves performance by almost 4%. these results demonstrate the promise of learning transformations to improve model robustness.\",\n",
       "  'categories': 'cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['mark ibrahim', 'diane bouchacourt', 'ari morcos'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13356'},\n",
       " {'title': 'novelty detection in time series via weak innovations representation: a   deep learning approach',\n",
       "  'id': '2210.13358',\n",
       "  'abstract': 'we consider novelty detection in time series with unknown and nonparametric probability structures. a deep learning approach is proposed to causally extract an innovations sequence consisting of novelty samples statistically independent of all past samples of the time series. a novelty detection algorithm is developed for the online detection of novel changes in the probability structure in the innovations sequence. a minimax optimality under a bayes risk measure is established for the proposed novelty detection method, and its robustness and efficacy are demonstrated in experiments using real and synthetic datasets.',\n",
       "  'categories': 'cs.lg eess.sp stat.ml',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['xinyi wang', 'mei-jen lee', 'qing zhao', 'lang tong'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13358'},\n",
       " {'title': 'nasa: neural architecture search and acceleration for hardware inspired   hybrid networks',\n",
       "  'id': '2210.13361',\n",
       "  'abstract': \"multiplication is arguably the most cost-dominant operation in modern deep neural networks (dnns), limiting their achievable efficiency and thus more extensive deployment in resource-constrained applications. to tackle this limitation, pioneering works have developed handcrafted multiplication-free dnns, which require expert knowledge and time-consuming manual iteration, calling for fast development tools. to this end, we propose a neural architecture search and acceleration framework dubbed nasa, which enables automated multiplication-reduced dnn development and integrates a dedicated multiplication-reduced accelerator for boosting dnns' achievable efficiency. specifically, nasa adopts neural architecture search (nas) spaces that augment the state-of-the-art one with hardware-inspired multiplication-free operators, such as shift and adder, armed with a novel progressive pretrain strategy (pgp) together with customized training recipes to automatically search for optimal multiplication-reduced dnns; on top of that, nasa further develops a dedicated accelerator, which advocates a chunk-based template and auto-mapper dedicated for nasa-nas resulting dnns to better leverage their algorithmic properties for boosting hardware efficiency. experimental results and ablation studies consistently validate the advantages of nasa's algorithm-hardware co-design framework in terms of achievable accuracy and efficiency tradeoffs. codes are available at https://github.com/rice-eic/nasa.\",\n",
       "  'categories': 'cs.ar cs.lg',\n",
       "  'doi': '10.1145/3508352.3549478',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['huihong shi',\n",
       "   'haoran you',\n",
       "   'yang zhao',\n",
       "   'zhongfeng wang',\n",
       "   'yingyan lin'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13361'},\n",
       " {'title': 'large batch and patch size training for medical image segmentation',\n",
       "  'id': '2210.13364',\n",
       "  'abstract': 'multi-organ segmentation enables organ evaluation, accounts the relationship between multiple organs, and facilitates accurate diagnosis and treatment decisions. however, only few models can perform segmentation accurately because of the lack of datasets and computational resources. on amos2022 challenge, which is a large-scale, clinical, and diverse abdominal multiorgan segmentation benchmark, we trained a 3d-unet model with large batch and patch sizes using multi-gpu distributed training. segmentation performance tended to increase for models with large batch and patch sizes compared with the baseline settings. the accuracy was further improved by using ensemble models that were trained with different settings. these results provide a reference for parameter selection in organ segmentation.',\n",
       "  'categories': 'eess.iv cs.cv cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['junya sato', 'shoji kido'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13364'},\n",
       " {'title': 'adlight: a universal approach of traffic signal control with augmented   data using reinforcement learning',\n",
       "  'id': '2210.13378',\n",
       "  'abstract': 'traffic signal control has the potential to reduce congestion in dynamic networks. recent studies show that traffic signal control with reinforcement learning (rl) methods can significantly reduce the average waiting time. however, a shortcoming of existing methods is that they require model retraining for new intersections with different structures. in this paper, we propose a novel reinforcement learning approach with augmented data (adlight) to train a universal model for intersections with different structures. we propose a new agent design incorporating features on movements and actions with set current phase duration to allow the generalized model to have the same structure for different intersections. a new data augmentation method named \\\\textit{movement shuffle} is developed to improve the generalization performance. we also test the universal model with new intersections in simulation of urban mobility (sumo). the results show that the performance of our approach is close to the models trained in a single environment directly (only a 5% loss of average waiting time), and we can reduce more than 80% of training time, which saves a lot of computational resources in scalable operations of traffic lights.',\n",
       "  'categories': 'eess.sy cs.ai cs.lg cs.sy',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['maonan wang',\n",
       "   'yutong xu',\n",
       "   'xi xiong',\n",
       "   'yuheng kan',\n",
       "   'chengcheng xu',\n",
       "   'man-on pun'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13378'},\n",
       " {'title': 'evaluating long-term memory in 3d mazes',\n",
       "  'id': '2210.13383',\n",
       "  'abstract': 'intelligent agents need to remember salient information to reason in partially-observed environments. for example, agents with a first-person view should remember the positions of relevant objects even if they go out of view. similarly, to effectively navigate through rooms agents need to remember the floor plan of how rooms are connected. however, most benchmark tasks in reinforcement learning do not test long-term memory in agents, slowing down progress in this important research direction. in this paper, we introduce the memory maze, a 3d domain of randomized mazes specifically designed for evaluating long-term memory in agents. unlike existing benchmarks, memory maze measures long-term memory separate from confounding agent abilities and requires the agent to localize itself by integrating information over time. with memory maze, we propose an online reinforcement learning benchmark, a diverse offline dataset, and an offline probing evaluation. recording a human player establishes a strong baseline and verifies the need to build up and retain memories, which is reflected in their gradually increasing rewards within each episode. we find that current algorithms benefit from training with truncated backpropagation through time and succeed on small mazes, but fall short of human performance on the large mazes, leaving room for future algorithmic designs to be evaluated on the memory maze.',\n",
       "  'categories': 'cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['jurgis pasukonis', 'timothy lillicrap', 'danijar hafner'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13383'},\n",
       " {'title': 'on the failure of variational score matching for vae models',\n",
       "  'id': '2210.13390',\n",
       "  'abstract': 'score matching (sm) is a convenient method for training flexible probabilistic models, which is often preferred over the traditional maximum-likelihood (ml) approach. however, these models are less interpretable than normalized models; as such, training robustness is in general difficult to assess. we present a critical study of existing variational sm objectives, showing catastrophic failure on a wide range of datasets and network architectures. our theoretical insights on the objectives emerge directly from their equivalent autoencoding losses when optimizing variational autoencoder (vae) models. first, we show that in the fisher autoencoder, sm produces far worse models than maximum-likelihood, and approximate inference by fisher divergence can lead to low-density local optima. however, with important modifications, this objective reduces to a regularized autoencoding loss that resembles the evidence lower bound (elbo). this analysis predicts that the modified sm algorithm should behave very similarly to elbo on gaussian vaes. we then review two other fd-based objectives from the literature and show that they reduce to uninterpretable autoencoding losses, likely leading to poor performance. the experiments verify our theoretical predictions and suggest that only elbo and the baseline objective robustly produce expected results, while previously proposed sm methods do not.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['li kevin wenliang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13390'},\n",
       " {'title': 'we need to talk about random seeds',\n",
       "  'id': '2210.13393',\n",
       "  'abstract': 'modern neural network libraries all take as a hyperparameter a random seed, typically used to determine the initial state of the model parameters. this opinion piece argues that there are some safe uses for random seeds: as part of the hyperparameter search to select a good model, creating an ensemble of several models, or measuring the sensitivity of the training algorithm to the random seed hyperparameter. it argues that some uses for random seeds are risky: using a fixed random seed for \"replicability\" and varying only the random seed to create score distributions for performance comparison. an analysis of 85 recent publications from the acl anthology finds that more than 50% contain risky uses of random seeds.',\n",
       "  'categories': 'cs.cl cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['steven bethard'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13393'},\n",
       " {'title': 'offline congestion games: how feedback type affects data coverage   requirement',\n",
       "  'id': '2210.13396',\n",
       "  'abstract': 'this paper investigates when one can efficiently recover an approximate nash equilibrium (ne) in offline congestion games.the existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. we consider three different types of feedback with decreasing revealed information. starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and give a pessimism-type algorithm that can recover an approximate ne. for the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. on the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate ne. lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate ne. together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback.',\n",
       "  'categories': 'cs.gt cs.lg cs.ma',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['haozhe jiang',\n",
       "   'qiwen cui',\n",
       "   'zhihan xiong',\n",
       "   'maryam fazel',\n",
       "   'simon s. du'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13396'},\n",
       " {'title': 'sampling with mollified interaction energy descent',\n",
       "  'id': '2210.13400',\n",
       "  'abstract': 'sampling from a target measure whose density is only known up to a normalization constant is a fundamental problem in computational statistics and machine learning. in this paper, we present a new optimization-based method for sampling called mollified interaction energy descent (mied). mied minimizes a new class of energies on probability measures called mollified interaction energies (mies). these energies rely on mollifier functions -- smooth approximations of the dirac delta originated from pde theory. we show that as the mollifier approaches the dirac delta, the mie converges to the chi-square divergence with respect to the target measure and the gradient flow of the mie agrees with that of the chi-square divergence. optimizing this energy with proper discretization yields a practical first-order particle-based algorithm for sampling in both unconstrained and constrained domains. we show experimentally that for unconstrained sampling problems our algorithm performs on par with existing particle-based algorithms like svgd, while for constrained sampling problems our method readily incorporates constrained optimization techniques to handle more flexible constraints with strong performance compared to alternatives.',\n",
       "  'categories': 'stat.ml cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['lingxiao li',\n",
       "   'qiang liu',\n",
       "   'anna korba',\n",
       "   'mikhail yurochkin',\n",
       "   'justin solomon'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13400'},\n",
       " {'title': 'thermodynamics-informed neural networks for physically realistic mixed   reality',\n",
       "  'id': '2210.13414',\n",
       "  'abstract': 'the imminent impact of immersive technologies in society urges for active research in real-time and interactive physics simulation for virtual worlds to be realistic. in this context, realistic means to be compliant to the laws of physics. in this paper we present a method for computing the dynamic response of (possibly non-linear and dissipative) deformable objects induced by real-time user interactions in mixed reality using deep learning. the graph-based architecture of the method ensures the thermodynamic consistency of the predictions, whereas the visualization pipeline allows a natural and realistic user experience. two examples of virtual solids interacting with virtual or physical solids in mixed reality scenarios are provided to prove the performance of the method.',\n",
       "  'categories': 'cs.gr cs.ai cs.ce cs.lg math.ds',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['quercus hernández',\n",
       "   'alberto badías',\n",
       "   'francisco chinesta',\n",
       "   'elías cueto'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13414'},\n",
       " {'title': 'deep learning approach for dynamic sampling for multichannel mass   spectrometry imaging',\n",
       "  'id': '2210.13415',\n",
       "  'abstract': \"mass spectrometry imaging (msi), using traditional rectilinear scanning, takes hours to days for high spatial resolution acquisitions. given that most pixels within a sample's field of view are often neither relevant to underlying biological structures nor chemically informative, msi presents as a prime candidate for integration with sparse and dynamic sampling algorithms. during a scan, stochastic models determine which locations probabilistically contain information critical to the generation of low-error reconstructions. decreasing the number of required physical measurements thereby minimizes overall acquisition times. a deep learning approach for dynamic sampling (dlads), utilizing a convolutional neural network (cnn) and encapsulating molecular mass intensity distributions within a third dimension, demonstrates a simulated 70% throughput improvement for nanospray desorption electrospray ionization (nano-desi) msi tissues. evaluations are conducted between dlads and a supervised learning approach for dynamic sampling, with least-squares regression (slads-ls) and a multi-layer perceptron (mlp) network (slads-net). when compared with slads-ls, limited to a single m/z channel, as well as multichannel slads-ls and slads-net, dlads respectively improves regression performance by 36.7%, 7.0%, and 6.2%, resulting in gains to reconstruction quality of 6.0%, 2.1%, and 3.4% for acquisition of targeted m/z.\",\n",
       "  'categories': 'eess.iv cs.cv cs.lg eess.sp',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['david helminiak', 'hang hu', 'julia laskin', 'dong hye ye'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13415'},\n",
       " {'title': 'avalon: a benchmark for rl generalization using procedurally generated   worlds',\n",
       "  'id': '2210.13417',\n",
       "  'abstract': 'despite impressive successes, deep reinforcement learning (rl) systems still fall short of human performance on generalization to new tasks and environments that differ from their training. as a benchmark tailored for studying rl generalization, we introduce avalon, a set of tasks in which embodied agents in highly diverse procedural 3d worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. avalon is unique among existing rl benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. this setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks. avalon includes a highly efficient simulator, a library of baselines, and a benchmark with scoring metrics evaluated against hundreds of hours of human performance, all of which are open-source and publicly available. we find that standard rl baselines make progress on most tasks but are still far from human performance, suggesting avalon is challenging enough to advance the quest for generalizable rl.',\n",
       "  'categories': 'cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['joshua albrecht',\n",
       "   'abraham j. fetterman',\n",
       "   'bryden fogelman',\n",
       "   'ellie kitanidis',\n",
       "   'bartosz wróblewski',\n",
       "   'nicole seo',\n",
       "   'michael rosenthal',\n",
       "   'maksis knutins',\n",
       "   'zachary polizzi',\n",
       "   'james b. simon',\n",
       "   'kanjun qiu'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13417'},\n",
       " {'title': 'dichotomy of control: separating what you can control from what you   cannot',\n",
       "  'id': '2210.13435',\n",
       "  'abstract': \"future- or return-conditioned supervised learning is an emerging paradigm for offline reinforcement learning (rl), where the future outcome (i.e., return) associated with an observed action sequence is used as input to a policy trained to imitate those same actions. while return-conditioning is at the heart of popular algorithms such as decision transformer (dt), these methods tend to perform poorly in highly stochastic environments, where an occasional high return can arise from randomness in the environment rather than the actions themselves. such situations can lead to a learned policy that is inconsistent with its conditioning inputs; i.e., using the policy to act in the environment, when conditioning on a specific desired return, leads to a distribution of real returns that is wildly different than desired. in this work, we propose the dichotomy of control (doc), a future-conditioned supervised learning framework that separates mechanisms within a policy's control (actions) from those beyond a policy's control (environment stochasticity). we achieve this separation by conditioning the policy on a latent variable representation of the future, and designing a mutual information constraint that removes any information from the latent variable associated with randomness in the environment. theoretically, we show that doc yields policies that are consistent with their conditioning inputs, ensuring that conditioning a learned policy on a desired high-return future outcome will correctly induce high-return behavior. empirically, we show that doc is able to achieve significantly better performance than dt on environments that have highly stochastic rewards and transition\",\n",
       "  'categories': 'cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['mengjiao yang',\n",
       "   'dale schuurmans',\n",
       "   'pieter abbeel',\n",
       "   'ofir nachum'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13435'},\n",
       " {'title': 'bridging machine learning and sciences: opportunities and challenges',\n",
       "  'id': '2210.13441',\n",
       "  'abstract': 'the application of machine learning in sciences has seen exciting advances in recent years. as a widely-applicable technique, anomaly detection has been long studied in the machine learning community. especially, deep neural nets-based out-of-distribution detection has made great progress for high-dimensional data. recently, these techniques have been showing their potential in scientific disciplines. we take a critical look at their applicative prospects including data universality, experimental protocols, model robustness, etc. we discuss examples that display transferable practices and domain-specific challenges simultaneously, providing a starting point for establishing a novel interdisciplinary research paradigm in the near future.',\n",
       "  'categories': 'stat.ml cs.lg hep-ex hep-ph physics.data-an',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['taoli cheng'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13441'},\n",
       " {'title': 'precision machine learning',\n",
       "  'id': '2210.13447',\n",
       "  'abstract': 'we explore unique considerations involved in fitting ml models to data with very high precision, as is often required for science applications. we empirically compare various function approximation methods and study how they scale with increasing parameters and data. we find that neural networks can often outperform classical approximation methods on high-dimensional examples, by auto-discovering and exploiting modular structures therein. however, neural networks trained with common optimizers are less powerful for low-dimensional cases, which motivates us to study the unique properties of neural network loss landscapes and the corresponding optimization challenges that arise in the high precision regime. to address the optimization issue in low dimensions, we develop training tricks which enable us to train neural networks to extremely low loss, close to the limits allowed by numerical precision.',\n",
       "  'categories': 'cs.lg physics.comp-ph',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['eric j. michaud', 'ziming liu', 'max tegmark'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13447'},\n",
       " {'title': 'metaformer baselines for vision',\n",
       "  'id': '2210.13452',\n",
       "  'abstract': \"metaformer, the abstracted architecture of transformer, has been found to play a significant role in achieving competitive performance. in this paper, we further explore the capacity of metaformer, again, without focusing on token mixer design: we introduce several baseline models under metaformer using the most basic or common mixers, and summarize our observations as follows: (1) metaformer ensures solid lower bound of performance. by merely adopting identity mapping as the token mixer, the metaformer model, termed identityformer, achieves >80% accuracy on imagenet-1k. (2) metaformer works well with arbitrary token mixers. when specifying the token mixer as even a random matrix to mix tokens, the resulting model randformer yields an accuracy of >81%, outperforming identityformer. rest assured of metaformer's results when new token mixers are adopted. (3) metaformer effortlessly offers state-of-the-art results. with just conventional token mixers dated back five years ago, the models instantiated from metaformer already beat state of the art. (a) convformer outperforms convnext. taking the common depthwise separable convolutions as the token mixer, the model termed convformer, which can be regarded as pure cnns, outperforms the strong cnn model convnext. (b) caformer sets new record on imagenet-1k. by simply applying depthwise separable convolutions as token mixer in the bottom stages and vanilla self-attention in the top stages, the resulting model caformer sets a new record on imagenet-1k: it achieves an accuracy of 85.5% at 224x224 resolution, under normal supervised training without external data or distillation. in our expedition to probe metaformer, we also find that a new activation, starrelu, reduces 71% flops of activation compared with gelu yet achieves better performance. we expect starrelu to find great potential in metaformer-like models alongside other neural networks.\",\n",
       "  'categories': 'cs.cv cs.ai cs.lg',\n",
       "  'doi': '',\n",
       "  'created': '2022-10-24',\n",
       "  'updated': '',\n",
       "  'authors': ['weihao yu',\n",
       "   'chenyang si',\n",
       "   'pan zhou',\n",
       "   'mi luo',\n",
       "   'yichen zhou',\n",
       "   'jiashi feng',\n",
       "   'shuicheng yan',\n",
       "   'xinchao wang'],\n",
       "  'affiliation': [],\n",
       "  'url': 'https://arxiv.org/abs/2210.13452'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2105.01937</td>\n",
       "      <td>flex: extrinsic parameters-free multi-view 3d ...</td>\n",
       "      <td>cs.cv cs.gr cs.lg</td>\n",
       "      <td>the increasing availability of video recording...</td>\n",
       "      <td></td>\n",
       "      <td>2021-05-05</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>[brian gordon, sigal raab, guy azov, raja giry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2105.15197</td>\n",
       "      <td>a simple and general debiased machine learning...</td>\n",
       "      <td>stat.ml cs.lg econ.em math.st stat.th</td>\n",
       "      <td>debiased machine learning is a meta algorithm ...</td>\n",
       "      <td></td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>[victor chernozhukov, whitney k. newey, rahul ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2106.01528</td>\n",
       "      <td>normalizing flows for knockoff-free controlled...</td>\n",
       "      <td>stat.ml cs.lg</td>\n",
       "      <td>controlled feature selection aims to discover ...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-02</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>[derek hansen, brian manzo, jeffrey regier]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2106.03157</td>\n",
       "      <td>self-supervision is all you need for solving r...</td>\n",
       "      <td>cs.lg</td>\n",
       "      <td>existing combinatorial search methods are ofte...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-06</td>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>[kyo takano]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2106.03725</td>\n",
       "      <td>stability to deformations of manifold filters ...</td>\n",
       "      <td>cs.lg</td>\n",
       "      <td>the paper defines and studies manifold (m) con...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>[zhiyang wang, luana ruiz, alejandro ribeiro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2106.06610</td>\n",
       "      <td>scalars are universal: equivariant machine lea...</td>\n",
       "      <td>cs.lg math-ph math.mp stat.ml</td>\n",
       "      <td>there has been enormous progress in the last f...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>[soledad villar, david w. hogg, kate storey-fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2106.06927</td>\n",
       "      <td>inverting adversarially robust networks for im...</td>\n",
       "      <td>cs.cv cs.lg cs.ne</td>\n",
       "      <td>despite unconditional feature inversion being ...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-13</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>[renan a. rojas-gomez, raymond a. yeh, minh n....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2106.07704</td>\n",
       "      <td>efficient (soft) q-learning for text generatio...</td>\n",
       "      <td>cs.cl cs.lg</td>\n",
       "      <td>maximum likelihood estimation (mle) is the pre...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>[han guo, bowen tan, zhengzhong liu, eric p. x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2106.10898</td>\n",
       "      <td>banditmf: multi-armed bandit based matrix fact...</td>\n",
       "      <td>cs.ir cs.lg</td>\n",
       "      <td>multi-armed bandits (mab) provide a principled...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-21</td>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>[shenghao xu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2106.12177</td>\n",
       "      <td>imitation learning: progress, taxonomies and c...</td>\n",
       "      <td>cs.lg cs.ro</td>\n",
       "      <td>imitation learning aims to extract knowledge f...</td>\n",
       "      <td></td>\n",
       "      <td>2021-06-23</td>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>[boyuan zheng, sunny verma, jianlong zhou, ivo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "20  2105.01937  flex: extrinsic parameters-free multi-view 3d ...   \n",
       "21  2105.15197  a simple and general debiased machine learning...   \n",
       "22  2106.01528  normalizing flows for knockoff-free controlled...   \n",
       "23  2106.03157  self-supervision is all you need for solving r...   \n",
       "24  2106.03725  stability to deformations of manifold filters ...   \n",
       "25  2106.06610  scalars are universal: equivariant machine lea...   \n",
       "26  2106.06927  inverting adversarially robust networks for im...   \n",
       "27  2106.07704  efficient (soft) q-learning for text generatio...   \n",
       "28  2106.10898  banditmf: multi-armed bandit based matrix fact...   \n",
       "29  2106.12177  imitation learning: progress, taxonomies and c...   \n",
       "\n",
       "                               categories  \\\n",
       "20                      cs.cv cs.gr cs.lg   \n",
       "21  stat.ml cs.lg econ.em math.st stat.th   \n",
       "22                          stat.ml cs.lg   \n",
       "23                                  cs.lg   \n",
       "24                                  cs.lg   \n",
       "25          cs.lg math-ph math.mp stat.ml   \n",
       "26                      cs.cv cs.lg cs.ne   \n",
       "27                            cs.cl cs.lg   \n",
       "28                            cs.ir cs.lg   \n",
       "29                            cs.lg cs.ro   \n",
       "\n",
       "                                             abstract doi     created  \\\n",
       "20  the increasing availability of video recording...      2021-05-05   \n",
       "21  debiased machine learning is a meta algorithm ...      2021-05-31   \n",
       "22  controlled feature selection aims to discover ...      2021-06-02   \n",
       "23  existing combinatorial search methods are ofte...      2021-06-06   \n",
       "24  the paper defines and studies manifold (m) con...      2021-06-07   \n",
       "25  there has been enormous progress in the last f...      2021-06-11   \n",
       "26  despite unconditional feature inversion being ...      2021-06-13   \n",
       "27  maximum likelihood estimation (mle) is the pre...      2021-06-14   \n",
       "28  multi-armed bandits (mab) provide a principled...      2021-06-21   \n",
       "29  imitation learning aims to extract knowledge f...      2021-06-23   \n",
       "\n",
       "       updated                                            authors  \n",
       "20  2022-10-21  [brian gordon, sigal raab, guy azov, raja giry...  \n",
       "21  2022-10-21  [victor chernozhukov, whitney k. newey, rahul ...  \n",
       "22  2022-10-21        [derek hansen, brian manzo, jeffrey regier]  \n",
       "23  2022-10-24                                       [kyo takano]  \n",
       "24  2022-10-21      [zhiyang wang, luana ruiz, alejandro ribeiro]  \n",
       "25  2022-10-20  [soledad villar, david w. hogg, kate storey-fi...  \n",
       "26  2022-10-21  [renan a. rojas-gomez, raymond a. yeh, minh n....  \n",
       "27  2022-10-22  [han guo, bowen tan, zhengzhong liu, eric p. x...  \n",
       "28  2022-10-24                                      [shenghao xu]  \n",
       "29  2022-10-20  [boyuan zheng, sunny verma, jianlong zhou, ivo...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ('id', 'title', 'categories', 'abstract', 'doi', 'created', 'updated', 'authors')\n",
    "df = pd.DataFrame(output, columns=cols)\n",
    "df[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>authors</th>\n",
       "      <th>url_pdf</th>\n",
       "      <th>url_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1606.04671</td>\n",
       "      <td>progressive neural networks</td>\n",
       "      <td>cs.lg</td>\n",
       "      <td>learning to solve complex sequences of tasks--...</td>\n",
       "      <td></td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>[andrei a. rusu, neil c. rabinowitz, guillaume...</td>\n",
       "      <td>https://arxiv.org/pdf/1606.04671.pdf</td>\n",
       "      <td>https://arxiv.org/abs/1606.04671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1903.09668</td>\n",
       "      <td>data augmentation for bayesian deep learning</td>\n",
       "      <td>stat.ml cs.lg stat.me</td>\n",
       "      <td>deep learning (dl) methods have emerged as one...</td>\n",
       "      <td>10.1214/22-ba1331</td>\n",
       "      <td>2019-03-22</td>\n",
       "      <td>2022-10-24</td>\n",
       "      <td>[yuexi wang, nicholas g. polson, vadim o. soko...</td>\n",
       "      <td>https://arxiv.org/pdf/1903.09668.pdf</td>\n",
       "      <td>https://arxiv.org/abs/1903.09668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904.10554</td>\n",
       "      <td>deep q-learning for nash equilibria: nash-dqn</td>\n",
       "      <td>cs.lg cs.gt q-fin.cp stat.ml</td>\n",
       "      <td>model-free learning for multi-agent stochastic...</td>\n",
       "      <td></td>\n",
       "      <td>2019-04-23</td>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>[philippe casgrain, brian ning, sebastian jaim...</td>\n",
       "      <td>https://arxiv.org/pdf/1904.10554.pdf</td>\n",
       "      <td>https://arxiv.org/abs/1904.10554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1908.05659</td>\n",
       "      <td>distributionally robust optimization: a review</td>\n",
       "      <td>math.oc cs.lg stat.ml</td>\n",
       "      <td>the concepts of risk-aversion, chance-constrai...</td>\n",
       "      <td>10.5802/ojmo.15</td>\n",
       "      <td>2019-08-12</td>\n",
       "      <td></td>\n",
       "      <td>[hamed rahimian, sanjay mehrotra]</td>\n",
       "      <td>https://arxiv.org/pdf/1908.05659.pdf</td>\n",
       "      <td>https://arxiv.org/abs/1908.05659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1909.00931</td>\n",
       "      <td>transfer fine-tuning: a bert case study</td>\n",
       "      <td>cs.cl cs.lg</td>\n",
       "      <td>a semantic equivalence assessment is defined a...</td>\n",
       "      <td>10.18653/v1/d19-1542</td>\n",
       "      <td>2019-09-02</td>\n",
       "      <td></td>\n",
       "      <td>[yuki arase, junichi tsujii]</td>\n",
       "      <td>https://arxiv.org/pdf/1909.00931.pdf</td>\n",
       "      <td>https://arxiv.org/abs/1909.00931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1911.03867</td>\n",
       "      <td>a modular deep learning pipeline for galaxy-sc...</td>\n",
       "      <td>astro-ph.im astro-ph.co cs.lg</td>\n",
       "      <td>upcoming large astronomical surveys are expect...</td>\n",
       "      <td></td>\n",
       "      <td>2019-11-10</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>[sandeep madireddy, nesar ramachandra, nan li,...</td>\n",
       "      <td>https://arxiv.org/pdf/1911.03867.pdf</td>\n",
       "      <td>https://arxiv.org/abs/1911.03867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2002.05905</td>\n",
       "      <td>magneto: fingerprinting usb flash drives via u...</td>\n",
       "      <td>cs.cr cs.lg</td>\n",
       "      <td>universal serial bus (usb) flash drives are no...</td>\n",
       "      <td>10.1145/3422308</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>2020-09-12</td>\n",
       "      <td>[omar adel ibrahim, savio sciancalepore, gabri...</td>\n",
       "      <td>https://arxiv.org/pdf/2002.05905.pdf</td>\n",
       "      <td>https://arxiv.org/abs/2002.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2004.05258</td>\n",
       "      <td>exploring optimal deep learning models for ima...</td>\n",
       "      <td>cs.cr cs.lg</td>\n",
       "      <td>analyzing a huge amount of malware is a major ...</td>\n",
       "      <td>10.1109/compsac54236.2022.00128</td>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>[rikima mitsuhashi, takahiro shinagawa]</td>\n",
       "      <td>https://arxiv.org/pdf/2004.05258.pdf</td>\n",
       "      <td>https://arxiv.org/abs/2004.05258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2005.02607</td>\n",
       "      <td>towards quantum advantage via topological data...</td>\n",
       "      <td>quant-ph cs.cc cs.lg</td>\n",
       "      <td>even after decades of quantum computing develo...</td>\n",
       "      <td></td>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>[casper gyurik, chris cade, vedran dunjko]</td>\n",
       "      <td>https://arxiv.org/pdf/2005.02607.pdf</td>\n",
       "      <td>https://arxiv.org/abs/2005.02607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2007.09855</td>\n",
       "      <td>wide boosting</td>\n",
       "      <td>cs.lg stat.ml</td>\n",
       "      <td>gradient boosting (gb) is a popular methodolog...</td>\n",
       "      <td></td>\n",
       "      <td>2020-07-19</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>[michael t. horrell]</td>\n",
       "      <td>https://arxiv.org/pdf/2007.09855.pdf</td>\n",
       "      <td>https://arxiv.org/abs/2007.09855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  1606.04671                        progressive neural networks   \n",
       "1  1903.09668       data augmentation for bayesian deep learning   \n",
       "2  1904.10554      deep q-learning for nash equilibria: nash-dqn   \n",
       "3  1908.05659     distributionally robust optimization: a review   \n",
       "4  1909.00931            transfer fine-tuning: a bert case study   \n",
       "5  1911.03867  a modular deep learning pipeline for galaxy-sc...   \n",
       "6  2002.05905  magneto: fingerprinting usb flash drives via u...   \n",
       "7  2004.05258  exploring optimal deep learning models for ima...   \n",
       "8  2005.02607  towards quantum advantage via topological data...   \n",
       "9  2007.09855                                      wide boosting   \n",
       "\n",
       "                      categories  \\\n",
       "0                          cs.lg   \n",
       "1          stat.ml cs.lg stat.me   \n",
       "2   cs.lg cs.gt q-fin.cp stat.ml   \n",
       "3          math.oc cs.lg stat.ml   \n",
       "4                    cs.cl cs.lg   \n",
       "5  astro-ph.im astro-ph.co cs.lg   \n",
       "6                    cs.cr cs.lg   \n",
       "7                    cs.cr cs.lg   \n",
       "8           quant-ph cs.cc cs.lg   \n",
       "9                  cs.lg stat.ml   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  learning to solve complex sequences of tasks--...   \n",
       "1  deep learning (dl) methods have emerged as one...   \n",
       "2  model-free learning for multi-agent stochastic...   \n",
       "3  the concepts of risk-aversion, chance-constrai...   \n",
       "4  a semantic equivalence assessment is defined a...   \n",
       "5  upcoming large astronomical surveys are expect...   \n",
       "6  universal serial bus (usb) flash drives are no...   \n",
       "7  analyzing a huge amount of malware is a major ...   \n",
       "8  even after decades of quantum computing develo...   \n",
       "9  gradient boosting (gb) is a popular methodolog...   \n",
       "\n",
       "                               doi     created     updated  \\\n",
       "0                                   2016-06-15  2022-10-22   \n",
       "1                10.1214/22-ba1331  2019-03-22  2022-10-24   \n",
       "2                                   2019-04-23  2022-10-23   \n",
       "3                  10.5802/ojmo.15  2019-08-12               \n",
       "4             10.18653/v1/d19-1542  2019-09-02               \n",
       "5                                   2019-11-10  2022-10-21   \n",
       "6                  10.1145/3422308  2020-02-14  2020-09-12   \n",
       "7  10.1109/compsac54236.2022.00128  2020-04-10  2022-10-23   \n",
       "8                                   2020-05-06  2022-10-21   \n",
       "9                                   2020-07-19  2022-10-22   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [andrei a. rusu, neil c. rabinowitz, guillaume...   \n",
       "1  [yuexi wang, nicholas g. polson, vadim o. soko...   \n",
       "2  [philippe casgrain, brian ning, sebastian jaim...   \n",
       "3                  [hamed rahimian, sanjay mehrotra]   \n",
       "4                       [yuki arase, junichi tsujii]   \n",
       "5  [sandeep madireddy, nesar ramachandra, nan li,...   \n",
       "6  [omar adel ibrahim, savio sciancalepore, gabri...   \n",
       "7            [rikima mitsuhashi, takahiro shinagawa]   \n",
       "8         [casper gyurik, chris cade, vedran dunjko]   \n",
       "9                               [michael t. horrell]   \n",
       "\n",
       "                                url_pdf                          url_html  \n",
       "0  https://arxiv.org/pdf/1606.04671.pdf  https://arxiv.org/abs/1606.04671  \n",
       "1  https://arxiv.org/pdf/1903.09668.pdf  https://arxiv.org/abs/1903.09668  \n",
       "2  https://arxiv.org/pdf/1904.10554.pdf  https://arxiv.org/abs/1904.10554  \n",
       "3  https://arxiv.org/pdf/1908.05659.pdf  https://arxiv.org/abs/1908.05659  \n",
       "4  https://arxiv.org/pdf/1909.00931.pdf  https://arxiv.org/abs/1909.00931  \n",
       "5  https://arxiv.org/pdf/1911.03867.pdf  https://arxiv.org/abs/1911.03867  \n",
       "6  https://arxiv.org/pdf/2002.05905.pdf  https://arxiv.org/abs/2002.05905  \n",
       "7  https://arxiv.org/pdf/2004.05258.pdf  https://arxiv.org/abs/2004.05258  \n",
       "8  https://arxiv.org/pdf/2005.02607.pdf  https://arxiv.org/abs/2005.02607  \n",
       "9  https://arxiv.org/pdf/2007.09855.pdf  https://arxiv.org/abs/2007.09855  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['url_pdf'] = 'https://arxiv.org/pdf/' + df['id'] + '.pdf'\n",
    "df['url_html'] = 'https://arxiv.org/abs/' + df['id']\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './case-studies/arxiv-corpus/mine50/'\n",
    "Path(path + 'pdf/').mkdir(parents=True, exist_ok=True)\n",
    "Path(path + 'html/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df[:5].iterrows():\n",
    "    response = requests.get(row['url_pdf'])\n",
    "    path_pdf = path + 'pdf/' + row['id'] + '.pdf'\n",
    "    with open(path_pdf, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    response = requests.get(row['url_html'])\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    filename = path + 'html/' + row['id'] + '.html'\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        f.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grobid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "path_corpus = './case-studies/arxiv-corpus/mine52/'\n",
    "client = GrobidClient(config_path=\"./grobid_client_python/config.json\")\n",
    "client.process(\"processFulltextDocument\", path_corpus + \"pdf/\", output=path_corpus + \"parsed_xml/\" , n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('repro-screener')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89308f92c98ad59917276c7071a7ee158bd27f5f51fe69fe9e9f685df9484ae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
