{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f940e8-5755-44de-aceb-1f928dcce12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "import spacy\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb22462-cf57-4a0d-bb0e-2a7c4569454c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'julia.md'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/adb/gitclones/repro-screener/find.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/adb/gitclones/repro-screener/find.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mjulia.md\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/adb/gitclones/repro-screener/find.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     all_text \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'julia.md'"
     ]
    }
   ],
   "source": [
    "with open('julia.md','r') as f:\n",
    "    all_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae711e4-6fda-4088-858f-ad5d179af12f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['![](assets_j/img-0147.jpg)\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0148.jpg)\\n',\n",
       " '\\n',\n",
       " 'Contents lists available at [ScienceDirect](http://www.ScienceDirect.com/) Journal of Computational Physics\\n',\n",
       " '\\n',\n",
       " '### www.elsevier.com/locate/jcp\\n',\n",
       " '\\n',\n",
       " 'A fast marching algorithm for the factored eikonal equation Eran Treister a,∗ Eldad Haber a,b  \\n',\n",
       " 'a *Department of Earth and Ocean Sciences, The University of British Columbia, Vancouver, BC, Canada*  \\n',\n",
       " 'b *Department of Mathematics, The University of British Columbia, Vancouver, BC, Canada*\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0149.jpg)\\n',\n",
       " '\\n',\n",
       " '### a r t i c l e i n f o\\n',\n",
       " '\\n',\n",
       " '*Article history:*  \\n',\n",
       " 'Received 14 April 2016  \\n',\n",
       " 'Received in revised form 5 August 2016 Accepted 8 August 2016  \\n',\n",
       " 'Available online 12 August 2016 *Keywords:*  \\n',\n",
       " 'Eikonal equation  \\n',\n",
       " 'Factored eikonal equation  \\n',\n",
       " 'Fast Marching  \\n',\n",
       " 'First arrival  \\n',\n",
       " 'Travel time tomography  \\n',\n",
       " 'Gauss–Newton  \\n',\n",
       " 'Seismic imaging\\n',\n",
       " '\\n',\n",
       " '##### 1. Introduction\\n',\n",
       " '\\n',\n",
       " 'a b s t r a c t\\n',\n",
       " '\\n',\n",
       " 'The eikonal equation is instrumental in many applications in several ﬁelds ranging from computer vision to geoscience. This equation can be eﬃciently solved using the iterative Fast Sweeping (FS) methods and the direct Fast Marching (FM) methods. However, when used for a point source, the original eikonal equation is known to yield inaccurate numerical solutions, because of a singularity at the source. In this case, the factored eikonal equation is often preferred, and is known to yield a more accurate numerical solution. One application that requires the solution of the eikonal equation for point sources is travel time tomography. This inverse problem may be formulated using the eikonal equation as a forward problem. While this problem has been solved using FS in the past, the more recent choice for applying it involves FM methods because of the eﬃciency in which sensitivities can be obtained using them. However, while several FS methods are available for solving the factored equation, the FM method is available only for the original eikonal equation. In this paper we develop a Fast Marching algorithm for the factored eikonal equation, using both ﬁrst and second order ﬁnite-difference schemes. Our algorithm follows the same lines as the original FM algorithm and requires the same computational effort. In addition, we show how to obtain sensitivities using this FM method and apply travel time tomography, formulated as an inverse factored eikonal equation. Numerical results in two and three dimensions show that our algorithm solves the factored eikonal equation eﬃciently, and demonstrate the achieved accuracy for computing the travel time. We also demonstrate a recovery of a 2D and 3D heterogeneous medium by travel time tomography using the eikonal equation for forward modeling and inversion by Gauss–Newton.  \\n',\n",
       " '© 2016 Elsevier Inc. All rights reserved.\\n',\n",
       " '\\n',\n",
       " 'The eikonal equation appears in many ﬁelds, ranging from computer vision [30,31,33,11], where it is used to track evolution of interfaces, to geoscience [19,12,22,14,24] where it describes the propagation of the ﬁrst arrival of a wave in a medium. The equation has the form\\n',\n",
       " '\\n',\n",
       " '# |∇ τ | 2 = κ( ? x) 2 ,\\n',\n",
       " '\\n',\n",
       " '################# (1.1)\\n',\n",
       " '\\n',\n",
       " 'where \\\\| ·\\\\| is the Euclidean norm. In the case of wave propagation, τ is the travel time of the wave and κ( ? *x)* is the slowness (inverse velocity) of the medium. The value of τ is usually given at some sub-region. For example, in this work we assume the wave propagates from a point source at location ? *x0,* for which the travel time is 0, and hence τ( ? *x0)* = 0.\\n',\n",
       " '\\n',\n",
       " '\\\\* Corresponding author.  \\n',\n",
       " '*E-mail addresses:* [erantreister@gmail.com](mailto:erantreister@gmail.com) (E. Treister), [haber@math.ubc.ca](mailto:haber@math.ubc.ca) (E. Haber). [http://dx.doi.org/10.1016/j.jcp.2016.08.012](http://dx.doi.org/10.1016/j.jcp.2016.08.012)  \\n',\n",
       " '0021-9991/© 2016 Elsevier Inc. All rights reserved.\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0637.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 1.** The *l2* norm of the approximation error \\\\|∇τ0 − *Dτ0* \\\\| around a source point at [0.5,0.5], where τ0 is the distance function and *D* is a central difference gradient operator on a mesh with *hx* = *hy* = 0.01.\\n',\n",
       " '\\n',\n",
       " 'Equation (1.1) is nonlinear, and as such may have multiple branches in its solution. One of these branches, which is the one of interest in the applications mentioned earlier, corresponds to the “ﬁrst-arrival” viscosity solution, and can be calculated eﬃciently [5,25]. One of the ways to compute it is by using the Fast Marching (FM) methods [36,30,31], which solve it directly using ﬁrst or second order schemes in O(n logn) operations. These methods are based on the monotonicity of the solution along the characteristics. Alternatively, (1.1) can be solved iteratively by Fast Sweeping (FS) methods, which may be seen as Gauss–Seidel method for (1.1). First order accurate solutions of (1.1) can be obtained very eﬃciently in 2 *d* Gauss–Seidel sweeps in O(n) operations, where *d* is the dimension of the problem [35,38]. An alternative for the mentioned approaches is to use FS to solve a Lax–Friedrichs approximation for (1.1), which involves adding artiﬁcial viscosity to the original equation [10]. This approach was suggested for general Hamilton–Jacobi equations, and is simple to implement. In [23], such Lax–Friedrichs approximation is obtained using FS up to third order accuracy using the weighted essentially non-oscillatory (WENO) approximations to the derivatives. For a performance comparison between some of the mentioned solvers see [7].  \\n',\n",
       " 'In some cases, the eikonal equation (1.1) is used to get a geometrical-optics ansatz of the solution of the Helmholtz equation in the high frequency regime [12,15,19,17]. This is done using the Rytov decomposition of the Helmholtz solution: *u(* ? *x)* = *a(* ? *x)* exp(iωτ( ? x)), where *a(* ? *x)* is the amplitude and τ( ? *x)* is the travel time. This approach involves solving (1.1) for the travel-time and solving the transport equation\\n',\n",
       " '\\n',\n",
       " '### ∇ τ ·∇a + 1 a?τ = 1 (∇ τ ·∇a +∇ · (a∇ τ)) = 0\\n',\n",
       " '\\n',\n",
       " '################# (1.2)\\n',\n",
       " '\\n',\n",
       " '2  \\n',\n",
       " '2  \\n',\n",
       " 'for the amplitude [15,19]. The resulting approximation includes only the ﬁrst arrival information of the wave propagation. Somewhat similarly, the work of [9] suggests using the eikonal solution to get a multigrid preconditioner for solving linear systems arising from discretization of the Helmholtz equation.  \\n',\n",
       " 'In many cases in seismic imaging, the eikonal equation is used for modeling the migration of seismic waves from a point source at some point ? *x0.* In this case, when solving (1.1) numerically by standard ﬁnite differences methods, the obtained solution has a strong singularity at the location of the source, which leads to large numerical errors [22,6]. Fig. 1 illustrates this phenomenon by showing the approximation error for the gradient of the distance function, which is the solution of (1.1) for κ = 1. It is clear that the largest approximation error for the gradient is located around the source point, and that its magnitude is rather large. In addition, it is observed that although τ may have more singularities in other places, the singularity at the source is more damaging and polluting for the numerical solution [22,6].  \\n',\n",
       " 'A rather easy treatment to the described phenomenon is suggested in [21,6], and achieved using a factored version of (1.1). That is, we deﬁne  \\n',\n",
       " 'τ = τ0τ1, (1.3) where τ0 is the distance function, τ0 = ?? *x* − ? *x0* ? 2, from the point source—the analytical solution for (1.1) in the case where κ( ? *x)* = 1 is a constant. Indeed, at the location of the source, the function τ0 is non-smooth. However, the computed factor τ1 is expected to be very smooth at the surrounding of the source, and can be approximated up to high order of accuracy [18]. Plugging (1.3) into (1.1) and applying the chain rule yields the *factored eikonal equation*\\n',\n",
       " '\\n',\n",
       " '# τ 2 |∇ τ1 | 2 + 2τ0τ1 ∇ τ0 ·∇ τ1 + τ 2 = κ( ? x) 2 .\\n',\n",
       " '\\n',\n",
       " '################# (1.4)\\n',\n",
       " '\\n',\n",
       " '0  \\n',\n",
       " '1  \\n',\n",
       " 'Similarly to it original version, Equation (1.4) can be solved by fast sweeping methods in ﬁrst order accuracy [6,16,18], or by a Lax–Friedrichs scheme up to third order of accuracy [19,15,18]. The recent works [18,20] suggest hybrid schemes where the factored eikonal is solved at the neighborhood of the source and the standard eikonal, which is computationally easier, is solved in the rest of the domain.  \\n',\n",
       " 'One geophysical tool that ﬁts the scenario described earlier is travel time tomography. One way to formulate it is by using the eikonal equation as a forward problem inside an inverse problem [29]. To solve the inverse problem, one should be able to solve (1.1) accurately for a point source, and to compute its sensitivities eﬃciently. The works of [13,34] computes\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " 'the tomography by FS, and require an FS iterative solution for computing the sensitivities. The more recent [14] uses the FM algorithm for forward modeling using the non-factored eikonal equation, because this way the sensitivities are obtained more eﬃciently by a simple solution of a lower triangular linear system. [3] suggests to use FS for forward modeling using the factored equation, but also eﬃciently obtain the sensitivities by approximating them using FM with the non-factored eikonal equation.  \\n',\n",
       " 'In this paper, we develop a Fast Marching algorithm for the factored eikonal equation (1.4), based on [30,31]. As in [31], our algorithm is able to solve (1.4) using ﬁrst order or second order schemes, in guaranteed O(n logn) running time. When using our method for forward modeling in travel time tomography, one achieves both worlds: (1) have an accurate forward modeling based on the factored eikonal equation, and (2) obtain the sensitivities of the (factored) forward modeling eﬃciently, by solving lower triangular linear systems in O(n) operations. Computationally, this is one of the most attractive ways to solve the inverse problem, since the cost of the inverse problem can be governed by the cost of applying the sensitivities.\\n',\n",
       " '\\n',\n",
       " 'Our paper is organized as follows. In the next section we brieﬂy review the FM method in [30,31], including some of its implementation details. Next, we show our extension to the FM method for the factored eikonal equation—in both ﬁrst and second order of accuracy—and provide some theoretical properties. Following that, we discuss the derivation of sensitivities using FM and brieﬂy present the travel time tomography problem. Last, we show some numerical results that demonstrate the effectiveness of the method in two and three dimensions, for both the forward and inverse problems.\\n',\n",
       " '\\n',\n",
       " '##### 2. The Fast Marching algorithm\\n',\n",
       " '\\n',\n",
       " 'We now review the FM algorithm of [30,31] in two dimensions. The extension to higher dimensions is straightforward. The FM algorithm is based on the Godunov upwind discretization [25] of (1.1). In two dimensions, this discretization is given by  \\n',\n",
       " '?  \\n',\n",
       " '?  \\n',\n",
       " '\\\\|∇ τ \\\\| 2 ≈ max\\\\{D −x τ,−D +x τ,0\\\\} 2 + max\\\\{D −y τ,−D +y τ,0\\\\} 2 = κ( ? *xij)* 2 , ? *xij* ∈ ?h,\\n',\n",
       " '\\n',\n",
       " '################# (2.5)\\n',\n",
       " '\\n',\n",
       " '*ij ij ij ij*\\n',\n",
       " '\\n',\n",
       " 'where in the simplest form *D* −x τ = *τi,j −τi−1,j* and *D* +x τ = *τi+1,j −τi,j* are the backward and forward ﬁrst derivative operators, *ij h ij h*  \\n',\n",
       " 'respectively. In principal, one can replace these operators with ones of higher order of accuracy.  \\n',\n",
       " 'The FM algorithm solves (2.5) in a sophisticated way, exploiting the fact that the upwind difference structure of (2.5) imposes a unique direction in which the information propagates—from smaller values of τ to larger values. Hence, the FM algorithm rests on solving (2.5) by building the solution outwards from the smallest τ value. It assumes that some initial value of τ is given at some region of ?h (or a point ? *x0)* and it propagates outwards from this initial region, by updating the next smallest value of τ at each step.  \\n',\n",
       " 'To apply the rule above, let us deﬁne three disjoint sets of variables: the knownvariables, the frontvariables (which are sometimes called the *trial* variables) and the unknownvariables. These three sets together contain all the grid points in the problem. For simplicity, let us assume that we solve (2.5) for a point source. That is, a source is located at point ? *x0,* for which τ( ? *x0)* = 0. Initially, knownis chosen as an empty set, frontis set to contain only ? *x0,* and unknownhas the rest of the variables for which τ is set to inﬁnity. At each step we choose the point ? *xij* in frontwith minimal value of τ( ? *xij)* and move it to known. Next, we move all of its neighbors which are in unknown to front, and solve (2.5) for all neighbors which are not in known. This way, we set all variables to be in known in *n* steps, and the algorithm ﬁnishes. A precise description of the algorithm is given in Algorithm 1.\\n',\n",
       " '\\n',\n",
       " '### Algorithm 1: Fast Marching\\n',\n",
       " '\\n',\n",
       " 'Initialize:  \\n',\n",
       " '*τij* = ∞ for all *?xij* ∈ ?h, τ(?x0) = 0, known← ∅, front← \\\\{?x0 \\\\}.  \\n',\n",
       " '**while** front? = ∅ **do**\\n',\n",
       " '\\n',\n",
       " '1. Find the minimal entry in front: *ximin,jmin* = argmin?xij *\\\\{τij* : *?xij* ∈front\\\\}\\n',\n",
       " '2. Add *?ximin,jmin* to knownand take it out of front: front←front\\\\\\\\\\\\{?ximin,jmin \\\\} ; known←known∪\\\\{?ximin,jmin \\\\}.\\n',\n",
       " '3. Add the unknown neighborhood of *?ximin,jmin* to front:  \\n',\n",
       " 'N = *\\\\{?ximin−1,jmin, ?ximin+1,jmin, ?ximin,jmin−1, ?ximin,jmin+1* \\\\}\\\\\\\\known *min*  \\n',\n",
       " 'front←front∪ N *min.*\\n',\n",
       " '4. **Foreach** *?xij* ∈ N *min*  \\n',\n",
       " 'Update *τij* by solving the quadratic (2.5), using only entries in known.\\n',\n",
       " '\\n',\n",
       " '- *ximin,jmin* = argmin?xij *\\\\{τij* : *?xij* ∈front\\\\}  \\n',\n",
       " 'Add *?ximin,jmin* to knownand take it out of front: front←front\\\\\\\\\\\\{?ximin,jmin \\\\} ; known←known∪\\\\{?ximin,jmin \\\\}.  \\n',\n",
       " 'Add the unknown neighborhood of *?ximin,jmin* to front:  \\n',\n",
       " 'N = *\\\\{?ximin−1,jmin, ?ximin+1,jmin, ?ximin,jmin−1, ?ximin,jmin+1* \\\\}\\\\\\\\known  \\n',\n",
       " '*min*  \\n',\n",
       " 'front←front∪ N *min.*  \\n',\n",
       " '**Foreach** *?xij* ∈ N *min*  \\n',\n",
       " 'Update *τij* by solving the quadratic (2.5), using only entries in known.\\n',\n",
       " '\\n',\n",
       " '### End\\n',\n",
       " '\\n',\n",
       " '### end\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0669.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 2.** A minimum heap and its implementation using array.\\n',\n",
       " '\\n',\n",
       " 'In [30] it was proved that Algorithm 1 produces a viable viscosity solution to (2.5) when using ﬁrst order approximations for the ﬁrst derivatives. Furthermore, it is proved that the values of τ in the order of which the points are set to knownin Step 2 are monotonically increasing.\\n',\n",
       " '\\n',\n",
       " '*2.1. Eﬃcient implementation using minimum heap*\\n',\n",
       " '\\n',\n",
       " 'Algorithm 1 has two main computational bottlenecks in Steps 1 and 4, which are repeated *n* times. For a d-dimensional problem, the set front contains a d-1 dimensional manifold of points, of size O(n *d−1* ). To ﬁnd the minimum of front *d*\\n',\n",
       " '\\n',\n",
       " 'eﬃciently, a minimum heap data structure is used [30,31]. A minimum heap is a binary tree with a property that a value at any node is less than or equal to the values at its two children. Consequently, the root of the tree holds the minimal value. The simplest implementation of such a tree is done by a sequential array of nodes, using the rule that if a node is located at entry *k,* then its two children are located at entries 2k and 2k + 1 (the ﬁrst element of the array is indexed by 1, and is the root of the tree). Equivalently, the parent of a node at entry *k* is located at entry ?k/2?. Fig. 2 shows an example of a minimum heap and its implementation using array. Generally, each element in the array can hold many properties, and one of these has to be deﬁned as a comparable “key”, which is used in the heap for sorting. In our case, each node holds a point ? *x* in the mesh, and its value τ( ? *x)* as a key.  \\n',\n",
       " 'In its simplest form, the minimum heap structure supports two basic operations: insert(element,key) and get- Min(). For example, this is the case in the C++ standard library implementation of the minimum heap structure. To apply getMin(), we remove the ﬁrst element in the array, and take the last element in the array and push it in the ﬁrst place. Then, to maintain the property of the heap, we repeatedly replace this value with its smaller child (the smaller of the two) until it reaches down in the tree to the point where it is smaller than its two children or it has no children. The insert(element,key) operation ﬁrst places a new element at the next empty space of the array. Then, it propagates this element upwards, each time replacing it with its parent until it reaches a point where the element is either the root or its key is larger than its parent’s key. Both of the described operations are performed in O(logm) complexity where *m* is the number of elements in the heap.  \\n',\n",
       " 'In Algorithm 1, the set frontis implemented using a min-heap. Steps 1–2 are trivially implemented using getMin(), and insert(element,key) is used in Step 3. However, Step 4 of Algorithm 1 requires updating values which are inside the heap but are not at the root. This operation is not supported in the standard deﬁnition of either priority queue or minimum heap. Indeed, the papers [30,31] use a variant of a priority queue, which includes back-links from points ? *xij* to their corresponding locations inside the heap, and a more “software-engineering friendly” implementation of this idea is suggested in [2], where those back-links are incorporated within the heap implementation, without any relation to the mesh. However, although this way an update of a key inside the heap can still be implemented in O(logm), it requires a specialized implementation of the heap, and encumbers the operations described earlier to maintain these back-links. In our implementation, we bypass this need for back-links, and implement Step 4 by reinserting elements to the heap if they are indeed smaller than their value in the heap. In Step 1, we simply ignore entries which are in known already. If the algorithm is indeed monotone, like the ﬁrst order version in [30], this implementation detail will not change the result of the algorithm. The downside of this change is that it enables frontto grow more than in the back-linked version. However, even if it grows four times compared to the back-linked version, then the heap tree is just two nodes higher, making the difference in running time insigniﬁcant.\\n',\n",
       " '\\n',\n",
       " '*2.2. Second order Fast Marching*\\n',\n",
       " '\\n',\n",
       " 'Solving the eikonal equation based on a ﬁrst order discretization in (2.5) provides guaranteed monotonicity and stability. However, it also provides a less accurate solution because of the added viscosity that is associated with the ﬁrst order approximation. To get a more accurate FM method, [31] suggests to use a second order upwind approximation in (2.5), e.g.\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '# D − τ = 3τi − 4τi−1 + τi−2 D + τ = −3τi + 4τi+1 − τi+2 .\\n',\n",
       " '\\n',\n",
       " '################# (2.6)\\n',\n",
       " '\\n',\n",
       " '*i* 2h *i* 2h  \\n',\n",
       " 'However, in some cases, the scheme may revert to ﬁrst order approximations from certain directions. The obvious case for that is when there are not enough knownpoints for the high order stencil. This case occurs for example when the given initial region contains only one point. Another condition for using second order operators is given in [31]:\\n',\n",
       " '\\n',\n",
       " '##### τi−1 ≥ τi−2 or τi+1 >τi+2,\\n',\n",
       " '\\n',\n",
       " '################# (2.7)\\n',\n",
       " '\\n',\n",
       " 'where the left condition is used for backward operators and the right one for forward operators. If (2.7) is not satisﬁed, the algorithm reverts to ﬁrst order operators. Later we show that this condition guarantees the monotonicity of the non-factored FM solution using second order scheme.\\n',\n",
       " '\\n',\n",
       " '##### 3. Fast Marching for the factored eikonal equation\\n',\n",
       " '\\n',\n",
       " 'Let us rewrite the factored eikonal equation (1.4) in a squared form, which is closer (1.1):\\n',\n",
       " '\\n',\n",
       " '# | τ0 ∇ τ1 + τ1 ∇ τ0 | 2 = κ( ? x) 2 .\\n',\n",
       " '\\n',\n",
       " '################# (3.8)\\n',\n",
       " '\\n',\n",
       " 'This writing is the key for deriving the FM algorithm for (1.4). Similarly to the Godunov upwind scheme in (2.5), we discretize (3.8) for τ1 using a derivative operator *D* ˆ instead of *D*  \\n',\n",
       " '?  \\n',\n",
       " '?\\n',\n",
       " '\\n',\n",
       " '### max{ D ˆ −x τ1,− D ˆ +x τ1,0} 2 + max{ D ˆ −y τ1,− D ˆ +y τ1,0} 2 = κ( ? xij) 2 , ? xij ∈ ?h.\\n',\n",
       " '\\n',\n",
       " '################# (3.9)\\n',\n",
       " '\\n',\n",
       " '*ij ij ij ij*\\n',\n",
       " '\\n',\n",
       " 'For example, the backward ﬁrst order factored derivative operator is given by\\n',\n",
       " '\\n',\n",
       " '###### D ˆ −x τ1 = (τ0)ij (τ1)i,j −(τ1)i−1,j +(p0)ij(τ1)ij,\\n',\n",
       " '\\n',\n",
       " '################# (3.10)\\n',\n",
       " '\\n',\n",
       " '*ij*  \\n',\n",
       " '*h*  \\n',\n",
       " 'where τ0 and *p0* = ∂τ0 are known. From this point we apply the Algorithm 1 as it is. We hold the values of τ0τ1 in front, and in Step 4 we update ∂x  \\n',\n",
       " '(τ1)ij with the solution of (3.9).  \\n',\n",
       " '**Initialization:** For the non-factored equation, Algorithm 1 is initialized by τ( ? *x0)* = 0 at the point source. In the factored equation, this is trivially fulﬁlled by deﬁnition, because at the source τ0( ? *x0)* = 0. Still, τ1( ? *x0)* should not be chosen arbitrarily since its value is used in the ﬁnite difference approximations when evaluating its neighbors. Examining (3.8) at the source yields τ1( ? *x0)* 2\\\\|∇ τ0 \\\\|2 = τ1( ? *x0)* 2 = κ( ? *x0)* 2 , since we choose τ0 such that \\\\|∇ τ0 \\\\|2 = 1, independently of κ. In some cases in the literature, i.e., [6], the value κ( ? *x0)* is absorbed in τ0, such that \\\\|∇ τ0 \\\\|2 = κ( ? *x0)* 2 . Then τ1( ? *x0)* should be chosen as 1. This is obviously equivalent for computing τ, however, it is much more convenient to choose τ0 independently of κ if one wants to obtain the sensitivities of the FM algorithm (for more details, see Section 4).  \\n',\n",
       " '**Second order discretization:** Similarly to the non-factored equation, the second order upwind approximations (2.6) can be used in (3.9)–(3.10) for τ1. Again, we revert to the ﬁrst order approximation in cases where the additional point needed for the second order approximation is not in known. We note that unlike the non-factored case, the solution τ1 is in most cases very smooth at the source (expected to be close to constant or linear). So, when we initialize the algorithm with the value of τ1 at the point source and revert to a ﬁrst order approximation for the neighbors, we do not introduce large discretization errors. In the non-factored case, the second derivative of τ is singular at the source, so using ﬁrst order approximation there signiﬁcantly pollutes the rest of the solution.\\n',\n",
       " '\\n',\n",
       " '*3.1. Solution of the piecewise quadratic equation*\\n',\n",
       " '\\n',\n",
       " 'We now describe how to solve both the non-factored and factored piecewise quadratic equations (2.5) and (3.9) respec- tively. This is required in Step 4 of Algorithm 1. Solving such an equation consists of the following four steps:\\n',\n",
       " '\\n',\n",
       " '1. Determine the order of approximation for each derivative in (2.5)/(3.9) (only required for high order schemes).\\n',\n",
       " '2. Determine which directions to choose (backward or forward) for each dimension (x, *y* or z).\\n',\n",
       " '3. Solve the quadratic equation in (2.5)/(3.9), assuming all terms are positive.\\n',\n",
       " '4. Make sure that the solution is valid, such that all max terms in (2.5)/(3.9) are indeed held with positive values. If not, some terms should be dropped, and the quadratic problem with the remaining terms is solved again.\\n',\n",
       " '\\n',\n",
       " 'Let us ﬁrst consider solving the non-factored ﬁrst order (2.5), for which the Step 1 is not relevant. In this case, Step 2 is simple: for each max\\\\{D − τ, −D + τ, 0\\\\} term, the smaller of the two values of τ from both sides (forward or backward) is *i j i j*  \\n',\n",
       " 'guaranteed to give a higher ﬁnite difference derivative. Furthermore, in Step 4, if some of the terms turn out negative after Step 3, then we can drop terms from (2.5) in decreasing order of the τ values, until a valid solution is reached. The same is true for a ﬁrst order factored version in (3.9).  \\n',\n",
       " 'However, using second order schemes (selectively) imposes additional complications on the solution of the piecewise quadratic equations (2.5) and (3.9). There are many options for order of accuracy vs directions in Steps 1–2, and in addition\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " 'it is not clear in which order to drop terms in Step 4 if negative terms are detected. Obviously, one can check all possibilities, but such an option may be costly in high dimensions. To simplify this we follow [31], and in Step 1 we use the second order approximation if the extra point is available in knownand fulﬁlls the condition (2.7), and revert to ﬁrst order approximation if not. Then, in Step 2 we determine the choice of directions considering the non-factored ﬁrst order approximation (2.5). That is, if (τ0τ1)i−1 < (τ0τ1)i+1, then we choose the backward upwind direction; otherwise we choose the forward direction. That is done correspondingly for each dimension.  \\n',\n",
       " 'Once Steps 1–2 are done, (3.9) reduces to a piecewise quadratic equation of the form  \\n',\n",
       " '?  \\n',\n",
       " 'max\\\\{ *αk(τij* −βk),0\\\\} 2 = κ( ? *xij)* 2 ,\\n',\n",
       " '\\n',\n",
       " '################# (3.11)\\n',\n",
       " '\\n',\n",
       " '*k*  \\n',\n",
       " 'where αk ≥ 0, βk ≥ 0 are non-negative constants that are coming from the ﬁnite difference approximations. For exam- ple, assuming that *k* = 1 corresponds to the *x* coordinate, then (3.10) would correspond to α1 = (τ0)i,j + (p0)i,j and *h*  \\n',\n",
       " 'β1 = (τ0)i,j(τ1)i−1,j . In Step 3 we simply ignore the max\\\\{·, 0\\\\} function and solve the equation assuming all terms are positive. *hα1*  \\n',\n",
       " 'We solve a simple quadratic function and choose the larger one of its two solutions for *τij.* If all chosen derivative terms are positive, the solution is valid; otherwise, we reduce the terms in (3.11) in decreasing order of βk, each time solving (3.11) with the remaining terms until a valid solution is reached. In three dimensions for example, this involves at most three quadratic solves. Algorithm 2 summarizes the solution of the piecewise quadratic equation.\\n',\n",
       " '\\n',\n",
       " '**Algorithm 2: Solution of the piecewise quadratic equation**  \\n',\n",
       " '**for** *each dimension x,y,...* **do**  \\n',\n",
       " '*% Choosing direction, forward or backward.*  \\n',\n",
       " '**if** *both forward and backward neighboring points are in* knownthen Choose the direction with smaller neighboring τ.\\n',\n",
       " '\\n',\n",
       " '### else\\n',\n",
       " '\\n',\n",
       " 'Otherwise, choose the direction in known.\\n',\n",
       " '\\n',\n",
       " '### end\\n',\n",
       " '\\n',\n",
       " '*% Choosing order of approximation, 1st or 2nd.* **if** *next neighboring point is in* knownthen Use second order approximation.\\n',\n",
       " '\\n',\n",
       " '### else\\n',\n",
       " '\\n',\n",
       " 'Use ﬁrst order approximation.\\n',\n",
       " '\\n',\n",
       " '### end\\n',\n",
       " '\\n',\n",
       " '### end\\n',\n",
       " '\\n',\n",
       " '*% Now all coeﬃcients of* αk *and* βk *of Equation* (3.11) *are known.*  \\n',\n",
       " 'Calculate (τ1)i,j by solving Equation (3.11).  \\n',\n",
       " '**while** *the solution* (τ1)i,j *is not valid* **do**  \\n',\n",
       " 'Remove the term with largest βk from the remaining terms in (3.11). Calculate (τ1)i,j by solving (3.11) with the remaining terms.\\n',\n",
       " '\\n',\n",
       " '### end\\n',\n",
       " '\\n',\n",
       " '*3.2. The monotonicity of the obtained solution*\\n',\n",
       " '\\n',\n",
       " 'It is known that the solution of (1.1) is monotone in the direction of the characteristics. We now show how to enforce the monotonicity of our solution using the FM method for the factored eikonal equation. To set the stage, we ﬁrst consider the FM method for the original non-factored equation.  \\n',\n",
       " 'In [30] the non-factored ﬁrst order discretization (2.5) is considered. In this case, each newly calculated value *τij* is guaranteed to be larger than its knownneighbors at the time of the calculation. To show this clearly, consider for example that the backward derivative is chosen in the *x* direction. Then,  \\n',\n",
       " '*τi−1,j* = *τi,j* −h *τi,j* − *τi−1,j* = *τi,j −hD* −x τ,\\n',\n",
       " '\\n',\n",
       " '################# (3.12)\\n',\n",
       " '\\n',\n",
       " '*h*\\n',\n",
       " '\\n',\n",
       " '*i,j*  \\n',\n",
       " 'so since *D* −x τ ≥ 0 in the solution of (2.5), we have that *τi,j* ≥ *τi−1,j.* This means that *τi,j* is greater or equal to its known *i,j*  \\n',\n",
       " 'neighbors. This property insures the monotonicity of the solution. The proof for this appears in [30], but here we can simplify it because unlike [30], we only calculate entries using knownvalues. We state the following lemma:\\n',\n",
       " '\\n',\n",
       " '**Lemma 1.** *Let* τ *be the result of Algorithm 1, for solving the (non-factored) ﬁrst order equation* (2.5). *Then the values of* τ *are mono- tonically non-decreasing in the order in which they are set to* known.\\n',\n",
       " '\\n',\n",
       " '**Proof.** Denote by ? *xk* an element that is set to knownat Step 2 of the k-th iteration of Algorithm 1. Assume by contradiction that there exists two elements ? *xp* and ? *xk,* such that τ( ? *xp)* > τ( ? *xk)* and *p* < *k.* Without loss of generality, assume that *k* is\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " 'the earliest iteration that this condition is fulﬁlled. Let *k* ¯ < *k* be the iteration in which the value of τ( ? *xk)* is updated in the last time and it is entered to front. We know that ? *x¯* is a neighbor of ? *xk.* By the algorithm, we know that at the k-th ¯ iteration ? *xp* is already set to known, otherwise ? *xk* would *k* have been chosen to knownat the p-th iteration instead of ? *xp.* By the assumption, we know that τ( ? *xp)* ≤ τ( ? *x¯* ), because otherwise ? *xk* would not have been the earliest element to violate the monotonicity. By the property in (3.12), we *k* know that τ( ? *x¯* ) ≤ τ( ? *xk),* and hence we reach τ( ? *xp)* ≤ τ( ? *xk),* which contradicts *k*  \\n',\n",
       " 'our assumption. ?\\n',\n",
       " '\\n',\n",
       " 'Furthermore, the lemma above can be extended for a Fast Marching solution of *any* equation (2.5) such that the dis- cretization operator *D* satisﬁes a monotonicity condition:\\n',\n",
       " '\\n',\n",
       " '# D −x τ ≥ 0 ⇒ τij ≥ τi−1,j and − D +x τ ≥ 0⇒ τij ≥ τi+1,j.\\n',\n",
       " '\\n',\n",
       " '################# (3.13)\\n',\n",
       " '\\n',\n",
       " '*ij*  ij*\\n',\n",
       " '\\n',\n",
       " 'The next corollary can be proved using the same arguments as in Lemma 1:\\n',\n",
       " '\\n',\n",
       " '**Corollary 1.** *Let* τ *be the result of Algorithm 1, for solving the Godunov upwind equation* (2.5) *using operators D which satisfy* (3.13). *Then the values of* τ *are monotonically non-decreasing in the order in which they are set to* known.\\n',\n",
       " '\\n',\n",
       " 'The condition (3.13) and the corollary above is violated when the second order operators (2.6) are used in (2.5). However, if we look at a single violation, then it is of order *h* 2 . To show this, we examine the backward difference derivative using Taylor expansion:\\n',\n",
       " '\\n',\n",
       " '### ? ?\\n',\n",
       " '\\n',\n",
       " '*τi−1,j* = *τi,j* −h ∂τ + *O(h* 2 ) = *τi,j −hD* −x τ + *O(h* 2 ),\\n',\n",
       " '\\n',\n",
       " '################# (3.14)\\n',\n",
       " '\\n',\n",
       " '∂x *i,j i,j*  \\n',\n",
       " '−x  \\n',\n",
       " 'where *D i,j* is given in (2.6) (the same arguments can be derived for the forward difference derivative). Assuming again that *D* −x τ > 0, this means that each newly calculated *τi,j* is generally greater than its knownneighbors, but may violate that up *i,j*  \\n',\n",
       " 'to magnitude *O(h* 2 ). Note that if *D* −x τ is suﬃciently bounded away from zero and the second derivative ∂ 2 τ is bounded in [xi−1, ], *i,j* ∂x 2 *xi* then *τi,j >τi−1,j* will be satisﬁed.  \\n',\n",
       " 'To correct this and obtain a monotone solution using (2.6), one may impose the condition (2.7) for using the second order scheme. If it is not satisﬁed, the scheme reverts to the ﬁrst order scheme, which satisﬁes (3.13). If (2.7) is satisﬁed, then (2.6) does satisfy (3.13), because\\n',\n",
       " '\\n',\n",
       " '*2hD* −x τ = *3τij* − *4τi−1,j* + *τi−2,j* < *3τij* − *3τi−1,j.*\\n',\n",
       " '\\n',\n",
       " '################# (3.15)\\n',\n",
       " '\\n',\n",
       " '*ij*\\n',\n",
       " '\\n',\n",
       " 'Note that the condition (2.7) is suggested in [31] but the monotonicity guarantee of the second order scheme is not exam- ined.  \\n',\n",
       " 'We now examine the monotonicity of the obtained factored solution τ0τ1 when using ﬁrst order operators in (3.9). Suppose that we are calculating (τ1)ij using the backward operator (3.10) in the *x* direction in (3.9). We again start with a Taylor expansion\\n',\n",
       " '\\n',\n",
       " '### ? ? ? ?? ? ? ?\\n',\n",
       " '\\n',\n",
       " '### (τ0τ1)i−1,j = (τ0)ij −h ∂τ0 + O(h 2 ) (τ1)ij −h ∂τ1 + O(h 2 )\\n',\n",
       " '\\n',\n",
       " '∂x ? *i,j* ?\\n',\n",
       " '\\n',\n",
       " '- ? ∂x *i,j*\\n',\n",
       " '\\n',\n",
       " '### = (τ0τ1)ij −h(τ0)ij ∂τ1 −h(τ1)ij ∂τ0 + O(h 2 )\\n',\n",
       " '\\n',\n",
       " '################# (3.16)\\n',\n",
       " '\\n',\n",
       " '∂x *i,j* ∂x *i,j*\\n',\n",
       " '\\n',\n",
       " '# = (τ0τ1)ij −h D ˆ −x τ1 + O(h 2 ),\\n',\n",
       " '\\n',\n",
       " '*ij*\\n',\n",
       " '\\n',\n",
       " '### ? ?\\n',\n",
       " '\\n',\n",
       " 'where the last equality is obtained by placing ∂τ1\\n',\n",
       " '\\n',\n",
       " '= (τ1)i,j −(τ1)i−1,j *+O(h).* This expansion shows that if the monotonicity ∂x *i,j h*  \\n',\n",
       " 'is not obtained, i.e., (τ0τ1)i,j −(τ0τ1)i−1,j < 0, then the non-factored derivative is negative, *D* −x (τ0τ1) < 0, while the factored *ij*  \\n',\n",
       " 'derivative is non-negative *D* ˆ −x τ1 ≥ 0 (otherwise it is not chosen in (3.9)). This means that the monotonicity may be violated *ij*  \\n',\n",
       " 'only up to an error of *O(h* 2 ). This holds for both ﬁrst and second order upwind approximations. In fact, (3.16) shows that this is a result of using the chain rule rather than the order of discretization of the operators *D,* ˆ since the monotonicity condition involves the value (τ0)i−1,j, while it does not appear in the discretization scheme. In any case, the magnitude of the error in the monotonicity violation is either of the same or of higher order as the error in τ1, using ﬁrst or second order schemes. Again, if *D* −x τ1 is suﬃciently bounded away from zero and ∂ 2 τ1 is bounded in [xi−1, *xi* ], then the monotonicity *i,j* ∂x 2  \\n',\n",
       " '(τ0τ1)i,j > (τ0τ1)i−1,j will be satisﬁed.  \\n',\n",
       " 'Nevertheless, in our algorithm we may enforce the monotonicity of the obtained solution by reverting to the non-factored operators in cases where the monotonicity is not satisﬁed, or, the factored and non-factored schemes do not agree in sign, for example: *D* ˆ −x τ1 ≥ 0, but *D* −x (τ0τ1) < 0. Note that in this case the numerical derivative is approximately zero, hence the *ij*  \\n',\n",
       " '*ij*  \\n',\n",
       " 'direction of the characteristic is almost parallel to the *y* direction. We apply this change using the same order of derivative which the algorithm chooses to use. That is, if the algorithm chooses a ﬁrst or second order factored stencil, we revert to a\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " 'standard ﬁrst or second order stencil, respectively. Following Corollary 1, this guarantees the monotonicity of the solution, because we enforce the condition (3.13) at all stages of the algorithm. We note that experimentally, this small correction does not inﬂuence the accuracy of the solution obtained with our algorithm in both ﬁrst and second order schemes in two and three dimensions.\\n',\n",
       " '\\n',\n",
       " '##### 4. Calculation of sensitivities and travel time tomography\\n',\n",
       " '\\n',\n",
       " 'Travel time tomography is a useful tool in some Geophysical applications. One way to obtain it is by using the eikonal equation as a forward problem inside an inverse problem [29]. To solve the inverse problem, one should be able to solve (1.1) accurately, and to compute its sensitivities. The works of [13,34] computes the tomography by FS, and require an FS iterative solution for computing the sensitivities. When using the FM algorithm for forward modeling, those are obtained more eﬃciently by a simple solution of a lower triangular linear system [14,3]. More explicitly, let us denote by boldface all the discretized values of the mentioned functions on a grid, and suppose that we set **m** to be the vector of the values of κ( ? *x)* 2 on this grid. By solving (3.9), we get a function τ1(m) for the values of τ1 on the grid. We wish to get a linearization for τ1(m), such that we can predict its change following a small change in **m.** That is, we wish to be able to apply an approximation\\n',\n",
       " '\\n',\n",
       " 'τ1(m+δm) ≈ τ1(m)+ **Jδm,** (4.17) where **J** is the sensitivity matrix (or Jacobian) deﬁned by\\n',\n",
       " '\\n',\n",
       " '*Jij* = (∇ *mτ1)ij* = ∂(τ1)i .\\n',\n",
       " '\\n',\n",
       " '################# (4.18) ∂mj\\n',\n",
       " '\\n',\n",
       " 'To obtain the sensitivity we ﬁrst rewrite (3.9) in implicit form\\n',\n",
       " '\\n',\n",
       " 'f(m,τ1) = ( **D** ˆ *x* τ1) 2 +( **D** ˆ *y* τ1) 2 − **m** = 0,\\n',\n",
       " '\\n',\n",
       " '################# (4.19)\\n',\n",
       " '\\n',\n",
       " 'where **D** ˆ *x* = diag(τ0)D *x* + diag(p0) and **D** ˆ *y* = diag(τ0) · **D** *y* + diag(q0) are the matrices that apply the ﬁnite difference derivatives that are chosen by the FM algorithm when applied for **m. p0** and **q0** are the analytical derivatives of τ0 with respect to *x* and *y* on the grid respectively, and diag(x) denotes a diagonal matrix whose diagonal elements are those of the vector **x.** We note that in the points where no derivative is chosen in the solution of (3.9), a zero row is set in the corresponding operator **D.** ˆ Also, at the row of the point source, we set each of **D** ˆ *x* and **D** ˆ *y* to have only one diagonal non-zero element, which equals to the values of **p0** and **q0** at the source. This way, (4.19) is exactly fulﬁlled for **D** ˆ *x* and **D** ˆ *y* and τ1.  \\n',\n",
       " 'To obtain the sensitivity, we apply the gradient operator to both sides of (4.19), yielding (∇ τ1f)(∇ **mτ1)** + ∇ **mf** = 0, and deﬁne [8]:\\n',\n",
       " '\\n',\n",
       " '### J(m) = ∇ mτ1 = −(∇ τ1f) −1 (∇ mf). (4.20)\\n',\n",
       " '\\n',\n",
       " 'This results in  \\n',\n",
       " '**J** = (diag(2 **D** ˆ *x* τ1) **D** ˆ *x* + diag(2 **D** ˆ *y* τ1) **D** ˆ *y* ) −1 ,\\n',\n",
       " '\\n',\n",
       " '################# (4.21)\\n',\n",
       " '\\n',\n",
       " 'following ∇ **mf** = −I, and since the operators **D** ˆ do not depend on **m** (we deﬁned τ0 and its derivatives so it does not depend on κ).  \\n',\n",
       " 'The matrix (4.21) can be multiplied with any vector eﬃciently given the order of variables (i, *j)* in which the FM algorithm set their values as known. To apply **J** on an arbitrary vector **x,** i.e. calculate **e** = **Jx,** a linear system **Ae** = **x** can be solved with **A** = **J** −1 (note that **A** is a sparse matrix). The equations of this linear system, which correspond to the rows of **A,** can be approached and solved sequentially in the FM order of variables. Since the FM algorithm uses only known variables for determining each new variable, then when looking at each row *i* of **A,** the non-zero entries in that row (except *i)* correspond to variables that where in knownwhen τ *i* was determined during the FM run. Therefore, if all those variables are known except *i,* then the i-th equation has only one unknown (ei) and can be trivially solved. In other words, if we permute **A** according to the FM order, we get a sparse lower triangular matrix, and the corresponding system can be solved eﬃciently in one forward substitution sweep in *O(n)* operations. For the non-factored equation one may use (4.21) with non-factored operators **D** *x* and **D** *y* instead of the factored ones [14].\\n',\n",
       " '\\n',\n",
       " '*4.1. Travel time tomography using Gauss–Newton*\\n',\n",
       " '\\n',\n",
       " 'Assume that we have several sources and receivers set on an open surface, and for each source we have traveltime data **d** *i*  \\n',\n",
       " 'given in the location of the receivers. Based on these observations we wish to compute the unknown slowness model obs  \\n',\n",
       " 'of the ground underneath. The inverse problem for this process, called travel time tomography, may be given by  \\n',\n",
       " '?\\n',\n",
       " '\\n',\n",
       " '?\\n',\n",
       " '\\n',\n",
       " '### ? ns\\n',\n",
       " '\\n',\n",
       " 'min φ(m) = min ?P ? τ *i* (m)− **d** *i* ? 2 + αR(m)\\n',\n",
       " '\\n',\n",
       " '################# (4.22)\\n',\n",
       " '\\n',\n",
       " '*mL<m<mH mL<m<mH* obs *i=1*\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " 'where\\n',\n",
       " '\\n',\n",
       " '# |∇ τ i | 2 = m( ? x) τ i ( ? xi) = 0 i = 1,...,ns\\n',\n",
       " '\\n',\n",
       " '################# (4.23)\\n',\n",
       " '\\n',\n",
       " 'Here τ *i* is the travel time from the point source ? *xi,* and **m(** ? *x)* = κ( ? *x)* 2 is the squared slowness model as in (1.1), only now it is unknown. The operator **P** ? is a projection to the set of receivers that gather the wave information. Here we assume that the information from all sources is available on all the receivers, i.e., the projection operator **P** does not change between sources. R(m) is a regularization term and α > 0 is its balancing parameter. The parameters *mL* and *mH* are positive lower and upper bounds needed for keeping the slowness of the medium physical. We note that the observations **d** *i* can be obs obtained manually from recorded seismic data or by automatic time picking—for more information see [28] and references therein.  \\n',\n",
       " 'Without the regularization term R(m), the problem (4.22) is ill-posed, i.e., many solutions **m** may ﬁt the predicted travel time to the measured data [37,32]. For this reason, in most cases we cannot expect to exactly recover the true model, but wish to recover a reasonable model by adding prior information using the regularization term R(m). This term aims to promote physical or meaningful solutions that we may expect to see in the recovered model. For example, in seismic exploration, one may expect to recover a layered model of the earth subsurface, hence may choose *R* to promote smooth or piecewise-smooth functions like the total variation regularization term [26].  \\n',\n",
       " 'There are several ways to solve (4.22), and most of them are gradient-based. Here we focus on Gauss–Newton. This method is computationally favorable here, since its cost is governed by the application of sensitivities, which are easy to obtain using FM. Given an approximation **m** (k) at the k-th iteration, we place (4.17) into (4.22) and get\\n',\n",
       " '\\n',\n",
       " '### 1 ? ns ? ? ?\\n',\n",
       " '\\n',\n",
       " '###### min ?P diag(τ i ) τ i (m (k) )+ J i δm − d i ? 2 + αR(m (k) +δm),\\n',\n",
       " '\\n',\n",
       " '################# (4.24)\\n',\n",
       " '\\n',\n",
       " 'δm 2 0 1 obs *i=1*\\n',\n",
       " '\\n',\n",
       " 'where **J** *i* is the sensitivity of τ *i* at **m** (k) . Minimizing this approximation for δm leads to computing the gradient 1\\n',\n",
       " '\\n',\n",
       " '- *ns* ? ?\\n',\n",
       " '\\n',\n",
       " '###### ∇ mφ(m (k) ) = (J i ) ? diag(τ i )P P ? diag(τ i )τ i − d i + α ∇ mR(m (k) ).\\n',\n",
       " '\\n',\n",
       " '################# (4.25)\\n',\n",
       " '\\n',\n",
       " '0 0 1 obs *i=1*\\n',\n",
       " '\\n',\n",
       " 'We then approximately solve the linear system\\n',\n",
       " '\\n',\n",
       " '**Hδm** = −∇ mφ(m (k) ) where\\n',\n",
       " '\\n',\n",
       " '### ? n\\n',\n",
       " '\\n',\n",
       " '#### H =\\n',\n",
       " '\\n',\n",
       " '###### (J i ) ? diag(τ i )PP ? diag(τ i )J i + α?mR(m (k) ).\\n',\n",
       " '\\n',\n",
       " '0  \\n',\n",
       " '0 *i* =1\\n',\n",
       " '\\n',\n",
       " 'The linear system is solved using the conjugate gradient method where only matrix vector products are computed. Finally, the model is updated, **m** ← **m** + μδm where μ ≤ 1 is a line search parameter that is chosen such that the objective function is decreased at each iteration.\\n',\n",
       " '\\n',\n",
       " '##### 5. Numerical results: solving the eikonal equation\\n',\n",
       " '\\n',\n",
       " 'In this section we demonstrate the FM algorithm using ﬁrst or second order upwind discretization for solving the fac- tored eikonal equation (1.4). We demonstrate both the accuracy of the obtained solution, and the computational cost of calculating it using the FM algorithm. The accuracy of the algorithm is demonstrated by two error norms: one in the maxi- mum norm *l∞,* and one is the mean *l2* norm deﬁned by the standard *l2* norm of the error divided by the square root of the total number of variables. Similarly to [31], we show these two measures to demonstrate the accuracy of the second order scheme. Showing the *l∞* norm of the error for this scheme may result in only ﬁrst order accuracy, because at some points our second order FM algorithm reverts to ﬁrst order operators, which may be picked by the *l∞* norm.  \\n',\n",
       " 'To demonstrate the eﬃciency of the computation, we measure the time in which the algorithm solves each test. We also show this timing in terms of work-units, where each work unit is deﬁned by the time that it takes to evaluate the equation (1.1) using given central difference gradient stencils (without memory allocation time). We note that the more reliable timings appear for the large scale examples.  \\n',\n",
       " 'We use analytical examples for media where there is a known analytical solution for a point source located at ? *x0.* The ﬁrst two appear in [6]. We show results for two and three dimensions. Our code is written in Julia language [4] version 0.4.5, and all our tests were calculated on a laptop machine using Windows 10 64 bit OS, with Intel core-i7 2.8 GHz CPU with 32 GB of RAM. Our code is publicly available in [https :/ /github .com /JuliaInv /FactoredEikonalFastMarching](https://github.com/JuliaInv/FactoredEikonalFastMarching.jl) .jl. We do not enforce the monotonicity in the results below, but those can be enforced in our package. The three test cases are listed below.\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0785.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 3.** The 2D slowness model κ(?x) of the three test cases and the corresponding contours of the 2D solutions.\\n',\n",
       " '\\n',\n",
       " '*Test case 1: constant gradient of squared slowness*\\n',\n",
       " '\\n',\n",
       " '### κ 2 ( ? x) = s 2 + 2a e1 ? ·( ? x−? x0),\\n',\n",
       " '\\n',\n",
       " '0\\n',\n",
       " '\\n',\n",
       " 'In this test case we set:\\n',\n",
       " '\\n',\n",
       " '################# (5.26)\\n',\n",
       " '\\n',\n",
       " 'where *e1* ? = (1, 0) is a unit vector, and · is the standard dot product. The parameters *a, s0,* the domain and the source location are chosen differently in 2D and 3D. The corresponding exact solution is given by  \\n',\n",
       " '*τexact(* ? *x)* = *S* ¯ 2 σ − 1 *a* 2 (σ 3 ),\\n',\n",
       " '\\n',\n",
       " '################# (5.27)\\n',\n",
       " '\\n',\n",
       " '6 where\\n',\n",
       " '\\n',\n",
       " '### S ¯ 2 ( ? x) = s 2 +a e1 ? ·( ? x−? x0)\\n',\n",
       " '\\n',\n",
       " '################# (5.28)\\n',\n",
       " '\\n',\n",
       " '- 0 ? ? −1\\n',\n",
       " '\\n',\n",
       " '### σ 2 ( ? x) = S ¯ 2 + S ¯ 4 −a 2?? x− x0 ? ?2 2?? x− x0 ? ? 2 .\\n',\n",
       " '\\n',\n",
       " '################# (5.29)\\n',\n",
       " '\\n',\n",
       " 'Figs. 3(a) and 3(d) show the model κ for this test case with the chosen parameters for 2D.\\n',\n",
       " '\\n',\n",
       " '*Test case 2: constant gradient of velocity*  \\n',\n",
       " 'In this test case we set:\\n',\n",
       " '\\n',\n",
       " '### ? ? −1\\n',\n",
       " '\\n',\n",
       " '### κ( ? x) = 1 +a e1 ? ·( ? x−? x0)\\n',\n",
       " '\\n',\n",
       " '################# (5.30)\\n',\n",
       " '\\n',\n",
       " '### s 0\\n',\n",
       " '\\n',\n",
       " 'where again *e1* ? = (1, 0), · is the dot product, and the parameters *a, s0,* the domain and the source location are chosen differently in 2D and 3D. The exact solution is given by  \\n',\n",
       " '?  \\n',\n",
       " '?  \\n',\n",
       " '*τexact(* ? *x)* = 1 acosh 1+ 1 *s0a* 2 κ( ? x)?? x−? *x0* ? 2 .\\n',\n",
       " '\\n',\n",
       " '################# (5.31)\\n',\n",
       " '\\n',\n",
       " '*a*  \\n',\n",
       " '2  \\n',\n",
       " 'Figs. 3(b) and 3(e) show the model κ for this test case with the chosen parameters for 2D.  \\n',\n",
       " '*Test case 3: Gaussian factor* In this test case we choose a function for τ *exact* and multiply it by τ0 to get *τexact.* We choose τ1 as a Gaussian function centered around a point *x1:* ?\\n',\n",
       " '\\n',\n",
       " '1 ?\\n',\n",
       " '\\n',\n",
       " '?\\n',\n",
       " '\\n',\n",
       " '# τ exact ( ? x) = 1 exp −( ? x−? x1) T ?( ? x−? x1) + 1\\n',\n",
       " '\\n',\n",
       " '################# (5.32)\\n',\n",
       " '\\n',\n",
       " '1 2 2  \\n',\n",
       " 'where ? is a 2 × 2 or 3 × 3 positive diagonal matrix. As before, the parameters ? *x1* and ?, the domain and the source location *x0* ? are chosen differently in 2D and 3D. Here κ( ? *x)* is deﬁned by (3.8), with τ0 being the distance function. Figs. 3(c) and 3(f) show the model κ for this test with the chosen parameters for 2D.\\n',\n",
       " '\\n',\n",
       " '*5.1. Two dimensional tests*\\n',\n",
       " '\\n',\n",
       " 'Now we show results for the two dimensional versions of the tests mentioned above. For all tests in 2D we choose the domain to be [0, 4] ×[0, 8], while *h* = *hx* = *hy* varies from large to small.\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '### Table 1\\n',\n",
       " '\\n',\n",
       " 'Results for 2D constant gradient of squared slowness (test case 1). The error measures are in the [l∞, mean *l2* ] norms.\\n',\n",
       " '\\n',\n",
       " '*h n* 1 *st* order\\n',\n",
       " '\\n',\n",
       " 'error in τ\\n',\n",
       " '\\n',\n",
       " '2 *nd* order\\n',\n",
       " '\\n',\n",
       " 'time (work) error in τ  \\n',\n",
       " 'time (work)\\n',\n",
       " '\\n',\n",
       " '1/40 161× 321  \\n',\n",
       " '[3.71e−03, 9.42e−04] 0.05 s (217) [9.33e−05, 9.26e−06] 0.05 s (202) 1/80 321× 641  \\n',\n",
       " '[1.85e−03, 4.69e−04] 0.19 s (199) [3.30e−05, 2.21e−06] 0.20 s (209) 1/160 641× 1281 [9.22e−04, 2.34e−04] 0.85 s (217) [1.14e−05, 5.32e−07] 0.85 s (218) 1/320 1281× 2561 [4.60e−04, 1.17e−04] 3.89 s (266) [4.06e−06, 1.28e−07] 3.84 s (262) 1/640 2561× 5121 [2.30e−04, 5.83e−05] 16.4 s (278) [1.47e−06, 3.12e−08] 17.1 s (289) 1/1280 5121× 10241 [1.15e−04, 2.92e−05] 76.6 s (316) [5.18e−07, 7.64e−09] 77.5 s (320)\\n',\n",
       " '\\n',\n",
       " '### Table 2\\n',\n",
       " '\\n',\n",
       " 'Results for the 2D constant gradient of velocity (test case 2). The error measures are in the [l∞, mean *l2* ] norms.\\n',\n",
       " '\\n',\n",
       " '*h n* 1 *st* order\\n',\n",
       " '\\n',\n",
       " 'error in τ\\n',\n",
       " '\\n',\n",
       " '2 *nd* order\\n',\n",
       " '\\n',\n",
       " 'time (work) error in τ  \\n',\n",
       " 'time (work)\\n',\n",
       " '\\n',\n",
       " '1/40 161× 321  \\n',\n",
       " '[2.66e−02, 1.01e−02] 0.05 s (205) [4.86e−04, 2.90e−04] 0.05 s (236) 1/80 321× 641  \\n',\n",
       " '[1.32e−02, 5.05e−03] 0.21 s (221) [1.67e−04, 7.38e−05]  \\n',\n",
       " '0.20 s (206) 1/160 641× 1281 [6.59e−03, 2.52e−03] 0.87 s (223) [5.18e−05, 1.85e−05] 0.86 s (221) 1/320 1281× 2561 [3.29e−03, 1.26e−03] 3.80 s (259) [1.90e−05, 4.61e−06] 3.88 s (265) 1/640 2561× 5121 [1.65e−03, 6.28e−04] 16.2 s (274) [6.58e−06, 1.15e−06] 16.6 s (280) 1/1280 5121× 10241 [8.22e−04, 3.14e−04] 73.8 s (304) [2.28e−06, 2.86e−07] 74.6 s (307)\\n',\n",
       " '\\n',\n",
       " '### Table 3\\n',\n",
       " '\\n',\n",
       " 'Results for the 2D Gaussian factor test case (test case 3). The error measures are in the [l∞, mean *l2* ] norms.\\n',\n",
       " '\\n',\n",
       " '*h n* 1 *st* order\\n',\n",
       " '\\n',\n",
       " 'error in τ\\n',\n",
       " '\\n',\n",
       " '2 *nd* order\\n',\n",
       " '\\n',\n",
       " 'time (work) error in τ  \\n',\n",
       " 'time (work)\\n',\n",
       " '\\n',\n",
       " '1/40 161× 321  \\n',\n",
       " '[6.15e−03, 3.86e−03] 0.05 s (205) [1.60e−04, 5.94e−05] 0.05 s (236) 1/80 321× 641  \\n',\n",
       " '[3.07e−03, 1.93e−03]  \\n',\n",
       " '0.21 s (221) [3.85e−05, 1.56e−05] 0.20 s (206) 1/160 641× 1281 [1.54e−03, 9.67e−04] 0.87 s (223) [1.08e−05, 4.03e−06] 0.86 s (221) 1/320 1281× 2561 [7.68e−04, 4.83e−04] 3.80 s (259) [3.18e−06, 1.04e−06] 3.88 s (265) 1/640 2561× 5121 [3.84e−04, 2.42e−04] 16.2 s (275) [9.59e−07, 2.66e−07] 16.6 s (280) 1/1280 5121× 10241 [1.92e−04, 1.21e−04] 73.8 s (304) [2.99e−07, 6.88e−08] 74.6 s (307)\\n',\n",
       " '\\n',\n",
       " '*Test case 1: constant gradient of squared slowness* For this 2D setting, we use the parameters *a* = −0.4, *s0* = 2.0, and the source location is ? *x0* = (0, 4). Table 1 summarizes the results for this test. On the ﬁrst order section we see a typical ﬁrst order convergence rate in both error norms. As the mesh size increases by two in each direction, the errors drop by a factor of two. In the second order section we see the typical behavior of the FM algorithm. At some points, ﬁrst order operators are used, and hence the error at those locations dominates the *l∞* norm. Still, we observe much better convergence compared to the ﬁrst order *l∞,* only it is not of second order. In the mean *l2* norm we see typical second order convergence—as the mesh size increases by two in each direction, the errors drop by a factor of four. In any case, the errors in the second order columns are much smaller than those in the ﬁrst order columns.  \\n',\n",
       " 'In terms of computational cost, the 2D FM algorithm exhibits favorable timings and work counts. Except the small cases, the cost of the algorithm is comparable to 200–300 function evaluations using standard difference stencils. This is maintained for all the considered mesh sizes. The difference in the computational cost between using ﬁrst and second order schemes is only about 10% of execution time.\\n',\n",
       " '\\n',\n",
       " '*Test case 2: constant gradient of velocity* For this 2D setting, we use the parameters *a* = 1.0, *s0* = 2.0, and the location of the source is again at ? *x0* = (0, 4). Table 2 summarizes the results for this test. The results here are almost identical to the previous test case. The ﬁrst order columns show typical ﬁrst order convergence in both error norms. The second order columns show better convergence and exhibits second order convergence in the mean *l2* norm column. The computational costs columns show timings which are almost identical to the previous test case.\\n',\n",
       " '\\n',\n",
       " '*Test case 3: Gaussian factor* For this setting, we use the parameters ? = diag(0.1, 0.4), ? *x1* = (4/3, 2) (ﬂoored to the closest grid point), and the source is located in the point ? *x0* = (1, 2). Table 3 summarizes the results for this test. Again we see ﬁrst order convergence at the ﬁrst order columns in both norms. On the second order columns we again see faster convergence, and in the mean *l2* norm column we see convergence rate that is close to second order—the error decreases by a factor of about 3.9 when the mesh size increases by a factor of 2 in each dimension. Again we see similar behavior in the computational cost columns.  \\n',\n",
       " 'We now wish to better illustrate the difference between the accuracy of the ﬁrst order scheme and the second order scheme. First, Fig. 4 shows contours of the exact and approximate solutions in certain regions of the domain for the second and third test cases. It is clear that the ﬁrst order approximation is less accurate than the second order one. Next, Fig. 5\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0478.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 4.** Contours of small regions in the exact, ﬁrst order accurate and second order accurate travel times τ using *h* = 0.1. The exact solution appears in black line. The ﬁrst order approximation appears in dotted red line and the second order approximation appears in a dashed blue line mostly right with the exact solution. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0479.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 5.** The accuracy of the FM approximations in logarithmic scales for the 2D cases. Red plots are used for ﬁrst order approximations, blue plots for second order approximations; dotted lines for *l∞* error norm and solid for mean *l2* norm. Black circles denote a reference for exact second order convergence rate. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)\\n',\n",
       " '\\n',\n",
       " 'shows plots of the errors in Tables 1–3 in logarithmic scales for both *h* and the error norms, where the order of conver- gence determines the slope of the lines. It is clear that in all cases, using the second order scheme we get second order convergence in mean *l2* norm, and a bit more than ﬁrst order convergence in the *l∞* norm.\\n',\n",
       " '\\n',\n",
       " '*5.2. Three dimensional tests*\\n',\n",
       " '\\n',\n",
       " 'We now show results for the same type of tests in three dimensions. For all the 3D tests we choose the domain to be [0, 0.8] ×[0, 1.6] ×[0, 1.6], and *h* = *hx* = *hy* = *hz* varies.\\n',\n",
       " '\\n',\n",
       " '*Test case 1: constant gradient of squared slowness* For the 3D version of this test case we use the parameters *a* = −1.65, and *s0* = 2.0, and the source is located at (0, 0.8, 0.8). Table 4 summarizes the results for this test case. Again, like in two dimensions, the ﬁrst order version of FM yields ﬁrst order convergence rate in both error norms. When using the second order scheme we get a super-linear convergence rate in the *l∞* column, and second order convergence in the mean *l2* column. We note that in 3D the FM algorithm reverts to ﬁrst order scheme on 2D manifolds, where the derivative in each dimension switches sign, and not on 1D curves as in 2D.  \\n',\n",
       " 'In terms of computational cost, it is obvious that the 3D problem is much more expensive than the 2D one. The compu- tational cost in seconds per grid-point in 3D is about 3 times higher than the corresponding cost in 2D. That is because the treatment of each grid point is more expensive (more neighbors and more derivative directions), and the number of grid points that are processed inside the heap is much larger (a 2D manifold of points compared to a 1D curve). As a result, when we normalize the timing by the cost of a 3D “work-unit” (evaluation of (1.1) in 3D), the cost grows a little when the mesh-size grows. Still, solving the problem requires 200–500 work units. Again, using the ﬁrst and second order schemes requires similar computational effort in our 3D implementation of the FM algorithm.\\n',\n",
       " '\\n',\n",
       " '*Test case 2: constant gradient of velocity*\\n',\n",
       " '\\n',\n",
       " 'For the 3D version of this test case we use the parameters *a* = 1.0, and *s0* = 2.0, and the source is located at (0, 0.8, 0.8). Table 5 summarizes the results for this test case. As in the previous case, we get ﬁrst order convergence when using the ﬁrst order scheme, in both error norms. Again, when using the second order scheme we get a super-linear convergence rate in the *l∞* column, and second order convergence in the mean *l2* column.\\n',\n",
       " '\\n',\n",
       " '*Test case 3: Gaussian factor* For this 3D test case we use the parameters ? = diag(0.2, 0.4, 0.1), ? *x1* = (0.4, 1.6 , 0.4) (ﬂoored to the closest grid point), and the source is located in the point ? *x0* = (0.2, 0.4, 0.4). Table 6 summarizes the 3 results for this\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '### Table 4\\n',\n",
       " '\\n',\n",
       " 'Results for the 3D constant gradient of squared slowness test case (test case 1). The error measures are in the [l∞, mean *l2* ] norms. *h n* 1 *st* order  \\n',\n",
       " 'error in τ\\n',\n",
       " '\\n',\n",
       " '2 *nd* order  \\n',\n",
       " 'time (work) error in τ  \\n',\n",
       " 'time (work) 1/20 17× 33× 33  \\n',\n",
       " '[5.41e−03, 1.46e−03] 0.04 s (236) [5.63e−4, 1.49e−04]  \\n',\n",
       " '0.04 s (234) 1/40 33× 65× 65  \\n',\n",
       " '[2.64e−03, 7.05e−04] 0.30 s (230) [2.00e−04, 3.52e−05] 0.32 s (235) 1/80 65× 129× 129 [1.30e−03, 3.46e−04] 2.88 s (332) [6.99e−05, 7.82e−06] 2.90 s (334) 1/160 129× 257× 257 [6.41e−04, 1.72e−04] 28.7 s (427) [2.51e−05, 1.68e−06] 29.0 s (432) 1/320 257× 513× 513 [3.19e−04, 8.55e−05] 264 s (481) [8.78e−06, 3.53e−07] 272 s (497)\\n',\n",
       " '\\n',\n",
       " '### Table 5\\n',\n",
       " '\\n',\n",
       " 'Results for the 3D constant gradient of velocity test case (test case 2). The error measures are in the [l∞, mean *l2* ] norms. *h n* 1 *st* order  \\n',\n",
       " 'error in τ  \\n',\n",
       " '1/20 17× 33× 33  \\n',\n",
       " '[1.35e−02, 5.04e−03]\\n',\n",
       " '\\n',\n",
       " '2 *nd* order  \\n',\n",
       " 'time (work) error in τ  \\n',\n",
       " 'time (work) 0.04 s (237) [2.34e−03, 9.36e−04] 0.04 s (255) 1/40 33× 65× 65  \\n',\n",
       " '[6.24e−03, 2.44e−03] 0.31 s (234) [5.12e−04, 1.72e−04]  \\n',\n",
       " '0.32 s (236) 1/80 65× 129× 129 [3.00e−03, 1.20e−03] 2.86 s (330) [1.70e−04, 3.82e−05] 2.89 s (334) 1/160 129× 257× 257 [1.47e−03, 5.99e−04] 27.6 s (411) [5.42e−05, 9.33e−06] 28.9 s (430) 1/320 257× 513× 513 [7.30e−04, 2.99e−04] 263 s (481) [1.95e−05, 2.29e−06] 271 s (496)\\n',\n",
       " '\\n',\n",
       " '### Table 6\\n',\n",
       " '\\n',\n",
       " 'Results for the 3D Gaussian factor test case (test case 3). The error measures are in the [l∞, mean *l2* ] norms. *h n* 1 *st* order  \\n',\n",
       " 'error in τ\\n',\n",
       " '\\n',\n",
       " '2 *nd* order  \\n',\n",
       " 'time (work) error in τ  \\n',\n",
       " 'time (work) 1/20 17× 33× 33  \\n',\n",
       " '[7.53e−03, 3.26e−03] 0.04 s (230) [3.65e−04, 1.27e−04]  \\n',\n",
       " '0.04 s (229) 1/40 33× 65× 65  \\n',\n",
       " '[3.69e−03, 1.56e−03] 0.33 s (245) [9.95e−05, 2.85e−05] 0.34 s (253) 1/80 65× 129× 129 [1.83e−03, 7.62e−04] 2.77 s (319) [3.22e−05, 7.50e−06] 2.80 s (323) 1/160 129× 257× 257 [9.11e−04, 3.77e−04] 26.4 s (393) [1.06e−05, 2.06e−06] 27.2 s (405) 1/320 257× 513× 513 [4.54e−04, 1.87e−04] 267 s (487) [3.54e−06, 5.66e−07] 276 s (504)\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0468.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 6.** The accuracy of the FM approximations in logarithmic scales for the 3D cases. Red and blue plots are used for ﬁrst and second order approximations, respectively; dotted lines for *l∞* error norm and solid for mean *l2* norm. Black circles denote an exact second order convergence rate. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)\\n',\n",
       " '\\n',\n",
       " 'test case. The results are similar to the previous test case in both the convergence (ﬁrst/second order using *l∞/l2* norms) and computational costs in seconds and work units.  \\n',\n",
       " 'Again we wish to demonstrate the order accuracy of the FM approximations using the ﬁrst and second order schemes. Fig. 6 shows the results in Tables 4–6 in logarithmic scales. Like in 2D, we observe second order convergence rate when the error is measure in mean *l2* norm. However, because the second order stencil reduces to ﬁrst order stencil in two dimensional manifolds, the error in *l∞* norm is higher in 3D than it is in 2D.\\n',\n",
       " '\\n',\n",
       " '##### 6. Numerical results: travel time tomography\\n',\n",
       " '\\n',\n",
       " 'In this section we demonstrate a solution of travel time tomography using synthetic travel time data dobs for a 2D and SEG/EAGE salt model given in [1] and presented in Fig. 7(a), using a 256 ×128 grid that represents an area of approximately 13.5 km × 4.2 km. We choose 51 equally distanced sources locations on the open surface (that is, they are located every 5 pixels on the top row), and 256 receivers (located in every pixel on the top row). We note that to have a reasonable solution using the ﬁrst arrivals for the inverse problem under this setup, the velocity in the interior has to be larger than\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0459.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 7.** 2D travel time tomography experiment, grid size 256× 128. Velocities are given in km/sec.\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0460.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 8.** Initial and ﬁnal data and residuals in the source-receiver domain.\\n',\n",
       " '\\n',\n",
       " 'that on the surface. This is to guarantee that the ﬁrst arrival rays obtained on the surface actually come from the interior but not only travel along the surface. To dobs we add white Gaussian noise with standard deviation of 0.01 × mean(\\\\|dobs \\\\|). To ﬁt the model to the data, we minimize (4.22) using Gauss–Newton (we perform 10 iterations, where in each we apply 8 CG steps for the Gauss–Newton direction problem). We use the general-propose inversion package [27], which is freely available in [https :/ /github .com /JuliaInv /jInv.jl,](https://github.com/JuliaInv/jInv.jl) together with our FM package mentioned earlier. For that, we ﬁrst generate an initial slowness model **m** (0) = *mref* , whose velocity model shown in Fig. 7(b). This corresponds to a velocity ﬁeld with a constant gradient in the *y* direction, similarly to the model in Fig. 3(b). To bound **m** from above and from below throughout the minimization, we invert for an auxiliary variable **m** ? and use the following scalar bounded bijective mapping that prevents **m** from being below *mL* or above *mH:*\\n',\n",
       " '\\n',\n",
       " '### ? ? ? ?\\n',\n",
       " '\\n',\n",
       " '*mbound(m* ? ) = *mH* −mL · tanh  \\n',\n",
       " '2\\n',\n",
       " '\\n',\n",
       " '### · m ? − mH +mL + 1 +mL.\\n',\n",
       " '\\n',\n",
       " '2 *mH* −mL 2\\n',\n",
       " '\\n',\n",
       " 'That is, instead of minimizing (4.22) as is, we minimize ?\\n',\n",
       " '\\n',\n",
       " '?\\n',\n",
       " '\\n',\n",
       " '#### ? ns\\n',\n",
       " '\\n',\n",
       " '### min φ(m ? ) = min ?P ? τ i (mbound(m ? ))− d i ? 2 + αR(m ? )\\n',\n",
       " '\\n',\n",
       " '################# (6.33)\\n',\n",
       " '\\n',\n",
       " '**m** ? **m** ? *i=1* obs\\n',\n",
       " '\\n',\n",
       " 'subject to the same constraints in (4.23). For the regularization *R* use a simple discrete central-differences Laplacian, and apply it for **m** ? ; that is\\n',\n",
       " '\\n',\n",
       " '*R(m* ? ) = 1 (m ? − **m** ?\\n',\n",
       " '\\n',\n",
       " '### ) ? ?h(m ? − m ? ),\\n',\n",
       " '\\n',\n",
       " '2 *ref ref*  \\n',\n",
       " 'where **m** ?  \\n',\n",
       " 'is the model such that *mbound(m* ? ) = *mref* . For ?h we use Neumann boundary conditions, since those lead to *r ef ref*  \\n',\n",
       " 'an effect of an automatic salt ﬂooding, which is a popular way to treat salt bodies. We set the regularization parameter to be α = 0.5. We note that other choices of *mref* and regularization terms may deﬁnitely be suitable here, but are beyond the scope of this paper. Fig. 7(c) shows the result model of the Gauss–Newton minimization, and Fig. 8 presents the initial and ﬁnal data and data residuals. In particular, Fig. 8(d) shows that the ﬁnal residual mostly contains the added Gaussian noise. Fig. 9 shows that the misﬁt was indeed reduced throughout the iterations, until the reduction stalls and the misﬁt reﬂect the noise level.  \\n',\n",
       " 'To demonstrate our algorithm in 3D, we use a 3D version of the same SEG/EAGE model, presented in Fig. 10(a), using a 256 × 256 × 128 grid that represents a volume of 13.5 km × 13.5 km × 4.2 km. We choose 144 equally distanced sources locations on the open surface, located every 23 pixels on the top surface, and 256 ×256 receivers located on the top surface.\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0439.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 9.** Convergence history of the inversion.\\n',\n",
       " '\\n',\n",
       " '![](assets_j/img-0440.jpg)\\n',\n",
       " '\\n',\n",
       " '**Fig. 10.** 3D travel time tomography experiment, grid size 256× 256× 128. Velocities are given in km/sec.\\n',\n",
       " '\\n',\n",
       " 'We use the same parameters as in the 2D experiment (bound function, regularization, initial 3D model, added noise to the data, number of iterations etc.). Fig. 10(b) shows the result of the inversion. Similarly to the 2D case, the top part of the model is recovered quite well, while a “salt ﬂooding” effect is evident in the bottom part of the model. We performed the inversion using a machine with two Intel(R) Xeon(R) E5-2670 v3 processors with 128 GB of RAM. Using 24 cores, we applied the inversion in approximately 15 hours, and the highest memory footprint of the algorithm reached around 30 GB.\\n',\n",
       " '\\n',\n",
       " '##### 7. Conclusions\\n',\n",
       " '\\n',\n",
       " 'In this paper we developed a Fast Marching algorithm for the factored eikonal equation, which in many cases yields a more accurate solution of the travel time than the original equation. Similarly to the original FM algorithm, our version solves the factored problem by exploiting the monotonicity of the solution along the characteristics. Our algorithm is capable of solving the problem using both ﬁrst and second order schemes. The advantages of our algorithm are (1) its favorable guaranteed *O(n* logn) running time, and (2) the easily computed sensitivity matrices for solving the inverse (factored) eikonal equation.\\n',\n",
       " '\\n',\n",
       " '##### Acknowledgements\\n',\n",
       " '\\n',\n",
       " 'The research leading to these results has received funding from the European Union’s – Seventh Framework Programme (FP7/2007-2013) under grant agreement no 623212 – MC Multiscale Inversion.\\n',\n",
       " '\\n',\n",
       " '##### References\\n',\n",
       " '\\n',\n",
       " '[1] [F. Aminzadeh, B. Jean, T. Kunz, 3-D Salt and Overthrust Models, Society of Exploration Geophysicists, 1997.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib616D696E7A616465683139393733s1)  \\n',\n",
       " '[2] [J.A. Bærentzen, On the implementation of fast marching methods for 3D lattices, Tech. Report IMM-TR-2001-13, Informatics and Mathematical Mod-  \\n',\n",
       " 'elling, Technical University of Denmark, DTU, 2001, Richard Petersens Plads, Building 321, DK- 2800 Kgs. Lyngby.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib494D4D323030312D30383431s1)   \\n',\n",
       " '[3] [A. Benaichouche, M. Noble, A. Gesret, First arrival traveltime tomography using the fast marching method and the adjoint state technique, in: 77th  \\n',\n",
       " 'EAGE Conference Proceedings, 2015.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib62656E616963686F75636865323031356669727374s1)   \\n',\n",
       " '[4] [J. Bezanzon, S. Karpinski, V. Shah, A. Edelman, Julia: a fast dynamic language for technical computing, in: Lang. NEXT, Apr. 2012.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib4A756C6961s1)  \\n',\n",
       " '[5] [M.G. Crandall, P.-L. Lions, Viscosity solutions of Hamilton–Jacobi equations, Trans. Am. Math. Soc. 277 (1983) 1–42.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6372616E64616C6C31393833766973636F73697479s1)  \\n',\n",
       " '[6] [S. Fomel, S. Luo, H. Zhao, Fast sweeping method for the factored eikonal equation, J. Comput. Phys. 228 (2009) 6440–6455.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib666F6D656C3230303966617374s1)  \\n',\n",
       " '[7] [P.A. Gremaud, C.M. Kuster, Computational study of fast methods for the eikonal equation, SIAM J. Sci. Comput. 27 (2006) 1803–1816.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6772656D61756432303036636F6D7075746174696F6E616Cs1)  \\n',\n",
       " '[8] [E. Haber, Computational Methods in Geophysical Electromagnetics, vol. 1, SIAM, 2014.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib686162657232303134636F6D7075746174696F6E616Cs1)  \\n',\n",
       " '[9] [E. Haber, S. MacLachlan, A fast method for the solution of the Helmholtz equation, J. Comput. Phys. 230 (2011) 4403–4418.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib68616265723230313166617374s1)  \\n',\n",
       " '[10] [C.Y. Kao, S. Osher, J. Qian, Lax–Friedrichs sweeping scheme for static Hamilton–Jacobi equations, J. Comput. Phys. 196 (2004) 367–391.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6B616F323030346C6178s1)\\n',\n",
       " '\\n',\n",
       " '---\\n',\n",
       " '\\n',\n",
       " '[11] [R. Kimmel, J.A. Sethian, Computing geodesic paths on manifolds, Proc. Natl. Acad. Sci. 95 (1998) 8431–8435.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6B696D6D656C31393938636F6D707574696E67s1)  \\n',\n",
       " '[12] [S. Leung, J. Qian, R. Burridge, Eulerian Gaussian beams for high-frequency wave propagation, Geophysics 72 (2007) SM61–SM76.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6C65756E673230303765756C657269616Es1)  \\n',\n",
       " '[13] [S. Leung, J. Qian, et al., An adjoint state method for three-dimensional transmission traveltime tomography using ﬁrst-arrivals, Commun. Math. Sci. 4  \\n',\n",
       " '(2006) 249–266.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6C65756E673230303661646A6F696E74s1)   \\n',\n",
       " '[14] [S. Li, A. Vladimirsky, S. Fomel, First-break traveltime tomography with the double-square-root eikonal equation, Geophysics 78 (2013) U89–U101.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6C69323031336669727374s1)  \\n',\n",
       " '[15] [S. Luo, J. Qian, Factored singularities and high-order Lax–Friedrichs sweeping schemes for point-source traveltimes and amplitudes, J. Comput. Phys.  \\n',\n",
       " '230 (2011) 4742–4755.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6C756F32303131666163746F726564s1)   \\n',\n",
       " '[16] [S. Luo, J. Qian, Fast sweeping methods for factored anisotropic eikonal equations: multiplicative and additive factors, J. Sci. Comput. 52 (2012) 360–382.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6C756F3230313266617374s1) [17] [S. Luo, J. Qian, R. Burridge, Fast Huygens sweeping methods for Helmholtz equations in inhomogeneous media in the high frequency regime, J. Comput.  \\n',\n",
       " 'Phys. 270 (2014) 378–401.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6C756F3230313466617374s1)   \\n',\n",
       " '[18] [S. Luo, J. Qian, R. Burridge, High-order factorization based high-order hybrid fast sweeping methods for point-source eikonal equations, SIAM J. Numer.  \\n',\n",
       " 'Anal. 52 (2014) 23–44.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib4C6F755169616E427572726964676532303134s1)   \\n',\n",
       " '[19] [S. Luo, J. Qian, H. Zhao, Higher-order schemes for 3d ﬁrst-arrival traveltimes and amplitudes, Geophysics 77 (2012) T47–T56.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6C756F32303132686967686572s1)  \\n',\n",
       " '[20] [M. Noble, A. Gesret, N. Belayouni, Accurate 3-d ﬁnite difference computation of traveltimes in strongly heterogeneous media, Geophys. J. Int. 199  \\n',\n",
       " '(2014) 1572–1585.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib6E6F626C65323031346163637572617465s1)   \\n',\n",
       " '[21] [A. Pica, et al., Fast and accurate ﬁnite-difference solutions of the 3d eikonal equation parametrized in celerity, in: 67th Ann. Internat. Mtg, Soc. of Expl.  \\n',\n",
       " 'Geophys, 1997, pp. 1774–1777.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib706963613139393766617374s1)   \\n',\n",
       " '[22] [J. Qian, W.W. Symes, An adaptive ﬁnite-difference method for traveltimes and amplitudes, Geophysics 67 (2002) 167–176.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7169616E323030326164617074697665s1)  \\n',\n",
       " '[23] [J. Qian, Y.-T. Zhang, H.-K. Zhao, A fast sweeping method for static convex Hamilton–Jacobi equations, J. Sci. Comput. 31 (2007) 237–271.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7169616E3230303766617374s1)  \\n',\n",
       " '[24] [N. Rawlinson, M. Sambridge, Wave front evolution in strongly heterogeneous layered media using the fast marching method, Geophys. J. Int. 156 (2004)  \\n',\n",
       " '631–647.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7261776C696E736F6E3230303477617665s1)   \\n',\n",
       " '[25] [E. Rouy, A. Tourin, A viscosity solutions approach to shape-from-shading, SIAM J. Numer. Anal. 29 (1992) 867–884.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib726F757931393932766973636F73697479s1)  \\n',\n",
       " '[26] [L.I. Rudin, S. Osher, E. Fatemi, Nonlinear total variation based noise removal algorithms, Physica D 60 (1992) 259–268.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib727564696E313939326E6F6E6C696E656172s1)  \\n',\n",
       " '[27] L. Ruthotto, E. Treister, E. Haber, jInv – a ﬂexible Julia package for PDE parameter estimation, 2016, submitted for publication.  \\n',\n",
       " '[28] [C. Saragiotis, T. Alkhalifah, S. Fomel, Automatic traveltime picking using instantaneous traveltime, Geophysics 78 (2013) T53–T58.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7361726167696F746973323031336175746F6D61746963s1)  \\n',\n",
       " '[29] [A. Sei, W.W. Symes, et al., Gradient calculation of the traveltime cost function without ray tracing, in: 65th Ann. Internat. Mtg., Expanded Abstracts,  \\n',\n",
       " 'Soc. Expl. Geophys, 1994, pp. 1351–1354.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib736569313939346772616469656E74s1)   \\n',\n",
       " '[30] [J.A. Sethian, A fast marching level set method for monotonically advancing fronts, Proc. Natl. Acad. Sci. 93 (1996) 1591–1595.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7365746869616E3139393666617374s1)  \\n',\n",
       " '[31] [J.A. Sethian, Fast marching methods, SIAM Rev. 41 (1999) 199–235.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7365746869616E3139393966617374s1)  \\n',\n",
       " '[32] [E. Somersalo, J. Kaipio, Statistical and Computational Inverse Problems, Appl. Math. Sci., vol. 160, 2004.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib736F6D657273616C6F32303034737461746973746963616Cs1)  \\n',\n",
       " '[33] [A. Spira, R. Kimmel, An eﬃcient solution to the eikonal equation on parametric manifolds, Interfaces Free Bound. 6 (2004) 315–328.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib737069726132303034656666696369656E74s1)  \\n',\n",
       " '[34] [C. Taillandier, M. Noble, H. Chauris, H. Calandra, First-arrival traveltime tomography based on the adjoint-state method, Geophysics 74 (2009) WCB1–  \\n',\n",
       " 'WCB10.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7461696C6C616E64696572323030396669727374s1)   \\n',\n",
       " '[35] [Y.-H.R. Tsai, L.-T. Cheng, S. Osher, H.-K. Zhao, Fast sweeping algorithms for a class of Hamilton–Jacobi equations, SIAM J. Numer. Anal. 41 (2003)  \\n',\n",
       " '673–694.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib747361693230303366617374s1)   \\n',\n",
       " '[36] [J.N. Tsitsiklis, Eﬃcient algorithms for globally optimal trajectories, IEEE Trans. Autom. Control 40 (1995) 1528–1538.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7473697473696B6C697331393935656666696369656E74s1)  \\n',\n",
       " '[37] [C.R. Vogel, Computational Methods for Inverse Problems, Frontiers Appl. Math., vol. 23, SIAM, Philadelphia, 2002.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib766F67656C32303032636F6D7075746174696F6E616Cs1)  \\n',\n",
       " '[38] [H. Zhao, A fast sweeping method for eikonal equations, Math. Comput. 74 (2005) 603–627.](http://refhub.elsevier.com/S0021-9991 16 30355-2/bib7A68616F3230303566617374s1)\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16884c3f-12b8-49a1-9f62-3e3ca1f193ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '[', ']', '(', 'assets_j', '/', 'img-0147.jpg', ')', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0148.jpg', ')', '\\n\\n', 'Contents', 'lists', 'available', 'at', '[', 'ScienceDirect](http://www', '.', 'ScienceDirect.com/', ')', 'Journal', 'of', 'Computational', 'Physics', '\\n\\n', '#', '#', '#', 'www.elsevier.com/locate/jcp', '\\n\\n', 'A', 'fast', 'marching', 'algorithm', 'for', 'the', 'factored', 'eikonal', 'equation', 'Eran', 'Treister', 'a,∗', 'Eldad', 'Haber', 'a', ',', 'b', ' \\n', 'a', '*', 'Department', 'of', 'Earth', 'and', 'Ocean', 'Sciences', ',', 'The', 'University', 'of', 'British', 'Columbia', ',', 'Vancouver', ',', 'BC', ',', 'Canada', '*', ' \\n', 'b', '*', 'Department', 'of', 'Mathematics', ',', 'The', 'University', 'of', 'British', 'Columbia', ',', 'Vancouver', ',', 'BC', ',', 'Canada', '*', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0149.jpg', ')', '\\n\\n', '#', '#', '#', 'a', 'r', 't', 'i', 'c', 'l', 'e', 'i', 'n', 'f', 'o', '\\n\\n', '*', 'Article', 'history', ':*', ' \\n', 'Received', '14', 'April', '2016', ' \\n', 'Received', 'in', 'revised', 'form', '5', 'August', '2016', 'Accepted', '8', 'August', '2016', ' \\n', 'Available', 'online', '12', 'August', '2016', '*', 'Keywords', ':*', ' \\n', 'Eikonal', 'equation', ' \\n', 'Factored', 'eikonal', 'equation', ' \\n', 'Fast', 'Marching', ' \\n', 'First', 'arrival', ' \\n', 'Travel', 'time', 'tomography', ' \\n', 'Gauss', '–', 'Newton', ' \\n', 'Seismic', 'imaging', '\\n\\n', '#', '#', '#', '#', '#', '1', '.', 'Introduction', '\\n\\n', 'a', 'b', 's', 't', 'r', 'a', 'c', 't', '\\n\\n', 'The', 'eikonal', 'equation', 'is', 'instrumental', 'in', 'many', 'applications', 'in', 'several', 'ﬁelds', 'ranging', 'from', 'computer', 'vision', 'to', 'geoscience', '.', 'This', 'equation', 'can', 'be', 'eﬃciently', 'solved', 'using', 'the', 'iterative', 'Fast', 'Sweeping', '(', 'FS', ')', 'methods', 'and', 'the', 'direct', 'Fast', 'Marching', '(', 'FM', ')', 'methods', '.', 'However', ',', 'when', 'used', 'for', 'a', 'point', 'source', ',', 'the', 'original', 'eikonal', 'equation', 'is', 'known', 'to', 'yield', 'inaccurate', 'numerical', 'solutions', ',', 'because', 'of', 'a', 'singularity', 'at', 'the', 'source', '.', 'In', 'this', 'case', ',', 'the', 'factored', 'eikonal', 'equation', 'is', 'often', 'preferred', ',', 'and', 'is', 'known', 'to', 'yield', 'a', 'more', 'accurate', 'numerical', 'solution', '.', 'One', 'application', 'that', 'requires', 'the', 'solution', 'of', 'the', 'eikonal', 'equation', 'for', 'point', 'sources', 'is', 'travel', 'time', 'tomography', '.', 'This', 'inverse', 'problem', 'may', 'be', 'formulated', 'using', 'the', 'eikonal', 'equation', 'as', 'a', 'forward', 'problem', '.', 'While', 'this', 'problem', 'has', 'been', 'solved', 'using', 'FS', 'in', 'the', 'past', ',', 'the', 'more', 'recent', 'choice', 'for', 'applying', 'it', 'involves', 'FM', 'methods', 'because', 'of', 'the', 'eﬃciency', 'in', 'which', 'sensitivities', 'can', 'be', 'obtained', 'using', 'them', '.', 'However', ',', 'while', 'several', 'FS', 'methods', 'are', 'available', 'for', 'solving', 'the', 'factored', 'equation', ',', 'the', 'FM', 'method', 'is', 'available', 'only', 'for', 'the', 'original', 'eikonal', 'equation', '.', 'In', 'this', 'paper', 'we', 'develop', 'a', 'Fast', 'Marching', 'algorithm', 'for', 'the', 'factored', 'eikonal', 'equation', ',', 'using', 'both', 'ﬁrst', 'and', 'second', 'order', 'ﬁnite', '-', 'difference', 'schemes', '.', 'Our', 'algorithm', 'follows', 'the', 'same', 'lines', 'as', 'the', 'original', 'FM', 'algorithm', 'and', 'requires', 'the', 'same', 'computational', 'effort', '.', 'In', 'addition', ',', 'we', 'show', 'how', 'to', 'obtain', 'sensitivities', 'using', 'this', 'FM', 'method', 'and', 'apply', 'travel', 'time', 'tomography', ',', 'formulated', 'as', 'an', 'inverse', 'factored', 'eikonal', 'equation', '.', 'Numerical', 'results', 'in', 'two', 'and', 'three', 'dimensions', 'show', 'that', 'our', 'algorithm', 'solves', 'the', 'factored', 'eikonal', 'equation', 'eﬃciently', ',', 'and', 'demonstrate', 'the', 'achieved', 'accuracy', 'for', 'computing', 'the', 'travel', 'time', '.', 'We', 'also', 'demonstrate', 'a', 'recovery', 'of', 'a', '2D', 'and', '3D', 'heterogeneous', 'medium', 'by', 'travel', 'time', 'tomography', 'using', 'the', 'eikonal', 'equation', 'for', 'forward', 'modeling', 'and', 'inversion', 'by', 'Gauss', '–', 'Newton', '.', ' \\n', '©', '2016', 'Elsevier', 'Inc.', 'All', 'rights', 'reserved', '.', '\\n\\n', 'The', 'eikonal', 'equation', 'appears', 'in', 'many', 'ﬁelds', ',', 'ranging', 'from', 'computer', 'vision', '[', '30,31,33,11', ']', ',', 'where', 'it', 'is', 'used', 'to', 'track', 'evolution', 'of', 'interfaces', ',', 'to', 'geoscience', '[', '19,12,22,14,24', ']', 'where', 'it', 'describes', 'the', 'propagation', 'of', 'the', 'ﬁrst', 'arrival', 'of', 'a', 'wave', 'in', 'a', 'medium', '.', 'The', 'equation', 'has', 'the', 'form', '\\n\\n', '#', '|∇', 'τ', '|', '2', '=', 'κ', '(', '?', 'x', ')', '2', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '1.1', ')', '\\n\\n', 'where', '\\\\|', '·', '\\\\|', 'is', 'the', 'Euclidean', 'norm', '.', 'In', 'the', 'case', 'of', 'wave', 'propagation', ',', 'τ', 'is', 'the', 'travel', 'time', 'of', 'the', 'wave', 'and', 'κ', '(', '?', '*', 'x', ')', '*', 'is', 'the', 'slowness', '(', 'inverse', 'velocity', ')', 'of', 'the', 'medium', '.', 'The', 'value', 'of', 'τ', 'is', 'usually', 'given', 'at', 'some', 'sub', '-', 'region', '.', 'For', 'example', ',', 'in', 'this', 'work', 'we', 'assume', 'the', 'wave', 'propagates', 'from', 'a', 'point', 'source', 'at', 'location', '?', '*', 'x0', ',', '*', 'for', 'which', 'the', 'travel', 'time', 'is', '0', ',', 'and', 'hence', 'τ', '(', '?', '*', 'x0', ')', '*', '=', '0', '.', '\\n\\n', '\\\\', '*', 'Corresponding', 'author', '.', ' \\n', '*', 'E', '-', 'mail', 'addresses', ':*', '[', 'erantreister@gmail.com](mailto:erantreister@gmail.com', ')', '(', 'E.', 'Treister', ')', ',', '[', 'haber@math.ubc.ca](mailto:haber@math.ubc.ca', ')', '(', 'E.', 'Haber', ')', '.', '[', 'http://dx.doi.org/10.1016/j.jcp.2016.08.012](http://dx.doi.org/10.1016/j.jcp.2016.08.012', ')', ' \\n', '0021', '-', '9991/', '©', '2016', 'Elsevier', 'Inc.', 'All', 'rights', 'reserved', '.', '\\n\\n', '---', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0637.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '1', '.', '*', '*', 'The', '*', 'l2', '*', 'norm', 'of', 'the', 'approximation', 'error', '\\\\|∇τ0', '−', '*', 'Dτ0', '*', '\\\\|', 'around', 'a', 'source', 'point', 'at', '[', '0.5,0.5', ']', ',', 'where', 'τ0', 'is', 'the', 'distance', 'function', 'and', '*', 'D', '*', 'is', 'a', 'central', 'difference', 'gradient', 'operator', 'on', 'a', 'mesh', 'with', '*', 'hx', '*', '=', '*', 'hy', '*', '=', '0.01', '.', '\\n\\n', 'Equation', '(', '1.1', ')', 'is', 'nonlinear', ',', 'and', 'as', 'such', 'may', 'have', 'multiple', 'branches', 'in', 'its', 'solution', '.', 'One', 'of', 'these', 'branches', ',', 'which', 'is', 'the', 'one', 'of', 'interest', 'in', 'the', 'applications', 'mentioned', 'earlier', ',', 'corresponds', 'to', 'the', '“', 'ﬁrst', '-', 'arrival', '”', 'viscosity', 'solution', ',', 'and', 'can', 'be', 'calculated', 'eﬃciently', '[', '5,25', ']', '.', 'One', 'of', 'the', 'ways', 'to', 'compute', 'it', 'is', 'by', 'using', 'the', 'Fast', 'Marching', '(', 'FM', ')', 'methods', '[', '36,30,31', ']', ',', 'which', 'solve', 'it', 'directly', 'using', 'ﬁrst', 'or', 'second', 'order', 'schemes', 'in', 'O(n', 'logn', ')', 'operations', '.', 'These', 'methods', 'are', 'based', 'on', 'the', 'monotonicity', 'of', 'the', 'solution', 'along', 'the', 'characteristics', '.', 'Alternatively', ',', '(', '1.1', ')', 'can', 'be', 'solved', 'iteratively', 'by', 'Fast', 'Sweeping', '(', 'FS', ')', 'methods', ',', 'which', 'may', 'be', 'seen', 'as', 'Gauss', '–', 'Seidel', 'method', 'for', '(', '1.1', ')', '.', 'First', 'order', 'accurate', 'solutions', 'of', '(', '1.1', ')', 'can', 'be', 'obtained', 'very', 'eﬃciently', 'in', '2', '*', 'd', '*', 'Gauss', '–', 'Seidel', 'sweeps', 'in', 'O(n', ')', 'operations', ',', 'where', '*', 'd', '*', 'is', 'the', 'dimension', 'of', 'the', 'problem', '[', '35,38', ']', '.', 'An', 'alternative', 'for', 'the', 'mentioned', 'approaches', 'is', 'to', 'use', 'FS', 'to', 'solve', 'a', 'Lax', '–', 'Friedrichs', 'approximation', 'for', '(', '1.1', ')', ',', 'which', 'involves', 'adding', 'artiﬁcial', 'viscosity', 'to', 'the', 'original', 'equation', '[', '10', ']', '.', 'This', 'approach', 'was', 'suggested', 'for', 'general', 'Hamilton', '–', 'Jacobi', 'equations', ',', 'and', 'is', 'simple', 'to', 'implement', '.', 'In', '[', '23', ']', ',', 'such', 'Lax', '–', 'Friedrichs', 'approximation', 'is', 'obtained', 'using', 'FS', 'up', 'to', 'third', 'order', 'accuracy', 'using', 'the', 'weighted', 'essentially', 'non', '-', 'oscillatory', '(', 'WENO', ')', 'approximations', 'to', 'the', 'derivatives', '.', 'For', 'a', 'performance', 'comparison', 'between', 'some', 'of', 'the', 'mentioned', 'solvers', 'see', '[', '7', ']', '.', ' \\n', 'In', 'some', 'cases', ',', 'the', 'eikonal', 'equation', '(', '1.1', ')', 'is', 'used', 'to', 'get', 'a', 'geometrical', '-', 'optics', 'ansatz', 'of', 'the', 'solution', 'of', 'the', 'Helmholtz', 'equation', 'in', 'the', 'high', 'frequency', 'regime', '[', '12,15,19,17', ']', '.', 'This', 'is', 'done', 'using', 'the', 'Rytov', 'decomposition', 'of', 'the', 'Helmholtz', 'solution', ':', '*', 'u', '(', '*', '?', '*', 'x', ')', '*', '=', '*', 'a', '(', '*', '?', '*', 'x', ')', '*', 'exp(iωτ', '(', '?', 'x', ')', ')', ',', 'where', '*', 'a', '(', '*', '?', '*', 'x', ')', '*', 'is', 'the', 'amplitude', 'and', 'τ', '(', '?', '*', 'x', ')', '*', 'is', 'the', 'travel', 'time', '.', 'This', 'approach', 'involves', 'solving', '(', '1.1', ')', 'for', 'the', 'travel', '-', 'time', 'and', 'solving', 'the', 'transport', 'equation', '\\n\\n', '#', '#', '#', '∇', 'τ', '·', '∇a', '+', '1', 'a?τ', '=', '1', '(', '∇', 'τ', '·', '∇a', '+', '∇', '·', '(', 'a∇', 'τ', ')', ')', '=', '0', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '1.2', ')', '\\n\\n', '2', ' \\n', '2', ' \\n', 'for', 'the', 'amplitude', '[', '15,19', ']', '.', 'The', 'resulting', 'approximation', 'includes', 'only', 'the', 'ﬁrst', 'arrival', 'information', 'of', 'the', 'wave', 'propagation', '.', 'Somewhat', 'similarly', ',', 'the', 'work', 'of', '[', '9', ']', 'suggests', 'using', 'the', 'eikonal', 'solution', 'to', 'get', 'a', 'multigrid', 'preconditioner', 'for', 'solving', 'linear', 'systems', 'arising', 'from', 'discretization', 'of', 'the', 'Helmholtz', 'equation', '.', ' \\n', 'In', 'many', 'cases', 'in', 'seismic', 'imaging', ',', 'the', 'eikonal', 'equation', 'is', 'used', 'for', 'modeling', 'the', 'migration', 'of', 'seismic', 'waves', 'from', 'a', 'point', 'source', 'at', 'some', 'point', '?', '*', 'x0', '.', '*', 'In', 'this', 'case', ',', 'when', 'solving', '(', '1.1', ')', 'numerically', 'by', 'standard', 'ﬁnite', 'differences', 'methods', ',', 'the', 'obtained', 'solution', 'has', 'a', 'strong', 'singularity', 'at', 'the', 'location', 'of', 'the', 'source', ',', 'which', 'leads', 'to', 'large', 'numerical', 'errors', '[', '22,6', ']', '.', 'Fig', '.', '1', 'illustrates', 'this', 'phenomenon', 'by', 'showing', 'the', 'approximation', 'error', 'for', 'the', 'gradient', 'of', 'the', 'distance', 'function', ',', 'which', 'is', 'the', 'solution', 'of', '(', '1.1', ')', 'for', 'κ', '=', '1', '.', 'It', 'is', 'clear', 'that', 'the', 'largest', 'approximation', 'error', 'for', 'the', 'gradient', 'is', 'located', 'around', 'the', 'source', 'point', ',', 'and', 'that', 'its', 'magnitude', 'is', 'rather', 'large', '.', 'In', 'addition', ',', 'it', 'is', 'observed', 'that', 'although', 'τ', 'may', 'have', 'more', 'singularities', 'in', 'other', 'places', ',', 'the', 'singularity', 'at', 'the', 'source', 'is', 'more', 'damaging', 'and', 'polluting', 'for', 'the', 'numerical', 'solution', '[', '22,6', ']', '.', ' \\n', 'A', 'rather', 'easy', 'treatment', 'to', 'the', 'described', 'phenomenon', 'is', 'suggested', 'in', '[', '21,6', ']', ',', 'and', 'achieved', 'using', 'a', 'factored', 'version', 'of', '(', '1.1', ')', '.', 'That', 'is', ',', 'we', 'deﬁne', ' \\n', 'τ', '=', 'τ0τ1', ',', '(', '1.3', ')', 'where', 'τ0', 'is', 'the', 'distance', 'function', ',', 'τ0', '=', '?', '?', '*', 'x', '*', '−', '?', '*', 'x0', '*', '?', '2', ',', 'from', 'the', 'point', 'source', '—', 'the', 'analytical', 'solution', 'for', '(', '1.1', ')', 'in', 'the', 'case', 'where', 'κ', '(', '?', '*', 'x', ')', '*', '=', '1', 'is', 'a', 'constant', '.', 'Indeed', ',', 'at', 'the', 'location', 'of', 'the', 'source', ',', 'the', 'function', 'τ0', 'is', 'non', '-', 'smooth', '.', 'However', ',', 'the', 'computed', 'factor', 'τ1', 'is', 'expected', 'to', 'be', 'very', 'smooth', 'at', 'the', 'surrounding', 'of', 'the', 'source', ',', 'and', 'can', 'be', 'approximated', 'up', 'to', 'high', 'order', 'of', 'accuracy', '[', '18', ']', '.', 'Plugging', '(', '1.3', ')', 'into', '(', '1.1', ')', 'and', 'applying', 'the', 'chain', 'rule', 'yields', 'the', '*', 'factored', 'eikonal', 'equation', '*', '\\n\\n', '#', 'τ', '2', '|∇', 'τ1', '|', '2', '+', '2τ0τ1', '∇', 'τ0', '·', '∇', 'τ1', '+', 'τ', '2', '=', 'κ', '(', '?', 'x', ')', '2', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '1.4', ')', '\\n\\n', '0', ' \\n', '1', ' \\n', 'Similarly', 'to', 'it', 'original', 'version', ',', 'Equation', '(', '1.4', ')', 'can', 'be', 'solved', 'by', 'fast', 'sweeping', 'methods', 'in', 'ﬁrst', 'order', 'accuracy', '[', '6,16,18', ']', ',', 'or', 'by', 'a', 'Lax', '–', 'Friedrichs', 'scheme', 'up', 'to', 'third', 'order', 'of', 'accuracy', '[', '19,15,18', ']', '.', 'The', 'recent', 'works', '[', '18,20', ']', 'suggest', 'hybrid', 'schemes', 'where', 'the', 'factored', 'eikonal', 'is', 'solved', 'at', 'the', 'neighborhood', 'of', 'the', 'source', 'and', 'the', 'standard', 'eikonal', ',', 'which', 'is', 'computationally', 'easier', ',', 'is', 'solved', 'in', 'the', 'rest', 'of', 'the', 'domain', '.', ' \\n', 'One', 'geophysical', 'tool', 'that', 'ﬁts', 'the', 'scenario', 'described', 'earlier', 'is', 'travel', 'time', 'tomography', '.', 'One', 'way', 'to', 'formulate', 'it', 'is', 'by', 'using', 'the', 'eikonal', 'equation', 'as', 'a', 'forward', 'problem', 'inside', 'an', 'inverse', 'problem', '[', '29', ']', '.', 'To', 'solve', 'the', 'inverse', 'problem', ',', 'one', 'should', 'be', 'able', 'to', 'solve', '(', '1.1', ')', 'accurately', 'for', 'a', 'point', 'source', ',', 'and', 'to', 'compute', 'its', 'sensitivities', 'eﬃciently', '.', 'The', 'works', 'of', '[', '13,34', ']', 'computes', '\\n\\n', '---', '\\n\\n', 'the', 'tomography', 'by', 'FS', ',', 'and', 'require', 'an', 'FS', 'iterative', 'solution', 'for', 'computing', 'the', 'sensitivities', '.', 'The', 'more', 'recent', '[', '14', ']', 'uses', 'the', 'FM', 'algorithm', 'for', 'forward', 'modeling', 'using', 'the', 'non', '-', 'factored', 'eikonal', 'equation', ',', 'because', 'this', 'way', 'the', 'sensitivities', 'are', 'obtained', 'more', 'eﬃciently', 'by', 'a', 'simple', 'solution', 'of', 'a', 'lower', 'triangular', 'linear', 'system', '.', '[', '3', ']', 'suggests', 'to', 'use', 'FS', 'for', 'forward', 'modeling', 'using', 'the', 'factored', 'equation', ',', 'but', 'also', 'eﬃciently', 'obtain', 'the', 'sensitivities', 'by', 'approximating', 'them', 'using', 'FM', 'with', 'the', 'non', '-', 'factored', 'eikonal', 'equation', '.', ' \\n', 'In', 'this', 'paper', ',', 'we', 'develop', 'a', 'Fast', 'Marching', 'algorithm', 'for', 'the', 'factored', 'eikonal', 'equation', '(', '1.4', ')', ',', 'based', 'on', '[', '30,31', ']', '.', 'As', 'in', '[', '31', ']', ',', 'our', 'algorithm', 'is', 'able', 'to', 'solve', '(', '1.4', ')', 'using', 'ﬁrst', 'order', 'or', 'second', 'order', 'schemes', ',', 'in', 'guaranteed', 'O(n', 'logn', ')', 'running', 'time', '.', 'When', 'using', 'our', 'method', 'for', 'forward', 'modeling', 'in', 'travel', 'time', 'tomography', ',', 'one', 'achieves', 'both', 'worlds', ':', '(', '1', ')', 'have', 'an', 'accurate', 'forward', 'modeling', 'based', 'on', 'the', 'factored', 'eikonal', 'equation', ',', 'and', '(', '2', ')', 'obtain', 'the', 'sensitivities', 'of', 'the', '(', 'factored', ')', 'forward', 'modeling', 'eﬃciently', ',', 'by', 'solving', 'lower', 'triangular', 'linear', 'systems', 'in', 'O(n', ')', 'operations', '.', 'Computationally', ',', 'this', 'is', 'one', 'of', 'the', 'most', 'attractive', 'ways', 'to', 'solve', 'the', 'inverse', 'problem', ',', 'since', 'the', 'cost', 'of', 'the', 'inverse', 'problem', 'can', 'be', 'governed', 'by', 'the', 'cost', 'of', 'applying', 'the', 'sensitivities', '.', '\\n\\n', 'Our', 'paper', 'is', 'organized', 'as', 'follows', '.', 'In', 'the', 'next', 'section', 'we', 'brieﬂy', 'review', 'the', 'FM', 'method', 'in', '[', '30,31', ']', ',', 'including', 'some', 'of', 'its', 'implementation', 'details', '.', 'Next', ',', 'we', 'show', 'our', 'extension', 'to', 'the', 'FM', 'method', 'for', 'the', 'factored', 'eikonal', 'equation', '—', 'in', 'both', 'ﬁrst', 'and', 'second', 'order', 'of', 'accuracy', '—', 'and', 'provide', 'some', 'theoretical', 'properties', '.', 'Following', 'that', ',', 'we', 'discuss', 'the', 'derivation', 'of', 'sensitivities', 'using', 'FM', 'and', 'brieﬂy', 'present', 'the', 'travel', 'time', 'tomography', 'problem', '.', 'Last', ',', 'we', 'show', 'some', 'numerical', 'results', 'that', 'demonstrate', 'the', 'effectiveness', 'of', 'the', 'method', 'in', 'two', 'and', 'three', 'dimensions', ',', 'for', 'both', 'the', 'forward', 'and', 'inverse', 'problems', '.', '\\n\\n', '#', '#', '#', '#', '#', '2', '.', 'The', 'Fast', 'Marching', 'algorithm', '\\n\\n', 'We', 'now', 'review', 'the', 'FM', 'algorithm', 'of', '[', '30,31', ']', 'in', 'two', 'dimensions', '.', 'The', 'extension', 'to', 'higher', 'dimensions', 'is', 'straightforward', '.', 'The', 'FM', 'algorithm', 'is', 'based', 'on', 'the', 'Godunov', 'upwind', 'discretization', '[', '25', ']', 'of', '(', '1.1', ')', '.', 'In', 'two', 'dimensions', ',', 'this', 'discretization', 'is', 'given', 'by', ' \\n', '?', ' \\n', '?', ' \\n', '\\\\|∇', 'τ', '\\\\|', '2', '≈', 'max\\\\{D', '−x', 'τ,−D', '+', 'x', 'τ,0\\\\', '}', '2', '+', 'max\\\\{D', '−y', 'τ,−D', '+', 'y', 'τ,0\\\\', '}', '2', '=', 'κ', '(', '?', '*', 'xij', ')', '*', '2', ',', '?', '*', 'xij', '*', '∈', '?', 'h', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '2.5', ')', '\\n\\n', '*', 'ij', 'ij', 'ij', 'ij', '*', '\\n\\n', 'where', 'in', 'the', 'simplest', 'form', '*', 'D', '*', '−x', 'τ', '=', '*', 'τi', ',', 'j', '−τi−1,j', '*', 'and', '*', 'D', '*', '+', 'x', 'τ', '=', '*', 'τi+1,j', '−τi', ',', 'j', '*', 'are', 'the', 'backward', 'and', 'forward', 'ﬁrst', 'derivative', 'operators', ',', '*', 'ij', 'h', 'ij', 'h', '*', ' \\n', 'respectively', '.', 'In', 'principal', ',', 'one', 'can', 'replace', 'these', 'operators', 'with', 'ones', 'of', 'higher', 'order', 'of', 'accuracy', '.', ' \\n', 'The', 'FM', 'algorithm', 'solves', '(', '2.5', ')', 'in', 'a', 'sophisticated', 'way', ',', 'exploiting', 'the', 'fact', 'that', 'the', 'upwind', 'difference', 'structure', 'of', '(', '2.5', ')', 'imposes', 'a', 'unique', 'direction', 'in', 'which', 'the', 'information', 'propagates', '—', 'from', 'smaller', 'values', 'of', 'τ', 'to', 'larger', 'values', '.', 'Hence', ',', 'the', 'FM', 'algorithm', 'rests', 'on', 'solving', '(', '2.5', ')', 'by', 'building', 'the', 'solution', 'outwards', 'from', 'the', 'smallest', 'τ', 'value', '.', 'It', 'assumes', 'that', 'some', 'initial', 'value', 'of', 'τ', 'is', 'given', 'at', 'some', 'region', 'of', '?', 'h', '(', 'or', 'a', 'point', '?', '*', 'x0', ')', '*', 'and', 'it', 'propagates', 'outwards', 'from', 'this', 'initial', 'region', ',', 'by', 'updating', 'the', 'next', 'smallest', 'value', 'of', 'τ', 'at', 'each', 'step', '.', ' \\n', 'To', 'apply', 'the', 'rule', 'above', ',', 'let', 'us', 'deﬁne', 'three', 'disjoint', 'sets', 'of', 'variables', ':', 'the', 'knownvariables', ',', 'the', 'frontvariables', '(', 'which', 'are', 'sometimes', 'called', 'the', '*', 'trial', '*', 'variables', ')', 'and', 'the', 'unknownvariables', '.', 'These', 'three', 'sets', 'together', 'contain', 'all', 'the', 'grid', 'points', 'in', 'the', 'problem', '.', 'For', 'simplicity', ',', 'let', 'us', 'assume', 'that', 'we', 'solve', '(', '2.5', ')', 'for', 'a', 'point', 'source', '.', 'That', 'is', ',', 'a', 'source', 'is', 'located', 'at', 'point', '?', '*', 'x0', ',', '*', 'for', 'which', 'τ', '(', '?', '*', 'x0', ')', '*', '=', '0', '.', 'Initially', ',', 'knownis', 'chosen', 'as', 'an', 'empty', 'set', ',', 'frontis', 'set', 'to', 'contain', 'only', '?', '*', 'x0', ',', '*', 'and', 'unknownhas', 'the', 'rest', 'of', 'the', 'variables', 'for', 'which', 'τ', 'is', 'set', 'to', 'inﬁnity', '.', 'At', 'each', 'step', 'we', 'choose', 'the', 'point', '?', '*', 'xij', '*', 'in', 'frontwith', 'minimal', 'value', 'of', 'τ', '(', '?', '*', 'xij', ')', '*', 'and', 'move', 'it', 'to', 'known', '.', 'Next', ',', 'we', 'move', 'all', 'of', 'its', 'neighbors', 'which', 'are', 'in', 'unknown', 'to', 'front', ',', 'and', 'solve', '(', '2.5', ')', 'for', 'all', 'neighbors', 'which', 'are', 'not', 'in', 'known', '.', 'This', 'way', ',', 'we', 'set', 'all', 'variables', 'to', 'be', 'in', 'known', 'in', '*', 'n', '*', 'steps', ',', 'and', 'the', 'algorithm', 'ﬁnishes', '.', 'A', 'precise', 'description', 'of', 'the', 'algorithm', 'is', 'given', 'in', 'Algorithm', '1', '.', '\\n\\n', '#', '#', '#', 'Algorithm', '1', ':', 'Fast', 'Marching', '\\n\\n', 'Initialize', ':', ' \\n', '*', 'τij', '*', '=', '∞', 'for', 'all', '*', '?', 'xij', '*', '∈', '?', 'h', ',', 'τ(?x0', ')', '=', '0', ',', 'known←', '∅', ',', 'front←', '\\\\{?x0', '\\\\', '}', '.', ' \\n', '*', '*', 'while', '*', '*', 'front', '?', '=', '∅', '*', '*', 'do', '*', '*', '\\n\\n', '1', '.', 'Find', 'the', 'minimal', 'entry', 'in', 'front', ':', '*', 'ximin', ',', 'jmin', '*', '=', 'argmin?xij', '*', '\\\\{τij', '*', ':', '*', '?', 'xij', '*', '∈front\\\\', '}', '\\n', '2', '.', 'Add', '*', '?', 'ximin', ',', 'jmin', '*', 'to', 'knownand', 'take', 'it', 'out', 'of', 'front', ':', 'front←front\\\\\\\\\\\\{?ximin', ',', 'jmin', '\\\\', '}', ';', 'known←known∪\\\\{?ximin', ',', 'jmin', '\\\\', '}', '.', '\\n', '3', '.', 'Add', 'the', 'unknown', 'neighborhood', 'of', '*', '?', 'ximin', ',', 'jmin', '*', 'to', 'front', ':', ' \\n', 'N', '=', '*', '\\\\{?ximin−1,jmin', ',', '?', 'ximin+1,jmin', ',', '?', 'ximin', ',', 'jmin−1', ',', '?', 'ximin', ',', 'jmin+1', '*', '\\\\}\\\\\\\\known', '*', 'min', '*', ' \\n', 'front←front∪', 'N', '*', 'min', '.', '*', '\\n', '4', '.', '*', '*', 'Foreach', '*', '*', '*', '?', 'xij', '*', '∈', 'N', '*', 'min', '*', ' \\n', 'Update', '*', 'τij', '*', 'by', 'solving', 'the', 'quadratic', '(', '2.5', ')', ',', 'using', 'only', 'entries', 'in', 'known', '.', '\\n\\n', '-', '*', 'ximin', ',', 'jmin', '*', '=', 'argmin?xij', '*', '\\\\{τij', '*', ':', '*', '?', 'xij', '*', '∈front\\\\', '}', ' \\n', 'Add', '*', '?', 'ximin', ',', 'jmin', '*', 'to', 'knownand', 'take', 'it', 'out', 'of', 'front', ':', 'front←front\\\\\\\\\\\\{?ximin', ',', 'jmin', '\\\\', '}', ';', 'known←known∪\\\\{?ximin', ',', 'jmin', '\\\\', '}', '.', ' \\n', 'Add', 'the', 'unknown', 'neighborhood', 'of', '*', '?', 'ximin', ',', 'jmin', '*', 'to', 'front', ':', ' \\n', 'N', '=', '*', '\\\\{?ximin−1,jmin', ',', '?', 'ximin+1,jmin', ',', '?', 'ximin', ',', 'jmin−1', ',', '?', 'ximin', ',', 'jmin+1', '*', '\\\\}\\\\\\\\known', ' \\n', '*', 'min', '*', ' \\n', 'front←front∪', 'N', '*', 'min', '.', '*', ' \\n', '*', '*', 'Foreach', '*', '*', '*', '?', 'xij', '*', '∈', 'N', '*', 'min', '*', ' \\n', 'Update', '*', 'τij', '*', 'by', 'solving', 'the', 'quadratic', '(', '2.5', ')', ',', 'using', 'only', 'entries', 'in', 'known', '.', '\\n\\n', '#', '#', '#', 'End', '\\n\\n', '#', '#', '#', 'end', '\\n\\n', '---', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0669.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '2', '.', '*', '*', 'A', 'minimum', 'heap', 'and', 'its', 'implementation', 'using', 'array', '.', '\\n\\n', 'In', '[', '30', ']', 'it', 'was', 'proved', 'that', 'Algorithm', '1', 'produces', 'a', 'viable', 'viscosity', 'solution', 'to', '(', '2.5', ')', 'when', 'using', 'ﬁrst', 'order', 'approximations', 'for', 'the', 'ﬁrst', 'derivatives', '.', 'Furthermore', ',', 'it', 'is', 'proved', 'that', 'the', 'values', 'of', 'τ', 'in', 'the', 'order', 'of', 'which', 'the', 'points', 'are', 'set', 'to', 'knownin', 'Step', '2', 'are', 'monotonically', 'increasing', '.', '\\n\\n', '*', '2.1', '.', 'Eﬃcient', 'implementation', 'using', 'minimum', 'heap', '*', '\\n\\n', 'Algorithm', '1', 'has', 'two', 'main', 'computational', 'bottlenecks', 'in', 'Steps', '1', 'and', '4', ',', 'which', 'are', 'repeated', '*', 'n', '*', 'times', '.', 'For', 'a', 'd', '-', 'dimensional', 'problem', ',', 'the', 'set', 'front', 'contains', 'a', 'd-1', 'dimensional', 'manifold', 'of', 'points', ',', 'of', 'size', 'O(n', '*', 'd−1', '*', ')', '.', 'To', 'ﬁnd', 'the', 'minimum', 'of', 'front', '*', 'd', '*', '\\n\\n', 'eﬃciently', ',', 'a', 'minimum', 'heap', 'data', 'structure', 'is', 'used', '[', '30,31', ']', '.', 'A', 'minimum', 'heap', 'is', 'a', 'binary', 'tree', 'with', 'a', 'property', 'that', 'a', 'value', 'at', 'any', 'node', 'is', 'less', 'than', 'or', 'equal', 'to', 'the', 'values', 'at', 'its', 'two', 'children', '.', 'Consequently', ',', 'the', 'root', 'of', 'the', 'tree', 'holds', 'the', 'minimal', 'value', '.', 'The', 'simplest', 'implementation', 'of', 'such', 'a', 'tree', 'is', 'done', 'by', 'a', 'sequential', 'array', 'of', 'nodes', ',', 'using', 'the', 'rule', 'that', 'if', 'a', 'node', 'is', 'located', 'at', 'entry', '*', 'k', ',', '*', 'then', 'its', 'two', 'children', 'are', 'located', 'at', 'entries', '2k', 'and', '2k', '+', '1', '(', 'the', 'ﬁrst', 'element', 'of', 'the', 'array', 'is', 'indexed', 'by', '1', ',', 'and', 'is', 'the', 'root', 'of', 'the', 'tree', ')', '.', 'Equivalently', ',', 'the', 'parent', 'of', 'a', 'node', 'at', 'entry', '*', 'k', '*', 'is', 'located', 'at', 'entry', '?', 'k/2', '?', '.', 'Fig', '.', '2', 'shows', 'an', 'example', 'of', 'a', 'minimum', 'heap', 'and', 'its', 'implementation', 'using', 'array', '.', 'Generally', ',', 'each', 'element', 'in', 'the', 'array', 'can', 'hold', 'many', 'properties', ',', 'and', 'one', 'of', 'these', 'has', 'to', 'be', 'deﬁned', 'as', 'a', 'comparable', '“', 'key', '”', ',', 'which', 'is', 'used', 'in', 'the', 'heap', 'for', 'sorting', '.', 'In', 'our', 'case', ',', 'each', 'node', 'holds', 'a', 'point', '?', '*', 'x', '*', 'in', 'the', 'mesh', ',', 'and', 'its', 'value', 'τ', '(', '?', '*', 'x', ')', '*', 'as', 'a', 'key', '.', ' \\n', 'In', 'its', 'simplest', 'form', ',', 'the', 'minimum', 'heap', 'structure', 'supports', 'two', 'basic', 'operations', ':', 'insert(element', ',', 'key', ')', 'and', 'get-', 'Min', '(', ')', '.', 'For', 'example', ',', 'this', 'is', 'the', 'case', 'in', 'the', 'C++', 'standard', 'library', 'implementation', 'of', 'the', 'minimum', 'heap', 'structure', '.', 'To', 'apply', 'getMin', '(', ')', ',', 'we', 'remove', 'the', 'ﬁrst', 'element', 'in', 'the', 'array', ',', 'and', 'take', 'the', 'last', 'element', 'in', 'the', 'array', 'and', 'push', 'it', 'in', 'the', 'ﬁrst', 'place', '.', 'Then', ',', 'to', 'maintain', 'the', 'property', 'of', 'the', 'heap', ',', 'we', 'repeatedly', 'replace', 'this', 'value', 'with', 'its', 'smaller', 'child', '(', 'the', 'smaller', 'of', 'the', 'two', ')', 'until', 'it', 'reaches', 'down', 'in', 'the', 'tree', 'to', 'the', 'point', 'where', 'it', 'is', 'smaller', 'than', 'its', 'two', 'children', 'or', 'it', 'has', 'no', 'children', '.', 'The', 'insert(element', ',', 'key', ')', 'operation', 'ﬁrst', 'places', 'a', 'new', 'element', 'at', 'the', 'next', 'empty', 'space', 'of', 'the', 'array', '.', 'Then', ',', 'it', 'propagates', 'this', 'element', 'upwards', ',', 'each', 'time', 'replacing', 'it', 'with', 'its', 'parent', 'until', 'it', 'reaches', 'a', 'point', 'where', 'the', 'element', 'is', 'either', 'the', 'root', 'or', 'its', 'key', 'is', 'larger', 'than', 'its', 'parent', '’s', 'key', '.', 'Both', 'of', 'the', 'described', 'operations', 'are', 'performed', 'in', 'O(logm', ')', 'complexity', 'where', '*', 'm', '*', 'is', 'the', 'number', 'of', 'elements', 'in', 'the', 'heap', '.', ' \\n', 'In', 'Algorithm', '1', ',', 'the', 'set', 'frontis', 'implemented', 'using', 'a', 'min', '-', 'heap', '.', 'Steps', '1–2', 'are', 'trivially', 'implemented', 'using', 'getMin', '(', ')', ',', 'and', 'insert(element', ',', 'key', ')', 'is', 'used', 'in', 'Step', '3', '.', 'However', ',', 'Step', '4', 'of', 'Algorithm', '1', 'requires', 'updating', 'values', 'which', 'are', 'inside', 'the', 'heap', 'but', 'are', 'not', 'at', 'the', 'root', '.', 'This', 'operation', 'is', 'not', 'supported', 'in', 'the', 'standard', 'deﬁnition', 'of', 'either', 'priority', 'queue', 'or', 'minimum', 'heap', '.', 'Indeed', ',', 'the', 'papers', '[', '30,31', ']', 'use', 'a', 'variant', 'of', 'a', 'priority', 'queue', ',', 'which', 'includes', 'back', '-', 'links', 'from', 'points', '?', '*', 'xij', '*', 'to', 'their', 'corresponding', 'locations', 'inside', 'the', 'heap', ',', 'and', 'a', 'more', '“', 'software', '-', 'engineering', 'friendly', '”', 'implementation', 'of', 'this', 'idea', 'is', 'suggested', 'in', '[', '2', ']', ',', 'where', 'those', 'back', '-', 'links', 'are', 'incorporated', 'within', 'the', 'heap', 'implementation', ',', 'without', 'any', 'relation', 'to', 'the', 'mesh', '.', 'However', ',', 'although', 'this', 'way', 'an', 'update', 'of', 'a', 'key', 'inside', 'the', 'heap', 'can', 'still', 'be', 'implemented', 'in', 'O(logm', ')', ',', 'it', 'requires', 'a', 'specialized', 'implementation', 'of', 'the', 'heap', ',', 'and', 'encumbers', 'the', 'operations', 'described', 'earlier', 'to', 'maintain', 'these', 'back', '-', 'links', '.', 'In', 'our', 'implementation', ',', 'we', 'bypass', 'this', 'need', 'for', 'back', '-', 'links', ',', 'and', 'implement', 'Step', '4', 'by', 'reinserting', 'elements', 'to', 'the', 'heap', 'if', 'they', 'are', 'indeed', 'smaller', 'than', 'their', 'value', 'in', 'the', 'heap', '.', 'In', 'Step', '1', ',', 'we', 'simply', 'ignore', 'entries', 'which', 'are', 'in', 'known', 'already', '.', 'If', 'the', 'algorithm', 'is', 'indeed', 'monotone', ',', 'like', 'the', 'ﬁrst', 'order', 'version', 'in', '[', '30', ']', ',', 'this', 'implementation', 'detail', 'will', 'not', 'change', 'the', 'result', 'of', 'the', 'algorithm', '.', 'The', 'downside', 'of', 'this', 'change', 'is', 'that', 'it', 'enables', 'frontto', 'grow', 'more', 'than', 'in', 'the', 'back', '-', 'linked', 'version', '.', 'However', ',', 'even', 'if', 'it', 'grows', 'four', 'times', 'compared', 'to', 'the', 'back', '-', 'linked', 'version', ',', 'then', 'the', 'heap', 'tree', 'is', 'just', 'two', 'nodes', 'higher', ',', 'making', 'the', 'difference', 'in', 'running', 'time', 'insigniﬁcant', '.', '\\n\\n', '*', '2.2', '.', 'Second', 'order', 'Fast', 'Marching', '*', '\\n\\n', 'Solving', 'the', 'eikonal', 'equation', 'based', 'on', 'a', 'ﬁrst', 'order', 'discretization', 'in', '(', '2.5', ')', 'provides', 'guaranteed', 'monotonicity', 'and', 'stability', '.', 'However', ',', 'it', 'also', 'provides', 'a', 'less', 'accurate', 'solution', 'because', 'of', 'the', 'added', 'viscosity', 'that', 'is', 'associated', 'with', 'the', 'ﬁrst', 'order', 'approximation', '.', 'To', 'get', 'a', 'more', 'accurate', 'FM', 'method', ',', '[', '31', ']', 'suggests', 'to', 'use', 'a', 'second', 'order', 'upwind', 'approximation', 'in', '(', '2.5', ')', ',', 'e.g.', '\\n\\n', '---', '\\n\\n', '#', 'D', '−', 'τ', '=', '3τi', '−', '4τi−1', '+', 'τi−2', 'D', '+', 'τ', '=', '−3τi', '+', '4τi+1', '−', 'τi+2', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '2.6', ')', '\\n\\n', '*', 'i', '*', '2h', '*', 'i', '*', '2h', ' \\n', 'However', ',', 'in', 'some', 'cases', ',', 'the', 'scheme', 'may', 'revert', 'to', 'ﬁrst', 'order', 'approximations', 'from', 'certain', 'directions', '.', 'The', 'obvious', 'case', 'for', 'that', 'is', 'when', 'there', 'are', 'not', 'enough', 'knownpoints', 'for', 'the', 'high', 'order', 'stencil', '.', 'This', 'case', 'occurs', 'for', 'example', 'when', 'the', 'given', 'initial', 'region', 'contains', 'only', 'one', 'point', '.', 'Another', 'condition', 'for', 'using', 'second', 'order', 'operators', 'is', 'given', 'in', '[', '31', ']', ':', '\\n\\n', '#', '#', '#', '#', '#', 'τi−1', '≥', 'τi−2', 'or', 'τi+1', '>', 'τi+2', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '2.7', ')', '\\n\\n', 'where', 'the', 'left', 'condition', 'is', 'used', 'for', 'backward', 'operators', 'and', 'the', 'right', 'one', 'for', 'forward', 'operators', '.', 'If', '(', '2.7', ')', 'is', 'not', 'satisﬁed', ',', 'the', 'algorithm', 'reverts', 'to', 'ﬁrst', 'order', 'operators', '.', 'Later', 'we', 'show', 'that', 'this', 'condition', 'guarantees', 'the', 'monotonicity', 'of', 'the', 'non', '-', 'factored', 'FM', 'solution', 'using', 'second', 'order', 'scheme', '.', '\\n\\n', '#', '#', '#', '#', '#', '3', '.', 'Fast', 'Marching', 'for', 'the', 'factored', 'eikonal', 'equation', '\\n\\n', 'Let', 'us', 'rewrite', 'the', 'factored', 'eikonal', 'equation', '(', '1.4', ')', 'in', 'a', 'squared', 'form', ',', 'which', 'is', 'closer', '(', '1.1', '):', '\\n\\n', '#', '|', 'τ0', '∇', 'τ1', '+', 'τ1', '∇', 'τ0', '|', '2', '=', 'κ', '(', '?', 'x', ')', '2', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.8', ')', '\\n\\n', 'This', 'writing', 'is', 'the', 'key', 'for', 'deriving', 'the', 'FM', 'algorithm', 'for', '(', '1.4', ')', '.', 'Similarly', 'to', 'the', 'Godunov', 'upwind', 'scheme', 'in', '(', '2.5', ')', ',', 'we', 'discretize', '(', '3.8', ')', 'for', 'τ1', 'using', 'a', 'derivative', 'operator', '*', 'D', '*', 'ˆ', 'instead', 'of', '*', 'D', '*', ' \\n', '?', ' \\n', '?', '\\n\\n', '#', '#', '#', 'max', '{', 'D', 'ˆ', '−x', 'τ1,−', 'D', 'ˆ', '+', 'x', 'τ1,0', '}', '2', '+', 'max', '{', 'D', 'ˆ', '−y', 'τ1,−', 'D', 'ˆ', '+', 'y', 'τ1,0', '}', '2', '=', 'κ', '(', '?', 'xij', ')', '2', ',', '?', 'xij', '∈', '?', 'h.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.9', ')', '\\n\\n', '*', 'ij', 'ij', 'ij', 'ij', '*', '\\n\\n', 'For', 'example', ',', 'the', 'backward', 'ﬁrst', 'order', 'factored', 'derivative', 'operator', 'is', 'given', 'by', '\\n\\n', '#', '#', '#', '#', '#', '#', 'D', 'ˆ', '−x', 'τ1', '=', '(', 'τ0)ij', '(', 'τ1)i', ',', 'j', '−(τ1)i−1,j', '+', '(', 'p0)ij(τ1)ij', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.10', ')', '\\n\\n', '*', 'ij', '*', ' \\n', '*', 'h', '*', ' \\n', 'where', 'τ0', 'and', '*', 'p0', '*', '=', '∂τ0', 'are', 'known', '.', 'From', 'this', 'point', 'we', 'apply', 'the', 'Algorithm', '1', 'as', 'it', 'is', '.', 'We', 'hold', 'the', 'values', 'of', 'τ0τ1', 'in', 'front', ',', 'and', 'in', 'Step', '4', 'we', 'update', '∂x', ' \\n', '(', 'τ1)ij', 'with', 'the', 'solution', 'of', '(', '3.9', ')', '.', ' \\n', '*', '*', 'Initialization', ':*', '*', 'For', 'the', 'non', '-', 'factored', 'equation', ',', 'Algorithm', '1', 'is', 'initialized', 'by', 'τ', '(', '?', '*', 'x0', ')', '*', '=', '0', 'at', 'the', 'point', 'source', '.', 'In', 'the', 'factored', 'equation', ',', 'this', 'is', 'trivially', 'fulﬁlled', 'by', 'deﬁnition', ',', 'because', 'at', 'the', 'source', 'τ0', '(', '?', '*', 'x0', ')', '*', '=', '0', '.', 'Still', ',', 'τ1', '(', '?', '*', 'x0', ')', '*', 'should', 'not', 'be', 'chosen', 'arbitrarily', 'since', 'its', 'value', 'is', 'used', 'in', 'the', 'ﬁnite', 'difference', 'approximations', 'when', 'evaluating', 'its', 'neighbors', '.', 'Examining', '(', '3.8', ')', 'at', 'the', 'source', 'yields', 'τ1', '(', '?', '*', 'x0', ')', '*', '2\\\\|∇', 'τ0', '\\\\|2', '=', 'τ1', '(', '?', '*', 'x0', ')', '*', '2', '=', 'κ', '(', '?', '*', 'x0', ')', '*', '2', ',', 'since', 'we', 'choose', 'τ0', 'such', 'that', '\\\\|∇', 'τ0', '\\\\|2', '=', '1', ',', 'independently', 'of', 'κ', '.', 'In', 'some', 'cases', 'in', 'the', 'literature', ',', 'i.e.', ',', '[', '6', ']', ',', 'the', 'value', 'κ', '(', '?', '*', 'x0', ')', '*', 'is', 'absorbed', 'in', 'τ0', ',', 'such', 'that', '\\\\|∇', 'τ0', '\\\\|2', '=', 'κ', '(', '?', '*', 'x0', ')', '*', '2', '.', 'Then', 'τ1', '(', '?', '*', 'x0', ')', '*', 'should', 'be', 'chosen', 'as', '1', '.', 'This', 'is', 'obviously', 'equivalent', 'for', 'computing', 'τ', ',', 'however', ',', 'it', 'is', 'much', 'more', 'convenient', 'to', 'choose', 'τ0', 'independently', 'of', 'κ', 'if', 'one', 'wants', 'to', 'obtain', 'the', 'sensitivities', 'of', 'the', 'FM', 'algorithm', '(', 'for', 'more', 'details', ',', 'see', 'Section', '4', ')', '.', ' \\n', '*', '*', 'Second', 'order', 'discretization', ':*', '*', 'Similarly', 'to', 'the', 'non', '-', 'factored', 'equation', ',', 'the', 'second', 'order', 'upwind', 'approximations', '(', '2.6', ')', 'can', 'be', 'used', 'in', '(', '3.9)–(3.10', ')', 'for', 'τ1', '.', 'Again', ',', 'we', 'revert', 'to', 'the', 'ﬁrst', 'order', 'approximation', 'in', 'cases', 'where', 'the', 'additional', 'point', 'needed', 'for', 'the', 'second', 'order', 'approximation', 'is', 'not', 'in', 'known', '.', 'We', 'note', 'that', 'unlike', 'the', 'non', '-', 'factored', 'case', ',', 'the', 'solution', 'τ1', 'is', 'in', 'most', 'cases', 'very', 'smooth', 'at', 'the', 'source', '(', 'expected', 'to', 'be', 'close', 'to', 'constant', 'or', 'linear', ')', '.', 'So', ',', 'when', 'we', 'initialize', 'the', 'algorithm', 'with', 'the', 'value', 'of', 'τ1', 'at', 'the', 'point', 'source', 'and', 'revert', 'to', 'a', 'ﬁrst', 'order', 'approximation', 'for', 'the', 'neighbors', ',', 'we', 'do', 'not', 'introduce', 'large', 'discretization', 'errors', '.', 'In', 'the', 'non', '-', 'factored', 'case', ',', 'the', 'second', 'derivative', 'of', 'τ', 'is', 'singular', 'at', 'the', 'source', ',', 'so', 'using', 'ﬁrst', 'order', 'approximation', 'there', 'signiﬁcantly', 'pollutes', 'the', 'rest', 'of', 'the', 'solution', '.', '\\n\\n', '*', '3.1', '.', 'Solution', 'of', 'the', 'piecewise', 'quadratic', 'equation', '*', '\\n\\n', 'We', 'now', 'describe', 'how', 'to', 'solve', 'both', 'the', 'non', '-', 'factored', 'and', 'factored', 'piecewise', 'quadratic', 'equations', '(', '2.5', ')', 'and', '(', '3.9', ')', 'respec-', 'tively', '.', 'This', 'is', 'required', 'in', 'Step', '4', 'of', 'Algorithm', '1', '.', 'Solving', 'such', 'an', 'equation', 'consists', 'of', 'the', 'following', 'four', 'steps', ':', '\\n\\n', '1', '.', 'Determine', 'the', 'order', 'of', 'approximation', 'for', 'each', 'derivative', 'in', '(', '2.5)/(3.9', ')', '(', 'only', 'required', 'for', 'high', 'order', 'schemes', ')', '.', '\\n', '2', '.', 'Determine', 'which', 'directions', 'to', 'choose', '(', 'backward', 'or', 'forward', ')', 'for', 'each', 'dimension', '(', 'x', ',', '*', 'y', '*', 'or', 'z', ')', '.', '\\n', '3', '.', 'Solve', 'the', 'quadratic', 'equation', 'in', '(', '2.5)/(3.9', ')', ',', 'assuming', 'all', 'terms', 'are', 'positive', '.', '\\n', '4', '.', 'Make', 'sure', 'that', 'the', 'solution', 'is', 'valid', ',', 'such', 'that', 'all', 'max', 'terms', 'in', '(', '2.5)/(3.9', ')', 'are', 'indeed', 'held', 'with', 'positive', 'values', '.', 'If', 'not', ',', 'some', 'terms', 'should', 'be', 'dropped', ',', 'and', 'the', 'quadratic', 'problem', 'with', 'the', 'remaining', 'terms', 'is', 'solved', 'again', '.', '\\n\\n', 'Let', 'us', 'ﬁrst', 'consider', 'solving', 'the', 'non', '-', 'factored', 'ﬁrst', 'order', '(', '2.5', ')', ',', 'for', 'which', 'the', 'Step', '1', 'is', 'not', 'relevant', '.', 'In', 'this', 'case', ',', 'Step', '2', 'is', 'simple', ':', 'for', 'each', 'max\\\\{D', '−', 'τ', ',', '−D', '+', 'τ', ',', '0\\\\', '}', 'term', ',', 'the', 'smaller', 'of', 'the', 'two', 'values', 'of', 'τ', 'from', 'both', 'sides', '(', 'forward', 'or', 'backward', ')', 'is', '*', 'i', 'j', 'i', 'j', '*', ' \\n', 'guaranteed', 'to', 'give', 'a', 'higher', 'ﬁnite', 'difference', 'derivative', '.', 'Furthermore', ',', 'in', 'Step', '4', ',', 'if', 'some', 'of', 'the', 'terms', 'turn', 'out', 'negative', 'after', 'Step', '3', ',', 'then', 'we', 'can', 'drop', 'terms', 'from', '(', '2.5', ')', 'in', 'decreasing', 'order', 'of', 'the', 'τ', 'values', ',', 'until', 'a', 'valid', 'solution', 'is', 'reached', '.', 'The', 'same', 'is', 'true', 'for', 'a', 'ﬁrst', 'order', 'factored', 'version', 'in', '(', '3.9', ')', '.', ' \\n', 'However', ',', 'using', 'second', 'order', 'schemes', '(', 'selectively', ')', 'imposes', 'additional', 'complications', 'on', 'the', 'solution', 'of', 'the', 'piecewise', 'quadratic', 'equations', '(', '2.5', ')', 'and', '(', '3.9', ')', '.', 'There', 'are', 'many', 'options', 'for', 'order', 'of', 'accuracy', 'vs', 'directions', 'in', 'Steps', '1–2', ',', 'and', 'in', 'addition', '\\n\\n', '---', '\\n\\n', 'it', 'is', 'not', 'clear', 'in', 'which', 'order', 'to', 'drop', 'terms', 'in', 'Step', '4', 'if', 'negative', 'terms', 'are', 'detected', '.', 'Obviously', ',', 'one', 'can', 'check', 'all', 'possibilities', ',', 'but', 'such', 'an', 'option', 'may', 'be', 'costly', 'in', 'high', 'dimensions', '.', 'To', 'simplify', 'this', 'we', 'follow', '[', '31', ']', ',', 'and', 'in', 'Step', '1', 'we', 'use', 'the', 'second', 'order', 'approximation', 'if', 'the', 'extra', 'point', 'is', 'available', 'in', 'knownand', 'fulﬁlls', 'the', 'condition', '(', '2.7', ')', ',', 'and', 'revert', 'to', 'ﬁrst', 'order', 'approximation', 'if', 'not', '.', 'Then', ',', 'in', 'Step', '2', 'we', 'determine', 'the', 'choice', 'of', 'directions', 'considering', 'the', 'non', '-', 'factored', 'ﬁrst', 'order', 'approximation', '(', '2.5', ')', '.', 'That', 'is', ',', 'if', '(', 'τ0τ1)i−1', '<', '(', 'τ0τ1)i+1', ',', 'then', 'we', 'choose', 'the', 'backward', 'upwind', 'direction', ';', 'otherwise', 'we', 'choose', 'the', 'forward', 'direction', '.', 'That', 'is', 'done', 'correspondingly', 'for', 'each', 'dimension', '.', ' \\n', 'Once', 'Steps', '1–2', 'are', 'done', ',', '(', '3.9', ')', 'reduces', 'to', 'a', 'piecewise', 'quadratic', 'equation', 'of', 'the', 'form', ' \\n', '?', ' \\n', 'max\\\\', '{', '*', 'αk(τij', '*', '−βk),0\\\\', '}', '2', '=', 'κ', '(', '?', '*', 'xij', ')', '*', '2', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.11', ')', '\\n\\n', '*', 'k', '*', ' \\n', 'where', 'αk', '≥', '0', ',', 'βk', '≥', '0', 'are', 'non', '-', 'negative', 'constants', 'that', 'are', 'coming', 'from', 'the', 'ﬁnite', 'difference', 'approximations', '.', 'For', 'exam-', 'ple', ',', 'assuming', 'that', '*', 'k', '*', '=', '1', 'corresponds', 'to', 'the', '*', 'x', '*', 'coordinate', ',', 'then', '(', '3.10', ')', 'would', 'correspond', 'to', 'α1', '=', '(', 'τ0)i', ',', 'j', '+', '(', 'p0)i', ',', 'j', 'and', '*', 'h', '*', ' \\n', 'β1', '=', '(', 'τ0)i', ',', 'j(τ1)i−1,j', '.', 'In', 'Step', '3', 'we', 'simply', 'ignore', 'the', 'max\\\\', '{', '·', ',', '0\\\\', '}', 'function', 'and', 'solve', 'the', 'equation', 'assuming', 'all', 'terms', 'are', 'positive', '.', '*', 'hα1', '*', ' \\n', 'We', 'solve', 'a', 'simple', 'quadratic', 'function', 'and', 'choose', 'the', 'larger', 'one', 'of', 'its', 'two', 'solutions', 'for', '*', 'τij', '.', '*', 'If', 'all', 'chosen', 'derivative', 'terms', 'are', 'positive', ',', 'the', 'solution', 'is', 'valid', ';', 'otherwise', ',', 'we', 'reduce', 'the', 'terms', 'in', '(', '3.11', ')', 'in', 'decreasing', 'order', 'of', 'βk', ',', 'each', 'time', 'solving', '(', '3.11', ')', 'with', 'the', 'remaining', 'terms', 'until', 'a', 'valid', 'solution', 'is', 'reached', '.', 'In', 'three', 'dimensions', 'for', 'example', ',', 'this', 'involves', 'at', 'most', 'three', 'quadratic', 'solves', '.', 'Algorithm', '2', 'summarizes', 'the', 'solution', 'of', 'the', 'piecewise', 'quadratic', 'equation', '.', '\\n\\n', '*', '*', 'Algorithm', '2', ':', 'Solution', 'of', 'the', 'piecewise', 'quadratic', 'equation', '*', '*', ' \\n', '*', '*', 'for', '*', '*', '*', 'each', 'dimension', 'x', ',', 'y', ',', '...', '*', '*', '*', 'do', '*', '*', ' \\n', '*', '%', 'Choosing', 'direction', ',', 'forward', 'or', 'backward', '.', '*', ' \\n', '*', '*', 'if', '*', '*', '*', 'both', 'forward', 'and', 'backward', 'neighboring', 'points', 'are', 'in', '*', 'knownthen', 'Choose', 'the', 'direction', 'with', 'smaller', 'neighboring', 'τ', '.', '\\n\\n', '#', '#', '#', 'else', '\\n\\n', 'Otherwise', ',', 'choose', 'the', 'direction', 'in', 'known', '.', '\\n\\n', '#', '#', '#', 'end', '\\n\\n', '*', '%', 'Choosing', 'order', 'of', 'approximation', ',', '1st', 'or', '2nd', '.', '*', '*', '*', 'if', '*', '*', '*', 'next', 'neighboring', 'point', 'is', 'in', '*', 'knownthen', 'Use', 'second', 'order', 'approximation', '.', '\\n\\n', '#', '#', '#', 'else', '\\n\\n', 'Use', 'ﬁrst', 'order', 'approximation', '.', '\\n\\n', '#', '#', '#', 'end', '\\n\\n', '#', '#', '#', 'end', '\\n\\n', '*', '%', 'Now', 'all', 'coeﬃcients', 'of', '*', 'αk', '*', 'and', '*', 'βk', '*', 'of', 'Equation', '*', '(', '3.11', ')', '*', 'are', 'known', '.', '*', ' \\n', 'Calculate', '(', 'τ1)i', ',', 'j', 'by', 'solving', 'Equation', '(', '3.11', ')', '.', ' \\n', '*', '*', 'while', '*', '*', '*', 'the', 'solution', '*', '(', 'τ1)i', ',', 'j', '*', 'is', 'not', 'valid', '*', '*', '*', 'do', '*', '*', ' \\n', 'Remove', 'the', 'term', 'with', 'largest', 'βk', 'from', 'the', 'remaining', 'terms', 'in', '(', '3.11', ')', '.', 'Calculate', '(', 'τ1)i', ',', 'j', 'by', 'solving', '(', '3.11', ')', 'with', 'the', 'remaining', 'terms', '.', '\\n\\n', '#', '#', '#', 'end', '\\n\\n', '*', '3.2', '.', 'The', 'monotonicity', 'of', 'the', 'obtained', 'solution', '*', '\\n\\n', 'It', 'is', 'known', 'that', 'the', 'solution', 'of', '(', '1.1', ')', 'is', 'monotone', 'in', 'the', 'direction', 'of', 'the', 'characteristics', '.', 'We', 'now', 'show', 'how', 'to', 'enforce', 'the', 'monotonicity', 'of', 'our', 'solution', 'using', 'the', 'FM', 'method', 'for', 'the', 'factored', 'eikonal', 'equation', '.', 'To', 'set', 'the', 'stage', ',', 'we', 'ﬁrst', 'consider', 'the', 'FM', 'method', 'for', 'the', 'original', 'non', '-', 'factored', 'equation', '.', ' \\n', 'In', '[', '30', ']', 'the', 'non', '-', 'factored', 'ﬁrst', 'order', 'discretization', '(', '2.5', ')', 'is', 'considered', '.', 'In', 'this', 'case', ',', 'each', 'newly', 'calculated', 'value', '*', 'τij', '*', 'is', 'guaranteed', 'to', 'be', 'larger', 'than', 'its', 'knownneighbors', 'at', 'the', 'time', 'of', 'the', 'calculation', '.', 'To', 'show', 'this', 'clearly', ',', 'consider', 'for', 'example', 'that', 'the', 'backward', 'derivative', 'is', 'chosen', 'in', 'the', '*', 'x', '*', 'direction', '.', 'Then', ',', ' \\n', '*', 'τi−1,j', '*', '=', '*', 'τi', ',', 'j', '*', '−h', '*', 'τi', ',', 'j', '*', '−', '*', 'τi−1,j', '*', '=', '*', 'τi', ',', 'j', '−hD', '*', '−x', 'τ', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.12', ')', '\\n\\n', '*', 'h', '*', '\\n\\n', '*', 'i', ',', 'j', '*', ' \\n', 'so', 'since', '*', 'D', '*', '−x', 'τ', '≥', '0', 'in', 'the', 'solution', 'of', '(', '2.5', ')', ',', 'we', 'have', 'that', '*', 'τi', ',', 'j', '*', '≥', '*', 'τi−1,j', '.', '*', 'This', 'means', 'that', '*', 'τi', ',', 'j', '*', 'is', 'greater', 'or', 'equal', 'to', 'its', 'known', '*', 'i', ',', 'j', '*', ' \\n', 'neighbors', '.', 'This', 'property', 'insures', 'the', 'monotonicity', 'of', 'the', 'solution', '.', 'The', 'proof', 'for', 'this', 'appears', 'in', '[', '30', ']', ',', 'but', 'here', 'we', 'can', 'simplify', 'it', 'because', 'unlike', '[', '30', ']', ',', 'we', 'only', 'calculate', 'entries', 'using', 'knownvalues', '.', 'We', 'state', 'the', 'following', 'lemma', ':', '\\n\\n', '*', '*', 'Lemma', '1', '.', '*', '*', '*', 'Let', '*', 'τ', '*', 'be', 'the', 'result', 'of', 'Algorithm', '1', ',', 'for', 'solving', 'the', '(', 'non', '-', 'factored', ')', 'ﬁrst', 'order', 'equation', '*', '(', '2.5', ')', '.', '*', 'Then', 'the', 'values', 'of', '*', 'τ', '*', 'are', 'mono-', 'tonically', 'non', '-', 'decreasing', 'in', 'the', 'order', 'in', 'which', 'they', 'are', 'set', 'to', '*', 'known', '.', '\\n\\n', '*', '*', 'Proof', '.', '*', '*', 'Denote', 'by', '?', '*', 'xk', '*', 'an', 'element', 'that', 'is', 'set', 'to', 'knownat', 'Step', '2', 'of', 'the', 'k', '-', 'th', 'iteration', 'of', 'Algorithm', '1', '.', 'Assume', 'by', 'contradiction', 'that', 'there', 'exists', 'two', 'elements', '?', '*', 'xp', '*', 'and', '?', '*', 'xk', ',', '*', 'such', 'that', 'τ', '(', '?', '*', 'xp', ')', '*', '>', 'τ', '(', '?', '*', 'xk', ')', '*', 'and', '*', 'p', '*', '<', '*', 'k.', '*', 'Without', 'loss', 'of', 'generality', ',', 'assume', 'that', '*', 'k', '*', 'is', '\\n\\n', '---', '\\n\\n', 'the', 'earliest', 'iteration', 'that', 'this', 'condition', 'is', 'fulﬁlled', '.', 'Let', '*', 'k', '*', '¯', '<', '*', 'k', '*', 'be', 'the', 'iteration', 'in', 'which', 'the', 'value', 'of', 'τ', '(', '?', '*', 'xk', ')', '*', 'is', 'updated', 'in', 'the', 'last', 'time', 'and', 'it', 'is', 'entered', 'to', 'front', '.', 'We', 'know', 'that', '?', '*', 'x¯', '*', 'is', 'a', 'neighbor', 'of', '?', '*', 'xk', '.', '*', 'By', 'the', 'algorithm', ',', 'we', 'know', 'that', 'at', 'the', 'k', '-', 'th', '¯', 'iteration', '?', '*', 'xp', '*', 'is', 'already', 'set', 'to', 'known', ',', 'otherwise', '?', '*', 'xk', '*', 'would', '*', 'k', '*', 'have', 'been', 'chosen', 'to', 'knownat', 'the', 'p', '-', 'th', 'iteration', 'instead', 'of', '?', '*', 'xp', '.', '*', 'By', 'the', 'assumption', ',', 'we', 'know', 'that', 'τ', '(', '?', '*', 'xp', ')', '*', '≤', 'τ', '(', '?', '*', 'x¯', '*', ')', ',', 'because', 'otherwise', '?', '*', 'xk', '*', 'would', 'not', 'have', 'been', 'the', 'earliest', 'element', 'to', 'violate', 'the', 'monotonicity', '.', 'By', 'the', 'property', 'in', '(', '3.12', ')', ',', 'we', '*', 'k', '*', 'know', 'that', 'τ', '(', '?', '*', 'x¯', '*', ')', '≤', 'τ', '(', '?', '*', 'xk', ')', ',', '*', 'and', 'hence', 'we', 'reach', 'τ', '(', '?', '*', 'xp', ')', '*', '≤', 'τ', '(', '?', '*', 'xk', ')', ',', '*', 'which', 'contradicts', '*', 'k', '*', ' \\n', 'our', 'assumption', '.', '?', '\\n\\n', 'Furthermore', ',', 'the', 'lemma', 'above', 'can', 'be', 'extended', 'for', 'a', 'Fast', 'Marching', 'solution', 'of', '*', 'any', '*', 'equation', '(', '2.5', ')', 'such', 'that', 'the', 'dis-', 'cretization', 'operator', '*', 'D', '*', 'satisﬁes', 'a', 'monotonicity', 'condition', ':', '\\n\\n', '#', 'D', '−x', 'τ', '≥', '0', '⇒', 'τij', '≥', 'τi−1,j', 'and', '−', 'D', '+', 'x', 'τ', '≥', '0⇒', 'τij', '≥', 'τi+1,j', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.13', ')', '\\n\\n', '*', 'ij', '*', ' ', 'ij', '*', '\\n\\n', 'The', 'next', 'corollary', 'can', 'be', 'proved', 'using', 'the', 'same', 'arguments', 'as', 'in', 'Lemma', '1', ':', '\\n\\n', '*', '*', 'Corollary', '1', '.', '*', '*', '*', 'Let', '*', 'τ', '*', 'be', 'the', 'result', 'of', 'Algorithm', '1', ',', 'for', 'solving', 'the', 'Godunov', 'upwind', 'equation', '*', '(', '2.5', ')', '*', 'using', 'operators', 'D', 'which', 'satisfy', '*', '(', '3.13', ')', '.', '*', 'Then', 'the', 'values', 'of', '*', 'τ', '*', 'are', 'monotonically', 'non', '-', 'decreasing', 'in', 'the', 'order', 'in', 'which', 'they', 'are', 'set', 'to', '*', 'known', '.', '\\n\\n', 'The', 'condition', '(', '3.13', ')', 'and', 'the', 'corollary', 'above', 'is', 'violated', 'when', 'the', 'second', 'order', 'operators', '(', '2.6', ')', 'are', 'used', 'in', '(', '2.5', ')', '.', 'However', ',', 'if', 'we', 'look', 'at', 'a', 'single', 'violation', ',', 'then', 'it', 'is', 'of', 'order', '*', 'h', '*', '2', '.', 'To', 'show', 'this', ',', 'we', 'examine', 'the', 'backward', 'difference', 'derivative', 'using', 'Taylor', 'expansion', ':', '\\n\\n', '#', '#', '#', '?', '?', '\\n\\n', '*', 'τi−1,j', '*', '=', '*', 'τi', ',', 'j', '*', '−h', '∂τ', '+', '*', 'O(h', '*', '2', ')', '=', '*', 'τi', ',', 'j', '−hD', '*', '−x', 'τ', '+', '*', 'O(h', '*', '2', ')', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.14', ')', '\\n\\n', '∂x', '*', 'i', ',', 'j', 'i', ',', 'j', '*', ' \\n', '−x', ' \\n', 'where', '*', 'D', 'i', ',', 'j', '*', 'is', 'given', 'in', '(', '2.6', ')', '(', 'the', 'same', 'arguments', 'can', 'be', 'derived', 'for', 'the', 'forward', 'difference', 'derivative', ')', '.', 'Assuming', 'again', 'that', '*', 'D', '*', '−x', 'τ', '>', '0', ',', 'this', 'means', 'that', 'each', 'newly', 'calculated', '*', 'τi', ',', 'j', '*', 'is', 'generally', 'greater', 'than', 'its', 'knownneighbors', ',', 'but', 'may', 'violate', 'that', 'up', '*', 'i', ',', 'j', '*', ' \\n', 'to', 'magnitude', '*', 'O(h', '*', '2', ')', '.', 'Note', 'that', 'if', '*', 'D', '*', '−x', 'τ', 'is', 'suﬃciently', 'bounded', 'away', 'from', 'zero', 'and', 'the', 'second', 'derivative', '∂', '2', 'τ', 'is', 'bounded', 'in', '[', 'xi−1', ',', ']', ',', '*', 'i', ',', 'j', '*', '∂x', '2', '*', 'xi', '*', 'then', '*', 'τi', ',', 'j', '>', 'τi−1,j', '*', 'will', 'be', 'satisﬁed', '.', ' \\n', 'To', 'correct', 'this', 'and', 'obtain', 'a', 'monotone', 'solution', 'using', '(', '2.6', ')', ',', 'one', 'may', 'impose', 'the', 'condition', '(', '2.7', ')', 'for', 'using', 'the', 'second', 'order', 'scheme', '.', 'If', 'it', 'is', 'not', 'satisﬁed', ',', 'the', 'scheme', 'reverts', 'to', 'the', 'ﬁrst', 'order', 'scheme', ',', 'which', 'satisﬁes', '(', '3.13', ')', '.', 'If', '(', '2.7', ')', 'is', 'satisﬁed', ',', 'then', '(', '2.6', ')', 'does', 'satisfy', '(', '3.13', ')', ',', 'because', '\\n\\n', '*', '2hD', '*', '−x', 'τ', '=', '*', '3τij', '*', '−', '*', '4τi−1,j', '*', '+', '*', 'τi−2,j', '*', '<', '*', '3τij', '*', '−', '*', '3τi−1,j', '.', '*', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.15', ')', '\\n\\n', '*', 'ij', '*', '\\n\\n', 'Note', 'that', 'the', 'condition', '(', '2.7', ')', 'is', 'suggested', 'in', '[', '31', ']', 'but', 'the', 'monotonicity', 'guarantee', 'of', 'the', 'second', 'order', 'scheme', 'is', 'not', 'exam-', 'ined', '.', ' \\n', 'We', 'now', 'examine', 'the', 'monotonicity', 'of', 'the', 'obtained', 'factored', 'solution', 'τ0τ1', 'when', 'using', 'ﬁrst', 'order', 'operators', 'in', '(', '3.9', ')', '.', 'Suppose', 'that', 'we', 'are', 'calculating', '(', 'τ1)ij', 'using', 'the', 'backward', 'operator', '(', '3.10', ')', 'in', 'the', '*', 'x', '*', 'direction', 'in', '(', '3.9', ')', '.', 'We', 'again', 'start', 'with', 'a', 'Taylor', 'expansion', '\\n\\n', '#', '#', '#', '?', '?', '?', '?', '?', '?', '?', '?', '\\n\\n', '#', '#', '#', '(', 'τ0τ1)i−1,j', '=', '(', 'τ0)ij', '−h', '∂τ0', '+', 'O(h', '2', ')', '(', 'τ1)ij', '−h', '∂τ1', '+', 'O(h', '2', ')', '\\n\\n', '∂x', '?', '*', 'i', ',', 'j', '*', '?', '\\n\\n', '-', '?', '∂x', '*', 'i', ',', 'j', '*', '\\n\\n', '#', '#', '#', '=', '(', 'τ0τ1)ij', '−h(τ0)ij', '∂τ1', '−h(τ1)ij', '∂τ0', '+', 'O(h', '2', ')', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '3.16', ')', '\\n\\n', '∂x', '*', 'i', ',', 'j', '*', '∂x', '*', 'i', ',', 'j', '*', '\\n\\n', '#', '=', '(', 'τ0τ1)ij', '−h', 'D', 'ˆ', '−x', 'τ1', '+', 'O(h', '2', ')', ',', '\\n\\n', '*', 'ij', '*', '\\n\\n', '#', '#', '#', '?', '?', '\\n\\n', 'where', 'the', 'last', 'equality', 'is', 'obtained', 'by', 'placing', '∂τ1', '\\n\\n', '=', '(', 'τ1)i', ',', 'j', '−(τ1)i−1,j', '*', '+', 'O(h', ')', '.', '*', 'This', 'expansion', 'shows', 'that', 'if', 'the', 'monotonicity', '∂x', '*', 'i', ',', 'j', 'h', '*', ' \\n', 'is', 'not', 'obtained', ',', 'i.e.', ',', '(', 'τ0τ1)i', ',', 'j', '−(τ0τ1)i−1,j', '<', '0', ',', 'then', 'the', 'non', '-', 'factored', 'derivative', 'is', 'negative', ',', '*', 'D', '*', '−x', '(', 'τ0τ1', ')', '<', '0', ',', 'while', 'the', 'factored', '*', 'ij', '*', ' \\n', 'derivative', 'is', 'non', '-', 'negative', '*', 'D', '*', 'ˆ', '−x', 'τ1', '≥', '0', '(', 'otherwise', 'it', 'is', 'not', 'chosen', 'in', '(', '3.9', ')', ')', '.', 'This', 'means', 'that', 'the', 'monotonicity', 'may', 'be', 'violated', '*', 'ij', '*', ' \\n', 'only', 'up', 'to', 'an', 'error', 'of', '*', 'O(h', '*', '2', ')', '.', 'This', 'holds', 'for', 'both', 'ﬁrst', 'and', 'second', 'order', 'upwind', 'approximations', '.', 'In', 'fact', ',', '(', '3.16', ')', 'shows', 'that', 'this', 'is', 'a', 'result', 'of', 'using', 'the', 'chain', 'rule', 'rather', 'than', 'the', 'order', 'of', 'discretization', 'of', 'the', 'operators', '*', 'D', ',', '*', 'ˆ', 'since', 'the', 'monotonicity', 'condition', 'involves', 'the', 'value', '(', 'τ0)i−1,j', ',', 'while', 'it', 'does', 'not', 'appear', 'in', 'the', 'discretization', 'scheme', '.', 'In', 'any', 'case', ',', 'the', 'magnitude', 'of', 'the', 'error', 'in', 'the', 'monotonicity', 'violation', 'is', 'either', 'of', 'the', 'same', 'or', 'of', 'higher', 'order', 'as', 'the', 'error', 'in', 'τ1', ',', 'using', 'ﬁrst', 'or', 'second', 'order', 'schemes', '.', 'Again', ',', 'if', '*', 'D', '*', '−x', 'τ1', 'is', 'suﬃciently', 'bounded', 'away', 'from', 'zero', 'and', '∂', '2', 'τ1', 'is', 'bounded', 'in', '[', 'xi−1', ',', '*', 'xi', '*', ']', ',', 'then', 'the', 'monotonicity', '*', 'i', ',', 'j', '*', '∂x', '2', ' \\n', '(', 'τ0τ1)i', ',', 'j', '>', '(', 'τ0τ1)i−1,j', 'will', 'be', 'satisﬁed', '.', ' \\n', 'Nevertheless', ',', 'in', 'our', 'algorithm', 'we', 'may', 'enforce', 'the', 'monotonicity', 'of', 'the', 'obtained', 'solution', 'by', 'reverting', 'to', 'the', 'non', '-', 'factored', 'operators', 'in', 'cases', 'where', 'the', 'monotonicity', 'is', 'not', 'satisﬁed', ',', 'or', ',', 'the', 'factored', 'and', 'non', '-', 'factored', 'schemes', 'do', 'not', 'agree', 'in', 'sign', ',', 'for', 'example', ':', '*', 'D', '*', 'ˆ', '−x', 'τ1', '≥', '0', ',', 'but', '*', 'D', '*', '−x', '(', 'τ0τ1', ')', '<', '0', '.', 'Note', 'that', 'in', 'this', 'case', 'the', 'numerical', 'derivative', 'is', 'approximately', 'zero', ',', 'hence', 'the', '*', 'ij', '*', ' \\n', '*', 'ij', '*', ' \\n', 'direction', 'of', 'the', 'characteristic', 'is', 'almost', 'parallel', 'to', 'the', '*', 'y', '*', 'direction', '.', 'We', 'apply', 'this', 'change', 'using', 'the', 'same', 'order', 'of', 'derivative', 'which', 'the', 'algorithm', 'chooses', 'to', 'use', '.', 'That', 'is', ',', 'if', 'the', 'algorithm', 'chooses', 'a', 'ﬁrst', 'or', 'second', 'order', 'factored', 'stencil', ',', 'we', 'revert', 'to', 'a', '\\n\\n', '---', '\\n\\n', 'standard', 'ﬁrst', 'or', 'second', 'order', 'stencil', ',', 'respectively', '.', 'Following', 'Corollary', '1', ',', 'this', 'guarantees', 'the', 'monotonicity', 'of', 'the', 'solution', ',', 'because', 'we', 'enforce', 'the', 'condition', '(', '3.13', ')', 'at', 'all', 'stages', 'of', 'the', 'algorithm', '.', 'We', 'note', 'that', 'experimentally', ',', 'this', 'small', 'correction', 'does', 'not', 'inﬂuence', 'the', 'accuracy', 'of', 'the', 'solution', 'obtained', 'with', 'our', 'algorithm', 'in', 'both', 'ﬁrst', 'and', 'second', 'order', 'schemes', 'in', 'two', 'and', 'three', 'dimensions', '.', '\\n\\n', '#', '#', '#', '#', '#', '4', '.', 'Calculation', 'of', 'sensitivities', 'and', 'travel', 'time', 'tomography', '\\n\\n', 'Travel', 'time', 'tomography', 'is', 'a', 'useful', 'tool', 'in', 'some', 'Geophysical', 'applications', '.', 'One', 'way', 'to', 'obtain', 'it', 'is', 'by', 'using', 'the', 'eikonal', 'equation', 'as', 'a', 'forward', 'problem', 'inside', 'an', 'inverse', 'problem', '[', '29', ']', '.', 'To', 'solve', 'the', 'inverse', 'problem', ',', 'one', 'should', 'be', 'able', 'to', 'solve', '(', '1.1', ')', 'accurately', ',', 'and', 'to', 'compute', 'its', 'sensitivities', '.', 'The', 'works', 'of', '[', '13,34', ']', 'computes', 'the', 'tomography', 'by', 'FS', ',', 'and', 'require', 'an', 'FS', 'iterative', 'solution', 'for', 'computing', 'the', 'sensitivities', '.', 'When', 'using', 'the', 'FM', 'algorithm', 'for', 'forward', 'modeling', ',', 'those', 'are', 'obtained', 'more', 'eﬃciently', 'by', 'a', 'simple', 'solution', 'of', 'a', 'lower', 'triangular', 'linear', 'system', '[', '14,3', ']', '.', 'More', 'explicitly', ',', 'let', 'us', 'denote', 'by', 'boldface', 'all', 'the', 'discretized', 'values', 'of', 'the', 'mentioned', 'functions', 'on', 'a', 'grid', ',', 'and', 'suppose', 'that', 'we', 'set', '*', '*', 'm', '*', '*', 'to', 'be', 'the', 'vector', 'of', 'the', 'values', 'of', 'κ', '(', '?', '*', 'x', ')', '*', '2', 'on', 'this', 'grid', '.', 'By', 'solving', '(', '3.9', ')', ',', 'we', 'get', 'a', 'function', 'τ1(m', ')', 'for', 'the', 'values', 'of', 'τ1', 'on', 'the', 'grid', '.', 'We', 'wish', 'to', 'get', 'a', 'linearization', 'for', 'τ1(m', ')', ',', 'such', 'that', 'we', 'can', 'predict', 'its', 'change', 'following', 'a', 'small', 'change', 'in', '*', '*', 'm.', '*', '*', 'That', 'is', ',', 'we', 'wish', 'to', 'be', 'able', 'to', 'apply', 'an', 'approximation', '\\n\\n', 'τ1(m+δm', ')', '≈', 'τ1(m)+', '*', '*', 'Jδm', ',', '*', '*', '(', '4.17', ')', 'where', '*', '*', 'J', '*', '*', 'is', 'the', 'sensitivity', 'matrix', '(', 'or', 'Jacobian', ')', 'deﬁned', 'by', '\\n\\n', '*', 'Jij', '*', '=', '(', '∇', '*', 'mτ1)ij', '*', '=', '∂(τ1)i', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '4.18', ')', '∂mj', '\\n\\n', 'To', 'obtain', 'the', 'sensitivity', 'we', 'ﬁrst', 'rewrite', '(', '3.9', ')', 'in', 'implicit', 'form', '\\n\\n', 'f(m', ',', 'τ1', ')', '=', '(', '*', '*', 'D', '*', '*', 'ˆ', '*', 'x', '*', 'τ1', ')', '2', '+', '(', '*', '*', 'D', '*', '*', 'ˆ', '*', 'y', '*', 'τ1', ')', '2', '−', '*', '*', 'm', '*', '*', '=', '0', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '4.19', ')', '\\n\\n', 'where', '*', '*', 'D', '*', '*', 'ˆ', '*', 'x', '*', '=', 'diag(τ0)D', '*', 'x', '*', '+', 'diag(p0', ')', 'and', '*', '*', 'D', '*', '*', 'ˆ', '*', 'y', '*', '=', 'diag(τ0', ')', '·', '*', '*', 'D', '*', '*', '*', 'y', '*', '+', 'diag(q0', ')', 'are', 'the', 'matrices', 'that', 'apply', 'the', 'ﬁnite', 'difference', 'derivatives', 'that', 'are', 'chosen', 'by', 'the', 'FM', 'algorithm', 'when', 'applied', 'for', '*', '*', 'm.', 'p0', '*', '*', 'and', '*', '*', 'q0', '*', '*', 'are', 'the', 'analytical', 'derivatives', 'of', 'τ0', 'with', 'respect', 'to', '*', 'x', '*', 'and', '*', 'y', '*', 'on', 'the', 'grid', 'respectively', ',', 'and', 'diag(x', ')', 'denotes', 'a', 'diagonal', 'matrix', 'whose', 'diagonal', 'elements', 'are', 'those', 'of', 'the', 'vector', '*', '*', 'x.', '*', '*', 'We', 'note', 'that', 'in', 'the', 'points', 'where', 'no', 'derivative', 'is', 'chosen', 'in', 'the', 'solution', 'of', '(', '3.9', ')', ',', 'a', 'zero', 'row', 'is', 'set', 'in', 'the', 'corresponding', 'operator', '*', '*', 'D.', '*', '*', 'ˆ', 'Also', ',', 'at', 'the', 'row', 'of', 'the', 'point', 'source', ',', 'we', 'set', 'each', 'of', '*', '*', 'D', '*', '*', 'ˆ', '*', 'x', '*', 'and', '*', '*', 'D', '*', '*', 'ˆ', '*', 'y', '*', 'to', 'have', 'only', 'one', 'diagonal', 'non', '-', 'zero', 'element', ',', 'which', 'equals', 'to', 'the', 'values', 'of', '*', '*', 'p0', '*', '*', 'and', '*', '*', 'q0', '*', '*', 'at', 'the', 'source', '.', 'This', 'way', ',', '(', '4.19', ')', 'is', 'exactly', 'fulﬁlled', 'for', '*', '*', 'D', '*', '*', 'ˆ', '*', 'x', '*', 'and', '*', '*', 'D', '*', '*', 'ˆ', '*', 'y', '*', 'and', 'τ1', '.', ' \\n', 'To', 'obtain', 'the', 'sensitivity', ',', 'we', 'apply', 'the', 'gradient', 'operator', 'to', 'both', 'sides', 'of', '(', '4.19', ')', ',', 'yielding', '(', '∇', 'τ1f)(∇', '*', '*', 'mτ1', ')', '*', '*', '+', '∇', '*', '*', 'mf', '*', '*', '=', '0', ',', 'and', 'deﬁne', '[', '8', ']', ':', '\\n\\n', '#', '#', '#', 'J(m', ')', '=', '∇', 'mτ1', '=', '−(∇', 'τ1f', ')', '−1', '(', '∇', 'mf', ')', '.', '(', '4.20', ')', '\\n\\n', 'This', 'results', 'in', ' \\n', '*', '*', 'J', '*', '*', '=', '(', 'diag(2', '*', '*', 'D', '*', '*', 'ˆ', '*', 'x', '*', 'τ1', ')', '*', '*', 'D', '*', '*', 'ˆ', '*', 'x', '*', '+', 'diag(2', '*', '*', 'D', '*', '*', 'ˆ', '*', 'y', '*', 'τ1', ')', '*', '*', 'D', '*', '*', 'ˆ', '*', 'y', '*', ')', '−1', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '4.21', ')', '\\n\\n', 'following', '∇', '*', '*', 'mf', '*', '*', '=', '−I', ',', 'and', 'since', 'the', 'operators', '*', '*', 'D', '*', '*', 'ˆ', 'do', 'not', 'depend', 'on', '*', '*', 'm', '*', '*', '(', 'we', 'deﬁned', 'τ0', 'and', 'its', 'derivatives', 'so', 'it', 'does', 'not', 'depend', 'on', 'κ', ')', '.', ' \\n', 'The', 'matrix', '(', '4.21', ')', 'can', 'be', 'multiplied', 'with', 'any', 'vector', 'eﬃciently', 'given', 'the', 'order', 'of', 'variables', '(', 'i', ',', '*', 'j', ')', '*', 'in', 'which', 'the', 'FM', 'algorithm', 'set', 'their', 'values', 'as', 'known', '.', 'To', 'apply', '*', '*', 'J', '*', '*', 'on', 'an', 'arbitrary', 'vector', '*', '*', 'x', ',', '*', '*', 'i.e.', 'calculate', '*', '*', 'e', '*', '*', '=', '*', '*', 'Jx', ',', '*', '*', 'a', 'linear', 'system', '*', '*', 'Ae', '*', '*', '=', '*', '*', 'x', '*', '*', 'can', 'be', 'solved', 'with', '*', '*', 'A', '*', '*', '=', '*', '*', 'J', '*', '*', '−1', '(', 'note', 'that', '*', '*', 'A', '*', '*', 'is', 'a', 'sparse', 'matrix', ')', '.', 'The', 'equations', 'of', 'this', 'linear', 'system', ',', 'which', 'correspond', 'to', 'the', 'rows', 'of', '*', '*', 'A', ',', '*', '*', 'can', 'be', 'approached', 'and', 'solved', 'sequentially', 'in', 'the', 'FM', 'order', 'of', 'variables', '.', 'Since', 'the', 'FM', 'algorithm', 'uses', 'only', 'known', 'variables', 'for', 'determining', 'each', 'new', 'variable', ',', 'then', 'when', 'looking', 'at', 'each', 'row', '*', 'i', '*', 'of', '*', '*', 'A', ',', '*', '*', 'the', 'non', '-', 'zero', 'entries', 'in', 'that', 'row', '(', 'except', '*', 'i', ')', '*', 'correspond', 'to', 'variables', 'that', 'where', 'in', 'knownwhen', 'τ', '*', 'i', '*', 'was', 'determined', 'during', 'the', 'FM', 'run', '.', 'Therefore', ',', 'if', 'all', 'those', 'variables', 'are', 'known', 'except', '*', 'i', ',', '*', 'then', 'the', 'i', '-', 'th', 'equation', 'has', 'only', 'one', 'unknown', '(', 'ei', ')', 'and', 'can', 'be', 'trivially', 'solved', '.', 'In', 'other', 'words', ',', 'if', 'we', 'permute', '*', '*', 'A', '*', '*', 'according', 'to', 'the', 'FM', 'order', ',', 'we', 'get', 'a', 'sparse', 'lower', 'triangular', 'matrix', ',', 'and', 'the', 'corresponding', 'system', 'can', 'be', 'solved', 'eﬃciently', 'in', 'one', 'forward', 'substitution', 'sweep', 'in', '*', 'O(n', ')', '*', 'operations', '.', 'For', 'the', 'non', '-', 'factored', 'equation', 'one', 'may', 'use', '(', '4.21', ')', 'with', 'non', '-', 'factored', 'operators', '*', '*', 'D', '*', '*', '*', 'x', '*', 'and', '*', '*', 'D', '*', '*', '*', 'y', '*', 'instead', 'of', 'the', 'factored', 'ones', '[', '14', ']', '.', '\\n\\n', '*', '4.1', '.', 'Travel', 'time', 'tomography', 'using', 'Gauss', '–', 'Newton', '*', '\\n\\n', 'Assume', 'that', 'we', 'have', 'several', 'sources', 'and', 'receivers', 'set', 'on', 'an', 'open', 'surface', ',', 'and', 'for', 'each', 'source', 'we', 'have', 'traveltime', 'data', '*', '*', 'd', '*', '*', '*', 'i', '*', ' \\n', 'given', 'in', 'the', 'location', 'of', 'the', 'receivers', '.', 'Based', 'on', 'these', 'observations', 'we', 'wish', 'to', 'compute', 'the', 'unknown', 'slowness', 'model', 'obs', ' \\n', 'of', 'the', 'ground', 'underneath', '.', 'The', 'inverse', 'problem', 'for', 'this', 'process', ',', 'called', 'travel', 'time', 'tomography', ',', 'may', 'be', 'given', 'by', ' \\n', '?', '\\n\\n', '?', '\\n\\n', '#', '#', '#', '?', 'ns', '\\n\\n', 'min', 'φ(m', ')', '=', 'min', '?', 'P', '?', 'τ', '*', 'i', '*', '(', 'm)−', '*', '*', 'd', '*', '*', '*', 'i', '*', '?', '2', '+', 'αR(m', ')', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '4.22', ')', '\\n\\n', '*', 'mL', '<', 'm', '<', 'mH', 'mL', '<', 'm', '<', 'mH', '*', 'obs', '*', 'i=1', '*', '\\n\\n', '---', '\\n\\n', 'where', '\\n\\n', '#', '|∇', 'τ', 'i', '|', '2', '=', 'm', '(', '?', 'x', ')', 'τ', 'i', '(', '?', 'xi', ')', '=', '0', 'i', '=', '1,', '...', ',ns', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '4.23', ')', '\\n\\n', 'Here', 'τ', '*', 'i', '*', 'is', 'the', 'travel', 'time', 'from', 'the', 'point', 'source', '?', '*', 'xi', ',', '*', 'and', '*', '*', 'm', '(', '*', '*', '?', '*', 'x', ')', '*', '=', 'κ', '(', '?', '*', 'x', ')', '*', '2', 'is', 'the', 'squared', 'slowness', 'model', 'as', 'in', '(', '1.1', ')', ',', 'only', 'now', 'it', 'is', 'unknown', '.', 'The', 'operator', '*', '*', 'P', '*', '*', '?', 'is', 'a', 'projection', 'to', 'the', 'set', 'of', 'receivers', 'that', 'gather', 'the', 'wave', 'information', '.', 'Here', 'we', 'assume', 'that', 'the', 'information', 'from', 'all', 'sources', 'is', 'available', 'on', 'all', 'the', 'receivers', ',', 'i.e.', ',', 'the', 'projection', 'operator', '*', '*', 'P', '*', '*', 'does', 'not', 'change', 'between', 'sources', '.', 'R(m', ')', 'is', 'a', 'regularization', 'term', 'and', 'α', '>', '0', 'is', 'its', 'balancing', 'parameter', '.', 'The', 'parameters', '*', 'mL', '*', 'and', '*', 'mH', '*', 'are', 'positive', 'lower', 'and', 'upper', 'bounds', 'needed', 'for', 'keeping', 'the', 'slowness', 'of', 'the', 'medium', 'physical', '.', 'We', 'note', 'that', 'the', 'observations', '*', '*', 'd', '*', '*', '*', 'i', '*', 'can', 'be', 'obs', 'obtained', 'manually', 'from', 'recorded', 'seismic', 'data', 'or', 'by', 'automatic', 'time', 'picking', '—', 'for', 'more', 'information', 'see', '[', '28', ']', 'and', 'references', 'therein', '.', ' \\n', 'Without', 'the', 'regularization', 'term', 'R(m', ')', ',', 'the', 'problem', '(', '4.22', ')', 'is', 'ill', '-', 'posed', ',', 'i.e.', ',', 'many', 'solutions', '*', '*', 'm', '*', '*', 'may', 'ﬁt', 'the', 'predicted', 'travel', 'time', 'to', 'the', 'measured', 'data', '[', '37,32', ']', '.', 'For', 'this', 'reason', ',', 'in', 'most', 'cases', 'we', 'can', 'not', 'expect', 'to', 'exactly', 'recover', 'the', 'true', 'model', ',', 'but', 'wish', 'to', 'recover', 'a', 'reasonable', 'model', 'by', 'adding', 'prior', 'information', 'using', 'the', 'regularization', 'term', 'R(m', ')', '.', 'This', 'term', 'aims', 'to', 'promote', 'physical', 'or', 'meaningful', 'solutions', 'that', 'we', 'may', 'expect', 'to', 'see', 'in', 'the', 'recovered', 'model', '.', 'For', 'example', ',', 'in', 'seismic', 'exploration', ',', 'one', 'may', 'expect', 'to', 'recover', 'a', 'layered', 'model', 'of', 'the', 'earth', 'subsurface', ',', 'hence', 'may', 'choose', '*', 'R', '*', 'to', 'promote', 'smooth', 'or', 'piecewise', '-', 'smooth', 'functions', 'like', 'the', 'total', 'variation', 'regularization', 'term', '[', '26', ']', '.', ' \\n', 'There', 'are', 'several', 'ways', 'to', 'solve', '(', '4.22', ')', ',', 'and', 'most', 'of', 'them', 'are', 'gradient', '-', 'based', '.', 'Here', 'we', 'focus', 'on', 'Gauss', '–', 'Newton', '.', 'This', 'method', 'is', 'computationally', 'favorable', 'here', ',', 'since', 'its', 'cost', 'is', 'governed', 'by', 'the', 'application', 'of', 'sensitivities', ',', 'which', 'are', 'easy', 'to', 'obtain', 'using', 'FM', '.', 'Given', 'an', 'approximation', '*', '*', 'm', '*', '*', '(', 'k', ')', 'at', 'the', 'k', '-', 'th', 'iteration', ',', 'we', 'place', '(', '4.17', ')', 'into', '(', '4.22', ')', 'and', 'get', '\\n\\n', '#', '#', '#', '1', '?', 'ns', '?', '?', '?', '\\n\\n', '#', '#', '#', '#', '#', '#', 'min', '?', 'P', 'diag(τ', 'i', ')', 'τ', 'i', '(', 'm', '(', 'k', ')', ')', '+', 'J', 'i', 'δm', '−', 'd', 'i', '?', '2', '+', 'αR(m', '(', 'k', ')', '+', 'δm', ')', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '4.24', ')', '\\n\\n', 'δm', '2', '0', '1', 'obs', '*', 'i=1', '*', '\\n\\n', 'where', '*', '*', 'J', '*', '*', '*', 'i', '*', 'is', 'the', 'sensitivity', 'of', 'τ', '*', 'i', '*', 'at', '*', '*', 'm', '*', '*', '(', 'k', ')', '.', 'Minimizing', 'this', 'approximation', 'for', 'δm', 'leads', 'to', 'computing', 'the', 'gradient', '1', '\\n\\n', '-', '*', 'ns', '*', '?', '?', '\\n\\n', '#', '#', '#', '#', '#', '#', '∇', 'mφ(m', '(', 'k', ')', ')', '=', '(', 'J', 'i', ')', '?', 'diag(τ', 'i', ')', 'P', 'P', '?', 'diag(τ', 'i', ')', 'τ', 'i', '−', 'd', 'i', '+', 'α', '∇', 'mR(m', '(', 'k', ')', ')', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '4.25', ')', '\\n\\n', '0', '0', '1', 'obs', '*', 'i=1', '*', '\\n\\n', 'We', 'then', 'approximately', 'solve', 'the', 'linear', 'system', '\\n\\n', '*', '*', 'Hδm', '*', '*', '=', '−∇', 'mφ(m', '(', 'k', ')', ')', 'where', '\\n\\n', '#', '#', '#', '?', 'n', '\\n\\n', '#', '#', '#', '#', 'H', '=', '\\n\\n', '#', '#', '#', '#', '#', '#', '(', 'J', 'i', ')', '?', 'diag(τ', 'i', ')', 'PP', '?', 'diag(τ', 'i', ')', 'J', 'i', '+', 'α?mR(m', '(', 'k', ')', ')', '.', '\\n\\n', '0', ' \\n', '0', '*', 'i', '*', '=', '1', '\\n\\n', 'The', 'linear', 'system', 'is', 'solved', 'using', 'the', 'conjugate', 'gradient', 'method', 'where', 'only', 'matrix', 'vector', 'products', 'are', 'computed', '.', 'Finally', ',', 'the', 'model', 'is', 'updated', ',', '*', '*', 'm', '*', '*', '←', '*', '*', 'm', '*', '*', '+', 'μδm', 'where', 'μ', '≤', '1', 'is', 'a', 'line', 'search', 'parameter', 'that', 'is', 'chosen', 'such', 'that', 'the', 'objective', 'function', 'is', 'decreased', 'at', 'each', 'iteration', '.', '\\n\\n', '#', '#', '#', '#', '#', '5', '.', 'Numerical', 'results', ':', 'solving', 'the', 'eikonal', 'equation', '\\n\\n', 'In', 'this', 'section', 'we', 'demonstrate', 'the', 'FM', 'algorithm', 'using', 'ﬁrst', 'or', 'second', 'order', 'upwind', 'discretization', 'for', 'solving', 'the', 'fac-', 'tored', 'eikonal', 'equation', '(', '1.4', ')', '.', 'We', 'demonstrate', 'both', 'the', 'accuracy', 'of', 'the', 'obtained', 'solution', ',', 'and', 'the', 'computational', 'cost', 'of', 'calculating', 'it', 'using', 'the', 'FM', 'algorithm', '.', 'The', 'accuracy', 'of', 'the', 'algorithm', 'is', 'demonstrated', 'by', 'two', 'error', 'norms', ':', 'one', 'in', 'the', 'maxi-', 'mum', 'norm', '*', 'l∞', ',', '*', 'and', 'one', 'is', 'the', 'mean', '*', 'l2', '*', 'norm', 'deﬁned', 'by', 'the', 'standard', '*', 'l2', '*', 'norm', 'of', 'the', 'error', 'divided', 'by', 'the', 'square', 'root', 'of', 'the', 'total', 'number', 'of', 'variables', '.', 'Similarly', 'to', '[', '31', ']', ',', 'we', 'show', 'these', 'two', 'measures', 'to', 'demonstrate', 'the', 'accuracy', 'of', 'the', 'second', 'order', 'scheme', '.', 'Showing', 'the', '*', 'l∞', '*', 'norm', 'of', 'the', 'error', 'for', 'this', 'scheme', 'may', 'result', 'in', 'only', 'ﬁrst', 'order', 'accuracy', ',', 'because', 'at', 'some', 'points', 'our', 'second', 'order', 'FM', 'algorithm', 'reverts', 'to', 'ﬁrst', 'order', 'operators', ',', 'which', 'may', 'be', 'picked', 'by', 'the', '*', 'l∞', '*', 'norm', '.', ' \\n', 'To', 'demonstrate', 'the', 'eﬃciency', 'of', 'the', 'computation', ',', 'we', 'measure', 'the', 'time', 'in', 'which', 'the', 'algorithm', 'solves', 'each', 'test', '.', 'We', 'also', 'show', 'this', 'timing', 'in', 'terms', 'of', 'work', '-', 'units', ',', 'where', 'each', 'work', 'unit', 'is', 'deﬁned', 'by', 'the', 'time', 'that', 'it', 'takes', 'to', 'evaluate', 'the', 'equation', '(', '1.1', ')', 'using', 'given', 'central', 'difference', 'gradient', 'stencils', '(', 'without', 'memory', 'allocation', 'time', ')', '.', 'We', 'note', 'that', 'the', 'more', 'reliable', 'timings', 'appear', 'for', 'the', 'large', 'scale', 'examples', '.', ' \\n', 'We', 'use', 'analytical', 'examples', 'for', 'media', 'where', 'there', 'is', 'a', 'known', 'analytical', 'solution', 'for', 'a', 'point', 'source', 'located', 'at', '?', '*', 'x0', '.', '*', 'The', 'ﬁrst', 'two', 'appear', 'in', '[', '6', ']', '.', 'We', 'show', 'results', 'for', 'two', 'and', 'three', 'dimensions', '.', 'Our', 'code', 'is', 'written', 'in', 'Julia', 'language', '[', '4', ']', 'version', '0.4.5', ',', 'and', 'all', 'our', 'tests', 'were', 'calculated', 'on', 'a', 'laptop', 'machine', 'using', 'Windows', '10', '64', 'bit', 'OS', ',', 'with', 'Intel', 'core', '-', 'i7', '2.8', 'GHz', 'CPU', 'with', '32', 'GB', 'of', 'RAM', '.', 'Our', 'code', 'is', 'publicly', 'available', 'in', '[', 'https', ':/', '/github', '.com', '/JuliaInv', '/FactoredEikonalFastMarching](https://github.com', '/', 'JuliaInv', '/', 'FactoredEikonalFastMarching.jl', ')', '.jl', '.', 'We', 'do', 'not', 'enforce', 'the', 'monotonicity', 'in', 'the', 'results', 'below', ',', 'but', 'those', 'can', 'be', 'enforced', 'in', 'our', 'package', '.', 'The', 'three', 'test', 'cases', 'are', 'listed', 'below', '.', '\\n\\n', '---', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0785.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '3', '.', '*', '*', 'The', '2D', 'slowness', 'model', 'κ(?x', ')', 'of', 'the', 'three', 'test', 'cases', 'and', 'the', 'corresponding', 'contours', 'of', 'the', '2D', 'solutions', '.', '\\n\\n', '*', 'Test', 'case', '1', ':', 'constant', 'gradient', 'of', 'squared', 'slowness', '*', '\\n\\n', '#', '#', '#', 'κ', '2', '(', '?', 'x', ')', '=', 's', '2', '+', '2a', 'e1', '?', '·', '(', '?', 'x−', '?', 'x0', ')', ',', '\\n\\n', '0', '\\n\\n', 'In', 'this', 'test', 'case', 'we', 'set', ':', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '5.26', ')', '\\n\\n', 'where', '*', 'e1', '*', '?', '=', '(', '1', ',', '0', ')', 'is', 'a', 'unit', 'vector', ',', 'and', '·', 'is', 'the', 'standard', 'dot', 'product', '.', 'The', 'parameters', '*', 'a', ',', 's0', ',', '*', 'the', 'domain', 'and', 'the', 'source', 'location', 'are', 'chosen', 'differently', 'in', '2D', 'and', '3D.', 'The', 'corresponding', 'exact', 'solution', 'is', 'given', 'by', ' \\n', '*', 'τexact', '(', '*', '?', '*', 'x', ')', '*', '=', '*', 'S', '*', '¯', '2', 'σ', '−', '1', '*', 'a', '*', '2', '(', 'σ', '3', ')', ',', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '5.27', ')', '\\n\\n', '6', 'where', '\\n\\n', '#', '#', '#', 'S', '¯', '2', '(', '?', 'x', ')', '=', 's', '2', '+', 'a', 'e1', '?', '·', '(', '?', 'x−', '?', 'x0', ')', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '5.28', ')', '\\n\\n', '-', '0', '?', '?', '−1', '\\n\\n', '#', '#', '#', 'σ', '2', '(', '?', 'x', ')', '=', 'S', '¯', '2', '+', 'S', '¯', '4', '−a', '2', '?', '?', 'x−', 'x0', '?', '?', '2', '2', '?', '?', 'x−', 'x0', '?', '?', '2', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '5.29', ')', '\\n\\n', 'Figs', '.', '3(a', ')', 'and', '3(d', ')', 'show', 'the', 'model', 'κ', 'for', 'this', 'test', 'case', 'with', 'the', 'chosen', 'parameters', 'for', '2D.', '\\n\\n', '*', 'Test', 'case', '2', ':', 'constant', 'gradient', 'of', 'velocity', '*', ' \\n', 'In', 'this', 'test', 'case', 'we', 'set', ':', '\\n\\n', '#', '#', '#', '?', '?', '−1', '\\n\\n', '#', '#', '#', 'κ', '(', '?', 'x', ')', '=', '1', '+', 'a', 'e1', '?', '·', '(', '?', 'x−', '?', 'x0', ')', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '5.30', ')', '\\n\\n', '#', '#', '#', 's', '0', '\\n\\n', 'where', 'again', '*', 'e1', '*', '?', '=', '(', '1', ',', '0', ')', ',', '·', 'is', 'the', 'dot', 'product', ',', 'and', 'the', 'parameters', '*', 'a', ',', 's0', ',', '*', 'the', 'domain', 'and', 'the', 'source', 'location', 'are', 'chosen', 'differently', 'in', '2D', 'and', '3D.', 'The', 'exact', 'solution', 'is', 'given', 'by', ' \\n', '?', ' \\n', '?', ' \\n', '*', 'τexact', '(', '*', '?', '*', 'x', ')', '*', '=', '1', 'acosh', '1', '+', '1', '*', 's0a', '*', '2', 'κ', '(', '?', 'x', ')', '?', '?', 'x−', '?', '*', 'x0', '*', '?', '2', '.', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '5.31', ')', '\\n\\n', '*', 'a', '*', ' \\n', '2', ' \\n', 'Figs', '.', '3(b', ')', 'and', '3(e', ')', 'show', 'the', 'model', 'κ', 'for', 'this', 'test', 'case', 'with', 'the', 'chosen', 'parameters', 'for', '2D.', ' \\n', '*', 'Test', 'case', '3', ':', 'Gaussian', 'factor', '*', 'In', 'this', 'test', 'case', 'we', 'choose', 'a', 'function', 'for', 'τ', '*', 'exact', '*', 'and', 'multiply', 'it', 'by', 'τ0', 'to', 'get', '*', 'τexact', '.', '*', 'We', 'choose', 'τ1', 'as', 'a', 'Gaussian', 'function', 'centered', 'around', 'a', 'point', '*', 'x1', ':*', '?', '\\n\\n', '1', '?', '\\n\\n', '?', '\\n\\n', '#', 'τ', 'exact', '(', '?', 'x', ')', '=', '1', 'exp', '−', '(', '?', 'x−', '?', 'x1', ')', 'T', '?', '(', '?', 'x−', '?', 'x1', ')', '+', '1', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '5.32', ')', '\\n\\n', '1', '2', '2', ' \\n', 'where', '?', 'is', 'a', '2', '×', '2', 'or', '3', '×', '3', 'positive', 'diagonal', 'matrix', '.', 'As', 'before', ',', 'the', 'parameters', '?', '*', 'x1', '*', 'and', '?', ',', 'the', 'domain', 'and', 'the', 'source', 'location', '*', 'x0', '*', '?', 'are', 'chosen', 'differently', 'in', '2D', 'and', '3D.', 'Here', 'κ', '(', '?', '*', 'x', ')', '*', 'is', 'deﬁned', 'by', '(', '3.8', ')', ',', 'with', 'τ0', 'being', 'the', 'distance', 'function', '.', 'Figs', '.', '3(c', ')', 'and', '3(f', ')', 'show', 'the', 'model', 'κ', 'for', 'this', 'test', 'with', 'the', 'chosen', 'parameters', 'for', '2D.', '\\n\\n', '*', '5.1', '.', 'Two', 'dimensional', 'tests', '*', '\\n\\n', 'Now', 'we', 'show', 'results', 'for', 'the', 'two', 'dimensional', 'versions', 'of', 'the', 'tests', 'mentioned', 'above', '.', 'For', 'all', 'tests', 'in', '2D', 'we', 'choose', 'the', 'domain', 'to', 'be', '[', '0', ',', '4', ']', '×[0', ',', '8', ']', ',', 'while', '*', 'h', '*', '=', '*', 'hx', '*', '=', '*', 'hy', '*', 'varies', 'from', 'large', 'to', 'small', '.', '\\n\\n', '---', '\\n\\n', '#', '#', '#', 'Table', '1', '\\n\\n', 'Results', 'for', '2D', 'constant', 'gradient', 'of', 'squared', 'slowness', '(', 'test', 'case', '1', ')', '.', 'The', 'error', 'measures', 'are', 'in', 'the', '[', 'l∞', ',', 'mean', '*', 'l2', '*', ']', 'norms', '.', '\\n\\n', '*', 'h', 'n', '*', '1', '*', 'st', '*', 'order', '\\n\\n', 'error', 'in', 'τ', '\\n\\n', '2', '*', 'nd', '*', 'order', '\\n\\n', 'time', '(', 'work', ')', 'error', 'in', 'τ', ' \\n', 'time', '(', 'work', ')', '\\n\\n', '1/40', '161×', '321', ' \\n', '[', '3.71e−03', ',', '9.42e−04', ']', '0.05', 's', '(', '217', ')', '[', '9.33e−05', ',', '9.26e−06', ']', '0.05', 's', '(', '202', ')', '1/80', '321×', '641', ' \\n', '[', '1.85e−03', ',', '4.69e−04', ']', '0.19', 's', '(', '199', ')', '[', '3.30e−05', ',', '2.21e−06', ']', '0.20', 's', '(', '209', ')', '1/160', '641×', '1281', '[', '9.22e−04', ',', '2.34e−04', ']', '0.85', 's', '(', '217', ')', '[', '1.14e−05', ',', '5.32e−07', ']', '0.85', 's', '(', '218', ')', '1/320', '1281×', '2561', '[', '4.60e−04', ',', '1.17e−04', ']', '3.89', 's', '(', '266', ')', '[', '4.06e−06', ',', '1.28e−07', ']', '3.84', 's', '(', '262', ')', '1/640', '2561×', '5121', '[', '2.30e−04', ',', '5.83e−05', ']', '16.4', 's', '(', '278', ')', '[', '1.47e−06', ',', '3.12e−08', ']', '17.1', 's', '(', '289', ')', '1/1280', '5121×', '10241', '[', '1.15e−04', ',', '2.92e−05', ']', '76.6', 's', '(', '316', ')', '[', '5.18e−07', ',', '7.64e−09', ']', '77.5', 's', '(', '320', ')', '\\n\\n', '#', '#', '#', 'Table', '2', '\\n\\n', 'Results', 'for', 'the', '2D', 'constant', 'gradient', 'of', 'velocity', '(', 'test', 'case', '2', ')', '.', 'The', 'error', 'measures', 'are', 'in', 'the', '[', 'l∞', ',', 'mean', '*', 'l2', '*', ']', 'norms', '.', '\\n\\n', '*', 'h', 'n', '*', '1', '*', 'st', '*', 'order', '\\n\\n', 'error', 'in', 'τ', '\\n\\n', '2', '*', 'nd', '*', 'order', '\\n\\n', 'time', '(', 'work', ')', 'error', 'in', 'τ', ' \\n', 'time', '(', 'work', ')', '\\n\\n', '1/40', '161×', '321', ' \\n', '[', '2.66e−02', ',', '1.01e−02', ']', '0.05', 's', '(', '205', ')', '[', '4.86e−04', ',', '2.90e−04', ']', '0.05', 's', '(', '236', ')', '1/80', '321×', '641', ' \\n', '[', '1.32e−02', ',', '5.05e−03', ']', '0.21', 's', '(', '221', ')', '[', '1.67e−04', ',', '7.38e−05', ']', ' \\n', '0.20', 's', '(', '206', ')', '1/160', '641×', '1281', '[', '6.59e−03', ',', '2.52e−03', ']', '0.87', 's', '(', '223', ')', '[', '5.18e−05', ',', '1.85e−05', ']', '0.86', 's', '(', '221', ')', '1/320', '1281×', '2561', '[', '3.29e−03', ',', '1.26e−03', ']', '3.80', 's', '(', '259', ')', '[', '1.90e−05', ',', '4.61e−06', ']', '3.88', 's', '(', '265', ')', '1/640', '2561×', '5121', '[', '1.65e−03', ',', '6.28e−04', ']', '16.2', 's', '(', '274', ')', '[', '6.58e−06', ',', '1.15e−06', ']', '16.6', 's', '(', '280', ')', '1/1280', '5121×', '10241', '[', '8.22e−04', ',', '3.14e−04', ']', '73.8', 's', '(', '304', ')', '[', '2.28e−06', ',', '2.86e−07', ']', '74.6', 's', '(', '307', ')', '\\n\\n', '#', '#', '#', 'Table', '3', '\\n\\n', 'Results', 'for', 'the', '2D', 'Gaussian', 'factor', 'test', 'case', '(', 'test', 'case', '3', ')', '.', 'The', 'error', 'measures', 'are', 'in', 'the', '[', 'l∞', ',', 'mean', '*', 'l2', '*', ']', 'norms', '.', '\\n\\n', '*', 'h', 'n', '*', '1', '*', 'st', '*', 'order', '\\n\\n', 'error', 'in', 'τ', '\\n\\n', '2', '*', 'nd', '*', 'order', '\\n\\n', 'time', '(', 'work', ')', 'error', 'in', 'τ', ' \\n', 'time', '(', 'work', ')', '\\n\\n', '1/40', '161×', '321', ' \\n', '[', '6.15e−03', ',', '3.86e−03', ']', '0.05', 's', '(', '205', ')', '[', '1.60e−04', ',', '5.94e−05', ']', '0.05', 's', '(', '236', ')', '1/80', '321×', '641', ' \\n', '[', '3.07e−03', ',', '1.93e−03', ']', ' \\n', '0.21', 's', '(', '221', ')', '[', '3.85e−05', ',', '1.56e−05', ']', '0.20', 's', '(', '206', ')', '1/160', '641×', '1281', '[', '1.54e−03', ',', '9.67e−04', ']', '0.87', 's', '(', '223', ')', '[', '1.08e−05', ',', '4.03e−06', ']', '0.86', 's', '(', '221', ')', '1/320', '1281×', '2561', '[', '7.68e−04', ',', '4.83e−04', ']', '3.80', 's', '(', '259', ')', '[', '3.18e−06', ',', '1.04e−06', ']', '3.88', 's', '(', '265', ')', '1/640', '2561×', '5121', '[', '3.84e−04', ',', '2.42e−04', ']', '16.2', 's', '(', '275', ')', '[', '9.59e−07', ',', '2.66e−07', ']', '16.6', 's', '(', '280', ')', '1/1280', '5121×', '10241', '[', '1.92e−04', ',', '1.21e−04', ']', '73.8', 's', '(', '304', ')', '[', '2.99e−07', ',', '6.88e−08', ']', '74.6', 's', '(', '307', ')', '\\n\\n', '*', 'Test', 'case', '1', ':', 'constant', 'gradient', 'of', 'squared', 'slowness', '*', 'For', 'this', '2D', 'setting', ',', 'we', 'use', 'the', 'parameters', '*', 'a', '*', '=', '−0.4', ',', '*', 's0', '*', '=', '2.0', ',', 'and', 'the', 'source', 'location', 'is', '?', '*', 'x0', '*', '=', '(', '0', ',', '4', ')', '.', 'Table', '1', 'summarizes', 'the', 'results', 'for', 'this', 'test', '.', 'On', 'the', 'ﬁrst', 'order', 'section', 'we', 'see', 'a', 'typical', 'ﬁrst', 'order', 'convergence', 'rate', 'in', 'both', 'error', 'norms', '.', 'As', 'the', 'mesh', 'size', 'increases', 'by', 'two', 'in', 'each', 'direction', ',', 'the', 'errors', 'drop', 'by', 'a', 'factor', 'of', 'two', '.', 'In', 'the', 'second', 'order', 'section', 'we', 'see', 'the', 'typical', 'behavior', 'of', 'the', 'FM', 'algorithm', '.', 'At', 'some', 'points', ',', 'ﬁrst', 'order', 'operators', 'are', 'used', ',', 'and', 'hence', 'the', 'error', 'at', 'those', 'locations', 'dominates', 'the', '*', 'l∞', '*', 'norm', '.', 'Still', ',', 'we', 'observe', 'much', 'better', 'convergence', 'compared', 'to', 'the', 'ﬁrst', 'order', '*', 'l∞', ',', '*', 'only', 'it', 'is', 'not', 'of', 'second', 'order', '.', 'In', 'the', 'mean', '*', 'l2', '*', 'norm', 'we', 'see', 'typical', 'second', 'order', 'convergence', '—', 'as', 'the', 'mesh', 'size', 'increases', 'by', 'two', 'in', 'each', 'direction', ',', 'the', 'errors', 'drop', 'by', 'a', 'factor', 'of', 'four', '.', 'In', 'any', 'case', ',', 'the', 'errors', 'in', 'the', 'second', 'order', 'columns', 'are', 'much', 'smaller', 'than', 'those', 'in', 'the', 'ﬁrst', 'order', 'columns', '.', ' \\n', 'In', 'terms', 'of', 'computational', 'cost', ',', 'the', '2D', 'FM', 'algorithm', 'exhibits', 'favorable', 'timings', 'and', 'work', 'counts', '.', 'Except', 'the', 'small', 'cases', ',', 'the', 'cost', 'of', 'the', 'algorithm', 'is', 'comparable', 'to', '200–300', 'function', 'evaluations', 'using', 'standard', 'difference', 'stencils', '.', 'This', 'is', 'maintained', 'for', 'all', 'the', 'considered', 'mesh', 'sizes', '.', 'The', 'difference', 'in', 'the', 'computational', 'cost', 'between', 'using', 'ﬁrst', 'and', 'second', 'order', 'schemes', 'is', 'only', 'about', '10', '%', 'of', 'execution', 'time', '.', '\\n\\n', '*', 'Test', 'case', '2', ':', 'constant', 'gradient', 'of', 'velocity', '*', 'For', 'this', '2D', 'setting', ',', 'we', 'use', 'the', 'parameters', '*', 'a', '*', '=', '1.0', ',', '*', 's0', '*', '=', '2.0', ',', 'and', 'the', 'location', 'of', 'the', 'source', 'is', 'again', 'at', '?', '*', 'x0', '*', '=', '(', '0', ',', '4', ')', '.', 'Table', '2', 'summarizes', 'the', 'results', 'for', 'this', 'test', '.', 'The', 'results', 'here', 'are', 'almost', 'identical', 'to', 'the', 'previous', 'test', 'case', '.', 'The', 'ﬁrst', 'order', 'columns', 'show', 'typical', 'ﬁrst', 'order', 'convergence', 'in', 'both', 'error', 'norms', '.', 'The', 'second', 'order', 'columns', 'show', 'better', 'convergence', 'and', 'exhibits', 'second', 'order', 'convergence', 'in', 'the', 'mean', '*', 'l2', '*', 'norm', 'column', '.', 'The', 'computational', 'costs', 'columns', 'show', 'timings', 'which', 'are', 'almost', 'identical', 'to', 'the', 'previous', 'test', 'case', '.', '\\n\\n', '*', 'Test', 'case', '3', ':', 'Gaussian', 'factor', '*', 'For', 'this', 'setting', ',', 'we', 'use', 'the', 'parameters', '?', '=', 'diag(0.1', ',', '0.4', ')', ',', '?', '*', 'x1', '*', '=', '(', '4/3', ',', '2', ')', '(', 'ﬂoored', 'to', 'the', 'closest', 'grid', 'point', ')', ',', 'and', 'the', 'source', 'is', 'located', 'in', 'the', 'point', '?', '*', 'x0', '*', '=', '(', '1', ',', '2', ')', '.', 'Table', '3', 'summarizes', 'the', 'results', 'for', 'this', 'test', '.', 'Again', 'we', 'see', 'ﬁrst', 'order', 'convergence', 'at', 'the', 'ﬁrst', 'order', 'columns', 'in', 'both', 'norms', '.', 'On', 'the', 'second', 'order', 'columns', 'we', 'again', 'see', 'faster', 'convergence', ',', 'and', 'in', 'the', 'mean', '*', 'l2', '*', 'norm', 'column', 'we', 'see', 'convergence', 'rate', 'that', 'is', 'close', 'to', 'second', 'order', '—', 'the', 'error', 'decreases', 'by', 'a', 'factor', 'of', 'about', '3.9', 'when', 'the', 'mesh', 'size', 'increases', 'by', 'a', 'factor', 'of', '2', 'in', 'each', 'dimension', '.', 'Again', 'we', 'see', 'similar', 'behavior', 'in', 'the', 'computational', 'cost', 'columns', '.', ' \\n', 'We', 'now', 'wish', 'to', 'better', 'illustrate', 'the', 'difference', 'between', 'the', 'accuracy', 'of', 'the', 'ﬁrst', 'order', 'scheme', 'and', 'the', 'second', 'order', 'scheme', '.', 'First', ',', 'Fig', '.', '4', 'shows', 'contours', 'of', 'the', 'exact', 'and', 'approximate', 'solutions', 'in', 'certain', 'regions', 'of', 'the', 'domain', 'for', 'the', 'second', 'and', 'third', 'test', 'cases', '.', 'It', 'is', 'clear', 'that', 'the', 'ﬁrst', 'order', 'approximation', 'is', 'less', 'accurate', 'than', 'the', 'second', 'order', 'one', '.', 'Next', ',', 'Fig', '.', '5', '\\n\\n', '---', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0478.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '4', '.', '*', '*', 'Contours', 'of', 'small', 'regions', 'in', 'the', 'exact', ',', 'ﬁrst', 'order', 'accurate', 'and', 'second', 'order', 'accurate', 'travel', 'times', 'τ', 'using', '*', 'h', '*', '=', '0.1', '.', 'The', 'exact', 'solution', 'appears', 'in', 'black', 'line', '.', 'The', 'ﬁrst', 'order', 'approximation', 'appears', 'in', 'dotted', 'red', 'line', 'and', 'the', 'second', 'order', 'approximation', 'appears', 'in', 'a', 'dashed', 'blue', 'line', 'mostly', 'right', 'with', 'the', 'exact', 'solution', '.', '(', 'For', 'interpretation', 'of', 'the', 'references', 'to', 'color', 'in', 'this', 'ﬁgure', 'legend', ',', 'the', 'reader', 'is', 'referred', 'to', 'the', 'web', 'version', 'of', 'this', 'article', '.', ')', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0479.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '5', '.', '*', '*', 'The', 'accuracy', 'of', 'the', 'FM', 'approximations', 'in', 'logarithmic', 'scales', 'for', 'the', '2D', 'cases', '.', 'Red', 'plots', 'are', 'used', 'for', 'ﬁrst', 'order', 'approximations', ',', 'blue', 'plots', 'for', 'second', 'order', 'approximations', ';', 'dotted', 'lines', 'for', '*', 'l∞', '*', 'error', 'norm', 'and', 'solid', 'for', 'mean', '*', 'l2', '*', 'norm', '.', 'Black', 'circles', 'denote', 'a', 'reference', 'for', 'exact', 'second', 'order', 'convergence', 'rate', '.', '(', 'For', 'interpretation', 'of', 'the', 'references', 'to', 'color', 'in', 'this', 'ﬁgure', 'legend', ',', 'the', 'reader', 'is', 'referred', 'to', 'the', 'web', 'version', 'of', 'this', 'article', '.', ')', '\\n\\n', 'shows', 'plots', 'of', 'the', 'errors', 'in', 'Tables', '1–3', 'in', 'logarithmic', 'scales', 'for', 'both', '*', 'h', '*', 'and', 'the', 'error', 'norms', ',', 'where', 'the', 'order', 'of', 'conver-', 'gence', 'determines', 'the', 'slope', 'of', 'the', 'lines', '.', 'It', 'is', 'clear', 'that', 'in', 'all', 'cases', ',', 'using', 'the', 'second', 'order', 'scheme', 'we', 'get', 'second', 'order', 'convergence', 'in', 'mean', '*', 'l2', '*', 'norm', ',', 'and', 'a', 'bit', 'more', 'than', 'ﬁrst', 'order', 'convergence', 'in', 'the', '*', 'l∞', '*', 'norm', '.', '\\n\\n', '*', '5.2', '.', 'Three', 'dimensional', 'tests', '*', '\\n\\n', 'We', 'now', 'show', 'results', 'for', 'the', 'same', 'type', 'of', 'tests', 'in', 'three', 'dimensions', '.', 'For', 'all', 'the', '3D', 'tests', 'we', 'choose', 'the', 'domain', 'to', 'be', '[', '0', ',', '0.8', ']', '×[0', ',', '1.6', ']', '×[0', ',', '1.6', ']', ',', 'and', '*', 'h', '*', '=', '*', 'hx', '*', '=', '*', 'hy', '*', '=', '*', 'hz', '*', 'varies', '.', '\\n\\n', '*', 'Test', 'case', '1', ':', 'constant', 'gradient', 'of', 'squared', 'slowness', '*', 'For', 'the', '3D', 'version', 'of', 'this', 'test', 'case', 'we', 'use', 'the', 'parameters', '*', 'a', '*', '=', '−1.65', ',', 'and', '*', 's0', '*', '=', '2.0', ',', 'and', 'the', 'source', 'is', 'located', 'at', '(', '0', ',', '0.8', ',', '0.8', ')', '.', 'Table', '4', 'summarizes', 'the', 'results', 'for', 'this', 'test', 'case', '.', 'Again', ',', 'like', 'in', 'two', 'dimensions', ',', 'the', 'ﬁrst', 'order', 'version', 'of', 'FM', 'yields', 'ﬁrst', 'order', 'convergence', 'rate', 'in', 'both', 'error', 'norms', '.', 'When', 'using', 'the', 'second', 'order', 'scheme', 'we', 'get', 'a', 'super', '-', 'linear', 'convergence', 'rate', 'in', 'the', '*', 'l∞', '*', 'column', ',', 'and', 'second', 'order', 'convergence', 'in', 'the', 'mean', '*', 'l2', '*', 'column', '.', 'We', 'note', 'that', 'in', '3D', 'the', 'FM', 'algorithm', 'reverts', 'to', 'ﬁrst', 'order', 'scheme', 'on', '2D', 'manifolds', ',', 'where', 'the', 'derivative', 'in', 'each', 'dimension', 'switches', 'sign', ',', 'and', 'not', 'on', '1D', 'curves', 'as', 'in', '2D.', ' \\n', 'In', 'terms', 'of', 'computational', 'cost', ',', 'it', 'is', 'obvious', 'that', 'the', '3D', 'problem', 'is', 'much', 'more', 'expensive', 'than', 'the', '2D', 'one', '.', 'The', 'compu-', 'tational', 'cost', 'in', 'seconds', 'per', 'grid', '-', 'point', 'in', '3D', 'is', 'about', '3', 'times', 'higher', 'than', 'the', 'corresponding', 'cost', 'in', '2D.', 'That', 'is', 'because', 'the', 'treatment', 'of', 'each', 'grid', 'point', 'is', 'more', 'expensive', '(', 'more', 'neighbors', 'and', 'more', 'derivative', 'directions', ')', ',', 'and', 'the', 'number', 'of', 'grid', 'points', 'that', 'are', 'processed', 'inside', 'the', 'heap', 'is', 'much', 'larger', '(', 'a', '2D', 'manifold', 'of', 'points', 'compared', 'to', 'a', '1D', 'curve', ')', '.', 'As', 'a', 'result', ',', 'when', 'we', 'normalize', 'the', 'timing', 'by', 'the', 'cost', 'of', 'a', '3D', '“', 'work', '-', 'unit', '”', '(', 'evaluation', 'of', '(', '1.1', ')', 'in', '3D', ')', ',', 'the', 'cost', 'grows', 'a', 'little', 'when', 'the', 'mesh', '-', 'size', 'grows', '.', 'Still', ',', 'solving', 'the', 'problem', 'requires', '200–500', 'work', 'units', '.', 'Again', ',', 'using', 'the', 'ﬁrst', 'and', 'second', 'order', 'schemes', 'requires', 'similar', 'computational', 'effort', 'in', 'our', '3D', 'implementation', 'of', 'the', 'FM', 'algorithm', '.', '\\n\\n', '*', 'Test', 'case', '2', ':', 'constant', 'gradient', 'of', 'velocity', '*', '\\n\\n', 'For', 'the', '3D', 'version', 'of', 'this', 'test', 'case', 'we', 'use', 'the', 'parameters', '*', 'a', '*', '=', '1.0', ',', 'and', '*', 's0', '*', '=', '2.0', ',', 'and', 'the', 'source', 'is', 'located', 'at', '(', '0', ',', '0.8', ',', '0.8', ')', '.', 'Table', '5', 'summarizes', 'the', 'results', 'for', 'this', 'test', 'case', '.', 'As', 'in', 'the', 'previous', 'case', ',', 'we', 'get', 'ﬁrst', 'order', 'convergence', 'when', 'using', 'the', 'ﬁrst', 'order', 'scheme', ',', 'in', 'both', 'error', 'norms', '.', 'Again', ',', 'when', 'using', 'the', 'second', 'order', 'scheme', 'we', 'get', 'a', 'super', '-', 'linear', 'convergence', 'rate', 'in', 'the', '*', 'l∞', '*', 'column', ',', 'and', 'second', 'order', 'convergence', 'in', 'the', 'mean', '*', 'l2', '*', 'column', '.', '\\n\\n', '*', 'Test', 'case', '3', ':', 'Gaussian', 'factor', '*', 'For', 'this', '3D', 'test', 'case', 'we', 'use', 'the', 'parameters', '?', '=', 'diag(0.2', ',', '0.4', ',', '0.1', ')', ',', '?', '*', 'x1', '*', '=', '(', '0.4', ',', '1.6', ',', '0.4', ')', '(', 'ﬂoored', 'to', 'the', 'closest', 'grid', 'point', ')', ',', 'and', 'the', 'source', 'is', 'located', 'in', 'the', 'point', '?', '*', 'x0', '*', '=', '(', '0.2', ',', '0.4', ',', '0.4', ')', '.', 'Table', '6', 'summarizes', 'the', '3', 'results', 'for', 'this', '\\n\\n', '---', '\\n\\n', '#', '#', '#', 'Table', '4', '\\n\\n', 'Results', 'for', 'the', '3D', 'constant', 'gradient', 'of', 'squared', 'slowness', 'test', 'case', '(', 'test', 'case', '1', ')', '.', 'The', 'error', 'measures', 'are', 'in', 'the', '[', 'l∞', ',', 'mean', '*', 'l2', '*', ']', 'norms', '.', '*', 'h', 'n', '*', '1', '*', 'st', '*', 'order', ' \\n', 'error', 'in', 'τ', '\\n\\n', '2', '*', 'nd', '*', 'order', ' \\n', 'time', '(', 'work', ')', 'error', 'in', 'τ', ' \\n', 'time', '(', 'work', ')', '1/20', '17×', '33×', '33', ' \\n', '[', '5.41e−03', ',', '1.46e−03', ']', '0.04', 's', '(', '236', ')', '[', '5.63e−4', ',', '1.49e−04', ']', ' \\n', '0.04', 's', '(', '234', ')', '1/40', '33×', '65×', '65', ' \\n', '[', '2.64e−03', ',', '7.05e−04', ']', '0.30', 's', '(', '230', ')', '[', '2.00e−04', ',', '3.52e−05', ']', '0.32', 's', '(', '235', ')', '1/80', '65×', '129×', '129', '[', '1.30e−03', ',', '3.46e−04', ']', '2.88', 's', '(', '332', ')', '[', '6.99e−05', ',', '7.82e−06', ']', '2.90', 's', '(', '334', ')', '1/160', '129×', '257×', '257', '[', '6.41e−04', ',', '1.72e−04', ']', '28.7', 's', '(', '427', ')', '[', '2.51e−05', ',', '1.68e−06', ']', '29.0', 's', '(', '432', ')', '1/320', '257×', '513×', '513', '[', '3.19e−04', ',', '8.55e−05', ']', '264', 's', '(', '481', ')', '[', '8.78e−06', ',', '3.53e−07', ']', '272', 's', '(', '497', ')', '\\n\\n', '#', '#', '#', 'Table', '5', '\\n\\n', 'Results', 'for', 'the', '3D', 'constant', 'gradient', 'of', 'velocity', 'test', 'case', '(', 'test', 'case', '2', ')', '.', 'The', 'error', 'measures', 'are', 'in', 'the', '[', 'l∞', ',', 'mean', '*', 'l2', '*', ']', 'norms', '.', '*', 'h', 'n', '*', '1', '*', 'st', '*', 'order', ' \\n', 'error', 'in', 'τ', ' \\n', '1/20', '17×', '33×', '33', ' \\n', '[', '1.35e−02', ',', '5.04e−03', ']', '\\n\\n', '2', '*', 'nd', '*', 'order', ' \\n', 'time', '(', 'work', ')', 'error', 'in', 'τ', ' \\n', 'time', '(', 'work', ')', '0.04', 's', '(', '237', ')', '[', '2.34e−03', ',', '9.36e−04', ']', '0.04', 's', '(', '255', ')', '1/40', '33×', '65×', '65', ' \\n', '[', '6.24e−03', ',', '2.44e−03', ']', '0.31', 's', '(', '234', ')', '[', '5.12e−04', ',', '1.72e−04', ']', ' \\n', '0.32', 's', '(', '236', ')', '1/80', '65×', '129×', '129', '[', '3.00e−03', ',', '1.20e−03', ']', '2.86', 's', '(', '330', ')', '[', '1.70e−04', ',', '3.82e−05', ']', '2.89', 's', '(', '334', ')', '1/160', '129×', '257×', '257', '[', '1.47e−03', ',', '5.99e−04', ']', '27.6', 's', '(', '411', ')', '[', '5.42e−05', ',', '9.33e−06', ']', '28.9', 's', '(', '430', ')', '1/320', '257×', '513×', '513', '[', '7.30e−04', ',', '2.99e−04', ']', '263', 's', '(', '481', ')', '[', '1.95e−05', ',', '2.29e−06', ']', '271', 's', '(', '496', ')', '\\n\\n', '#', '#', '#', 'Table', '6', '\\n\\n', 'Results', 'for', 'the', '3D', 'Gaussian', 'factor', 'test', 'case', '(', 'test', 'case', '3', ')', '.', 'The', 'error', 'measures', 'are', 'in', 'the', '[', 'l∞', ',', 'mean', '*', 'l2', '*', ']', 'norms', '.', '*', 'h', 'n', '*', '1', '*', 'st', '*', 'order', ' \\n', 'error', 'in', 'τ', '\\n\\n', '2', '*', 'nd', '*', 'order', ' \\n', 'time', '(', 'work', ')', 'error', 'in', 'τ', ' \\n', 'time', '(', 'work', ')', '1/20', '17×', '33×', '33', ' \\n', '[', '7.53e−03', ',', '3.26e−03', ']', '0.04', 's', '(', '230', ')', '[', '3.65e−04', ',', '1.27e−04', ']', ' \\n', '0.04', 's', '(', '229', ')', '1/40', '33×', '65×', '65', ' \\n', '[', '3.69e−03', ',', '1.56e−03', ']', '0.33', 's', '(', '245', ')', '[', '9.95e−05', ',', '2.85e−05', ']', '0.34', 's', '(', '253', ')', '1/80', '65×', '129×', '129', '[', '1.83e−03', ',', '7.62e−04', ']', '2.77', 's', '(', '319', ')', '[', '3.22e−05', ',', '7.50e−06', ']', '2.80', 's', '(', '323', ')', '1/160', '129×', '257×', '257', '[', '9.11e−04', ',', '3.77e−04', ']', '26.4', 's', '(', '393', ')', '[', '1.06e−05', ',', '2.06e−06', ']', '27.2', 's', '(', '405', ')', '1/320', '257×', '513×', '513', '[', '4.54e−04', ',', '1.87e−04', ']', '267', 's', '(', '487', ')', '[', '3.54e−06', ',', '5.66e−07', ']', '276', 's', '(', '504', ')', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0468.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '6', '.', '*', '*', 'The', 'accuracy', 'of', 'the', 'FM', 'approximations', 'in', 'logarithmic', 'scales', 'for', 'the', '3D', 'cases', '.', 'Red', 'and', 'blue', 'plots', 'are', 'used', 'for', 'ﬁrst', 'and', 'second', 'order', 'approximations', ',', 'respectively', ';', 'dotted', 'lines', 'for', '*', 'l∞', '*', 'error', 'norm', 'and', 'solid', 'for', 'mean', '*', 'l2', '*', 'norm', '.', 'Black', 'circles', 'denote', 'an', 'exact', 'second', 'order', 'convergence', 'rate', '.', '(', 'For', 'interpretation', 'of', 'the', 'references', 'to', 'color', 'in', 'this', 'ﬁgure', 'legend', ',', 'the', 'reader', 'is', 'referred', 'to', 'the', 'web', 'version', 'of', 'this', 'article', '.', ')', '\\n\\n', 'test', 'case', '.', 'The', 'results', 'are', 'similar', 'to', 'the', 'previous', 'test', 'case', 'in', 'both', 'the', 'convergence', '(', 'ﬁrst', '/', 'second', 'order', 'using', '*', 'l∞/l2', '*', 'norms', ')', 'and', 'computational', 'costs', 'in', 'seconds', 'and', 'work', 'units', '.', ' \\n', 'Again', 'we', 'wish', 'to', 'demonstrate', 'the', 'order', 'accuracy', 'of', 'the', 'FM', 'approximations', 'using', 'the', 'ﬁrst', 'and', 'second', 'order', 'schemes', '.', 'Fig', '.', '6', 'shows', 'the', 'results', 'in', 'Tables', '4–6', 'in', 'logarithmic', 'scales', '.', 'Like', 'in', '2D', ',', 'we', 'observe', 'second', 'order', 'convergence', 'rate', 'when', 'the', 'error', 'is', 'measure', 'in', 'mean', '*', 'l2', '*', 'norm', '.', 'However', ',', 'because', 'the', 'second', 'order', 'stencil', 'reduces', 'to', 'ﬁrst', 'order', 'stencil', 'in', 'two', 'dimensional', 'manifolds', ',', 'the', 'error', 'in', '*', 'l∞', '*', 'norm', 'is', 'higher', 'in', '3D', 'than', 'it', 'is', 'in', '2D.', '\\n\\n', '#', '#', '#', '#', '#', '6', '.', 'Numerical', 'results', ':', 'travel', 'time', 'tomography', '\\n\\n', 'In', 'this', 'section', 'we', 'demonstrate', 'a', 'solution', 'of', 'travel', 'time', 'tomography', 'using', 'synthetic', 'travel', 'time', 'data', 'dobs', 'for', 'a', '2D', 'and', 'SEG', '/', 'EAGE', 'salt', 'model', 'given', 'in', '[', '1', ']', 'and', 'presented', 'in', 'Fig', '.', '7(a', ')', ',', 'using', 'a', '256', '×128', 'grid', 'that', 'represents', 'an', 'area', 'of', 'approximately', '13.5', 'km', '×', '4.2', 'km', '.', 'We', 'choose', '51', 'equally', 'distanced', 'sources', 'locations', 'on', 'the', 'open', 'surface', '(', 'that', 'is', ',', 'they', 'are', 'located', 'every', '5', 'pixels', 'on', 'the', 'top', 'row', ')', ',', 'and', '256', 'receivers', '(', 'located', 'in', 'every', 'pixel', 'on', 'the', 'top', 'row', ')', '.', 'We', 'note', 'that', 'to', 'have', 'a', 'reasonable', 'solution', 'using', 'the', 'ﬁrst', 'arrivals', 'for', 'the', 'inverse', 'problem', 'under', 'this', 'setup', ',', 'the', 'velocity', 'in', 'the', 'interior', 'has', 'to', 'be', 'larger', 'than', '\\n\\n', '---', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0459.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '7', '.', '*', '*', '2D', 'travel', 'time', 'tomography', 'experiment', ',', 'grid', 'size', '256×', '128', '.', 'Velocities', 'are', 'given', 'in', 'km', '/', 'sec', '.', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0460.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '8', '.', '*', '*', 'Initial', 'and', 'ﬁnal', 'data', 'and', 'residuals', 'in', 'the', 'source', '-', 'receiver', 'domain', '.', '\\n\\n', 'that', 'on', 'the', 'surface', '.', 'This', 'is', 'to', 'guarantee', 'that', 'the', 'ﬁrst', 'arrival', 'rays', 'obtained', 'on', 'the', 'surface', 'actually', 'come', 'from', 'the', 'interior', 'but', 'not', 'only', 'travel', 'along', 'the', 'surface', '.', 'To', 'dobs', 'we', 'add', 'white', 'Gaussian', 'noise', 'with', 'standard', 'deviation', 'of', '0.01', '×', 'mean(\\\\|dobs', '\\\\|', ')', '.', 'To', 'ﬁt', 'the', 'model', 'to', 'the', 'data', ',', 'we', 'minimize', '(', '4.22', ')', 'using', 'Gauss', '–', 'Newton', '(', 'we', 'perform', '10', 'iterations', ',', 'where', 'in', 'each', 'we', 'apply', '8', 'CG', 'steps', 'for', 'the', 'Gauss', '–', 'Newton', 'direction', 'problem', ')', '.', 'We', 'use', 'the', 'general', '-', 'propose', 'inversion', 'package', '[', '27', ']', ',', 'which', 'is', 'freely', 'available', 'in', '[', 'https', ':/', '/github', '.com', '/JuliaInv', '/jInv.jl,](https://github.com', '/', 'JuliaInv', '/', 'jInv.jl', ')', 'together', 'with', 'our', 'FM', 'package', 'mentioned', 'earlier', '.', 'For', 'that', ',', 'we', 'ﬁrst', 'generate', 'an', 'initial', 'slowness', 'model', '*', '*', 'm', '*', '*', '(', '0', ')', '=', '*', 'mref', '*', ',', 'whose', 'velocity', 'model', 'shown', 'in', 'Fig', '.', '7(b', ')', '.', 'This', 'corresponds', 'to', 'a', 'velocity', 'ﬁeld', 'with', 'a', 'constant', 'gradient', 'in', 'the', '*', 'y', '*', 'direction', ',', 'similarly', 'to', 'the', 'model', 'in', 'Fig', '.', '3(b', ')', '.', 'To', 'bound', '*', '*', 'm', '*', '*', 'from', 'above', 'and', 'from', 'below', 'throughout', 'the', 'minimization', ',', 'we', 'invert', 'for', 'an', 'auxiliary', 'variable', '*', '*', 'm', '*', '*', '?', 'and', 'use', 'the', 'following', 'scalar', 'bounded', 'bijective', 'mapping', 'that', 'prevents', '*', '*', 'm', '*', '*', 'from', 'being', 'below', '*', 'mL', '*', 'or', 'above', '*', 'mH', ':*', '\\n\\n', '#', '#', '#', '?', '?', '?', '?', '\\n\\n', '*', 'mbound(m', '*', '?', ')', '=', '*', 'mH', '*', '−mL', '·', 'tanh', ' \\n', '2', '\\n\\n', '#', '#', '#', '·', 'm', '?', '−', 'mH', '+', 'mL', '+', '1', '+', 'mL.', '\\n\\n', '2', '*', 'mH', '*', '−mL', '2', '\\n\\n', 'That', 'is', ',', 'instead', 'of', 'minimizing', '(', '4.22', ')', 'as', 'is', ',', 'we', 'minimize', '?', '\\n\\n', '?', '\\n\\n', '#', '#', '#', '#', '?', 'ns', '\\n\\n', '#', '#', '#', 'min', 'φ(m', '?', ')', '=', 'min', '?', 'P', '?', 'τ', 'i', '(', 'mbound(m', '?', ')', ')', '−', 'd', 'i', '?', '2', '+', 'αR(m', '?', ')', '\\n\\n', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '#', '(', '6.33', ')', '\\n\\n', '*', '*', 'm', '*', '*', '?', '*', '*', 'm', '*', '*', '?', '*', 'i=1', '*', 'obs', '\\n\\n', 'subject', 'to', 'the', 'same', 'constraints', 'in', '(', '4.23', ')', '.', 'For', 'the', 'regularization', '*', 'R', '*', 'use', 'a', 'simple', 'discrete', 'central', '-', 'differences', 'Laplacian', ',', 'and', 'apply', 'it', 'for', '*', '*', 'm', '*', '*', '?', ';', 'that', 'is', '\\n\\n', '*', 'R(m', '*', '?', ')', '=', '1', '(', 'm', '?', '−', '*', '*', 'm', '*', '*', '?', '\\n\\n', '#', '#', '#', ')', '?', '?', 'h(m', '?', '−', 'm', '?', ')', ',', '\\n\\n', '2', '*', 'ref', 'ref', '*', ' \\n', 'where', '*', '*', 'm', '*', '*', '?', ' \\n', 'is', 'the', 'model', 'such', 'that', '*', 'mbound(m', '*', '?', ')', '=', '*', 'mref', '*', '.', 'For', '?', 'h', 'we', 'use', 'Neumann', 'boundary', 'conditions', ',', 'since', 'those', 'lead', 'to', '*', 'r', 'ef', 'ref', '*', ' \\n', 'an', 'effect', 'of', 'an', 'automatic', 'salt', 'ﬂooding', ',', 'which', 'is', 'a', 'popular', 'way', 'to', 'treat', 'salt', 'bodies', '.', 'We', 'set', 'the', 'regularization', 'parameter', 'to', 'be', 'α', '=', '0.5', '.', 'We', 'note', 'that', 'other', 'choices', 'of', '*', 'mref', '*', 'and', 'regularization', 'terms', 'may', 'deﬁnitely', 'be', 'suitable', 'here', ',', 'but', 'are', 'beyond', 'the', 'scope', 'of', 'this', 'paper', '.', 'Fig', '.', '7(c', ')', 'shows', 'the', 'result', 'model', 'of', 'the', 'Gauss', '–', 'Newton', 'minimization', ',', 'and', 'Fig', '.', '8', 'presents', 'the', 'initial', 'and', 'ﬁnal', 'data', 'and', 'data', 'residuals', '.', 'In', 'particular', ',', 'Fig', '.', '8(d', ')', 'shows', 'that', 'the', 'ﬁnal', 'residual', 'mostly', 'contains', 'the', 'added', 'Gaussian', 'noise', '.', 'Fig', '.', '9', 'shows', 'that', 'the', 'misﬁt', 'was', 'indeed', 'reduced', 'throughout', 'the', 'iterations', ',', 'until', 'the', 'reduction', 'stalls', 'and', 'the', 'misﬁt', 'reﬂect', 'the', 'noise', 'level', '.', ' \\n', 'To', 'demonstrate', 'our', 'algorithm', 'in', '3D', ',', 'we', 'use', 'a', '3D', 'version', 'of', 'the', 'same', 'SEG', '/', 'EAGE', 'model', ',', 'presented', 'in', 'Fig', '.', '10(a', ')', ',', 'using', 'a', '256', '×', '256', '×', '128', 'grid', 'that', 'represents', 'a', 'volume', 'of', '13.5', 'km', '×', '13.5', 'km', '×', '4.2', 'km', '.', 'We', 'choose', '144', 'equally', 'distanced', 'sources', 'locations', 'on', 'the', 'open', 'surface', ',', 'located', 'every', '23', 'pixels', 'on', 'the', 'top', 'surface', ',', 'and', '256', '×256', 'receivers', 'located', 'on', 'the', 'top', 'surface', '.', '\\n\\n', '---', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0439.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '9', '.', '*', '*', 'Convergence', 'history', 'of', 'the', 'inversion', '.', '\\n\\n', '!', '[', ']', '(', 'assets_j', '/', 'img-0440.jpg', ')', '\\n\\n', '*', '*', 'Fig', '.', '10', '.', '*', '*', '3D', 'travel', 'time', 'tomography', 'experiment', ',', 'grid', 'size', '256×', '256×', '128', '.', 'Velocities', 'are', 'given', 'in', 'km', '/', 'sec', '.', '\\n\\n', 'We', 'use', 'the', 'same', 'parameters', 'as', 'in', 'the', '2D', 'experiment', '(', 'bound', 'function', ',', 'regularization', ',', 'initial', '3D', 'model', ',', 'added', 'noise', 'to', 'the', 'data', ',', 'number', 'of', 'iterations', 'etc', '.', ')', '.', 'Fig', '.', '10(b', ')', 'shows', 'the', 'result', 'of', 'the', 'inversion', '.', 'Similarly', 'to', 'the', '2D', 'case', ',', 'the', 'top', 'part', 'of', 'the', 'model', 'is', 'recovered', 'quite', 'well', ',', 'while', 'a', '“', 'salt', 'ﬂooding', '”', 'effect', 'is', 'evident', 'in', 'the', 'bottom', 'part', 'of', 'the', 'model', '.', 'We', 'performed', 'the', 'inversion', 'using', 'a', 'machine', 'with', 'two', 'Intel(R', ')', 'Xeon(R', ')', 'E5', '-', '2670', 'v3', 'processors', 'with', '128', 'GB', 'of', 'RAM', '.', 'Using', '24', 'cores', ',', 'we', 'applied', 'the', 'inversion', 'in', 'approximately', '15', 'hours', ',', 'and', 'the', 'highest', 'memory', 'footprint', 'of', 'the', 'algorithm', 'reached', 'around', '30', 'GB', '.', '\\n\\n', '#', '#', '#', '#', '#', '7', '.', 'Conclusions', '\\n\\n', 'In', 'this', 'paper', 'we', 'developed', 'a', 'Fast', 'Marching', 'algorithm', 'for', 'the', 'factored', 'eikonal', 'equation', ',', 'which', 'in', 'many', 'cases', 'yields', 'a', 'more', 'accurate', 'solution', 'of', 'the', 'travel', 'time', 'than', 'the', 'original', 'equation', '.', 'Similarly', 'to', 'the', 'original', 'FM', 'algorithm', ',', 'our', 'version', 'solves', 'the', 'factored', 'problem', 'by', 'exploiting', 'the', 'monotonicity', 'of', 'the', 'solution', 'along', 'the', 'characteristics', '.', 'Our', 'algorithm', 'is', 'capable', 'of', 'solving', 'the', 'problem', 'using', 'both', 'ﬁrst', 'and', 'second', 'order', 'schemes', '.', 'The', 'advantages', 'of', 'our', 'algorithm', 'are', '(', '1', ')', 'its', 'favorable', 'guaranteed', '*', 'O(n', '*', 'logn', ')', 'running', 'time', ',', 'and', '(', '2', ')', 'the', 'easily', 'computed', 'sensitivity', 'matrices', 'for', 'solving', 'the', 'inverse', '(', 'factored', ')', 'eikonal', 'equation', '.', '\\n\\n', '#', '#', '#', '#', '#', 'Acknowledgements', '\\n\\n', 'The', 'research', 'leading', 'to', 'these', 'results', 'has', 'received', 'funding', 'from', 'the', 'European', 'Union', '’s', '–', 'Seventh', 'Framework', 'Programme', '(', 'FP7/2007', '-', '2013', ')', 'under', 'grant', 'agreement', 'no', '623212', '–', 'MC', 'Multiscale', 'Inversion', '.', '\\n\\n', '#', '#', '#', '#', '#', 'References', '\\n\\n', '[', '1', ']', '[', 'F.', 'Aminzadeh', ',', 'B.', 'Jean', ',', 'T.', 'Kunz', ',', '3', '-', 'D', 'Salt', 'and', 'Overthrust', 'Models', ',', 'Society', 'of', 'Exploration', 'Geophysicists', ',', '1997.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib616D696E7A616465683139393733s1', ')', ' \\n', '[', '2', ']', '[', 'J.A.', 'Bærentzen', ',', 'On', 'the', 'implementation', 'of', 'fast', 'marching', 'methods', 'for', '3D', 'lattices', ',', 'Tech', '.', 'Report', 'IMM', '-', 'TR-2001', '-', '13', ',', 'Informatics', 'and', 'Mathematical', 'Mod-', ' \\n', 'elling', ',', 'Technical', 'University', 'of', 'Denmark', ',', 'DTU', ',', '2001', ',', 'Richard', 'Petersens', 'Plads', ',', 'Building', '321', ',', 'DK-', '2800', 'Kgs', '.', 'Lyngby.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib494D4D323030312D30383431s1', ')', '  \\n', '[', '3', ']', '[', 'A.', 'Benaichouche', ',', 'M.', 'Noble', ',', 'A.', 'Gesret', ',', 'First', 'arrival', 'traveltime', 'tomography', 'using', 'the', 'fast', 'marching', 'method', 'and', 'the', 'adjoint', 'state', 'technique', ',', 'in', ':', '77th', ' \\n', 'EAGE', 'Conference', 'Proceedings', ',', '2015.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib62656E616963686F75636865323031356669727374s1', ')', '  \\n', '[', '4', ']', '[', 'J.', 'Bezanzon', ',', 'S.', 'Karpinski', ',', 'V.', 'Shah', ',', 'A.', 'Edelman', ',', 'Julia', ':', 'a', 'fast', 'dynamic', 'language', 'for', 'technical', 'computing', ',', 'in', ':', 'Lang', '.', 'NEXT', ',', 'Apr.', '2012.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib4A756C6961s1', ')', ' \\n', '[', '5', ']', '[', 'M.G.', 'Crandall', ',', 'P.-L.', 'Lions', ',', 'Viscosity', 'solutions', 'of', 'Hamilton', '–', 'Jacobi', 'equations', ',', 'Trans', '.', 'Am', '.', 'Math', '.', 'Soc', '.', '277', '(', '1983', ')', '1–42.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6372616E64616C6C31393833766973636F73697479s1', ')', ' \\n', '[', '6', ']', '[', 'S.', 'Fomel', ',', 'S.', 'Luo', ',', 'H.', 'Zhao', ',', 'Fast', 'sweeping', 'method', 'for', 'the', 'factored', 'eikonal', 'equation', ',', 'J.', 'Comput', '.', 'Phys', '.', '228', '(', '2009', ')', '6440–6455.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib666F6D656C3230303966617374s1', ')', ' \\n', '[', '7', ']', '[', 'P.A.', 'Gremaud', ',', 'C.M.', 'Kuster', ',', 'Computational', 'study', 'of', 'fast', 'methods', 'for', 'the', 'eikonal', 'equation', ',', 'SIAM', 'J.', 'Sci', '.', 'Comput', '.', '27', '(', '2006', ')', '1803–1816.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6772656D61756432303036636F6D7075746174696F6E616Cs1', ')', ' \\n', '[', '8', ']', '[', 'E.', 'Haber', ',', 'Computational', 'Methods', 'in', 'Geophysical', 'Electromagnetics', ',', 'vol', '.', '1', ',', 'SIAM', ',', '2014.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib686162657232303134636F6D7075746174696F6E616Cs1', ')', ' \\n', '[', '9', ']', '[', 'E.', 'Haber', ',', 'S.', 'MacLachlan', ',', 'A', 'fast', 'method', 'for', 'the', 'solution', 'of', 'the', 'Helmholtz', 'equation', ',', 'J.', 'Comput', '.', 'Phys', '.', '230', '(', '2011', ')', '4403–4418.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib68616265723230313166617374s1', ')', ' \\n', '[', '10', ']', '[', 'C.Y.', 'Kao', ',', 'S.', 'Osher', ',', 'J.', 'Qian', ',', 'Lax', '–', 'Friedrichs', 'sweeping', 'scheme', 'for', 'static', 'Hamilton', '–', 'Jacobi', 'equations', ',', 'J.', 'Comput', '.', 'Phys', '.', '196', '(', '2004', ')', '367–391.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6B616F323030346C6178s1', ')', '\\n\\n', '---', '\\n\\n', '[', '11', ']', '[', 'R.', 'Kimmel', ',', 'J.A.', 'Sethian', ',', 'Computing', 'geodesic', 'paths', 'on', 'manifolds', ',', 'Proc', '.', 'Natl', '.', 'Acad', '.', 'Sci', '.', '95', '(', '1998', ')', '8431–8435.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6B696D6D656C31393938636F6D707574696E67s1', ')', ' \\n', '[', '12', ']', '[', 'S.', 'Leung', ',', 'J.', 'Qian', ',', 'R.', 'Burridge', ',', 'Eulerian', 'Gaussian', 'beams', 'for', 'high', '-', 'frequency', 'wave', 'propagation', ',', 'Geophysics', '72', '(', '2007', ')', 'SM61', '–', 'SM76.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6C65756E673230303765756C657269616Es1', ')', ' \\n', '[', '13', ']', '[', 'S.', 'Leung', ',', 'J.', 'Qian', ',', 'et', 'al', '.', ',', 'An', 'adjoint', 'state', 'method', 'for', 'three', '-', 'dimensional', 'transmission', 'traveltime', 'tomography', 'using', 'ﬁrst', '-', 'arrivals', ',', 'Commun', '.', 'Math', '.', 'Sci', '.', '4', ' \\n', '(', '2006', ')', '249–266.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6C65756E673230303661646A6F696E74s1', ')', '  \\n', '[', '14', ']', '[', 'S.', 'Li', ',', 'A.', 'Vladimirsky', ',', 'S.', 'Fomel', ',', 'First', '-', 'break', 'traveltime', 'tomography', 'with', 'the', 'double', '-', 'square', '-', 'root', 'eikonal', 'equation', ',', 'Geophysics', '78', '(', '2013', ')', 'U89', '–', 'U101.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6C69323031336669727374s1', ')', ' \\n', '[', '15', ']', '[', 'S.', 'Luo', ',', 'J.', 'Qian', ',', 'Factored', 'singularities', 'and', 'high', '-', 'order', 'Lax', '–', 'Friedrichs', 'sweeping', 'schemes', 'for', 'point', '-', 'source', 'traveltimes', 'and', 'amplitudes', ',', 'J.', 'Comput', '.', 'Phys', '.', ' \\n', '230', '(', '2011', ')', '4742–4755.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6C756F32303131666163746F726564s1', ')', '  \\n', '[', '16', ']', '[', 'S.', 'Luo', ',', 'J.', 'Qian', ',', 'Fast', 'sweeping', 'methods', 'for', 'factored', 'anisotropic', 'eikonal', 'equations', ':', 'multiplicative', 'and', 'additive', 'factors', ',', 'J.', 'Sci', '.', 'Comput', '.', '52', '(', '2012', ')', '360–382.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6C756F3230313266617374s1', ')', '[', '17', ']', '[', 'S.', 'Luo', ',', 'J.', 'Qian', ',', 'R.', 'Burridge', ',', 'Fast', 'Huygens', 'sweeping', 'methods', 'for', 'Helmholtz', 'equations', 'in', 'inhomogeneous', 'media', 'in', 'the', 'high', 'frequency', 'regime', ',', 'J.', 'Comput', '.', ' \\n', 'Phys', '.', '270', '(', '2014', ')', '378–401.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6C756F3230313466617374s1', ')', '  \\n', '[', '18', ']', '[', 'S.', 'Luo', ',', 'J.', 'Qian', ',', 'R.', 'Burridge', ',', 'High', '-', 'order', 'factorization', 'based', 'high', '-', 'order', 'hybrid', 'fast', 'sweeping', 'methods', 'for', 'point', '-', 'source', 'eikonal', 'equations', ',', 'SIAM', 'J.', 'Numer', '.', ' \\n', 'Anal', '.', '52', '(', '2014', ')', '23–44.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib4C6F755169616E427572726964676532303134s1', ')', '  \\n', '[', '19', ']', '[', 'S.', 'Luo', ',', 'J.', 'Qian', ',', 'H.', 'Zhao', ',', 'Higher', '-', 'order', 'schemes', 'for', '3d', 'ﬁrst', '-', 'arrival', 'traveltimes', 'and', 'amplitudes', ',', 'Geophysics', '77', '(', '2012', ')', 'T47', '–', 'T56.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6C756F32303132686967686572s1', ')', ' \\n', '[', '20', ']', '[', 'M.', 'Noble', ',', 'A.', 'Gesret', ',', 'N.', 'Belayouni', ',', 'Accurate', '3', '-', 'd', 'ﬁnite', 'difference', 'computation', 'of', 'traveltimes', 'in', 'strongly', 'heterogeneous', 'media', ',', 'Geophys', '.', 'J.', 'Int', '.', '199', ' \\n', '(', '2014', ')', '1572–1585.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib6E6F626C65323031346163637572617465s1', ')', '  \\n', '[', '21', ']', '[', 'A.', 'Pica', ',', 'et', 'al', '.', ',', 'Fast', 'and', 'accurate', 'ﬁnite', '-', 'difference', 'solutions', 'of', 'the', '3d', 'eikonal', 'equation', 'parametrized', 'in', 'celerity', ',', 'in', ':', '67th', 'Ann', '.', 'Internat', '.', 'Mtg', ',', 'Soc', '.', 'of', 'Expl', '.', ' \\n', 'Geophys', ',', '1997', ',', 'pp', '.', '1774–1777.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib706963613139393766617374s1', ')', '  \\n', '[', '22', ']', '[', 'J.', 'Qian', ',', 'W.W.', 'Symes', ',', 'An', 'adaptive', 'ﬁnite', '-', 'difference', 'method', 'for', 'traveltimes', 'and', 'amplitudes', ',', 'Geophysics', '67', '(', '2002', ')', '167–176.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7169616E323030326164617074697665s1', ')', ' \\n', '[', '23', ']', '[', 'J.', 'Qian', ',', 'Y.-T.', 'Zhang', ',', 'H.-K.', 'Zhao', ',', 'A', 'fast', 'sweeping', 'method', 'for', 'static', 'convex', 'Hamilton', '–', 'Jacobi', 'equations', ',', 'J.', 'Sci', '.', 'Comput', '.', '31', '(', '2007', ')', '237–271.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7169616E3230303766617374s1', ')', ' \\n', '[', '24', ']', '[', 'N.', 'Rawlinson', ',', 'M.', 'Sambridge', ',', 'Wave', 'front', 'evolution', 'in', 'strongly', 'heterogeneous', 'layered', 'media', 'using', 'the', 'fast', 'marching', 'method', ',', 'Geophys', '.', 'J.', 'Int', '.', '156', '(', '2004', ')', ' \\n', '631–647.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7261776C696E736F6E3230303477617665s1', ')', '  \\n', '[', '25', ']', '[', 'E.', 'Rouy', ',', 'A.', 'Tourin', ',', 'A', 'viscosity', 'solutions', 'approach', 'to', 'shape', '-', 'from', '-', 'shading', ',', 'SIAM', 'J.', 'Numer', '.', 'Anal', '.', '29', '(', '1992', ')', '867–884.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib726F757931393932766973636F73697479s1', ')', ' \\n', '[', '26', ']', '[', 'L.I.', 'Rudin', ',', 'S.', 'Osher', ',', 'E.', 'Fatemi', ',', 'Nonlinear', 'total', 'variation', 'based', 'noise', 'removal', 'algorithms', ',', 'Physica', 'D', '60', '(', '1992', ')', '259–268.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib727564696E313939326E6F6E6C696E656172s1', ')', ' \\n', '[', '27', ']', 'L.', 'Ruthotto', ',', 'E.', 'Treister', ',', 'E.', 'Haber', ',', 'jInv', '–', 'a', 'ﬂexible', 'Julia', 'package', 'for', 'PDE', 'parameter', 'estimation', ',', '2016', ',', 'submitted', 'for', 'publication', '.', ' \\n', '[', '28', ']', '[', 'C.', 'Saragiotis', ',', 'T.', 'Alkhalifah', ',', 'S.', 'Fomel', ',', 'Automatic', 'traveltime', 'picking', 'using', 'instantaneous', 'traveltime', ',', 'Geophysics', '78', '(', '2013', ')', 'T53', '–', 'T58.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7361726167696F746973323031336175746F6D61746963s1', ')', ' \\n', '[', '29', ']', '[', 'A.', 'Sei', ',', 'W.W.', 'Symes', ',', 'et', 'al', '.', ',', 'Gradient', 'calculation', 'of', 'the', 'traveltime', 'cost', 'function', 'without', 'ray', 'tracing', ',', 'in', ':', '65th', 'Ann', '.', 'Internat', '.', 'Mtg', '.', ',', 'Expanded', 'Abstracts', ',', ' \\n', 'Soc', '.', 'Expl', '.', 'Geophys', ',', '1994', ',', 'pp', '.', '1351–1354.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib736569313939346772616469656E74s1', ')', '  \\n', '[', '30', ']', '[', 'J.A.', 'Sethian', ',', 'A', 'fast', 'marching', 'level', 'set', 'method', 'for', 'monotonically', 'advancing', 'fronts', ',', 'Proc', '.', 'Natl', '.', 'Acad', '.', 'Sci', '.', '93', '(', '1996', ')', '1591–1595.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7365746869616E3139393666617374s1', ')', ' \\n', '[', '31', ']', '[', 'J.A.', 'Sethian', ',', 'Fast', 'marching', 'methods', ',', 'SIAM', 'Rev.', '41', '(', '1999', ')', '199–235.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7365746869616E3139393966617374s1', ')', ' \\n', '[', '32', ']', '[', 'E.', 'Somersalo', ',', 'J.', 'Kaipio', ',', 'Statistical', 'and', 'Computational', 'Inverse', 'Problems', ',', 'Appl', '.', 'Math', '.', 'Sci', '.', ',', 'vol', '.', '160', ',', '2004.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib736F6D657273616C6F32303034737461746973746963616Cs1', ')', ' \\n', '[', '33', ']', '[', 'A.', 'Spira', ',', 'R.', 'Kimmel', ',', 'An', 'eﬃcient', 'solution', 'to', 'the', 'eikonal', 'equation', 'on', 'parametric', 'manifolds', ',', 'Interfaces', 'Free', 'Bound', '.', '6', '(', '2004', ')', '315–328.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib737069726132303034656666696369656E74s1', ')', ' \\n', '[', '34', ']', '[', 'C.', 'Taillandier', ',', 'M.', 'Noble', ',', 'H.', 'Chauris', ',', 'H.', 'Calandra', ',', 'First', '-', 'arrival', 'traveltime', 'tomography', 'based', 'on', 'the', 'adjoint', '-', 'state', 'method', ',', 'Geophysics', '74', '(', '2009', ')', 'WCB1', '–', ' \\n', 'WCB10.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7461696C6C616E64696572323030396669727374s1', ')', '  \\n', '[', '35', ']', '[', 'Y.-H.R.', 'Tsai', ',', 'L.-T.', 'Cheng', ',', 'S.', 'Osher', ',', 'H.-K.', 'Zhao', ',', 'Fast', 'sweeping', 'algorithms', 'for', 'a', 'class', 'of', 'Hamilton', '–', 'Jacobi', 'equations', ',', 'SIAM', 'J.', 'Numer', '.', 'Anal', '.', '41', '(', '2003', ')', ' \\n', '673–694.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib747361693230303366617374s1', ')', '  \\n', '[', '36', ']', '[', 'J.N.', 'Tsitsiklis', ',', 'Eﬃcient', 'algorithms', 'for', 'globally', 'optimal', 'trajectories', ',', 'IEEE', 'Trans', '.', 'Autom', '.', 'Control', '40', '(', '1995', ')', '1528–1538.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7473697473696B6C697331393935656666696369656E74s1', ')', ' \\n', '[', '37', ']', '[', 'C.R.', 'Vogel', ',', 'Computational', 'Methods', 'for', 'Inverse', 'Problems', ',', 'Frontiers', 'Appl', '.', 'Math', '.', ',', 'vol', '.', '23', ',', 'SIAM', ',', 'Philadelphia', ',', '2002.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib766F67656C32303032636F6D7075746174696F6E616Cs1', ')', ' \\n', '[', '38', ']', '[', 'H.', 'Zhao', ',', 'A', 'fast', 'sweeping', 'method', 'for', 'eikonal', 'equations', ',', 'Math', '.', 'Comput', '.', '74', '(', '2005', ')', '603–627.](http://refhub.elsevier.com', '/', 'S0021', '-', '9991', '16', '30355', '-', '2', '/', 'bib7A68616F3230303566617374s1', ')', '\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "all_text = open('julia.md','r').read()\n",
    "all_doc = nlp(all_text)\n",
    "print([token.text for token in all_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1035d6-bd9d-42fd-a600-35f38c374604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elem_to_text(elem, default=''):\n",
    "    if elem:\n",
    "        return elem.getText()\n",
    "    else:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e619bd",
   "metadata": {},
   "source": [
    "## Regex parsing with bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8a21cb-7ccd-4cb3-accb-70fe0f68d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup\n",
    "\n",
    "def search(text,find,n):\n",
    "    '''Searches for text, and retrieves n words either side of the text, which are retuned seperatly'''\n",
    "    word = r\"\\W*([\\w]+)\"\n",
    "    try:\n",
    "        groups = re.search(r'{}\\W*{}{}'.format(word*n,find,word*n), text).groups()\n",
    "        return groups[:n],groups[n:]\n",
    "    except:\n",
    "        return \"Not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb2ea30a-cec6-4e04-aae8-47f04a7f4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = read_tei('./case-studies/arxiv-corpus/mine5/parsed_xml/1606.04671.tei.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf6a9d19-8372-45ed-8365-50952574ab35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Progressive Neural Networks'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e2caa6-138d-4a50-95e4-1b0393d90373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning to solve complex sequences of tasks-while both leveraging transfer and avoiding catastrophic forgetting-remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.getText(separator=' ', strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced89546-42c7-44d4-9514-e6361bc76aa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getText'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/adb/gitclones/repro-screener/find.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/adb/gitclones/repro-screener/find.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m idno_elem \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39midno\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDOI\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/adb/gitclones/repro-screener/find.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe DOI is \u001b[39m\u001b[39m{\u001b[39;00midno_elem\u001b[39m.\u001b[39mgetText()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getText'"
     ]
    }
   ],
   "source": [
    "idno_elem = soup.find('idno', type='DOI')\n",
    "print(f\"The DOI is {idno_elem.getText()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e0b0c07-0ffd-48c2-af1a-b6e7961815a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = [t.getText(separator=' ', strip=True) for t in soup.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d4beaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = [t.getText(separator=' ', strip=True) for t in soup.find_all('email')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc88031a-e584-4239-91d9-6410b77e2d2a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning to solve complex sequences of tasks-while both leveraging transfer and avoiding catastrophic forgetting-remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.',\n",
       " 'Finetuning remains the method of choice for transfer learning with neural networks: a model is pretrained on a source domain (where data is often abundant), the output layers of the model are adapted to the target domain, and the network is finetuned via backpropagation. This approach was pioneered in [7] by transferring knowledge from a generative to a discriminative model, and has since been generalized with great success [11] . Unfortunately, the approach has drawbacks which make it unsuitable for transferring across multiple tasks: if we wish to leverage knowledge acquired over a sequence of experiences, which model should we use to initialize subsequent models? This seems to require not only a learning method that can support transfer learning without catastrophic forgetting, but also foreknowledge of task similarity. Furthermore, while finetuning may allow us to recover expert performance in the target domain, it is a destructive process which discards the previously learned function. One could copy each model before finetuning to explicitly remember all previous tasks, but the issue of selecting a proper initialization remains. While distillation [8] offers one potential solution to multitask learning [17] , it requires a reservoir of persistent training data for all tasks, an assumption which may not always hold. This paper introduces progressive networks, a novel model architecture with explicit support for transfer across sequences of tasks. While finetuning incorporates prior knowledge only at initialization, progressive networks retain a pool of pretrained models throughout training, and learn lateral connections from these to extract useful features for the new task. By combining previously learned features in this manner, progressive networks achieve a richer compositionality, in which prior knowledge is no longer transient and can be integrated at each layer of the feature hierarchy. Moreover, the addition of new capacity alongside pretrained networks gives these models the flexibility to both reuse old computations and learn new ones. As we will show, progressive networks naturally accumulate experiences and are immune to catastrophic forgetting by design, making them an ideal springboard for tackling long-standing problems of continual or lifelong learning.',\n",
       " 'The contributions of this paper are threefold. While many of the individual ingredients used in progressive nets can be found in the literature, their combination and use in solving complex sequences arXiv:1606.04671v4 [cs.LG] 22 Oct 2022 of tasks is novel. Second, we extensively evaluate the model in complex reinforcement learning domains. In the process, we also evaluate alternative approaches to transfer (such as finetuning) within the RL domain. In particular, we show that progressive networks provide comparable (if not slightly better) transfer performance to traditional finetuning, but without the destructive consequences. Finally, we develop a novel analysis based on Fisher Information and perturbation which allows us to analyse in detail how and where transfer occurs across tasks.',\n",
       " 'Continual learning is a long-standing goal of machine learning, where agents not only learn (and remember) a series of tasks experienced in sequence, but also have the ability to transfer knowledge from previous tasks to improve convergence speed [20] . Progressive networks integrate these desiderata directly into the model architecture: catastrophic forgetting is prevented by instantiating a new neural network (a column) for each task being solved, while transfer is enabled via lateral connections to features of previously learned columns. The scalability of this approach is addressed at the end of this section.',\n",
       " 'A progressive network starts with a single column: a deep neural network having L layers with hidden activations h',\n",
       " '(1) i ∈ R ni , with n i the number of units at layer i ≤ L, and parameters Θ (1)  trained to convergence. When switching to a second task, the parameters Θ (1) are \"frozen\" and a new column with parameters Θ (2) is instantiated (with random initialization), where layer h',\n",
       " 'i−1 via lateral connections. This generalizes to K tasks as follows: 1 :',\n",
       " 'where',\n",
       " '∈ R ni×nj are the lateral connections from layer i − 1 of column j, to layer i of column k and h 0 is the network input. f is an element-wise non-linearity: we use f (x) = max(0, x) for all intermediate layers. A progressive network with K = 3 is shown in Figure 1 . These modelling decisions are informed by our desire to: (1) solve K independent tasks at the end of training;',\n",
       " '(2) accelerate learning via transfer when possible; and (3) avoid catastrophic forgetting.',\n",
       " 'In the standard pretrain-and-finetune paradigm, there is often an implicit assumption of \"overlap\" between the tasks. Finetuning is efficient in this setting, as parameters need only be adjusted slightly to the target domain, and often only the top layer is retrained [23] . In contrast, we make no assumptions about the relationship between tasks, which may in practice be orthogonal or even adversarial. While the finetuning stage could potentially unlearn these features, this may prove difficult. Progressive networks side-step this issue by allocating a new column for each new task, whose weights are initialized randomly. Compared to the task-relevant initialization of pretraining, columns in progressive networks are free to reuse, modify or ignore previously learned features via the lateral connections. As the lateral connections U (k:j) i are only from column k to columns j < k, previous columns are not affected by the newly learned features in the forward pass. Because also the parameters {Θ (j) ; j < k} are kept frozen (i.e. are constants for the optimizer) when training Θ (k) , there is no interference between tasks and hence no catastrophic forgetting.',\n",
       " 'Application to Reinforcement Learning. Although progressive networks are widely applicable, this paper focuses on their application to deep reinforcement learning. In this case, each column is trained to solve a particular Markov Decision Process (MDP): the k-th column thus defines a policy π (k) (a | s) taking as input a state s given by the environment, and generating probabilities over actions π (k) (a | s) := h (k) L (s). At each time-step, an action is sampled from this distribution and taken in the environment, yielding the subsequent state. This policy implicitly defines a stationary distribution ρ π (k) (s, a) over states and actions.',\n",
       " 'Adapters. In practice, we augment the progressive network layer of Equation 2 with non-linear lateral connections which we call adapters. They serve both to improve initial conditioning and perform dimensionality reduction. Defining the vector of anterior features h',\n",
       " 'i−1 , in the case of dense layers, we replace the linear lateral connection with a single hidden layer MLP. Before feeding the lateral activations into the MLP, we multiply them by a learned scalar, initialized by a random small value. Its role is to adjust for the different scales of the different inputs. The hidden layer of the non-linear adapter is a projection onto an n i dimensional subspace. As the index k grows, this ensures that the number of parameters stemming from the lateral connections is in the same order as Θ (1) . Omitting bias terms, we get:',\n",
       " 'where',\n",
       " 'is the projection matrix. For convolutional layers, dimensionality reduction is performed via 1 × 1 convolutions [10] .',\n",
       " 'Limitations. Progressive networks are a stepping stone towards a full continual learning agent: they contain the necessary ingredients to learn multiple tasks, in sequence, while enabling transfer and being immune to catastrophic forgetting. A downside of the approach is the growth in number of parameters with the number of tasks. The analysis of Appendix 2 reveals that only a fraction of the new capacity is actually utilized, and that this trend increases with more columns. This suggests that growth can be addressed, e.g. by adding fewer layers or less capacity, by pruning [9] , or by online compression [17] during learning. Furthermore, while progressive networks retain the ability to solve all K tasks at test time, choosing which column to use for inference requires knowledge of the task label. These issues are left as future work.',\n",
       " 'Unlike finetuning, progressive nets do not destroy the features learned on prior tasks. This enables us to study in detail which features and at which depth transfer actually occurs. We explored two related methods: an intuitive, but slow method based on a perturbation analysis, and a faster analytical method derived from the Fisher Information [2] .',\n",
       " 'Average Perturbation Sensitivity (APS). To evaluate the degree to which source columns contribute to the target task, we can inject Gaussian noise at isolated points in the architecture (e.g. a given layer of a single column) and measure the impact of this perturbation on performance. A significant drop in performance indicates that the final prediction is heavily reliant on the feature map or layer. We find that this method yields similar results to the faster Fisher-based method presented below. We thus relegate details and results of the perturbation analysis to the appendix.',\n",
       " 'Average Fisher Sensitivity (AFS). We can get a local approximation to the perturbation sensitivity by using the Fisher Information matrix [2] . While the Fisher matrix is typically computed with respect to the model parameters, we compute a modified diagonal Fisher F of the network policy π with respect to the normalized activations 2 at each layer ĥ(k) i . For convolutional layers, we define F to implicitly perform a summation over pixel locations. F can be interpreted as the sensitivity of the policy to small changes in the representation. We define the diagonal matrix F , having elements F (m, m), and the derived Average Fisher Sensitivity (AFS) of feature m in layer i of column k as:',\n",
       " 'where the expectation is over the joint state-action distribution ρ(s, a) induced by the progressive network trained on the target task. In practice, it is often useful to consider the AFS score per-layer AFS(i, k) = m AFS(i, k, m), i.e. summing over all features of layer i. The AFS and APS thus estimate how much the network relies on each feature or column in a layer to compute its output.',\n",
       " 'There exist many different paradigms for transfer and multi-task reinforcement learning, as these have long been recognized as critical challenges in AI research [15, 19, 20] . Many methods for transfer learning rely on linear and other simple models (e.g. [18] ), which is a limiting factor to their applicability. Recently, there have been new methods proposed for multi-task or transfer learning with deep RL: [22, 17, 14] . In this work we present an architecture for deep reinforcement learning that in sequential task regimes that enables learning without forgetting while supporting individual feature transfer from previous learned tasks.',\n",
       " 'Pretraining and finetuning was proposed in [7] and applied to transfer learning in [4, 11] , generally in unsupervised-to-supervised or supervised-to-supervised settings. The actor-mimic approach [14] applied these principles to reinforcement learning, by fine-tuning a DQN multi-task network on new Atari games and showing that some responded with faster learning, while others did not. Progressive networks differ from the finetuning direction substantially, since capacity is added as new tasks are learned.',\n",
       " 'Progressive nets are related to the incremental and constructive architectures proposed in neural network literature. The cascade-correlation architecture was designed to eliminate forgetting while incrementally adding and refining feature extractors [6] . Auto-encoders such as [24] use incremental feature augmentation to track concept drift, and deep architectures such as [16] have been designed that specifically support feature transfer. More recently, in [1] , columns are separately trained on individual noise types, then linearly combined, and [5] use columns for image classification. The block-modular architecture of [21] has many similarities to our approach but focuses on a visual discrimination task. The progressive net approach, in contrast, uses lateral connections to access previously learned features for deep compositionality. It can be used in any sequential learning setting but is especially valuable in RL.',\n",
       " 'We evaluate progressive networks across three different RL domains. First, we consider synthetic versions of Pong, altered to have visual or control-level similarities. Next, we experiment broadly with random sequences of Atari games and perform a feature-level transfer analysis. Lastly, we demonstrate performance on a set of 3D maze games. Fig. 2 shows examples from selected tasks.',\n",
       " 'We rely on the Async Advantage Actor-Critic (A3C) framework introduced in [13] . Compared to DQN [12] , the model simultaneously learns a policy and a value function for predicting expected future rewards. A3C is trained on CPU using multiple threads and has been shown to converge faster than DQN on GPU. This made it a more natural fit for the large amount of sequential experiments required for this work. We report results by averaging the top 3 out of 25 jobs, each having different seeds and random hyper-parameter sampling. Performance is evaluated by measuring the area under the learning curve (average score per episode during training), rather than final score. The transfer score is then defined as the relative performance of an architecture compared with a single column baseline, trained only on the target task (baseline 1). We present transfer score curves for selected source-target games, and summarize all such pairs in transfer matrices. Models and baselines we consider are illustrated in Figure 3 . Details of the experimental setup are provided in section 3 of the Appendix.',\n",
       " 'The first evaluation domain is a set of synthetic variants of the Atari game of Pong (\"Pong Soup\") where the visuals and gameplay have been altered, thus providing a setting where we can be confident that there are transferable aspects of the tasks. We can make several observations from these results. Baseline 2 (single column, only output layer is finetuned; see Fig. 3 ) fails to learn the target task in most experiments and thus has negative transfer. This approach is quite standard in supervised learning settings, where features from ImageNet-trained nets are routinely repurposed for new domains. As expected, we observe high positive transfer with baseline 3 (single column, full finetuning), a well established paradigm for transfer. Progressive networks outperform this baseline however in terms of both median and mean score, with the difference being more pronounced for the latter. As the mean is more sensitive to outliers, this suggests that progressive networks are better able to exploit transfer when transfer is possible (i.e. when source and target domains are compatible). Fig. 4 (b) lends weight to this hypothesis, where progressive networks are shown to significantly outperform the baselines for particular game pairs. Progressive nets also compare favourably to baseline 4, confirming that progressive nets are indeed taking advantage of the features learned in previous columns. We use the metric derived in Sec. 3 to analyse what features are being transferred between Pong variants. We see that when switching from Pong to H-Flip, the network reuses the same components of low and mid-level vision (the outputs of the two convolutional layers; Figure 5a ). However, the fully connected layer must be largely re-learned, as the policy relevant features of the task (the relative locations/velocities of the paddle and ball) are now in a new location. When switching from Pong to Zoom, on the other hand, low-level vision is reused for the new task, but new mid-level vision features are learned. Interestingly, only one low-level feature appears to be reused: (see Fig. 5b ): this is a spatio-temporal filter with a considerable temporal DC component. This appears sufficient for detecting both ball motion and paddle position in the original, flipped, and zoomed Pongs.',\n",
       " 'Finally, when switching from Pong to Noisy, some new low-level vision is relearned. This is likely because the first layer filter learned on the clean task is not sufficiently tolerant to the added noise. In contrast, this problem does not apply when moving from Noisy to Pong (Figure 5a , rightmost column), where all of vision transfers to the new task.',\n",
       " 'We next investigate feature transfer between randomly selected Atari games [3] . This is an interesting question, because the visuals of Atari games are quite different from each other, as are the controls and required strategy. Though games like Pong and Breakout are conceptually similar (both involve hitting a ball with a paddle), Pong is vertically aligned while Breakout is horizontal: a potentially insurmountable feature-level difference. Other Atari game pairs have no discernible overlap, even at a conceptual level.',\n",
       " 'To this end we start by training single columns on three source games (Pong, River Raid, and Seaquest) 3 and assess if the learned features transfer to a different subset of randomly selected target games (Alien, Asterix, Boxing, Centipede, Gopher, Hero, James Bond, Krull, Robotank, Road Runner, Star Gunner, and Wizard of Wor). We evaluate progressive networks with 2, 3 and 4 columns, Pong Soup Atari Labyrinth Mean (%) Median (%) Mean (%) Median (%) Mean (%) Median (%) comparing to the baselines of Figure 3 ). The transfer matrix and selected transfer curves are shown in Figure 6 , and the results summarized in Table 1 .',\n",
       " 'Across all games, we observe from Fig. 6 , that progressive nets result in positive transfer in 8 out of 12 target tasks, with only two cases of negative transfer. This compares favourably to baseline 3, which yields positive transfer in only 5 of 12 games. This trend is reflected in Table 1 , where progressive networks convincingly outperform baseline 3 when using additional columns. This is especially promising as we show in the Appendix that progressive network use a diminishing amount of capacity with each added column, pointing a clear path to online compression or pruning as a means to mitigate the growth in model size.',\n",
       " 'Now consider the specific sequence Seaquest-to-Gopher, an example of two dissimilar games. Here, the pretrain/finetune paradigm (baseline 3) exhibits negative transfer, unlike progressive networks (see Fig. 6b , bottom), perhaps because they are more able to ignore the irrelevant features. For the sequence Seaquest[+River Raid][+Pong]-to-Boxing, using additional columns in the progressive networks can yield a significant increase in transfer (see Fig. 6b , top).',\n",
       " 'Figure 6 demonstrates that both positive and negative transfer is possible with progressive nets. To differentiate these cases, we consider the Average Fisher Sensitivity for the 3 column case (e.g., see Fig. 7a ). A clear pattern emerges amongst these and other examples: the most negative transfer coincides with complete dependence on the convolutional layers of the previous columns, and no learning of new visual features in the new column. In contrast, the most positive transfer occurs when the features of the first two columns are augmented by new features. The statistics across all 3-column nets (Figure 7b ) show that positive transfer in Atari occurs at a \"sweet spot\" between heavy reliance on features from the source task, and heavy reliance on all new features for the target task.',\n",
       " 'At first glance, this result appears unintuitive: if a progressive net finds a valuable feature set from a source task, shouldn\\'t we expect a high degree of transfer? We offer two hypotheses. First, this may simply reflect an optimization difficulty, where the source features offer fast convergence to a poor local minimum. This is a known challenge in transfer learning [20] : learned source tasks confer an inductive bias that can either help or hinder in different cases. Second, this may reflect a problem of exploration, where the transfered representation is \"good enough\" for a functional, but sub-optimal policy.',\n",
       " \"The final experimental setting for progressive networks is Labyrinth, a 3D maze environment where the inputs are rendered images granting partial observability and the agent outputs discrete actions, including looking up, down, left, or right and moving forward, backwards, left, or right. The tasks as well as the level maps are diverse and involve getting positive scores for 'eating' good items (apples, strawberries) and negative scores for eating bad items (mushrooms, lemons). Details can be found in the appendix. While there is conceptual and visual overlap between the different tasks, the tasks present a challenging set of diverse game elements (Figure 2 ). As in the other domains, the progressive approach yields more positive transfer than any of the baselines (see Fig. 8a and Table 1 ). We observe less transfer on the Seek Track levels, which have dense reward items throughout the maze and are easily learned. Note that even for these easy cases, baseline 2 shows negative transfer because it cannot learn new low-level visual features, which are important because the reward items change from task to task. The learning curves in Fig. 8b exemplify the typical results seen in this domain: on simpler games, such as Track 1 and 2, learning is rapid and stable by all agents. On more difficult games, with more complex game structure, the baselines struggle and progressive nets have an advantage.\",\n",
       " 'Continual learning, the ability to accumulate and transfer knowledge to new domains, is a core characteristic of intelligent beings. Progressive neural networks are a stepping stone towards continual learning, and this work has demonstrated their potential through experiments and analysis across three RL domains, including Atari, which contains orthogonal or even adversarial tasks. We believe that we are the first to show positive transfer in deep RL agents within a continual learning framework. Moreover, we have shown that the progressive approach is able to effectively exploit transfer for compatible source and task domains; that the approach is robust to harmful features learned in incompatible tasks; and that positive transfer increases with the number of columns, thus corroborating the constructive, rather than destructive, nature of the progressive architecture.',\n",
       " 'We explored two related methods for analysing transfer in progressive networks. One based on Fisher information yields the Average Fisher Sensitivity (AFS) and is described in Section 3 of the paper. We describe the second method based on perturbation analysis in this appendix, as it proved too slow to use at scale. Given its intuitive appeal however, we provide details of the method along with results on Pong Variants (see Section 5.2), as a means to corroborate the AFS score.',\n",
       " 'Our perturbation analysis aims to estimate which components of the source columns materially contribute to the performance of the final column on the target tasks. To this end, we injected Gaussian noise into each of the (post-ReLU) hidden representations, with a new sample on every forward pass, and calculated the average effect of these perturbations on the game score over 10 episodes. We did this at a coarse scale, by adding noise across all features of a given layer, though a fine scale analysis is also possible per feature (map). In order to be invariant to any arbitrary scale factors in the network weights, we scale the noise variance proportional to the variance of the activations in each feature map and fully-connected neuron. Scaling the variance in this manner is analogous to computing the Fisher w.r.t. normalized activations for the AFS score.',\n",
       " 'Figure 9 : (a) Perturbation analysis for the two second-layer convolutional representations in the two columns of the Pong/Pong-noise net. Blue: adding noise to second convolutional layer from column 1; green: from column 2. Grey line determines critical noise magnitude for each representation, σ 2 i . (b-c) Comparison of per-layer sensitivities obtained using the APS method (b) and the AFS method (c; as per main text). These are highly similar.',\n",
       " 'as the precision of the noise injected at layer i of column k, which results in a 50% drop in performance. The Average Perturbation Sensitivity (APS) for this layer is simply:',\n",
       " 'Note that this value is normalized across columns for a given layer. The APS score can thus be interpreted as the responsibility of each column in a given layer to final performance. The APS score of 2-column progressive networks trained on Pong Variants is shown in Fig9 (b). These clearly corroborate the AFS shown in (c).',\n",
       " 'As described in the main text, one of the limitations of progressive networks is the growth in the size of the network with added tasks. In the basic approach we pursue in the main text, the number of hidden units and feature maps grows linearly with the number of columns, and the number of parameters grows quadratically.',\n",
       " 'Here, we sought to determine the degree to which this full capacity is actually used by the network. We leveraged the Average Fisher Sensitivity measure to study how increasing the number of columns in the Atari task set changes the need for additional resources. In Figure 10a , we measure the average fractional use of existing feature maps in a given layer (here, layer 2). We do this for each network by concatenating the per-feature-map AFS values from all source columns in this layer, sorting the values to produce a spectrum, and then averaging across networks. We find that as the number of columns increases, the average spectrum becomes sparser: the network relies on a smaller proportion of features from the source columns. Similar results were found for all layers.',\n",
       " \"Similarly, in Figure 10b , we measure the capacity required in the final added column as a function of the total number of columns. Again, we measure the spectrum of AFS values in an example layer, but here from only the final column. As the progressive network grows, the new column's features are both less important overall (indicated by the declining area under the graph), and have a sparser AFS spectrum. Combined, these results suggest that significant pruning of lateral connections is possible, and the quadratic growth of parameters might be contained.\",\n",
       " 'In our grid we sample hyper-parameters from categorical distributions:',\n",
       " '• Learning rate was sampled from {10 −3 , 5 • 10 −4 , 10 −4 }.',\n",
       " '• Strength of the entropy regularization from {10 −2 , 10 −3 , 10 −4 } • Gradient clipping cut-off from {20, 40}',\n",
       " '• scalar multiplier on the lateral feature is initialized randomly to one from {1, 10 −1 , 10 −2 }',\n",
       " 'For the Atari experiments we used a model with 3 convolutional layers followed by a fully connected layer and from which we predict the policy and value function. The convolutional layers are as follows. All have 12 feature maps. The first convolutional layer has a kernel of size 8x8 and a stride of 4x4. The second layer has a kernel of size 4 and a stride of 2. The last convolutional layer has size 3x4 with a stride of 1. The fully connected layer has 256 hidden units.',\n",
       " 'Learning follows closely the paradigm described in [13] . We use 16 workers and the same RMSProp algorithm without momentum or centring of the variance. The score for each point of a training curve is the average over all the episodes the model gets to finish in 25e4 environment steps.',\n",
       " 'The whole experiments are run for a maximum of 1.6e8 environment step. The agent has an action repeat of 4 as in [13] , which means that for 4 consecutive steps the agent will use the same action picked at the beginning of the series. For this reason through out the paper we actually report results in terms of agent perceived steps rather than environment steps. That is, the maximal number of agent perceived step that we do for any particular run is 4e7.',\n",
       " 'Figure 11 shows training curves for all the target games in the Atari domain. We plot learning curves for two column, three column and four column progressive networks alongside Baseline 3 (gray dashed line), a model pretrained on Seaquest and then finetuned on the particular target game and Baseline 1 (gray dotted line), where a single column is trained on the source game Seaquest.',\n",
       " 'We can see that overall baseline 3 performs well. However there are situations when having features learned from more previous task actually helps with transfer (e.g. when target game is Boxing).  Figure 12 shows how two-column progressive networks perform as compared to Baseline 3 (gray dashed line), a model pretrained on the source game, here standard Pong, and then finetuned on a particular target game, and Baseline 1 (black dotted line), where a single column is trained on standard Pong. Figure 13 shows two-column progressive networks and baselines on Labyrinth tasks; the source game was Maze Y.',\n",
       " 'Section 5.4 evaluates progressive networks on foraging tasks in complex 3D maze environments. Positive rewards are given to the agent for collecting apples and strawberries, and negative rewards for mushrooms and lemons. Episodes terminate when either all (positive) rewards are collected, or after a fixed time interval.',\n",
       " 'Levels differ in their maze layout, the type of items present and the sparsity of the reward structure. The levels we employed can be characterized as follows:',\n",
       " '• Seek Track 1: simple corridor with many apples']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7ea7b-d562-4081-a862-e89ce1b95f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "count = 0\n",
    "for i in range(len(paras)):\n",
    "    find = search(paras[i], 'Problem', 1)\n",
    "    if (find!=\"Not found\"):\n",
    "        l.append(find)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e87381",
   "metadata": {},
   "source": [
    "## Manual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2d102624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "905201d5-a478-4bad-9dc5-e6350cfe9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_articles=50\n",
    "path_corpus = './case-studies/arxiv-corpus/mine50/'\n",
    "scrape_df = read_csv(path_corpus + 'scrape_df_' + str(max_articles)+ '.csv')\n",
    "\n",
    "# TODO index label for scrape_df\n",
    "# scrape_df['id'][0:50].to_csv(path_corpus + 'scrape_id.csv')\n",
    "scrape_df[['id']].to_csv('scrape_id.csv',index_label=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "eb2478de",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './case-studies/arxiv-corpus/mine50/manual_eval.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/adb/gitclones/repro-screener/find.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/adb/gitclones/repro-screener/find.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m manual_eval \u001b[39m=\u001b[39m  read_csv(path_corpus \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mmanual_eval.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/adb/gitclones/repro-screener/find.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m gunderson_vars \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mproblem\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mresearch_method\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mresearch_questions\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mpseudocode\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtraining_data\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mvalidation_data\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtest_data\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mhypothesis\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mmethod_source_code\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mhardware_specifications\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msoftware_dependencies\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mexperiment_setup\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mexperiment_source_code\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mresearch_type\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39maffiliation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/adb/gitclones/repro-screener/find.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(manual_eval\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/repro-screener/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/repro-screener/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/repro-screener/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/repro-screener/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/repro-screener/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/repro-screener/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1730\u001b[0m     f,\n\u001b[1;32m   1731\u001b[0m     mode,\n\u001b[1;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1738\u001b[0m )\n\u001b[1;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/repro-screener/lib/python3.9/site-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    858\u001b[0m             handle,\n\u001b[1;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    863\u001b[0m         )\n\u001b[1;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './case-studies/arxiv-corpus/mine50/manual_eval.csv'"
     ]
    }
   ],
   "source": [
    "manual_eval =  read_csv(path_corpus + 'manual_eval.csv')\n",
    "gunderson_vars = [\"id\",\"problem\",\"objective\",\"research_method\",\"research_questions\",\"pseudocode\",\"training_data\",\"validation_data\",\"test_data\",\"results\",\"hypothesis\",\"prediction\",\"method_source_code\",\"hardware_specifications\",\"software_dependencies\",\"experiment_setup\",\"experiment_source_code\",\"research_type\",\"affiliation\"]\n",
    "\n",
    "print(manual_eval.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6d77b",
   "metadata": {},
   "source": [
    "## Flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6000bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "import exrex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84aeb79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test Data', 'Test data', 'test Data', 'test data']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_problem = list(exrex.generate('((((P|p)roblem) ((S|s)tatement)))|((((R|r)esearch) ((P|p)roblem)))|((P|p)roblems?)'))\n",
    "keys_objective = list(exrex.generate('((((O|o)bjective)))|((((G|g)oal)|((((R|r)esearch) ((O|o)bjective))))|((((R|r)esearch) ((G|g)oal))))'))\n",
    "keys_research_method = list(exrex.generate('((R|r)esearch (M|m)ethods?)|'\\\n",
    "    '(M|m)ethods?'))\n",
    "keys_research_questions = list(exrex.generate('((((R|r)esearch)))|((((Q|q)uestions?)|'\\\n",
    "    '((((R|r)esearch) ((O|o)bjective))))|'\\\n",
    "    '((((R|r)esearch) ((G|g)oal))))'))\n",
    "keys_pseudocode = list(exrex.generate('((P|p)seudo-?code)|(Algorithm [1-9])'))\n",
    "keys_experiment_setup = list(exrex.generate('(E|e)xperimental( |-)(S|s)etup|'\\\n",
    "                                            '((H|h)yper( |-)(P|p)arameters)'))\n",
    "keys_hypothesis = list(exrex.generate('(H|h)ypothes(i|e)s'))\n",
    "keys_prediction = list(exrex.generate('(P|p)redictions?'))\n",
    "keys_hardware_specifications = list(exrex.generate('(H|h)ardware (S|s)pecification(s)'))\n",
    "keys_software_dependencies = list(exrex.generate('(S|s)oftware (D|d)ependencies'))\n",
    "keys_affiliation = list(exrex.generate('@edu'))\n",
    "keys_method_source_code = list(exrex.generate('(G|g)it( )?(H|h)ub|'\\\n",
    "    '(B|b)it(B|b)ucket|'\\\n",
    "        '(G|g)it( )?(L|l)ab'\n",
    "))\n",
    "keys_training_data = list(exrex.generate('(T|t)raining (D|d)ata'))\n",
    "keys_validation_data = list(exrex.generate('(V|v)alidation (D|d)ata'))\n",
    "keys_test_data = list(exrex.generate('(T|t)est (D|d)ata'))\n",
    "\n",
    "keys_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa7bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "adc\n",
      "['abc', 'Abc', 'adc', 'Adc']\n"
     ]
    }
   ],
   "source": [
    "def caps_combination(input_str):\n",
    "    words = input_str.split(\" \")\n",
    "    result = []\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        # result.append(list(map(''.join, itertools.product(*((c.lower(), c.upper()) for c in word[0])))))\n",
    "        # result.append(word[0].upper)\n",
    "        result.append(word.lower())\n",
    "        result.append(word.title())\n",
    "    return result\n",
    "st =\"abc adc\"\n",
    "print(caps_combination(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1e304a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[], [], [], [], []], [])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_processor = KeywordProcessor(case_sensitive=True)\n",
    "keyword_dict = {\n",
    "    \"problem\": keys_problem,\n",
    "    \"objective\": keys_objective,\n",
    "    \"research_method\": keys_research_method,\n",
    "    \"research_questions\": keys_research_questions,\n",
    "    \"pseudocode\": keys_pseudocode,\n",
    "    \n",
    "    \"hypothesis\": keys_hypothesis,\n",
    "    \"prediction\": keys_prediction,\n",
    "    \"method_source_code\": keys_method_source_code,\n",
    "    \"hardware_specifications\": keys_hardware_specifications,\n",
    "    \"software_dependencies\": keys_software_dependencies,\n",
    "    \"experiment_setup\": keys_experiment_setup,\n",
    "    \n",
    "    \"affiliation\": keys_affiliation,\n",
    "}\n",
    "keyword_processor.add_keywords_from_dict(keyword_dict)\n",
    "\n",
    "all_found_paras = []\n",
    "edu_emails = []\n",
    "\n",
    "for i in range(len(paras)):\n",
    "    all_found_paras.append(keyword_processor.extract_keywords(paras[i], span_info=True))\n",
    "for i in range(len(emails)):\n",
    "    edu_emails.append(keyword_processor.extract_keywords(emails[i], span_info=True))\n",
    "    \n",
    "non_empty_found_paras = [x for x in all_found_paras if x != []]\n",
    "non_empty_found_emails = [x for x in edu_emails if x != []]\n",
    "edu_emails, non_empty_found_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36dfe8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('research_method', 23, 29)\n",
      "Finetuning remains the ***____method___*** of choice for transfer learning with neural networks: a model is pretrained on a source domain (where data is often abundant), the output layers of the model are adapted to the target domain, and the network is finetuned via backpropagation. This approach was pioneered in [7] by transferring knowledge from a generative to a discriminative model, and has since been generalized with great success [11] . Unfortunately, the approach has drawbacks which make it unsuitable for transferring across multiple tasks: if we wish to leverage knowledge acquired over a sequence of experiences, which model should we use to initialize subsequent models? This seems to require not only a learning method that can support transfer learning without catastrophic forgetting, but also foreknowledge of task similarity. Furthermore, while finetuning may allow us to recover expert performance in the target domain, it is a destructive process which discards the previously learned function. One could copy each model before finetuning to explicitly remember all previous tasks, but the issue of selecting a proper initialization remains. While distillation [8] offers one potential solution to multitask learning [17] , it requires a reservoir of persistent training data for all tasks, an assumption which may not always hold. This paper introduces progressive networks, a novel model architecture with explicit support for transfer across sequences of tasks. While finetuning incorporates prior knowledge only at initialization, progressive networks retain a pool of pretrained models throughout training, and learn lateral connections from these to extract useful features for the new task. By combining previously learned features in this manner, progressive networks achieve a richer compositionality, in which prior knowledge is no longer transient and can be integrated at each layer of the feature hierarchy. Moreover, the addition of new capacity alongside pretrained networks gives these models the flexibility to both reuse old computations and learn new ones. As we will show, progressive networks naturally accumulate experiences and are immune to catastrophic forgetting by design, making them an ideal springboard for tackling long-standing problems of continual or lifelong learning.\n",
      "('research_method', 717, 723)\n",
      "Finetuning remains the method of choice for transfer learning with neural networks: a model is pretrained on a source domain (where data is often abundant), the output layers of the model are adapted to the target domain, and the network is finetuned via backpropagation. This approach was pioneered in [7] by transferring knowledge from a generative to a discriminative model, and has since been generalized with great success [11] . Unfortunately, the approach has drawbacks which make it unsuitable for transferring across multiple tasks: if we wish to leverage knowledge acquired over a sequence of experiences, which model should we use to initialize subsequent models? This seems to require not only a learning ***____method___*** that can support transfer learning without catastrophic forgetting, but also foreknowledge of task similarity. Furthermore, while finetuning may allow us to recover expert performance in the target domain, it is a destructive process which discards the previously learned function. One could copy each model before finetuning to explicitly remember all previous tasks, but the issue of selecting a proper initialization remains. While distillation [8] offers one potential solution to multitask learning [17] , it requires a reservoir of persistent training data for all tasks, an assumption which may not always hold. This paper introduces progressive networks, a novel model architecture with explicit support for transfer across sequences of tasks. While finetuning incorporates prior knowledge only at initialization, progressive networks retain a pool of pretrained models throughout training, and learn lateral connections from these to extract useful features for the new task. By combining previously learned features in this manner, progressive networks achieve a richer compositionality, in which prior knowledge is no longer transient and can be integrated at each layer of the feature hierarchy. Moreover, the addition of new capacity alongside pretrained networks gives these models the flexibility to both reuse old computations and learn new ones. As we will show, progressive networks naturally accumulate experiences and are immune to catastrophic forgetting by design, making them an ideal springboard for tackling long-standing problems of continual or lifelong learning.\n",
      "('problem', 2271, 2279)\n",
      "Finetuning remains the method of choice for transfer learning with neural networks: a model is pretrained on a source domain (where data is often abundant), the output layers of the model are adapted to the target domain, and the network is finetuned via backpropagation. This approach was pioneered in [7] by transferring knowledge from a generative to a discriminative model, and has since been generalized with great success [11] . Unfortunately, the approach has drawbacks which make it unsuitable for transferring across multiple tasks: if we wish to leverage knowledge acquired over a sequence of experiences, which model should we use to initialize subsequent models? This seems to require not only a learning method that can support transfer learning without catastrophic forgetting, but also foreknowledge of task similarity. Furthermore, while finetuning may allow us to recover expert performance in the target domain, it is a destructive process which discards the previously learned function. One could copy each model before finetuning to explicitly remember all previous tasks, but the issue of selecting a proper initialization remains. While distillation [8] offers one potential solution to multitask learning [17] , it requires a reservoir of persistent training data for all tasks, an assumption which may not always hold. This paper introduces progressive networks, a novel model architecture with explicit support for transfer across sequences of tasks. While finetuning incorporates prior knowledge only at initialization, progressive networks retain a pool of pretrained models throughout training, and learn lateral connections from these to extract useful features for the new task. By combining previously learned features in this manner, progressive networks achieve a richer compositionality, in which prior knowledge is no longer transient and can be integrated at each layer of the feature hierarchy. Moreover, the addition of new capacity alongside pretrained networks gives these models the flexibility to both reuse old computations and learn new ones. As we will show, progressive networks naturally accumulate experiences and are immune to catastrophic forgetting by design, making them an ideal springboard for tackling long-standing ***____problems___*** of continual or lifelong learning.\n"
     ]
    }
   ],
   "source": [
    "for found_word in non_empty_found_paras[0]:\n",
    "    print(found_word)\n",
    "    print(paras[1][:found_word[1]] + \"***____\" + paras[1][found_word[1]:found_word[2]] + \"___***\" + paras[1][found_word[2]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2c78d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n",
      "['andreirusu@google.com', 'gdesjardins@google.com', 'soyer@google.com', 'kirkpatrick@google.com', 'korayk@google.com']\n"
     ]
    }
   ],
   "source": [
    "print(all_found_emails)\n",
    "print(emails)\n",
    "for found_email in non_empty_found_emails:\n",
    "    print(found_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7df51560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('research_method', 23, 29), ('research_method', 717, 723), ('problem', 2271, 2279)], [('objective', 38, 42)], [('research_method', 207, 214), ('research_method', 239, 245), ('research_method', 304, 310)], [('prediction', 345, 355), ('research_method', 422, 428), ('research_method', 479, 485)], [('research_questions', 153, 161), ('research_method', 182, 189), ('research_method', 346, 353)], [('experiment_setup', 1037, 1055)], [('hypothesis', 1129, 1139)], [('problem', 223, 230)], [('research_questions', 104, 112)], [('hypothesis', 180, 190), ('problem', 503, 510)], [('research_method', 24, 31), ('research_method', 223, 229), ('research_method', 380, 386)], [('research_method', 371, 377), ('research_method', 394, 400)], [('experiment_setup', 22, 38)]]\n"
     ]
    }
   ],
   "source": [
    "print(non_empty_found_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7557cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_articles=50\n",
    "path_corpus = './case-studies/arxiv-corpus/mine50v2/'\n",
    "\n",
    "manual_eval =  read_csv(path_corpus + 'manual_eval.csv')\n",
    "gunderson_vars = [\"id\",\"problem\",\"objective\",\"research_method\",\"research_questions\",\"pseudocode\",\"training_data\",\"validation_data\",\"test_data\",\"results\",\"hypothesis\",\"prediction\",\"method_source_code\",\"hardware_specifications\",\"software_dependencies\",\"experiment_setup\",\"experiment_source_code\",\"research_type\",\"affiliation\"]\n",
    "\n",
    "# print(manual_eval.iloc[0])\n",
    "repro_eval = read_csv(path_corpus + 'scrape_df_' + str(max_articles)+ '.csv')\n",
    "\n",
    "# TODO index label for repro_eval\n",
    "# repro_eval['id'][0:50].to_csv(path_corpus + 'scrape_id.csv')\n",
    "repro_eval = repro_eval[['id']]\n",
    "repro_eval = repro_eval.reindex(gunderson_vars, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fc714c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>objective</th>\n",
       "      <th>research_method</th>\n",
       "      <th>research_questions</th>\n",
       "      <th>pseudocode</th>\n",
       "      <th>training_data</th>\n",
       "      <th>validation_data</th>\n",
       "      <th>test_data</th>\n",
       "      <th>results</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>prediction</th>\n",
       "      <th>method_source_code</th>\n",
       "      <th>hardware_specifications</th>\n",
       "      <th>software_dependencies</th>\n",
       "      <th>experiment_setup</th>\n",
       "      <th>experiment_source_code</th>\n",
       "      <th>research_type</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1606.04671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1903.09668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904.10554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1908.05659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1909.00931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1911.03867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2002.05905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2004.05258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2005.02607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2007.09855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2009.01947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2010.04261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010.04855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2011.11576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012.09302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2101.07354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2102.11887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2104.11893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2104.12546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2105.01099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2105.01937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2105.15197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2106.01528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2106.03157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2106.03725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2106.06610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2106.06927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2106.07704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2106.10898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2106.12177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2106.12936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2106.13823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2107.01131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2108.09779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2109.01372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2109.12784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2109.14855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2110.02343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2110.02474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2110.03135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2110.05169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2110.08255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2110.08432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2110.09902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2110.11688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2110.13624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2110.14241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2111.02997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2111.03289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2111.03664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  problem  objective  research_method  research_questions  \\\n",
       "0   1606.04671      NaN        NaN              NaN                 NaN   \n",
       "1   1903.09668      NaN        NaN              NaN                 NaN   \n",
       "2   1904.10554      NaN        NaN              NaN                 NaN   \n",
       "3   1908.05659      NaN        NaN              NaN                 NaN   \n",
       "4   1909.00931      NaN        NaN              NaN                 NaN   \n",
       "5   1911.03867      NaN        NaN              NaN                 NaN   \n",
       "6   2002.05905      NaN        NaN              NaN                 NaN   \n",
       "7   2004.05258      NaN        NaN              NaN                 NaN   \n",
       "8   2005.02607      NaN        NaN              NaN                 NaN   \n",
       "9   2007.09855      NaN        NaN              NaN                 NaN   \n",
       "10  2009.01947      NaN        NaN              NaN                 NaN   \n",
       "11  2010.04261      NaN        NaN              NaN                 NaN   \n",
       "12  2010.04855      NaN        NaN              NaN                 NaN   \n",
       "13  2011.11576      NaN        NaN              NaN                 NaN   \n",
       "14  2012.09302      NaN        NaN              NaN                 NaN   \n",
       "15  2101.07354      NaN        NaN              NaN                 NaN   \n",
       "16  2102.11887      NaN        NaN              NaN                 NaN   \n",
       "17  2104.11893      NaN        NaN              NaN                 NaN   \n",
       "18  2104.12546      NaN        NaN              NaN                 NaN   \n",
       "19  2105.01099      NaN        NaN              NaN                 NaN   \n",
       "20  2105.01937      NaN        NaN              NaN                 NaN   \n",
       "21  2105.15197      NaN        NaN              NaN                 NaN   \n",
       "22  2106.01528      NaN        NaN              NaN                 NaN   \n",
       "23  2106.03157      NaN        NaN              NaN                 NaN   \n",
       "24  2106.03725      NaN        NaN              NaN                 NaN   \n",
       "25  2106.06610      NaN        NaN              NaN                 NaN   \n",
       "26  2106.06927      NaN        NaN              NaN                 NaN   \n",
       "27  2106.07704      NaN        NaN              NaN                 NaN   \n",
       "28  2106.10898      NaN        NaN              NaN                 NaN   \n",
       "29  2106.12177      NaN        NaN              NaN                 NaN   \n",
       "30  2106.12936      NaN        NaN              NaN                 NaN   \n",
       "31  2106.13823      NaN        NaN              NaN                 NaN   \n",
       "32  2107.01131      NaN        NaN              NaN                 NaN   \n",
       "33  2108.09779      NaN        NaN              NaN                 NaN   \n",
       "34  2109.01372      NaN        NaN              NaN                 NaN   \n",
       "35  2109.12784      NaN        NaN              NaN                 NaN   \n",
       "36  2109.14855      NaN        NaN              NaN                 NaN   \n",
       "37  2110.02343      NaN        NaN              NaN                 NaN   \n",
       "38  2110.02474      NaN        NaN              NaN                 NaN   \n",
       "39  2110.03135      NaN        NaN              NaN                 NaN   \n",
       "40  2110.05169      NaN        NaN              NaN                 NaN   \n",
       "41  2110.08255      NaN        NaN              NaN                 NaN   \n",
       "42  2110.08432      NaN        NaN              NaN                 NaN   \n",
       "43  2110.09902      NaN        NaN              NaN                 NaN   \n",
       "44  2110.11688      NaN        NaN              NaN                 NaN   \n",
       "45  2110.13624      NaN        NaN              NaN                 NaN   \n",
       "46  2110.14241      NaN        NaN              NaN                 NaN   \n",
       "47  2111.02997      NaN        NaN              NaN                 NaN   \n",
       "48  2111.03289      NaN        NaN              NaN                 NaN   \n",
       "49  2111.03664      NaN        NaN              NaN                 NaN   \n",
       "\n",
       "    pseudocode  training_data  validation_data  test_data  results  \\\n",
       "0          NaN            NaN              NaN        NaN      NaN   \n",
       "1          NaN            NaN              NaN        NaN      NaN   \n",
       "2          NaN            NaN              NaN        NaN      NaN   \n",
       "3          NaN            NaN              NaN        NaN      NaN   \n",
       "4          NaN            NaN              NaN        NaN      NaN   \n",
       "5          NaN            NaN              NaN        NaN      NaN   \n",
       "6          NaN            NaN              NaN        NaN      NaN   \n",
       "7          NaN            NaN              NaN        NaN      NaN   \n",
       "8          NaN            NaN              NaN        NaN      NaN   \n",
       "9          NaN            NaN              NaN        NaN      NaN   \n",
       "10         NaN            NaN              NaN        NaN      NaN   \n",
       "11         NaN            NaN              NaN        NaN      NaN   \n",
       "12         NaN            NaN              NaN        NaN      NaN   \n",
       "13         NaN            NaN              NaN        NaN      NaN   \n",
       "14         NaN            NaN              NaN        NaN      NaN   \n",
       "15         NaN            NaN              NaN        NaN      NaN   \n",
       "16         NaN            NaN              NaN        NaN      NaN   \n",
       "17         NaN            NaN              NaN        NaN      NaN   \n",
       "18         NaN            NaN              NaN        NaN      NaN   \n",
       "19         NaN            NaN              NaN        NaN      NaN   \n",
       "20         NaN            NaN              NaN        NaN      NaN   \n",
       "21         NaN            NaN              NaN        NaN      NaN   \n",
       "22         NaN            NaN              NaN        NaN      NaN   \n",
       "23         NaN            NaN              NaN        NaN      NaN   \n",
       "24         NaN            NaN              NaN        NaN      NaN   \n",
       "25         NaN            NaN              NaN        NaN      NaN   \n",
       "26         NaN            NaN              NaN        NaN      NaN   \n",
       "27         NaN            NaN              NaN        NaN      NaN   \n",
       "28         NaN            NaN              NaN        NaN      NaN   \n",
       "29         NaN            NaN              NaN        NaN      NaN   \n",
       "30         NaN            NaN              NaN        NaN      NaN   \n",
       "31         NaN            NaN              NaN        NaN      NaN   \n",
       "32         NaN            NaN              NaN        NaN      NaN   \n",
       "33         NaN            NaN              NaN        NaN      NaN   \n",
       "34         NaN            NaN              NaN        NaN      NaN   \n",
       "35         NaN            NaN              NaN        NaN      NaN   \n",
       "36         NaN            NaN              NaN        NaN      NaN   \n",
       "37         NaN            NaN              NaN        NaN      NaN   \n",
       "38         NaN            NaN              NaN        NaN      NaN   \n",
       "39         NaN            NaN              NaN        NaN      NaN   \n",
       "40         NaN            NaN              NaN        NaN      NaN   \n",
       "41         NaN            NaN              NaN        NaN      NaN   \n",
       "42         NaN            NaN              NaN        NaN      NaN   \n",
       "43         NaN            NaN              NaN        NaN      NaN   \n",
       "44         NaN            NaN              NaN        NaN      NaN   \n",
       "45         NaN            NaN              NaN        NaN      NaN   \n",
       "46         NaN            NaN              NaN        NaN      NaN   \n",
       "47         NaN            NaN              NaN        NaN      NaN   \n",
       "48         NaN            NaN              NaN        NaN      NaN   \n",
       "49         NaN            NaN              NaN        NaN      NaN   \n",
       "\n",
       "    hypothesis  prediction  method_source_code  hardware_specifications  \\\n",
       "0          NaN         NaN                 NaN                      NaN   \n",
       "1          NaN         NaN                 NaN                      NaN   \n",
       "2          NaN         NaN                 NaN                      NaN   \n",
       "3          NaN         NaN                 NaN                      NaN   \n",
       "4          NaN         NaN                 NaN                      NaN   \n",
       "5          NaN         NaN                 NaN                      NaN   \n",
       "6          NaN         NaN                 NaN                      NaN   \n",
       "7          NaN         NaN                 NaN                      NaN   \n",
       "8          NaN         NaN                 NaN                      NaN   \n",
       "9          NaN         NaN                 NaN                      NaN   \n",
       "10         NaN         NaN                 NaN                      NaN   \n",
       "11         NaN         NaN                 NaN                      NaN   \n",
       "12         NaN         NaN                 NaN                      NaN   \n",
       "13         NaN         NaN                 NaN                      NaN   \n",
       "14         NaN         NaN                 NaN                      NaN   \n",
       "15         NaN         NaN                 NaN                      NaN   \n",
       "16         NaN         NaN                 NaN                      NaN   \n",
       "17         NaN         NaN                 NaN                      NaN   \n",
       "18         NaN         NaN                 NaN                      NaN   \n",
       "19         NaN         NaN                 NaN                      NaN   \n",
       "20         NaN         NaN                 NaN                      NaN   \n",
       "21         NaN         NaN                 NaN                      NaN   \n",
       "22         NaN         NaN                 NaN                      NaN   \n",
       "23         NaN         NaN                 NaN                      NaN   \n",
       "24         NaN         NaN                 NaN                      NaN   \n",
       "25         NaN         NaN                 NaN                      NaN   \n",
       "26         NaN         NaN                 NaN                      NaN   \n",
       "27         NaN         NaN                 NaN                      NaN   \n",
       "28         NaN         NaN                 NaN                      NaN   \n",
       "29         NaN         NaN                 NaN                      NaN   \n",
       "30         NaN         NaN                 NaN                      NaN   \n",
       "31         NaN         NaN                 NaN                      NaN   \n",
       "32         NaN         NaN                 NaN                      NaN   \n",
       "33         NaN         NaN                 NaN                      NaN   \n",
       "34         NaN         NaN                 NaN                      NaN   \n",
       "35         NaN         NaN                 NaN                      NaN   \n",
       "36         NaN         NaN                 NaN                      NaN   \n",
       "37         NaN         NaN                 NaN                      NaN   \n",
       "38         NaN         NaN                 NaN                      NaN   \n",
       "39         NaN         NaN                 NaN                      NaN   \n",
       "40         NaN         NaN                 NaN                      NaN   \n",
       "41         NaN         NaN                 NaN                      NaN   \n",
       "42         NaN         NaN                 NaN                      NaN   \n",
       "43         NaN         NaN                 NaN                      NaN   \n",
       "44         NaN         NaN                 NaN                      NaN   \n",
       "45         NaN         NaN                 NaN                      NaN   \n",
       "46         NaN         NaN                 NaN                      NaN   \n",
       "47         NaN         NaN                 NaN                      NaN   \n",
       "48         NaN         NaN                 NaN                      NaN   \n",
       "49         NaN         NaN                 NaN                      NaN   \n",
       "\n",
       "    software_dependencies  experiment_setup  experiment_source_code  \\\n",
       "0                     NaN               NaN                     NaN   \n",
       "1                     NaN               NaN                     NaN   \n",
       "2                     NaN               NaN                     NaN   \n",
       "3                     NaN               NaN                     NaN   \n",
       "4                     NaN               NaN                     NaN   \n",
       "5                     NaN               NaN                     NaN   \n",
       "6                     NaN               NaN                     NaN   \n",
       "7                     NaN               NaN                     NaN   \n",
       "8                     NaN               NaN                     NaN   \n",
       "9                     NaN               NaN                     NaN   \n",
       "10                    NaN               NaN                     NaN   \n",
       "11                    NaN               NaN                     NaN   \n",
       "12                    NaN               NaN                     NaN   \n",
       "13                    NaN               NaN                     NaN   \n",
       "14                    NaN               NaN                     NaN   \n",
       "15                    NaN               NaN                     NaN   \n",
       "16                    NaN               NaN                     NaN   \n",
       "17                    NaN               NaN                     NaN   \n",
       "18                    NaN               NaN                     NaN   \n",
       "19                    NaN               NaN                     NaN   \n",
       "20                    NaN               NaN                     NaN   \n",
       "21                    NaN               NaN                     NaN   \n",
       "22                    NaN               NaN                     NaN   \n",
       "23                    NaN               NaN                     NaN   \n",
       "24                    NaN               NaN                     NaN   \n",
       "25                    NaN               NaN                     NaN   \n",
       "26                    NaN               NaN                     NaN   \n",
       "27                    NaN               NaN                     NaN   \n",
       "28                    NaN               NaN                     NaN   \n",
       "29                    NaN               NaN                     NaN   \n",
       "30                    NaN               NaN                     NaN   \n",
       "31                    NaN               NaN                     NaN   \n",
       "32                    NaN               NaN                     NaN   \n",
       "33                    NaN               NaN                     NaN   \n",
       "34                    NaN               NaN                     NaN   \n",
       "35                    NaN               NaN                     NaN   \n",
       "36                    NaN               NaN                     NaN   \n",
       "37                    NaN               NaN                     NaN   \n",
       "38                    NaN               NaN                     NaN   \n",
       "39                    NaN               NaN                     NaN   \n",
       "40                    NaN               NaN                     NaN   \n",
       "41                    NaN               NaN                     NaN   \n",
       "42                    NaN               NaN                     NaN   \n",
       "43                    NaN               NaN                     NaN   \n",
       "44                    NaN               NaN                     NaN   \n",
       "45                    NaN               NaN                     NaN   \n",
       "46                    NaN               NaN                     NaN   \n",
       "47                    NaN               NaN                     NaN   \n",
       "48                    NaN               NaN                     NaN   \n",
       "49                    NaN               NaN                     NaN   \n",
       "\n",
       "    research_type  affiliation  \n",
       "0             NaN          NaN  \n",
       "1             NaN          NaN  \n",
       "2             NaN          NaN  \n",
       "3             NaN          NaN  \n",
       "4             NaN          NaN  \n",
       "5             NaN          NaN  \n",
       "6             NaN          NaN  \n",
       "7             NaN          NaN  \n",
       "8             NaN          NaN  \n",
       "9             NaN          NaN  \n",
       "10            NaN          NaN  \n",
       "11            NaN          NaN  \n",
       "12            NaN          NaN  \n",
       "13            NaN          NaN  \n",
       "14            NaN          NaN  \n",
       "15            NaN          NaN  \n",
       "16            NaN          NaN  \n",
       "17            NaN          NaN  \n",
       "18            NaN          NaN  \n",
       "19            NaN          NaN  \n",
       "20            NaN          NaN  \n",
       "21            NaN          NaN  \n",
       "22            NaN          NaN  \n",
       "23            NaN          NaN  \n",
       "24            NaN          NaN  \n",
       "25            NaN          NaN  \n",
       "26            NaN          NaN  \n",
       "27            NaN          NaN  \n",
       "28            NaN          NaN  \n",
       "29            NaN          NaN  \n",
       "30            NaN          NaN  \n",
       "31            NaN          NaN  \n",
       "32            NaN          NaN  \n",
       "33            NaN          NaN  \n",
       "34            NaN          NaN  \n",
       "35            NaN          NaN  \n",
       "36            NaN          NaN  \n",
       "37            NaN          NaN  \n",
       "38            NaN          NaN  \n",
       "39            NaN          NaN  \n",
       "40            NaN          NaN  \n",
       "41            NaN          NaN  \n",
       "42            NaN          NaN  \n",
       "43            NaN          NaN  \n",
       "44            NaN          NaN  \n",
       "45            NaN          NaN  \n",
       "46            NaN          NaN  \n",
       "47            NaN          NaN  \n",
       "48            NaN          NaN  \n",
       "49            NaN          NaN  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repro_eval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1278cd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1.]),\n",
       " array([1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_vars = set()\n",
    "for i in non_empty_found_paras:\n",
    "    for j in i:\n",
    "        found_vars.add(j[0])\n",
    "\n",
    "for i in repro_eval.columns[2:]:\n",
    "    if i in found_vars:\n",
    "        repro_eval[i][0] = 1\n",
    "    else:\n",
    "        repro_eval[i][0] = 0\n",
    "[manual_eval.iloc[0][2:].values, repro_eval.iloc[0][2:].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "529c1c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>objective</th>\n",
       "      <th>research_method</th>\n",
       "      <th>research_questions</th>\n",
       "      <th>pseudocode</th>\n",
       "      <th>training_data</th>\n",
       "      <th>validation_data</th>\n",
       "      <th>test_data</th>\n",
       "      <th>results</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>prediction</th>\n",
       "      <th>method_source_code</th>\n",
       "      <th>hardware_specifications</th>\n",
       "      <th>software_dependencies</th>\n",
       "      <th>experiment_setup</th>\n",
       "      <th>experiment_source_code</th>\n",
       "      <th>research_type</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>manual_eval</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repro_eval</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             problem  objective  research_method  research_questions  \\\n",
       "manual_eval      1.0        1.0              1.0                 0.0   \n",
       "repro_eval       1.0        1.0              1.0                 0.0   \n",
       "\n",
       "             pseudocode  training_data  validation_data  test_data  results  \\\n",
       "manual_eval         0.0            0.0              0.0        0.0      1.0   \n",
       "repro_eval          0.0            0.0              0.0        0.0      1.0   \n",
       "\n",
       "             hypothesis  prediction  method_source_code  \\\n",
       "manual_eval         0.0         0.0                 0.0   \n",
       "repro_eval          1.0         0.0                 0.0   \n",
       "\n",
       "             hardware_specifications  software_dependencies  experiment_setup  \\\n",
       "manual_eval                      0.0                    0.0               1.0   \n",
       "repro_eval                       0.0                    1.0               0.0   \n",
       "\n",
       "             experiment_source_code  research_type  affiliation  \n",
       "manual_eval                     0.0            0.0          1.0  \n",
       "repro_eval                      0.0            0.0          NaN  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([manual_eval.iloc[0][2:].values, repro_eval.iloc[0][2:].values], columns=gunderson_vars[1:], index=['manual_eval','repro_eval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "78ca1b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>objective</th>\n",
       "      <th>research_method</th>\n",
       "      <th>research_questions</th>\n",
       "      <th>pseudocode</th>\n",
       "      <th>training_data</th>\n",
       "      <th>validation_data</th>\n",
       "      <th>test_data</th>\n",
       "      <th>results</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>prediction</th>\n",
       "      <th>method_source_code</th>\n",
       "      <th>hardware_specifications</th>\n",
       "      <th>software_dependencies</th>\n",
       "      <th>experiment_setup</th>\n",
       "      <th>experiment_source_code</th>\n",
       "      <th>research_type</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1606.04671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1903.09668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1904.10554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1908.05659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1909.00931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1911.03867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2002.05905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2004.05258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2005.02607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2007.09855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2009.01947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2010.04261</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2010.04855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2011.11576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2012.09302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2101.07354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2102.11887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2104.11893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2104.12546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2105.01099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2105.01937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2105.15197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2106.01528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2106.03157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2106.03725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2106.06610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2106.06927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2106.07704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2106.10898</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2106.12177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>2106.12936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>2106.13823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>2107.01131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>2107.07110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>2108.09779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>2109.01372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>2109.12784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>2109.14855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>2110.02343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>2110.02474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>2110.03135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>2110.05169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>2110.08255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>2110.08432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>2110.09902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>2110.11688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>2110.13624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>2110.14241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>2111.02997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>2111.03289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index          id  problem  objective  research_method  \\\n",
       "0       0  1606.04671      1.0        1.0              1.0   \n",
       "1       1  1903.09668      NaN        NaN              NaN   \n",
       "2       2  1904.10554      NaN        NaN              NaN   \n",
       "3       3  1908.05659      NaN        NaN              NaN   \n",
       "4       4  1909.00931      NaN        NaN              NaN   \n",
       "5       5  1911.03867      NaN        NaN              NaN   \n",
       "6       6  2002.05905      NaN        NaN              NaN   \n",
       "7       7  2004.05258      NaN        NaN              NaN   \n",
       "8       8  2005.02607      NaN        NaN              NaN   \n",
       "9       9  2007.09855      NaN        NaN              NaN   \n",
       "10     10  2009.01947      NaN        NaN              NaN   \n",
       "11     11  2010.04261      NaN        NaN              NaN   \n",
       "12     12  2010.04855      NaN        NaN              NaN   \n",
       "13     13  2011.11576      NaN        NaN              NaN   \n",
       "14     14  2012.09302      NaN        NaN              NaN   \n",
       "15     15  2101.07354      NaN        NaN              NaN   \n",
       "16     16  2102.11887      NaN        NaN              NaN   \n",
       "17     17  2104.11893      NaN        NaN              NaN   \n",
       "18     18  2104.12546      NaN        NaN              NaN   \n",
       "19     19  2105.01099      NaN        NaN              NaN   \n",
       "20     20  2105.01937      NaN        NaN              NaN   \n",
       "21     21  2105.15197      NaN        NaN              NaN   \n",
       "22     22  2106.01528      NaN        NaN              NaN   \n",
       "23     23  2106.03157      NaN        NaN              NaN   \n",
       "24     24  2106.03725      NaN        NaN              NaN   \n",
       "25     25  2106.06610      NaN        NaN              NaN   \n",
       "26     26  2106.06927      NaN        NaN              NaN   \n",
       "27     27  2106.07704      NaN        NaN              NaN   \n",
       "28     28  2106.10898      NaN        NaN              NaN   \n",
       "29     29  2106.12177      NaN        NaN              NaN   \n",
       "30     30  2106.12936      NaN        NaN              NaN   \n",
       "31     31  2106.13823      NaN        NaN              NaN   \n",
       "32     32  2107.01131      NaN        NaN              NaN   \n",
       "33     33  2107.07110      NaN        NaN              NaN   \n",
       "34     34  2108.09779      NaN        NaN              NaN   \n",
       "35     35  2109.01372      NaN        NaN              NaN   \n",
       "36     36  2109.12784      NaN        NaN              NaN   \n",
       "37     37  2109.14855      NaN        NaN              NaN   \n",
       "38     38  2110.02343      NaN        NaN              NaN   \n",
       "39     39  2110.02474      NaN        NaN              NaN   \n",
       "40     40  2110.03135      NaN        NaN              NaN   \n",
       "41     41  2110.05169      NaN        NaN              NaN   \n",
       "42     42  2110.08255      NaN        NaN              NaN   \n",
       "43     43  2110.08432      NaN        NaN              NaN   \n",
       "44     44  2110.09902      NaN        NaN              NaN   \n",
       "45     45  2110.11688      NaN        NaN              NaN   \n",
       "46     46  2110.13624      NaN        NaN              NaN   \n",
       "47     47  2110.14241      NaN        NaN              NaN   \n",
       "48     48  2111.02997      NaN        NaN              NaN   \n",
       "49     49  2111.03289      NaN        NaN              NaN   \n",
       "\n",
       "    research_questions  pseudocode  training_data  validation_data  test_data  \\\n",
       "0                  0.0         0.0            0.0              0.0        0.0   \n",
       "1                  NaN         NaN            NaN              NaN        NaN   \n",
       "2                  NaN         NaN            NaN              NaN        NaN   \n",
       "3                  NaN         NaN            NaN              NaN        NaN   \n",
       "4                  NaN         NaN            NaN              NaN        NaN   \n",
       "5                  NaN         NaN            NaN              NaN        NaN   \n",
       "6                  NaN         NaN            NaN              NaN        NaN   \n",
       "7                  NaN         NaN            NaN              NaN        NaN   \n",
       "8                  NaN         NaN            NaN              NaN        NaN   \n",
       "9                  NaN         NaN            NaN              NaN        NaN   \n",
       "10                 NaN         NaN            NaN              NaN        NaN   \n",
       "11                 NaN         NaN            NaN              NaN        NaN   \n",
       "12                 NaN         NaN            NaN              NaN        NaN   \n",
       "13                 NaN         NaN            NaN              NaN        NaN   \n",
       "14                 NaN         NaN            NaN              NaN        NaN   \n",
       "15                 NaN         NaN            NaN              NaN        NaN   \n",
       "16                 NaN         NaN            NaN              NaN        NaN   \n",
       "17                 NaN         NaN            NaN              NaN        NaN   \n",
       "18                 NaN         NaN            NaN              NaN        NaN   \n",
       "19                 NaN         NaN            NaN              NaN        NaN   \n",
       "20                 NaN         NaN            NaN              NaN        NaN   \n",
       "21                 NaN         NaN            NaN              NaN        NaN   \n",
       "22                 NaN         NaN            NaN              NaN        NaN   \n",
       "23                 NaN         NaN            NaN              NaN        NaN   \n",
       "24                 NaN         NaN            NaN              NaN        NaN   \n",
       "25                 NaN         NaN            NaN              NaN        NaN   \n",
       "26                 NaN         NaN            NaN              NaN        NaN   \n",
       "27                 NaN         NaN            NaN              NaN        NaN   \n",
       "28                 NaN         NaN            NaN              NaN        NaN   \n",
       "29                 NaN         NaN            NaN              NaN        NaN   \n",
       "30                 NaN         NaN            NaN              NaN        NaN   \n",
       "31                 NaN         NaN            NaN              NaN        NaN   \n",
       "32                 NaN         NaN            NaN              NaN        NaN   \n",
       "33                 NaN         NaN            NaN              NaN        NaN   \n",
       "34                 NaN         NaN            NaN              NaN        NaN   \n",
       "35                 NaN         NaN            NaN              NaN        NaN   \n",
       "36                 NaN         NaN            NaN              NaN        NaN   \n",
       "37                 NaN         NaN            NaN              NaN        NaN   \n",
       "38                 NaN         NaN            NaN              NaN        NaN   \n",
       "39                 NaN         NaN            NaN              NaN        NaN   \n",
       "40                 NaN         NaN            NaN              NaN        NaN   \n",
       "41                 NaN         NaN            NaN              NaN        NaN   \n",
       "42                 NaN         NaN            NaN              NaN        NaN   \n",
       "43                 NaN         NaN            NaN              NaN        NaN   \n",
       "44                 NaN         NaN            NaN              NaN        NaN   \n",
       "45                 NaN         NaN            NaN              NaN        NaN   \n",
       "46                 NaN         NaN            NaN              NaN        NaN   \n",
       "47                 NaN         NaN            NaN              NaN        NaN   \n",
       "48                 NaN         NaN            NaN              NaN        NaN   \n",
       "49                 NaN         NaN            NaN              NaN        NaN   \n",
       "\n",
       "    results  hypothesis  prediction  method_source_code  \\\n",
       "0       1.0         0.0         0.0                 0.0   \n",
       "1       NaN         NaN         NaN                 NaN   \n",
       "2       NaN         NaN         NaN                 NaN   \n",
       "3       NaN         NaN         NaN                 NaN   \n",
       "4       NaN         NaN         NaN                 NaN   \n",
       "5       NaN         NaN         NaN                 NaN   \n",
       "6       NaN         NaN         NaN                 NaN   \n",
       "7       NaN         NaN         NaN                 NaN   \n",
       "8       NaN         NaN         NaN                 NaN   \n",
       "9       NaN         NaN         NaN                 NaN   \n",
       "10      NaN         NaN         NaN                 NaN   \n",
       "11      NaN         NaN         NaN                 NaN   \n",
       "12      NaN         NaN         NaN                 NaN   \n",
       "13      NaN         NaN         NaN                 NaN   \n",
       "14      NaN         NaN         NaN                 NaN   \n",
       "15      NaN         NaN         NaN                 NaN   \n",
       "16      NaN         NaN         NaN                 NaN   \n",
       "17      NaN         NaN         NaN                 NaN   \n",
       "18      NaN         NaN         NaN                 NaN   \n",
       "19      NaN         NaN         NaN                 NaN   \n",
       "20      NaN         NaN         NaN                 NaN   \n",
       "21      NaN         NaN         NaN                 NaN   \n",
       "22      NaN         NaN         NaN                 NaN   \n",
       "23      NaN         NaN         NaN                 NaN   \n",
       "24      NaN         NaN         NaN                 NaN   \n",
       "25      NaN         NaN         NaN                 NaN   \n",
       "26      NaN         NaN         NaN                 NaN   \n",
       "27      NaN         NaN         NaN                 NaN   \n",
       "28      NaN         NaN         NaN                 NaN   \n",
       "29      NaN         NaN         NaN                 NaN   \n",
       "30      NaN         NaN         NaN                 NaN   \n",
       "31      NaN         NaN         NaN                 NaN   \n",
       "32      NaN         NaN         NaN                 NaN   \n",
       "33      NaN         NaN         NaN                 NaN   \n",
       "34      NaN         NaN         NaN                 NaN   \n",
       "35      NaN         NaN         NaN                 NaN   \n",
       "36      NaN         NaN         NaN                 NaN   \n",
       "37      NaN         NaN         NaN                 NaN   \n",
       "38      NaN         NaN         NaN                 NaN   \n",
       "39      NaN         NaN         NaN                 NaN   \n",
       "40      NaN         NaN         NaN                 NaN   \n",
       "41      NaN         NaN         NaN                 NaN   \n",
       "42      NaN         NaN         NaN                 NaN   \n",
       "43      NaN         NaN         NaN                 NaN   \n",
       "44      NaN         NaN         NaN                 NaN   \n",
       "45      NaN         NaN         NaN                 NaN   \n",
       "46      NaN         NaN         NaN                 NaN   \n",
       "47      NaN         NaN         NaN                 NaN   \n",
       "48      NaN         NaN         NaN                 NaN   \n",
       "49      NaN         NaN         NaN                 NaN   \n",
       "\n",
       "    hardware_specifications  software_dependencies  experiment_setup  \\\n",
       "0                       0.0                    0.0               1.0   \n",
       "1                       NaN                    NaN               NaN   \n",
       "2                       NaN                    NaN               NaN   \n",
       "3                       NaN                    NaN               NaN   \n",
       "4                       NaN                    NaN               NaN   \n",
       "5                       NaN                    NaN               NaN   \n",
       "6                       NaN                    NaN               NaN   \n",
       "7                       NaN                    NaN               NaN   \n",
       "8                       NaN                    NaN               NaN   \n",
       "9                       NaN                    NaN               NaN   \n",
       "10                      NaN                    NaN               NaN   \n",
       "11                      NaN                    NaN               NaN   \n",
       "12                      NaN                    NaN               NaN   \n",
       "13                      NaN                    NaN               NaN   \n",
       "14                      NaN                    NaN               NaN   \n",
       "15                      NaN                    NaN               NaN   \n",
       "16                      NaN                    NaN               NaN   \n",
       "17                      NaN                    NaN               NaN   \n",
       "18                      NaN                    NaN               NaN   \n",
       "19                      NaN                    NaN               NaN   \n",
       "20                      NaN                    NaN               NaN   \n",
       "21                      NaN                    NaN               NaN   \n",
       "22                      NaN                    NaN               NaN   \n",
       "23                      NaN                    NaN               NaN   \n",
       "24                      NaN                    NaN               NaN   \n",
       "25                      NaN                    NaN               NaN   \n",
       "26                      NaN                    NaN               NaN   \n",
       "27                      NaN                    NaN               NaN   \n",
       "28                      NaN                    NaN               NaN   \n",
       "29                      NaN                    NaN               NaN   \n",
       "30                      NaN                    NaN               NaN   \n",
       "31                      NaN                    NaN               NaN   \n",
       "32                      NaN                    NaN               NaN   \n",
       "33                      NaN                    NaN               NaN   \n",
       "34                      NaN                    NaN               NaN   \n",
       "35                      NaN                    NaN               NaN   \n",
       "36                      NaN                    NaN               NaN   \n",
       "37                      NaN                    NaN               NaN   \n",
       "38                      NaN                    NaN               NaN   \n",
       "39                      NaN                    NaN               NaN   \n",
       "40                      NaN                    NaN               NaN   \n",
       "41                      NaN                    NaN               NaN   \n",
       "42                      NaN                    NaN               NaN   \n",
       "43                      NaN                    NaN               NaN   \n",
       "44                      NaN                    NaN               NaN   \n",
       "45                      NaN                    NaN               NaN   \n",
       "46                      NaN                    NaN               NaN   \n",
       "47                      NaN                    NaN               NaN   \n",
       "48                      NaN                    NaN               NaN   \n",
       "49                      NaN                    NaN               NaN   \n",
       "\n",
       "    experiment_source_code  research_type  affiliation  \n",
       "0                      0.0            0.0          1.0  \n",
       "1                      NaN            NaN          NaN  \n",
       "2                      NaN            NaN          NaN  \n",
       "3                      NaN            NaN          NaN  \n",
       "4                      NaN            NaN          NaN  \n",
       "5                      NaN            NaN          NaN  \n",
       "6                      NaN            NaN          NaN  \n",
       "7                      NaN            NaN          NaN  \n",
       "8                      NaN            NaN          NaN  \n",
       "9                      NaN            NaN          NaN  \n",
       "10                     NaN            NaN          NaN  \n",
       "11                     NaN            NaN          NaN  \n",
       "12                     NaN            NaN          NaN  \n",
       "13                     NaN            NaN          NaN  \n",
       "14                     NaN            NaN          NaN  \n",
       "15                     NaN            NaN          NaN  \n",
       "16                     NaN            NaN          NaN  \n",
       "17                     NaN            NaN          NaN  \n",
       "18                     NaN            NaN          NaN  \n",
       "19                     NaN            NaN          NaN  \n",
       "20                     NaN            NaN          NaN  \n",
       "21                     NaN            NaN          NaN  \n",
       "22                     NaN            NaN          NaN  \n",
       "23                     NaN            NaN          NaN  \n",
       "24                     NaN            NaN          NaN  \n",
       "25                     NaN            NaN          NaN  \n",
       "26                     NaN            NaN          NaN  \n",
       "27                     NaN            NaN          NaN  \n",
       "28                     NaN            NaN          NaN  \n",
       "29                     NaN            NaN          NaN  \n",
       "30                     NaN            NaN          NaN  \n",
       "31                     NaN            NaN          NaN  \n",
       "32                     NaN            NaN          NaN  \n",
       "33                     NaN            NaN          NaN  \n",
       "34                     NaN            NaN          NaN  \n",
       "35                     NaN            NaN          NaN  \n",
       "36                     NaN            NaN          NaN  \n",
       "37                     NaN            NaN          NaN  \n",
       "38                     NaN            NaN          NaN  \n",
       "39                     NaN            NaN          NaN  \n",
       "40                     NaN            NaN          NaN  \n",
       "41                     NaN            NaN          NaN  \n",
       "42                     NaN            NaN          NaN  \n",
       "43                     NaN            NaN          NaN  \n",
       "44                     NaN            NaN          NaN  \n",
       "45                     NaN            NaN          NaN  \n",
       "46                     NaN            NaN          NaN  \n",
       "47                     NaN            NaN          NaN  \n",
       "48                     NaN            NaN          NaN  \n",
       "49                     NaN            NaN          NaN  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_eval.drop(['index'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('repro-screener')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "89308f92c98ad59917276c7071a7ee158bd27f5f51fe69fe9e9f685df9484ae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
