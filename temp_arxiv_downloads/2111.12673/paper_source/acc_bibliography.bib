@inproceedings{td3,
  Author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  booktitle={International Conference on Machine Learning},
  pages={1582--1591},
  year={2018}
}

@InProceedings{Thrun+Schwartz:1993,
  author =       "Thrun, Sebastian and Schwartz, Anton",
  title =        "Issues in Using Function Approximation for Reinforcement Learning",
  booktitle =    "Proceedings of the 1993 Connectionist Models Summer School",
  year =         "1993",
  pages =     "255--263"
}



@book{introdrl2018,
  author    = {Richard S. Sutton and
               Andrew G. Barto},
  title     = {Reinforcement learning: An introduction},
  publisher = {{MIT} Press},
  year      = {2018},
  isbn      = {78-0262039246},
}



@article{gymopenai,
  title={OpenAI Gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@inproceedings{mujoco,
  added-at = {2018-11-14T00:00:00.000+0100},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle = {IROS},
  pages = {5026-5033},
  publisher = {IEEE},
  title = {MuJoCo: A physics engine for model-based control.},
  year = 2012
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}


@article{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, C.J.C.H.},
  journal={PhD thesis, Cambridge University},
  year={1989}
}

@Article{suttontd88,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44"
}

@Book{bellman57,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
  address =   "Princeton, NJ, USA",
  edition =   "1"
}


@InProceedings{silverdpg14,
  title = 	 {Deterministic Policy Gradient Algorithms},
  author = 	 {David Silver and Guy Lever and Nicolas Heess and Thomas Degris and Daan Wierstra and Martin Riedmiller},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {387--395},
  year = 	 {2014}
}

@article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}


@article{dqn15,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2613--2621},
  year={2010}
}

@inproceedings{hasselt2016deepdouble,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@inproceedings{tqc,
  title={Controlling overestimation bias with truncated mixture of continuous distributional quantile critics},
  author={Kuznetsov, Arsenii and Shvechikov, Pavel and Grishin, Alexander and Vetrov, Dmitry},
  booktitle={International Conference on Machine Learning},
  pages={5556--5566},
  year={2020},
  organization={PMLR}
}





% experiments



@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{yu2020meta,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}


@InProceedings{SAC,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author = 	 {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018}

}

@article{SACalgapp,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Kristian Hartikainen and
               George Tucker and
               Sehoon Ha and
               Jie Tan and
               Vikash Kumar and
               Henry Zhu and
               Abhishek Gupta and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic Algorithms and Applications},
  journal   = {CoRR},
  volume    = {abs/1812.05905},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.05905},
  archivePrefix = {arXiv},
  eprint    = {1812.05905},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-05905},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%intro
@article{silver2016go,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@misc{openaidota,
  title = {OpenAI Five Description},
  howpublished = {\url{https://openai.com/blog/how-to-train-your-openai-five/}},
  note = {Accessed: 2019-05-19}
}

@misc{starcraftdm,
  title = {AlphaStar: Mastering the Real-Time Strategy Game StarCraft II},
  howpublished = {\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  note = {Accessed: 2019-05-19}
}

@article{hasseltdeadlytriad,
  author    = {Hado van Hasselt and
               Yotam Doron and
               Florian Strub and
               Matteo Hessel and
               Nicolas Sonnerat and
               Joseph Modayil},
  title     = {Deep Reinforcement Learning and the Deadly Triad},
  journal   = {CoRR},
  volume    = {abs/1812.02648},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1812.02648}
}



@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}



% distributional
@inproceedings{bellemare2017distributional,
  title={A Distributional Perspective on Reinforcement Learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={449--458},
  year={2017}
}

@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc and Munos, R{\'e}mi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}




% related work

% comnbine on- and off-policy RL


@article{bhatt2019crossnorm,
  title={Crossnorm: Normalization for off-policy td reinforcement learning},
  author={Bhatt, Aditya and Argus, Max and Amiranashvili, Artemij and Brox, Thomas},
  journal={arXiv preprint arXiv:1902.05605},
  year={2019}
}

@inproceedings{hausknecht2016policy,
  title={On-policy vs. off-policy updates for deep reinforcement learning},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={Deep Reinforcement Learning: Frontiers and Challenges, IJCAI 2016 Workshop},
  year={2016}
}


 @InProceedings{fakoor20a, title = {P3O: Policy-on Policy-off Policy Optimization}, author = {Fakoor, Rasool and Chaudhari, Pratik and Smola, Alexander J.}, booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference}, pages = {1017--1027}, year = {2020}, volume = {115}, series = {Proceedings of Machine Learning Research},  publisher = {PMLR} } 
 
 
@inproceedings{NIPS2017_IPG,
 author = {Gu, Shixiang (Shane) and Lillicrap, Timothy and Turner, Richard E and Ghahramani, Zoubin and Sch\"{o}lkopf, Bernhard and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3846--3855},
 title = {Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning},
 volume = {30},
 year = {2017}
}

@inproceedings{NIPS2010_35cf8659,
 author = {Jie, Tang and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {1000--1008},
 title = {On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient},
 volume = {23},
 year = {2010}
}

@inproceedings{degris2012off,
  title={Off-Policy Actor-Critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard},
  booktitle={International Conference on Machine Learning},
  year={2012}
}

@inproceedings{o2016combining,
  title={Combining policy gradient and q-learning},
  author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{Wang2017SampleEA,
  title={Sample Efficient Actor-Critic with Experience Replay},
  author={Ziyu Wang and V. Bapst and N. Heess and V. Mnih and R. Munos and K. Kavukcuoglu and N. D. Freitas},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{NIPS2014_be53ee61,
 author = {Mahmood, A. Rupam and van Hasselt, Hado P and Sutton, Richard S},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3014--3022},
 title = {Weighted importance sampling for off-policy learning with linear function approximation},
 volume = {27},
 year = {2014}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@inproceedings{NIPS2016_c3992e9a,
 author = {Munos, Remi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1054--1062},
 title = {Safe and Efficient Off-Policy Reinforcement Learning},
 volume = {29},
 year = {2016}
}



% overestimation

@article{cetin2021learning,
  title={Learning pessimism for robust and efficient off-policy reinforcement learning},
  author={Cetin, Edoardo and Celiktutan, Oya},
  journal={arXiv preprint arXiv:2110.03375},
  year={2021}
}

@inproceedings{NIPS17-ishand,
title={TD Learning with Constrained Gradients},
author={Ishan Durugkar and Peter Stone},
booktitle={Proceedings of the Deep Reinforcement Learning Symposium, NIPS 2017},
month={December},
address={Long Beach, CA, USA},
url="http://www.cs.utexas.edu/users/ai-lab?NIPS17-ishand",
year={2017}
}


@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}

@inproceedings{kumarStabilizing19,
 author = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {11784--11794},
 title = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
 volume = {32},
 year = {2019}
}

@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}





@inproceedings{
Lan2020Maxmin,
title={Maxmin Q-learning: Controlling the Estimation Bias of Q-learning},
author={Qingfeng Lan and Yangchen Pan and Alona Fyshe and Martha White},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Bkg0u3Etwr}
}


@ARTICLE{lvSDDQ19,  author={P. {Lv} and X. {Wang} and Y. {Cheng} and Z. {Duan}},  journal={IEEE Access},   title={Stochastic Double Deep Q-Network},   year={2019},  volume={7},  number={},  pages={79446-79454},  doi={10.1109/ACCESS.2019.2922706}}




@InProceedings{avgDQN17,
  title = 	 {Averaged-{DQN}: Variance Reduction and Stabilization for Deep Reinforcement Learning},
  author = 	 {Oron Anschel and Nir Baram and Nahum Shimkin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {176--185},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR}
}

@inproceedings{weightedQlearning,
 author = {Zhang, Zongzhang and Pan, Zhiyuan and Kochenderfer, Mykel J.},
 title = {Weighted Double Q-learning},
 booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
 series = {IJCAI'17},
 year = {2017},
 isbn = {978-0-9992411-0-3},
 location = {Melbourne, Australia},
 pages = {3455--3461},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3172077.3172372},
 acmid = {3172372},
 publisher = {AAAI Press},
} 

@inproceedings{lee2013bias,
  title={Bias-corrected q-learning to control max-operator bias in q-learning},
  author={Lee, Donghun and Defourny, Boris and Powell, Warren B},
  booktitle={2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={93--99},
  year={2013},
  organization={IEEE}
}

@article{achiamdivergenceqlearning,
  author    = {Joshua Achiam and
               Ethan Knight and
               Pieter Abbeel},
  title     = {Towards Characterizing Divergence in Deep Q-Learning},
  journal   = {CoRR},
  volume    = {abs/1903.08894},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.08894},
  archivePrefix = {arXiv}
}

@article{fuBottlenecksqlearning,
  author    = {Justin Fu and
               Aviral Kumar and
               Matthew Soh and
               Sergey Levine},
  title     = {Diagnosing Bottlenecks in Deep Q-learning Algorithms},
  journal   = {CoRR},
  volume    = {abs/1902.10250},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10250},
  archivePrefix = {arXiv}
}

@article{shao2020grac,
  title={GRAC: Self-Guided and Self-Regularized Actor-Critic},
  author={Shao, Lin and You, Yifan and Yan, Mengyuan and Sun, Qingyun and Bohg, Jeannette},
  journal={arXiv preprint arXiv:2009.08973},
  year={2020}
}





% hyperparameter optimization for RL

@inproceedings{xu2018meta,
  title={Meta-Gradient Reinforcement Learning},
  author={Xu, Zhongwen and van Hasselt, Hado P and Silver, David},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{zhang2021importance,
  title={On the importance of hyperparameter optimization for model-based reinforcement learning},
  author={Zhang, Baohe and Rajan, Raghu and Pineda, Luis and Lambert, Nathan and Biedenkapp, Andr{\'e} and Chua, Kurtland and Hutter, Frank and Calandra, Roberto},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4015--4023},
  year={2021},
  organization={PMLR}
}


@InProceedings{falkner18a,
  title = 	 {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},
  author =       {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1437--1446},
  year = 	 {2018}
}

@ARTICLE{chiang19,  
author={Chiang, Hao-Tien Lewis and Faust, Aleksandra and Fiser, Marek and Francis, Anthony},  
journal={IEEE Robotics and Automation Letters},   
title={Learning Navigation Behaviors End-to-End With AutoRL},   
year={2019},  volume={4},  number={2},  pages={2007-2014},  doi={10.1109/LRA.2019.2899918}}

@article{jaderberg2017population,
  title={Population based training of neural networks},
  author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1711.09846},
  year={2017}
}


% ral rebuttal
@article{cini2020deep,
  title={Deep reinforcement learning with weighted Q-Learning},
  author={Cini, Andrea and D'Eramo, Carlo and Peters, Jan and Alippi, Cesare},
  journal={arXiv preprint arXiv:2003.09280},
  year={2020}
}

@inproceedings{d2017estimating,
  title={Estimating the maximum expected value in continuous reinforcement learning problems},
  author={D'Eramo, Carlo and Nuara, Alessandro and Pirotta, Matteo and Restelli, Marcello},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

