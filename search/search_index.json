{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to reproscreener's documentation!","text":"<p><code>reproscreener</code> aims to address challenges in robustness, transparency, and interpretability of ML models by automating verification of machine learning models at scale <sup>1</sup>.</p> <p>Note</p> <p>This project is under active development.</p> <ol> <li> <p>Bhaskar, A. and Stodden, V. 2024. Reproscreener: Leveraging LLMs for Assessing Computational Reproducibility of Machine Learning Pipelines. Proceedings of the 2nd ACM Conference on Reproducibility and Replicability (New York, NY, USA, Jul. 2024), 101--109.\u00a0\u21a9</p> </li> </ol>"},{"location":"02_manual_eval_explorer/","title":"02 manual eval explorer","text":"In\u00a0[\u00a0]: Copied! <pre>import marimo\n</pre> import marimo In\u00a0[\u00a0]: Copied! <pre>__generated_with = \"0.15.2\"\napp = marimo.App(width=\"full\")\n</pre> __generated_with = \"0.15.2\" app = marimo.App(width=\"full\") In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _():\n    import marimo as mo\n    return (mo,)\n</pre> @app.cell def _():     import marimo as mo     return (mo,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"# Manual Evaluation Explorer\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"# Manual Evaluation Explorer\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _():\n    import pandas as pd\n    import numpy as np\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import ListedColormap\n    from textwrap import wrap\n\n    all_columns = [\n      \"problem\",\n      \"problem_agreement\",\n      \"objective\",\n      \"objective_agreement\",\n      \"research_method\",\n      \"research_method_agreement\",\n      \"research_questions\",\n      \"research_questions_agreement\",\n      \"pseudocode\",\n      \"pseudocode_agreement\",\n      \"dataset\",\n      \"dataset_agreement\",\n      \"hypothesis\",\n      \"hypothesis_agreement\",\n      \"prediction\",\n      \"prediction_agreement\",\n      \"code_available\",\n      \"code_available_agreement\",\n      \"software_dependencies\",\n      \"software_dependencies_agreement\",\n      \"experiment_setup\",\n      \"experiment_setup_agreement\"\n    ]\n    metric_columns = [col for col in all_columns if not col.endswith(\"_agreement\")]\n    return ListedColormap, metric_columns, np, pd, plt, sns, wrap\n</pre> @app.cell(hide_code=True) def _():     import pandas as pd     import numpy as np     import seaborn as sns     import matplotlib.pyplot as plt     from matplotlib.colors import ListedColormap     from textwrap import wrap      all_columns = [       \"problem\",       \"problem_agreement\",       \"objective\",       \"objective_agreement\",       \"research_method\",       \"research_method_agreement\",       \"research_questions\",       \"research_questions_agreement\",       \"pseudocode\",       \"pseudocode_agreement\",       \"dataset\",       \"dataset_agreement\",       \"hypothesis\",       \"hypothesis_agreement\",       \"prediction\",       \"prediction_agreement\",       \"code_available\",       \"code_available_agreement\",       \"software_dependencies\",       \"software_dependencies_agreement\",       \"experiment_setup\",       \"experiment_setup_agreement\"     ]     metric_columns = [col for col in all_columns if not col.endswith(\"_agreement\")]     return ListedColormap, metric_columns, np, pd, plt, sns, wrap In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(pd):\n    df_manuscript_manual = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/manuscript.csv\")\n    df_manuscript_manual.set_index(\"paper_id\", inplace=True)\n    df_manuscript_manual = df_manuscript_manual.drop(\n        columns=[\"evaluation_type\", \"source_file\"]\n        + [col for col in df_manuscript_manual.columns if \"_description\" in col]\n    )\n    df_manuscript_manual = df_manuscript_manual.rename(columns={\"code_available_in_article\": \"code_available\"})\n    df_manuscript_manual\n    return (df_manuscript_manual,)\n</pre> @app.cell def _(pd):     df_manuscript_manual = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/manuscript.csv\")     df_manuscript_manual.set_index(\"paper_id\", inplace=True)     df_manuscript_manual = df_manuscript_manual.drop(         columns=[\"evaluation_type\", \"source_file\"]         + [col for col in df_manuscript_manual.columns if \"_description\" in col]     )     df_manuscript_manual = df_manuscript_manual.rename(columns={\"code_available_in_article\": \"code_available\"})     df_manuscript_manual     return (df_manuscript_manual,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"## Reproscreener (regex) vs. Manual evaluations of full manuscripts\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"## Reproscreener (regex) vs. Manual evaluations of full manuscripts\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(metric_columns, pd):\n    df_manuscript_regex = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\")\n    df_manuscript_regex = df_manuscript_regex.rename(columns={\"method_source_code\": \"code_available\", \"id\": \"paper_id\"})\n    df_manuscript_regex.set_index(\"paper_id\", inplace=True)\n    df_manuscript_regex['dataset'] = df_manuscript_regex['training_data'].astype(bool) + df_manuscript_regex['test_data'].astype(bool) + df_manuscript_regex['validation_data'].astype(bool) + df_manuscript_regex['training_data'].astype(bool)\n    df_manuscript_regex = df_manuscript_regex.drop(columns=[\"index\",\"training_data\", \"test_data\", \"validation_data\", \"title\"])\n    df_manuscript_regex[metric_columns] = df_manuscript_regex[metric_columns].astype(bool)\n    return (df_manuscript_regex,)\n</pre> @app.cell(hide_code=True) def _(metric_columns, pd):     df_manuscript_regex = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\")     df_manuscript_regex = df_manuscript_regex.rename(columns={\"method_source_code\": \"code_available\", \"id\": \"paper_id\"})     df_manuscript_regex.set_index(\"paper_id\", inplace=True)     df_manuscript_regex['dataset'] = df_manuscript_regex['training_data'].astype(bool) + df_manuscript_regex['test_data'].astype(bool) + df_manuscript_regex['validation_data'].astype(bool) + df_manuscript_regex['training_data'].astype(bool)     df_manuscript_regex = df_manuscript_regex.drop(columns=[\"index\",\"training_data\", \"test_data\", \"validation_data\", \"title\"])     df_manuscript_regex[metric_columns] = df_manuscript_regex[metric_columns].astype(bool)     return (df_manuscript_regex,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(df_manuscript_manual, df_manuscript_regex, metric_columns, mo, pd):\n    # Align indices across manual and regex dataframes and compute agreement\n    common_idx = df_manuscript_manual.index.intersection(df_manuscript_regex.index)\n    manual_bool = df_manuscript_manual.loc[common_idx, metric_columns].astype(bool)\n    regex_bool = df_manuscript_regex.loc[common_idx, metric_columns].astype(bool)\n\n    results_manuscript = {}\n    for manuscript_metric in metric_columns:\n        manuscript_regex_vals = regex_bool[manuscript_metric]\n        manuscript_manual_vals = manual_bool[manuscript_metric]\n\n        results_manuscript[manuscript_metric] = {\n            'regex_sum': int(manuscript_regex_vals.sum()),\n            'manual_sum': int(manuscript_manual_vals.sum()),\n            'regex_proportion': float(manuscript_regex_vals.mean()),\n            'regex_manual_agreement': float((manuscript_regex_vals == manuscript_manual_vals).mean()),\n            'manual_proportion': float(manuscript_manual_vals.mean()),\n            'total_n': int(len(manuscript_regex_vals))\n        }\n\n    manuscript_results_df = pd.DataFrame(results_manuscript).T\n\n    tab_decimal_manu = manuscript_results_df\n    tab_percent_manu = manuscript_results_df.copy()\n    tab_percent_manu['regex_proportion'] = tab_percent_manu['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent_manu['manual_proportion'] = tab_percent_manu['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent_manu['regex_manual_agreement'] = tab_percent_manu['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\n\n    tabs_manu = mo.ui.tabs({\"percent\": tab_percent_manu, \"decimal\": tab_decimal_manu})\n    tabs_manu\n    return\n</pre> @app.cell(hide_code=True) def _(df_manuscript_manual, df_manuscript_regex, metric_columns, mo, pd):     # Align indices across manual and regex dataframes and compute agreement     common_idx = df_manuscript_manual.index.intersection(df_manuscript_regex.index)     manual_bool = df_manuscript_manual.loc[common_idx, metric_columns].astype(bool)     regex_bool = df_manuscript_regex.loc[common_idx, metric_columns].astype(bool)      results_manuscript = {}     for manuscript_metric in metric_columns:         manuscript_regex_vals = regex_bool[manuscript_metric]         manuscript_manual_vals = manual_bool[manuscript_metric]          results_manuscript[manuscript_metric] = {             'regex_sum': int(manuscript_regex_vals.sum()),             'manual_sum': int(manuscript_manual_vals.sum()),             'regex_proportion': float(manuscript_regex_vals.mean()),             'regex_manual_agreement': float((manuscript_regex_vals == manuscript_manual_vals).mean()),             'manual_proportion': float(manuscript_manual_vals.mean()),             'total_n': int(len(manuscript_regex_vals))         }      manuscript_results_df = pd.DataFrame(results_manuscript).T      tab_decimal_manu = manuscript_results_df     tab_percent_manu = manuscript_results_df.copy()     tab_percent_manu['regex_proportion'] = tab_percent_manu['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent_manu['manual_proportion'] = tab_percent_manu['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent_manu['regex_manual_agreement'] = tab_percent_manu['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')      tabs_manu = mo.ui.tabs({\"percent\": tab_percent_manu, \"decimal\": tab_decimal_manu})     tabs_manu     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"## Reproscreener (regex) vs. Manual evaluations of abstracts\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"## Reproscreener (regex) vs. Manual evaluations of abstracts\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(metric_columns, pd):\n    # Load regex results for abstracts computed locally\n    df_abstract_regex = pd.read_csv(\"reports/tables/abstract_regex_gs.csv\")\n    df_abstract_regex = df_abstract_regex.set_index(\"paper_id\")\n    # Ensure boolean dtype for metrics\n    available_cols = [c for c in metric_columns if c in df_abstract_regex.columns]\n    df_abstract_regex[available_cols] = df_abstract_regex[available_cols].astype(bool)\n    df_abstract_regex\n    return (df_abstract_regex,)\n</pre> @app.cell(hide_code=True) def _(metric_columns, pd):     # Load regex results for abstracts computed locally     df_abstract_regex = pd.read_csv(\"reports/tables/abstract_regex_gs.csv\")     df_abstract_regex = df_abstract_regex.set_index(\"paper_id\")     # Ensure boolean dtype for metrics     available_cols = [c for c in metric_columns if c in df_abstract_regex.columns]     df_abstract_regex[available_cols] = df_abstract_regex[available_cols].astype(bool)     df_abstract_regex     return (df_abstract_regex,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(metric_columns, np, pd):\n    # Load GPT agreement for abstracts and derive manual columns from agreement\n    df_abs_gpt_agreement = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")\n\n    # Clean up columns - exclude metadata and description columns\n    df_abs_gpt_agreement = df_abs_gpt_agreement.set_index(\"paper_id\")\n    df_abs_gpt_agreement = df_abs_gpt_agreement.drop(\n        columns=[\"evaluation_type\", \"source_file\"]\n        + [col for col in df_abs_gpt_agreement.columns if \"_description\" in col]\n    )\n\n    # Remove gpt_ prefix from column names\n    df_abs_gpt_agreement = df_abs_gpt_agreement.rename(\n        columns={\n            col: col.replace(\"gpt_\", \"\")\n            for col in df_abs_gpt_agreement.columns\n            if col.startswith(\"gpt_\")\n        }\n    )\n\n    # Compute manual_&lt;metric&gt; using agreement flip rule\n    for abs_metric in metric_columns:\n        abs_agreement_col = f\"{abs_metric}_agreement\"\n        if abs_metric in df_abs_gpt_agreement.columns and abs_agreement_col in df_abs_gpt_agreement.columns:\n            abs_gpt_vals = df_abs_gpt_agreement[abs_metric].astype(bool)\n            abs_agreement_vals = df_abs_gpt_agreement[abs_agreement_col]\n            abs_manual_vals = np.where(abs_agreement_vals == 1, abs_gpt_vals, ~abs_gpt_vals)\n            df_abs_gpt_agreement[f\"manual_{abs_metric}\"] = abs_manual_vals.astype(bool)\n    return (df_abs_gpt_agreement,)\n</pre> @app.cell(hide_code=True) def _(metric_columns, np, pd):     # Load GPT agreement for abstracts and derive manual columns from agreement     df_abs_gpt_agreement = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")      # Clean up columns - exclude metadata and description columns     df_abs_gpt_agreement = df_abs_gpt_agreement.set_index(\"paper_id\")     df_abs_gpt_agreement = df_abs_gpt_agreement.drop(         columns=[\"evaluation_type\", \"source_file\"]         + [col for col in df_abs_gpt_agreement.columns if \"_description\" in col]     )      # Remove gpt_ prefix from column names     df_abs_gpt_agreement = df_abs_gpt_agreement.rename(         columns={             col: col.replace(\"gpt_\", \"\")             for col in df_abs_gpt_agreement.columns             if col.startswith(\"gpt_\")         }     )      # Compute manual_ using agreement flip rule     for abs_metric in metric_columns:         abs_agreement_col = f\"{abs_metric}_agreement\"         if abs_metric in df_abs_gpt_agreement.columns and abs_agreement_col in df_abs_gpt_agreement.columns:             abs_gpt_vals = df_abs_gpt_agreement[abs_metric].astype(bool)             abs_agreement_vals = df_abs_gpt_agreement[abs_agreement_col]             abs_manual_vals = np.where(abs_agreement_vals == 1, abs_gpt_vals, ~abs_gpt_vals)             df_abs_gpt_agreement[f\"manual_{abs_metric}\"] = abs_manual_vals.astype(bool)     return (df_abs_gpt_agreement,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(df_abs_gpt_agreement, metric_columns):\n    # Build manual abstract evaluations from df_abs_gpt_agreement manual_ columns\n    abs_manual_cols_map = {\n        f\"manual_{m}\": m for m in metric_columns if f\"manual_{m}\" in df_abs_gpt_agreement.columns\n    }\n    df_abstract_manual = df_abs_gpt_agreement[list(abs_manual_cols_map.keys())].rename(columns=abs_manual_cols_map)\n    # Ensure boolean dtype\n    df_abstract_manual = df_abstract_manual.astype(bool)\n    return (df_abstract_manual,)\n</pre> @app.cell(hide_code=True) def _(df_abs_gpt_agreement, metric_columns):     # Build manual abstract evaluations from df_abs_gpt_agreement manual_ columns     abs_manual_cols_map = {         f\"manual_{m}\": m for m in metric_columns if f\"manual_{m}\" in df_abs_gpt_agreement.columns     }     df_abstract_manual = df_abs_gpt_agreement[list(abs_manual_cols_map.keys())].rename(columns=abs_manual_cols_map)     # Ensure boolean dtype     df_abstract_manual = df_abstract_manual.astype(bool)     return (df_abstract_manual,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(df_abstract_manual, df_abstract_regex, metric_columns, mo, pd):\n    # Align indices and compute agreement for abstracts\n    common_idx_abs = df_abstract_manual.index.intersection(df_abstract_regex.index)\n    manual_bool_abs = df_abstract_manual.loc[common_idx_abs]\n    regex_bool_abs = df_abstract_regex.loc[common_idx_abs]\n\n    # abstract_metrics = [m for m in metric_columns if m in manual_bool_abs.columns and m in regex_bool_abs.columns]\n\n    results_abs = {}\n    for metric in metric_columns:\n        regex_vals = regex_bool_abs[metric].astype(bool)\n        manual_vals = manual_bool_abs[metric].astype(bool)\n\n        results_abs[metric] = {\n            'regex_sum': int(regex_vals.sum()),\n            'manual_sum': int(manual_vals.sum()),\n            'regex_proportion': float(regex_vals.mean()),\n            'regex_manual_agreement': float((regex_vals == manual_vals).mean()),\n            'manual_proportion': float(manual_vals.mean()),\n            'total_n': int(len(regex_vals)),\n        }\n\n    abstract_results_regex_df = pd.DataFrame(results_abs).T\n\n    tab_decimal_abs = abstract_results_regex_df\n    tab_percent_abs = abstract_results_regex_df.copy()\n    tab_percent_abs['regex_proportion'] = tab_percent_abs['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent_abs['manual_proportion'] = tab_percent_abs['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent_abs['regex_manual_agreement'] = tab_percent_abs['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\n\n    tabs_abs = mo.ui.tabs({\"percent\": tab_percent_abs, \"decimal\": tab_decimal_abs})\n    tabs_abs\n    return (abstract_results_regex_df,)\n</pre> @app.cell(hide_code=True) def _(df_abstract_manual, df_abstract_regex, metric_columns, mo, pd):     # Align indices and compute agreement for abstracts     common_idx_abs = df_abstract_manual.index.intersection(df_abstract_regex.index)     manual_bool_abs = df_abstract_manual.loc[common_idx_abs]     regex_bool_abs = df_abstract_regex.loc[common_idx_abs]      # abstract_metrics = [m for m in metric_columns if m in manual_bool_abs.columns and m in regex_bool_abs.columns]      results_abs = {}     for metric in metric_columns:         regex_vals = regex_bool_abs[metric].astype(bool)         manual_vals = manual_bool_abs[metric].astype(bool)          results_abs[metric] = {             'regex_sum': int(regex_vals.sum()),             'manual_sum': int(manual_vals.sum()),             'regex_proportion': float(regex_vals.mean()),             'regex_manual_agreement': float((regex_vals == manual_vals).mean()),             'manual_proportion': float(manual_vals.mean()),             'total_n': int(len(regex_vals)),         }      abstract_results_regex_df = pd.DataFrame(results_abs).T      tab_decimal_abs = abstract_results_regex_df     tab_percent_abs = abstract_results_regex_df.copy()     tab_percent_abs['regex_proportion'] = tab_percent_abs['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent_abs['manual_proportion'] = tab_percent_abs['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent_abs['regex_manual_agreement'] = tab_percent_abs['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')      tabs_abs = mo.ui.tabs({\"percent\": tab_percent_abs, \"decimal\": tab_decimal_abs})     tabs_abs     return (abstract_results_regex_df,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"## GPT-4 vs. Manual evaluations of manuscript abstracts\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"## GPT-4 vs. Manual evaluations of manuscript abstracts\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(metric_columns, mo, np, pd):\n    df_gpt_agreement_manu = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")\n\n    # Clean up columns - exclude metadata and description columns\n    # Make \"paper_id\" the index\n    df_gpt_agreement_manu = df_gpt_agreement_manu.set_index(\"paper_id\")\n\n    df_gpt_agreement_manu = df_gpt_agreement_manu.drop(\n        columns=[\"evaluation_type\", \"source_file\"]\n        + [col for col in df_gpt_agreement_manu.columns if \"_description\" in col]\n    )\n\n    # Remove gpt_ prefix from column names\n    df_gpt_agreement_manu = df_gpt_agreement_manu.rename(\n        columns={\n            col: col.replace(\"gpt_\", \"\")\n            for col in df_gpt_agreement_manu.columns\n            if col.startswith(\"gpt_\")\n        }\n    )\n\n    results = {}\n    for manu_metric in metric_columns:\n        gpt_col = manu_metric\n        agreement_col = f\"{manu_metric}_agreement\"\n\n        if agreement_col in df_gpt_agreement_manu.columns:\n            gpt_vals = df_gpt_agreement_manu[gpt_col].astype(bool)\n            agreement_vals = df_gpt_agreement_manu[agreement_col]\n\n            # Calculate revised manual evaluation: keep GPT when agreement=1, invert when agreement=0\n            manual_vals_gpt_abs = np.where(agreement_vals == 1, gpt_vals, ~gpt_vals)\n\n            # Add manual_vals_gpt_abs to the agreement_gpt DataFrame\n            df_gpt_agreement_manu[f\"manual_{manu_metric}\"] = manual_vals_gpt_abs.astype(bool)\n\n            results[manu_metric] = {\n                'gpt_sum': gpt_vals.sum(),\n                'manual_sum': manual_vals_gpt_abs.sum(),\n                'gpt_proportion': gpt_vals.mean(),\n                'gpt_manual_agreement': agreement_vals.mean(),\n                'manual_proportion': manual_vals_gpt_abs.mean(),\n                'total_n': len(gpt_vals)\n            }\n    abstract_results_gpt4_df = pd.DataFrame(results).T\n\n    tab_decimal = abstract_results_gpt4_df\n    tab_percent = abstract_results_gpt4_df.copy()\n    tab_percent['gpt_proportion'] = tab_percent['gpt_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent['manual_proportion'] = tab_percent['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent['gpt_manual_agreement'] = tab_percent['gpt_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\n\n    tabs = mo.ui.tabs({\"percent\": tab_percent, \"decimal\": tab_decimal})\n    tabs\n    return abstract_results_gpt4_df, df_gpt_agreement_manu\n</pre> @app.cell(hide_code=True) def _(metric_columns, mo, np, pd):     df_gpt_agreement_manu = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")      # Clean up columns - exclude metadata and description columns     # Make \"paper_id\" the index     df_gpt_agreement_manu = df_gpt_agreement_manu.set_index(\"paper_id\")      df_gpt_agreement_manu = df_gpt_agreement_manu.drop(         columns=[\"evaluation_type\", \"source_file\"]         + [col for col in df_gpt_agreement_manu.columns if \"_description\" in col]     )      # Remove gpt_ prefix from column names     df_gpt_agreement_manu = df_gpt_agreement_manu.rename(         columns={             col: col.replace(\"gpt_\", \"\")             for col in df_gpt_agreement_manu.columns             if col.startswith(\"gpt_\")         }     )      results = {}     for manu_metric in metric_columns:         gpt_col = manu_metric         agreement_col = f\"{manu_metric}_agreement\"          if agreement_col in df_gpt_agreement_manu.columns:             gpt_vals = df_gpt_agreement_manu[gpt_col].astype(bool)             agreement_vals = df_gpt_agreement_manu[agreement_col]              # Calculate revised manual evaluation: keep GPT when agreement=1, invert when agreement=0             manual_vals_gpt_abs = np.where(agreement_vals == 1, gpt_vals, ~gpt_vals)              # Add manual_vals_gpt_abs to the agreement_gpt DataFrame             df_gpt_agreement_manu[f\"manual_{manu_metric}\"] = manual_vals_gpt_abs.astype(bool)              results[manu_metric] = {                 'gpt_sum': gpt_vals.sum(),                 'manual_sum': manual_vals_gpt_abs.sum(),                 'gpt_proportion': gpt_vals.mean(),                 'gpt_manual_agreement': agreement_vals.mean(),                 'manual_proportion': manual_vals_gpt_abs.mean(),                 'total_n': len(gpt_vals)             }     abstract_results_gpt4_df = pd.DataFrame(results).T      tab_decimal = abstract_results_gpt4_df     tab_percent = abstract_results_gpt4_df.copy()     tab_percent['gpt_proportion'] = tab_percent['gpt_proportion'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent['manual_proportion'] = tab_percent['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent['gpt_manual_agreement'] = tab_percent['gpt_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')      tabs = mo.ui.tabs({\"percent\": tab_percent, \"decimal\": tab_decimal})     tabs     return abstract_results_gpt4_df, df_gpt_agreement_manu In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(metric_columns, pd):\n    # Load LLaMA 3.2 abstract results\n    df_abstract_llama32 = pd.read_csv(\"../llama3/outputs_json/20250222-215739/analysis_summary_reproscreener.csv\")\n    df_abstract_llama32 = df_abstract_llama32.set_index(\"paper_id\")\n    # Ensure boolean dtype for metrics present in this dataframe\n    available_cols_llama32 = [c for c in metric_columns if c in df_abstract_llama32.columns]\n    df_abstract_llama32[available_cols_llama32] = df_abstract_llama32[available_cols_llama32].astype(bool)\n    df_abstract_llama32\n    return (df_abstract_llama32,)\n</pre> @app.cell def _(metric_columns, pd):     # Load LLaMA 3.2 abstract results     df_abstract_llama32 = pd.read_csv(\"../llama3/outputs_json/20250222-215739/analysis_summary_reproscreener.csv\")     df_abstract_llama32 = df_abstract_llama32.set_index(\"paper_id\")     # Ensure boolean dtype for metrics present in this dataframe     available_cols_llama32 = [c for c in metric_columns if c in df_abstract_llama32.columns]     df_abstract_llama32[available_cols_llama32] = df_abstract_llama32[available_cols_llama32].astype(bool)     df_abstract_llama32     return (df_abstract_llama32,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(df_abstract_llama32, df_abstract_manual, metric_columns, mo, pd):\n    common_idx_abs_llama32 = df_abstract_manual.index.intersection(df_abstract_llama32.index)\n    manual_bool_abs_llama32 = df_abstract_manual.loc[common_idx_abs_llama32]\n    llama32_bool_abs = df_abstract_llama32.loc[common_idx_abs_llama32]\n\n    metrics_llama32_shared = [\n        m for m in metric_columns\n        if m in manual_bool_abs_llama32.columns and m in llama32_bool_abs.columns\n    ]\n\n    results_abs_llama32 = {}\n    for metric_llama32 in metrics_llama32_shared:\n        llama32_vals = llama32_bool_abs[metric_llama32].astype(bool)\n        manual_vals_llama32 = manual_bool_abs_llama32[metric_llama32].astype(bool)\n\n        results_abs_llama32[metric_llama32] = {\n            'llama32_sum': int(llama32_vals.sum()),\n            'manual_sum_llama32': int(manual_vals_llama32.sum()),\n            'llama32_proportion': float(llama32_vals.mean()),\n            'llama32_manual_agreement': float((llama32_vals == manual_vals_llama32).mean()),\n            'manual_proportion_llama32': float(manual_vals_llama32.mean()),\n            'total_n_llama32': int(len(llama32_vals)),\n        }\n\n    abstract_results_llama32_df = pd.DataFrame(results_abs_llama32).T\n\n    tab_decimal_abs_llama32 = abstract_results_llama32_df\n    tab_percent_abs_llama32 = abstract_results_llama32_df.copy()\n    tab_percent_abs_llama32['llama32_proportion'] = tab_percent_abs_llama32['llama32_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent_abs_llama32['manual_proportion_llama32'] = tab_percent_abs_llama32['manual_proportion_llama32'].mul(100).round(0).astype(int).astype(str).add('%')\n    tab_percent_abs_llama32['llama32_manual_agreement'] = tab_percent_abs_llama32['llama32_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\n\n    tabs_abs_llama32 = mo.ui.tabs({\"percent\": tab_percent_abs_llama32, \"decimal\": tab_decimal_abs_llama32})\n    tabs_abs_llama32\n    return (abstract_results_llama32_df,)\n</pre> @app.cell def _(df_abstract_llama32, df_abstract_manual, metric_columns, mo, pd):     common_idx_abs_llama32 = df_abstract_manual.index.intersection(df_abstract_llama32.index)     manual_bool_abs_llama32 = df_abstract_manual.loc[common_idx_abs_llama32]     llama32_bool_abs = df_abstract_llama32.loc[common_idx_abs_llama32]      metrics_llama32_shared = [         m for m in metric_columns         if m in manual_bool_abs_llama32.columns and m in llama32_bool_abs.columns     ]      results_abs_llama32 = {}     for metric_llama32 in metrics_llama32_shared:         llama32_vals = llama32_bool_abs[metric_llama32].astype(bool)         manual_vals_llama32 = manual_bool_abs_llama32[metric_llama32].astype(bool)          results_abs_llama32[metric_llama32] = {             'llama32_sum': int(llama32_vals.sum()),             'manual_sum_llama32': int(manual_vals_llama32.sum()),             'llama32_proportion': float(llama32_vals.mean()),             'llama32_manual_agreement': float((llama32_vals == manual_vals_llama32).mean()),             'manual_proportion_llama32': float(manual_vals_llama32.mean()),             'total_n_llama32': int(len(llama32_vals)),         }      abstract_results_llama32_df = pd.DataFrame(results_abs_llama32).T      tab_decimal_abs_llama32 = abstract_results_llama32_df     tab_percent_abs_llama32 = abstract_results_llama32_df.copy()     tab_percent_abs_llama32['llama32_proportion'] = tab_percent_abs_llama32['llama32_proportion'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent_abs_llama32['manual_proportion_llama32'] = tab_percent_abs_llama32['manual_proportion_llama32'].mul(100).round(0).astype(int).astype(str).add('%')     tab_percent_abs_llama32['llama32_manual_agreement'] = tab_percent_abs_llama32['llama32_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')      tabs_abs_llama32 = mo.ui.tabs({\"percent\": tab_percent_abs_llama32, \"decimal\": tab_decimal_abs_llama32})     tabs_abs_llama32     return (abstract_results_llama32_df,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(ListedColormap, df_abstract_llama32, plt, sns):\n    def plot_heatmap_llama():\n        # Map metric names to custom display labels (fallback: Title Case)\n        metrics_display_map = {\n            \"experiment_setup\": \"Experimental setup\",\n            \"hypothesis\": \"Hypothesis\",\n            \"method_source_code\": \"Method source code\",\n            \"objective\": \"Objective/Goal\",\n            \"prediction\": \"Prediction\",\n            \"problem\": \"Research problem\",\n            \"pseudocode\": \"Pseudocode\",\n            \"research_method\": \"Research method\",\n            \"research_questions\": \"Research questions\",\n            \"test_data\": \"Test data\",\n            \"training_data\": \"Training data\",\n            \"validation_data\": \"Validation data\",\n            \"dataset\": \"Dataset\",\n            \"software_dependencies\": \"Software dependencies\",\n        }\n\n        # Metrics on rows, papers on columns\n        heatmap_df = df_abstract_llama32.astype(float).T\n        heatmap_df.index = [metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]\n\n        # Two-color scheme (empty, filled)\n        custom_cmap = ListedColormap([\"#FFF0F0\", \"#E74C3C\"])\n\n        fig, ax = plt.subplots(figsize=(12, 4), tight_layout={\"pad\": 1.5})\n\n        # Black frame\n        ax.axhline(y=0, color=\"k\", linewidth=1)\n        ax.axvline(x=0, color=\"k\", linewidth=1)\n        ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1)\n        ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1)\n\n        sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1, ax=ax)\n\n        ax.set(xlabel=\"Paper\", ylabel=\"Metric\")\n        plt.title(\"LLama 3.2 evaluations on manuscript abstracts\", pad=15)\n        plt.subplots_adjust(top=0.95, left=0.15, right=0.95)\n        plt.tight_layout()\n\n        # Optional:\n        plt.savefig(\"heatmap_metric_presence_llama32_direct.png\", dpi=320, bbox_inches=\"tight\")\n        plt.show()\n\n    plot_heatmap_llama()\n    return\n</pre> @app.cell def _(ListedColormap, df_abstract_llama32, plt, sns):     def plot_heatmap_llama():         # Map metric names to custom display labels (fallback: Title Case)         metrics_display_map = {             \"experiment_setup\": \"Experimental setup\",             \"hypothesis\": \"Hypothesis\",             \"method_source_code\": \"Method source code\",             \"objective\": \"Objective/Goal\",             \"prediction\": \"Prediction\",             \"problem\": \"Research problem\",             \"pseudocode\": \"Pseudocode\",             \"research_method\": \"Research method\",             \"research_questions\": \"Research questions\",             \"test_data\": \"Test data\",             \"training_data\": \"Training data\",             \"validation_data\": \"Validation data\",             \"dataset\": \"Dataset\",             \"software_dependencies\": \"Software dependencies\",         }          # Metrics on rows, papers on columns         heatmap_df = df_abstract_llama32.astype(float).T         heatmap_df.index = [metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]          # Two-color scheme (empty, filled)         custom_cmap = ListedColormap([\"#FFF0F0\", \"#E74C3C\"])          fig, ax = plt.subplots(figsize=(12, 4), tight_layout={\"pad\": 1.5})          # Black frame         ax.axhline(y=0, color=\"k\", linewidth=1)         ax.axvline(x=0, color=\"k\", linewidth=1)         ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1)         ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1)          sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1, ax=ax)          ax.set(xlabel=\"Paper\", ylabel=\"Metric\")         plt.title(\"LLama 3.2 evaluations on manuscript abstracts\", pad=15)         plt.subplots_adjust(top=0.95, left=0.15, right=0.95)         plt.tight_layout()          # Optional:         plt.savefig(\"heatmap_metric_presence_llama32_direct.png\", dpi=320, bbox_inches=\"tight\")         plt.show()      plot_heatmap_llama()     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"## Select a metric to view evaluation results for each paper\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"## Select a metric to view evaluation results for each paper\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(metric_columns, mo):\n    dropdown = mo.ui.dropdown(metric_columns, value=metric_columns[0] if metric_columns else None)\n    dropdown\n    return (dropdown,)\n</pre> @app.cell(hide_code=True) def _(metric_columns, mo):     dropdown = mo.ui.dropdown(metric_columns, value=metric_columns[0] if metric_columns else None)     dropdown     return (dropdown,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(df_gpt_agreement_manu, dropdown, pd):\n    if dropdown.value:\n        selected_metric = dropdown.value\n        selected_metric_df = pd.DataFrame({\n            'gpt': df_gpt_agreement_manu[selected_metric].astype(bool),\n            'manual': df_gpt_agreement_manu[f\"manual_{selected_metric}\"].astype(bool),\n            'agreement': df_gpt_agreement_manu[f\"{selected_metric}_agreement\"],\n        })\n        selected_metric_df\n    else:\n        pd.DataFrame()\n    selected_metric_df\n    return\n</pre> @app.cell(hide_code=True) def _(df_gpt_agreement_manu, dropdown, pd):     if dropdown.value:         selected_metric = dropdown.value         selected_metric_df = pd.DataFrame({             'gpt': df_gpt_agreement_manu[selected_metric].astype(bool),             'manual': df_gpt_agreement_manu[f\"manual_{selected_metric}\"].astype(bool),             'agreement': df_gpt_agreement_manu[f\"{selected_metric}_agreement\"],         })         selected_metric_df     else:         pd.DataFrame()     selected_metric_df     return In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(\n    abstract_results_gpt4_df,\n    abstract_results_llama32_df,\n    abstract_results_regex_df,\n    pd,\n):\n\n\n    # plot all 3 agreement metrics\n    abstract_results_regex_df.regex_manual_agreement\n    abstract_results_gpt4_df.gpt_manual_agreement\n    abstract_results_llama32_df.llama32_manual_agreement\n\n    merged_agreement_results = pd.DataFrame({\n        'Reproscreener (regex)': abstract_results_regex_df['regex_manual_agreement'],\n        'GPT-4': abstract_results_gpt4_df['gpt_manual_agreement'],\n        'LLaMA 3.2': abstract_results_llama32_df['llama32_manual_agreement']\n    })\n    merged_agreement_results.index.name = 'Metric'\n    merged_agreement_results\n    return (merged_agreement_results,)\n</pre> @app.cell def _(     abstract_results_gpt4_df,     abstract_results_llama32_df,     abstract_results_regex_df,     pd, ):       # plot all 3 agreement metrics     abstract_results_regex_df.regex_manual_agreement     abstract_results_gpt4_df.gpt_manual_agreement     abstract_results_llama32_df.llama32_manual_agreement      merged_agreement_results = pd.DataFrame({         'Reproscreener (regex)': abstract_results_regex_df['regex_manual_agreement'],         'GPT-4': abstract_results_gpt4_df['gpt_manual_agreement'],         'LLaMA 3.2': abstract_results_llama32_df['llama32_manual_agreement']     })     merged_agreement_results.index.name = 'Metric'     merged_agreement_results     return (merged_agreement_results,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(merged_agreement_results):\n    merged_agreement_results_melt = merged_agreement_results.reset_index().melt(id_vars='Metric', var_name='Method', value_name='Agreement')\n    merged_agreement_results_melt\n    return (merged_agreement_results_melt,)\n</pre> @app.cell def _(merged_agreement_results):     merged_agreement_results_melt = merged_agreement_results.reset_index().melt(id_vars='Metric', var_name='Method', value_name='Agreement')     merged_agreement_results_melt     return (merged_agreement_results_melt,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(merged_agreement_results_melt, np, plt, wrap):\n    # import numpy as np\n    # import matplotlib.pyplot as plt\n    # import scienceplots\n    # plt.style.use('ieee')\n    metric_order = [\n        \"problem\", \"objective\", \"research_method\", \"research_questions\",\n        \"pseudocode\", \"dataset\", \"hypothesis\", \"prediction\",\n        \"code_available\", \"software_dependencies\", \"experiment_setup\"\n    ]# assumes df from above\n    df_sorted = merged_agreement_results_melt.sort_values([\"Metric\",\"Method\"])\n    methods = df_sorted[\"Method\"].unique()\n    metrics = metric_order\n    x = np.arange(len(metrics))\n    width = 0.8 / len(methods)\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    for i, m in enumerate(methods):\n        sub = df_sorted[df_sorted[\"Method\"] == m].set_index(\"Metric\").reindex(metrics)\n        ax.bar(x + i*width - (len(methods)-1)*width/2, sub[\"Agreement\"].values, width, label=m)\n\n    ax.set_xticks(x)\n    ax.set_xticklabels([ \"\\n\".join(wrap(m, 12)) for m in metrics ])\n    ax.set_ylim(0, 1.05)\n    ax.set_ylabel(\"Agreement\")\n    ax.set_title(\"Agreement by Metric \u2014 All Methods\")\n    ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n    return\n</pre> @app.cell def _(merged_agreement_results_melt, np, plt, wrap):     # import numpy as np     # import matplotlib.pyplot as plt     # import scienceplots     # plt.style.use('ieee')     metric_order = [         \"problem\", \"objective\", \"research_method\", \"research_questions\",         \"pseudocode\", \"dataset\", \"hypothesis\", \"prediction\",         \"code_available\", \"software_dependencies\", \"experiment_setup\"     ]# assumes df from above     df_sorted = merged_agreement_results_melt.sort_values([\"Metric\",\"Method\"])     methods = df_sorted[\"Method\"].unique()     metrics = metric_order     x = np.arange(len(metrics))     width = 0.8 / len(methods)      fig, ax = plt.subplots(figsize=(12, 6))     for i, m in enumerate(methods):         sub = df_sorted[df_sorted[\"Method\"] == m].set_index(\"Metric\").reindex(metrics)         ax.bar(x + i*width - (len(methods)-1)*width/2, sub[\"Agreement\"].values, width, label=m)      ax.set_xticks(x)     ax.set_xticklabels([ \"\\n\".join(wrap(m, 12)) for m in metrics ])     ax.set_ylim(0, 1.05)     ax.set_ylabel(\"Agreement\")     ax.set_title(\"Agreement by Metric \u2014 All Methods\")     ax.yaxis.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.7)     ax.legend()     plt.tight_layout()     plt.show()     return In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    app.run()\n</pre> if __name__ == \"__main__\":     app.run()"},{"location":"case-studies/","title":"Case studies","text":"<ol> <li> <p><code>mine50</code> contains the 50 most recent articles from arxiv.org in both the cs.LG and stat.ML categories, between the dates 2022-10-24 and 2022-10-25 and contained 570 search results at the time of the dataset creation. The search result is sorted by date in descending order</p> <p>Note</p> <p>The date being queried for is the last updated date and not the date of paper submission</p> </li> <li> <p><code>mine50-csLG</code> contains the results using the same method as <code>mine50</code> but without looking for articles in both cs.LG and stat.ML.</p> </li> </ol>"},{"location":"dev-notes/","title":"Development notes","text":"<ul> <li>Non binary values in JSON guidance format would require custom keyword seach functions</li> <li>Include a way to add keywords to JSON and have it assigned to keywordparser</li> <li>Refactor so keywords functions are independent of guidance type</li> <li>MKDocs over Sphinx?</li> <li>Using shutil.copyfileobj to merge all tex files in an article's source folder into 1 file and then running keyword search on it. Would likely be more efficient than search over each file and combining the scores.</li> <li>[] Dynamic progress (tests/dynamic_progress.py)</li> <li>Encoding error due to latin charaters in tex files (UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 58: invalid continuation byte)</li> </ul> <pre><code>poetry add arxivscraper requests pandas numpy pathlib flashtext exrex lxml rich typer gitpython pydrive2 urlextract uritools beautifulsoup4\n</code></pre> <pre><code>poetry add flake8 \"black[jupyter]\" mypy loguru beautifulsoup4 bandit seaborn types-requests pytest pytest-cov --group dev\n</code></pre> <pre><code>poetry add ipykernel streamlit types-tabulate pdfx jupyter-contrib-nbextensions tabulate mkdocs mkdocs-bibtex mkdocs-material pymdown-extensions mkdocstrings-python mkdocs-jupyter mkdocs-macros-plugin markdown-it-py linkify-it-py plotly graphviz pygraphviz --group explore\n</code></pre> <pre><code>idna platformdirs dnspython\n</code></pre> <p>uv run reproscreener --gold-abstracts-dir ../gold_standard/abstracts --out-csv reports/tables/abstract_regex_gs.csv</p>"},{"location":"development/","title":"Install for development","text":"<p>For a devlopment environment, first clone the repository <code>git@github.com:Machine-Learning-Pipelines/reproscreener.git</code>.</p> <ol> <li>Install <code>uv</code>:</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>For installation methods, see the uv documentation</p> <ol> <li>Create a virtual environment, activate it, and install the dependencies:</li> </ol> <pre><code>uv venv\nsource .venv/bin/activate\nuv sync\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol>"},{"location":"docstrings/","title":"Architecture","text":"<p>This page documents the public API of the three core modules that power reproscreener.</p>"},{"location":"docstrings/#paper_analyzer-module","title":"<code>paper_analyzer</code> module","text":"<p>The <code>paper_analyzer</code> module is responsible for analysing individual research papers hosted on arXiv. It can:</p> <ul> <li>Parse canonical arXiv identifiers from arbitrary arXiv URLs.</li> <li>Download either the TeX source bundle (e-print) or the PDF of the paper.</li> <li>Optionally convert PDFs to Markdown via <code>docling</code> for easier text processing.</li> <li>Extract reproducibility variables (problem statements, dataset mentions, hypotheses, etc.).</li> <li>Detect external links such as source-code or data repositories contained in the manuscript.</li> <li>Return the results as a convenient <code>pandas.DataFrame</code>.</li> </ul>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.analyze_abstract_file","title":"<code>analyze_abstract_file(abstract_path)</code>","text":"<p>Analyze a single abstract .txt file and return a row dict for the CSV. The paper_id is derived from the filename stem.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.analyze_abstracts_directory","title":"<code>analyze_abstracts_directory(abstracts_dir)</code>","text":"<p>Analyze all .txt abstracts in a directory and return a DataFrame with the expected CSV columns.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.analyze_arxiv_paper","title":"<code>analyze_arxiv_paper(arxiv_url, download_dir, url_type='tex')</code>","text":"<p>Main function to download, extract, and analyze an arXiv paper.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.analyze_arxiv_paper--parameters","title":"Parameters:","text":"<p>arxiv_url : str     The arXiv URL of the paper download_dir : Path     Directory to download and store the paper url_type : str     Type of arXiv URL, either \"tex\" or \"pdf\"</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.analyze_arxiv_paper--returns","title":"Returns:","text":"<p>pd.DataFrame     Analysis results as a DataFrame</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.analyze_content","title":"<code>analyze_content(folder_path, paper_id, title)</code>","text":"<p>Evaluate a paper by extracting variables and URLs from its files. Returns a DataFrame with evaluation results.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.combine_files_in_folder","title":"<code>combine_files_in_folder(folder_path, file_extensions=['.tex', '.md', '.txt'])</code>","text":"<p>Combine all files with specified extensions in a given directory into a single file.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.download_extract_source","title":"<code>download_extract_source(arxiv_url, path_download)</code>","text":"<p>Downloads and extracts the source code of an ArXiv paper from its URL. Also retrieves the paper title from the arXiv API. Returns the paper title and the download directory path.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.download_pdf_and_convert","title":"<code>download_pdf_and_convert(arxiv_url, path_download)</code>","text":"<p>Downloads a PDF from arXiv and converts it to markdown using docling. Returns the paper title and the path to the markdown file.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.extract_category_presence_from_text","title":"<code>extract_category_presence_from_text(text)</code>","text":"<p>Given raw text (e.g., an abstract), return boolean presence for each category relevant to the gold-standard CSV output.</p> <p>Output keys (CSV columns): - problem - objective - research_method - research_questions - pseudocode - dataset - hypothesis - prediction - code_available (mapped from method_source_code) - software_dependencies - experiment_setup</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.extract_urls","title":"<code>extract_urls(combined_path)</code>","text":"<p>Extract URLs from the combined file.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.find_data_repository_links","title":"<code>find_data_repository_links(url_list, allowed_domains=['github', 'gitlab', 'bitbucket', 'zenodo'])</code>","text":"<p>Find URLs belonging to allowed domains.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.find_variables","title":"<code>find_variables(combined_path)</code>","text":"<p>Return a list of (variable_category, matched_phrase) pairs found in the paper.</p>"},{"location":"docstrings/#reproscreener.analysis.paper_analyzer.parse_arxiv_id","title":"<code>parse_arxiv_id(arxiv_url)</code>","text":"<p>Extract the canonical arXiv identifier (without version or extension) from arxiv_url which may be any of: \u2022 https://arxiv.org/abs/1909.00066v1 \u2022 https://arxiv.org/pdf/1909.00066.pdf \u2022 https://arxiv.org/pdf/1909.00066v2.pdf \u2022 https://arxiv.org/src/1909.00066 \u2022 https://arxiv.org/e-print/1909.00066 Returns the bare identifier, e.g. <code>1909.00066</code>.</p>"},{"location":"docstrings/#repo_analyzer-module","title":"<code>repo_analyzer</code> module","text":"<p>The <code>repo_analyzer</code> module evaluates the structure of a Git repository that claims to implement the research. Its main tasks are:</p> <ul> <li>Cloning public repositories (GitHub, GitLab, Bitbucket, \u2026).</li> <li>Searching for dependency specification files (e.g. <code>requirements.txt</code>, <code>environment.yml</code>, <code>pyproject.toml</code>, <code>Dockerfile</code>, \u2026).</li> <li>Detecting wrapper scripts or entry-point files (<code>run.py</code>, <code>main.sh</code>, <code>Makefile</code>, \u2026).</li> <li>Parsing the project's <code>README</code> for sections that describe installation or requirements.</li> <li>Aggregating the findings into a tabular report.</li> </ul>"},{"location":"docstrings/#reproscreener.analysis.repo_analyzer.analyze_github_repo","title":"<code>analyze_github_repo(repo_url, clone_dir)</code>","text":"<p>Main function to clone and analyze a GitHub repository.</p>"},{"location":"docstrings/#reproscreener.analysis.repo_analyzer.analyze_repository_structure","title":"<code>analyze_repository_structure(repo_path)</code>","text":"<p>Evaluate a repository by checking the existence of certain files and sections in README. Returns a DataFrame with the evaluation results.</p>"},{"location":"docstrings/#reproscreener.analysis.repo_analyzer.check_files","title":"<code>check_files(dir_path, files_to_check, current_ext_mapping)</code>","text":"<p>Check if the given files exist in the directory based on ext_mapping.</p>"},{"location":"docstrings/#reproscreener.analysis.repo_analyzer.clone_repo","title":"<code>clone_repo(repo_url, cloned_path, overwrite=False)</code>","text":"<p>Clone a repository from the given URL to the given path. If the repository already exists, it won't be overwritten unless specified.</p>"},{"location":"docstrings/#keywords-module","title":"<code>keywords</code> module","text":"<p>The <code>keywords</code> module generates the lists of keywords/regular-expression patterns that are used by the analyser modules to identify important concepts inside paper text. Currently it implements the metrics from <sup>1</sup>.</p> <ol> <li> <p>Bhaskar, A. and Stodden, V. 2024. Reproscreener: Leveraging LLMs for Assessing Computational Reproducibility of Machine Learning Pipelines. Proceedings of the 2nd ACM Conference on Reproducibility and Replicability (New York, NY, USA, Jul. 2024), 101--109.\u00a0\u21a9</p> </li> </ol>"},{"location":"docstrings/#reproscreener.analysis.keywords.generate_gunderson_dict","title":"<code>generate_gunderson_dict()</code>","text":"<p>Generate a dictionary of Gunderson variables with regex patterns.</p> <p>Returns:</p> Name Type Description <code>_type_</code> <p>A dictionary of keywords and regex patterns.</p>"},{"location":"evaluation_results/","title":"Case studies","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom IPython.display import display\nfrom pathlib import Path\nimport sys\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, cohen_kappa_score\nfrom reproscreener.gold_standard import summary_table, tex_map_dict, repo_map_dict, prepare_pivot, compare_with_manual, split_parsed_readme\n\nsys.path.append(str(Path.cwd().parent / \"src/reproscrener\"))\n\nfrom reproscreener.plots.repo_eval_heatmaps import prepare_repo_heatmap_df, plot_repo_heatmap, plot_repo_clustermap\nfrom reproscreener.plots.tex_eval_heatmaps import prepare_tex_heatmap_df, plot_tex_heatmap\nfrom reproscreener.repo_eval import get_all_repo_eval_dict\nfrom reproscreener.tex_eval import get_all_tex_eval_dict\nfrom reproscreener.gdrive_downloader import gdrive_get_manual_eval\nfrom reproscreener.utils import reverse_mapping\n</pre> import pandas as pd import numpy as np from IPython.display import display from pathlib import Path import sys import seaborn as sns from matplotlib import pyplot as plt from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, cohen_kappa_score from reproscreener.gold_standard import summary_table, tex_map_dict, repo_map_dict, prepare_pivot, compare_with_manual, split_parsed_readme  sys.path.append(str(Path.cwd().parent / \"src/reproscrener\"))  from reproscreener.plots.repo_eval_heatmaps import prepare_repo_heatmap_df, plot_repo_heatmap, plot_repo_clustermap from reproscreener.plots.tex_eval_heatmaps import prepare_tex_heatmap_df, plot_tex_heatmap from reproscreener.repo_eval import get_all_repo_eval_dict from reproscreener.tex_eval import get_all_tex_eval_dict from reproscreener.gdrive_downloader import gdrive_get_manual_eval from reproscreener.utils import reverse_mapping In\u00a0[2]: Copied! <pre>path_repo = Path(\"../case-studies/arxiv-corpus/gold_standard/repo\")\npath_tex = Path(\"../case-studies/arxiv-corpus/gold_standard/source\")\npath_manual = Path(\"../case-studies/arxiv-corpus/manual_eval.csv\")\n\nmanual_eval = gdrive_get_manual_eval(overwrite=False, manual_path=path_manual)\ngold_standard_ids = manual_eval[\"paper\"].unique()\n</pre> path_repo = Path(\"../case-studies/arxiv-corpus/gold_standard/repo\") path_tex = Path(\"../case-studies/arxiv-corpus/gold_standard/source\") path_manual = Path(\"../case-studies/arxiv-corpus/manual_eval.csv\")  manual_eval = gdrive_get_manual_eval(overwrite=False, manual_path=path_manual) gold_standard_ids = manual_eval[\"paper\"].unique() <pre>Manual eval file already exists, use the overwrite flag to download\n</pre> In\u00a0[3]: Copied! <pre>repo_evaluation_dict = get_all_repo_eval_dict(path_repo)\nrepo_heatmap_df = prepare_repo_heatmap_df(repo_evaluation_dict, gold_standard_ids)\nplot_repo_heatmap(repo_heatmap_df, filename=\"heatmap_repo_eval.png\", path_plots=None, sort_x=True, sort_y=True)\n</pre> repo_evaluation_dict = get_all_repo_eval_dict(path_repo) repo_heatmap_df = prepare_repo_heatmap_df(repo_evaluation_dict, gold_standard_ids) plot_repo_heatmap(repo_heatmap_df, filename=\"heatmap_repo_eval.png\", path_plots=None, sort_x=True, sort_y=True) In\u00a0[4]: Copied! <pre>plot_repo_clustermap(repo_heatmap_df, filename=\"clustermap_repo_eval.png\", path_plots=None)\n</pre> plot_repo_clustermap(repo_heatmap_df, filename=\"clustermap_repo_eval.png\", path_plots=None) In\u00a0[5]: Copied! <pre>repo_heatmap_df.head(10).drop(columns=[\"Display_Label\"])\n</pre> repo_heatmap_df.head(10).drop(columns=[\"Display_Label\"]) Out[5]: Paper_ID Matched_File Category 0 1606.04671 Code provided but no matches Others 1 1903.09668 readme_dependencies Parsed Readme 2 1904.10554 Code provided but no matches Others 3 1908.05659 requirements.txt Dependencies 4 1908.05659 readme_install Parsed Readme 5 1909.00931 Code provided but no matches Others 6 1911.03867 environment.yml Dependencies 7 1911.03867 requirements.txt Dependencies 8 1911.03867 readme_requirements Parsed Readme 9 2002.05905 Code provided but no matches Others In\u00a0[6]: Copied! <pre>number_of_papers = len(repo_heatmap_df[\"Paper_ID\"].unique())\nprint(f\"Total number of papers in the gold standard: {len(gold_standard_ids)}\")\n</pre> number_of_papers = len(repo_heatmap_df[\"Paper_ID\"].unique()) print(f\"Total number of papers in the gold standard: {len(gold_standard_ids)}\") <pre>Total number of papers in the gold standard: 50\n</pre> In\u00a0[7]: Copied! <pre>summary_table(repo_heatmap_df, \"Matched_File\", number_of_papers)\n</pre> summary_table(repo_heatmap_df, \"Matched_File\", number_of_papers) Out[7]: Reproscreener_Article_Count Reproscreener_Percentage No code provided 28 56.00% Code provided but no matches 9 18.00% requirements.txt 6 12.00% readme_install 4 8.00% readme_requirements 3 6.00% readme_setup 3 6.00% readme_dependencies 2 4.00% environment.yml 1 2.00% conda_reqs.txt 1 2.00% pip_reqs.txt 1 2.00% run_experiments.py 1 2.00% main.py 1 2.00% <p>The variables are grouped by the following categories defined in <code>reverse_mapping</code>:</p> <ul> <li>Dependencies: Files related to the dependencies of the repository.</li> <li>Wrapper Scripts: Files that combine various stages of the workflow.</li> <li>Parsed Readme: Headers present in the README file of the repository that provide instructions about the code/data.</li> <li>Others: Contains <code>No code provided</code> or <code>Code provided but no matches</code>. The latter is used when the code is provided but files from any of the other categories were found in the repository.</li> </ul> In\u00a0[8]: Copied! <pre>reverse_mapping_df = pd.DataFrame.from_dict(reverse_mapping, orient='index', columns=['Category'])\nreverse_mapping_df.index.name = 'Matched_File'\nreverse_mapping_df\n</pre> reverse_mapping_df = pd.DataFrame.from_dict(reverse_mapping, orient='index', columns=['Category']) reverse_mapping_df.index.name = 'Matched_File' reverse_mapping_df Out[8]: Category Matched_File requirements.txt Dependencies setup.py Dependencies environment.yml Dependencies pyproject.toml Dependencies pip_reqs.txt Dependencies conda_reqs.txt Dependencies run.py Wrapper Scripts run.sh Wrapper Scripts main.py Wrapper Scripts main.sh Wrapper Scripts run_all.py Wrapper Scripts run_all.sh Wrapper Scripts run_experiments.py Wrapper Scripts run_experiments.sh Wrapper Scripts readme_requirements Parsed Readme readme_dependencies Parsed Readme readme_setup Parsed Readme readme_install Parsed Readme No code provided Others Code provided but no matches Others In\u00a0[9]: Copied! <pre>summary_table(repo_heatmap_df, \"Category\", number_of_papers)\n</pre> summary_table(repo_heatmap_df, \"Category\", number_of_papers) Out[9]: Reproscreener_Article_Count Reproscreener_Percentage Others 37 74.00% Parsed Readme 12 24.00% Dependencies 9 18.00% Wrapper Scripts 2 4.00% In\u00a0[10]: Copied! <pre>no_code_provided_counts = len(repo_heatmap_df[repo_heatmap_df[\"Matched_File\"] == \"No code provided\"])\ncode_provided_counts = number_of_papers - no_code_provided_counts\ncode_provided_percentage = (code_provided_counts / number_of_papers) * 100\nprint(f\"{code_provided_counts}/{number_of_papers} ({code_provided_percentage:.2f}%) of the papers have provided some code\")\n</pre> no_code_provided_counts = len(repo_heatmap_df[repo_heatmap_df[\"Matched_File\"] == \"No code provided\"]) code_provided_counts = number_of_papers - no_code_provided_counts code_provided_percentage = (code_provided_counts / number_of_papers) * 100 print(f\"{code_provided_counts}/{number_of_papers} ({code_provided_percentage:.2f}%) of the papers have provided some code\") <pre>22/50 (44.00%) of the papers have provided some code\n</pre> In\u00a0[11]: Copied! <pre>tex_evaluation_dict = get_all_tex_eval_dict(path_tex)\ntex_heatmap_df = prepare_tex_heatmap_df(tex_evaluation_dict, gold_standard_ids)\n</pre> tex_evaluation_dict = get_all_tex_eval_dict(path_tex) tex_heatmap_df = prepare_tex_heatmap_df(tex_evaluation_dict, gold_standard_ids) <pre>Output()</pre> <pre></pre> <pre>\n</pre> In\u00a0[12]: Copied! <pre>plot_tex_heatmap(tex_heatmap_df, filename=\"heatmap_tex_eval.png\", path_plots=None, sort_x=True, sort_y=True)\n</pre> plot_tex_heatmap(tex_heatmap_df, filename=\"heatmap_tex_eval.png\", path_plots=None, sort_x=True, sort_y=True) In\u00a0[13]: Copied! <pre>tex_heatmap_df.head(10)\n</pre> tex_heatmap_df.head(10) Out[13]: Paper_ID Found_Variable 0 1606.04671 Research questions 1 1606.04671 Research method 2 1606.04671 Experimental setup 3 1606.04671 Research problem 4 1606.04671 Prediction 5 1606.04671 Training data 6 1606.04671 Hypothesis 7 1606.04671 Objective/Goal 8 1903.09668 Research questions 9 1903.09668 Research method In\u00a0[14]: Copied! <pre>summary_table(tex_heatmap_df, \"Found_Variable\", number_of_papers)\n</pre> summary_table(tex_heatmap_df, \"Found_Variable\", number_of_papers) Out[14]: Reproscreener_Article_Count Reproscreener_Percentage Research questions 44 88.00% Research problem 44 88.00% Research method 43 86.00% Objective/Goal 39 78.00% Prediction 34 68.00% Method source code 23 46.00% Hypothesis 21 42.00% Training data 18 36.00% Experimental setup 15 30.00% Test data 7 14.00% Pseudocode 6 12.00% Validation data 2 4.00% No variables found 1 2.00% In\u00a0[15]: Copied! <pre>manual_eval = split_parsed_readme(manual_eval, 'parsed_readme')\nmanual_eval.rename(columns=repo_map_dict, inplace=True)\nmanual_eval.rename(columns={\"paper\": \"Paper_ID\"}, inplace=True)\nmanual_eval.head()\nmanual_eval.columns\n</pre> manual_eval = split_parsed_readme(manual_eval, 'parsed_readme') manual_eval.rename(columns=repo_map_dict, inplace=True) manual_eval.rename(columns={\"paper\": \"Paper_ID\"}, inplace=True) manual_eval.head() manual_eval.columns Out[15]: <pre>Index(['Paper_ID', 'Unnamed: 1', 'paper_url', 'notes', 'empirical_dataset',\n       'code_avail_article', 'code_avail_article_desc', 'code_avail_url',\n       'pwc_link_avail', 'pwc_link_match', 'pwc_link_desc',\n       'result_replication_code_avail', 'code_language', 'package',\n       'wrapper_scripts', 'wrapper_scripts_desc', 'hardware_specifications',\n       'software_dependencies', 'software_dependencies_desc',\n       'will_it_reproduce', 'will_it_reproduce_desc', 'parsed_readme',\n       'problem', 'problem_desc', 'objective', 'objective_desc',\n       'research_method', 'research_method_desc', 'research_questions',\n       'research_questions_desc', 'pseudocode', 'pseudocode_desc', 'dataset',\n       'dataset_desc', 'hypothesis', 'hypothesis_desc', 'prediction',\n       'experiment_setup', 'experiment_setup_desc', 'nan',\n       'readme_dependencies', 'readme_install', 'readme_requirements',\n       'readme_setup'],\n      dtype='object')</pre> In\u00a0[16]: Copied! <pre>repo_heatmap_pivot = prepare_pivot(repo_heatmap_df, 'Paper_ID', repo_map_dict, var_column='Category', match_column='Matched_File')\nauto_eval_df = repo_heatmap_pivot.copy()\nauto_eval_df.columns = [f\"{col}_reproscreener\" if col != \"Paper_ID\" else col for col in auto_eval_df.columns]\n\nmanual_eval_df = manual_eval.copy()\nmanual_eval_df.columns = [f\"{col}_manual\" if col != \"Paper_ID\" else col for col in manual_eval_df.columns]\n\ncompare_with_manual(auto_eval_df, manual_eval_df, repo_map_dict)\n</pre> repo_heatmap_pivot = prepare_pivot(repo_heatmap_df, 'Paper_ID', repo_map_dict, var_column='Category', match_column='Matched_File') auto_eval_df = repo_heatmap_pivot.copy() auto_eval_df.columns = [f\"{col}_reproscreener\" if col != \"Paper_ID\" else col for col in auto_eval_df.columns]  manual_eval_df = manual_eval.copy() manual_eval_df.columns = [f\"{col}_manual\" if col != \"Paper_ID\" else col for col in manual_eval_df.columns]  compare_with_manual(auto_eval_df, manual_eval_df, repo_map_dict) Out[16]: Variable False_Positives False_Negatives Total_Mistakes Reproscreener_Found Manual_Found 0 Dependencies 5 1 6 7.0 14.0 0 Wrapper Scripts 2 3 5 2.0 18.0 0 Parsed Readme - Requirements 3 1 4 3.0 2.0 0 Parsed Readme - Dependencies 2 1 3 2.0 9.0 0 Parsed Readme - Setup 3 0 3 3.0 2.0 0 Parsed Readme - Install 4 0 4 4.0 3.0 <ul> <li>Where n = 50 for <code>Reproscreener_Article_Count</code> and <code>Manual_Article_Count</code></li> <li>False positives - Reproscreener found something that wasn't manually found</li> <li>False negatives - Reproscreener didn't find something that was manually found</li> <li>Total mistakes - False positives + False negatives</li> </ul> In\u00a0[17]: Copied! <pre>compare_with_manual(auto_eval_df, manual_eval_df, repo_map_dict, output_format=\"percent\")\n</pre> compare_with_manual(auto_eval_df, manual_eval_df, repo_map_dict, output_format=\"percent\") Out[17]: Variable False_Positives False_Negatives Total_Mistakes Reproscreener_Found Manual_Found 0 Dependencies 10.0 2.0 12.0 14.0 28.0 0 Wrapper Scripts 4.0 6.0 10.0 4.0 36.0 0 Parsed Readme - Requirements 6.0 2.0 8.0 6.0 4.0 0 Parsed Readme - Dependencies 4.0 2.0 6.0 4.0 18.0 0 Parsed Readme - Setup 6.0 0.0 6.0 6.0 4.0 0 Parsed Readme - Install 8.0 0.0 8.0 8.0 6.0 In\u00a0[18]: Copied! <pre>tex_heatmap_pivot = prepare_pivot(tex_heatmap_df, 'Paper_ID', tex_map_dict, var_column='Found_Variable')\n\nauto_eval_df = tex_heatmap_pivot.copy()\nauto_eval_df.columns = [f\"{col}_reproscreener\" if col != \"Paper_ID\" else col for col in auto_eval_df.columns]\n\nmanual_eval_df = manual_eval.copy()\nmanual_eval_df.columns = [f\"{col}_manual\" if col != \"Paper_ID\" else col for col in manual_eval_df.columns]\n\ncompare_with_manual(auto_eval_df, manual_eval_df, tex_map_dict)\n</pre> tex_heatmap_pivot = prepare_pivot(tex_heatmap_df, 'Paper_ID', tex_map_dict, var_column='Found_Variable')  auto_eval_df = tex_heatmap_pivot.copy() auto_eval_df.columns = [f\"{col}_reproscreener\" if col != \"Paper_ID\" else col for col in auto_eval_df.columns]  manual_eval_df = manual_eval.copy() manual_eval_df.columns = [f\"{col}_manual\" if col != \"Paper_ID\" else col for col in manual_eval_df.columns]  compare_with_manual(auto_eval_df, manual_eval_df, tex_map_dict) Out[18]: Variable False_Positives False_Negatives Total_Mistakes Reproscreener_Found Manual_Found 0 Research questions 41 0 41 44.0 3.0 0 Research problem 30 1 31 44.0 15.0 0 Research method 34 1 35 43.0 10.0 0 Objective/Goal 35 0 35 39.0 4.0 0 Prediction 34 0 34 34.0 0.0 0 Method source code 5 4 9 23.0 22.0 0 Hypothesis 16 3 19 21.0 8.0 0 Training data 6 19 25 18.0 31.0 0 Experimental setup 0 22 22 15.0 37.0 <ul> <li>Where n = 50 for <code>Reproscreener_Article_Count</code> and <code>Manual_Article_Count</code></li> <li>False positives - Reproscreener found something that wasn't manually found</li> <li>False negatives - Reproscreener didn't find something that was manually found</li> <li>Total mistakes - False positives + False negatives</li> </ul> In\u00a0[19]: Copied! <pre>compare_with_manual(auto_eval_df, manual_eval_df, tex_map_dict, output_format=\"percent\")\n</pre> compare_with_manual(auto_eval_df, manual_eval_df, tex_map_dict, output_format=\"percent\") Out[19]: Variable False_Positives False_Negatives Total_Mistakes Reproscreener_Found Manual_Found 0 Research questions 82.0 0.0 82.0 88.0 6.0 0 Research problem 60.0 2.0 62.0 88.0 30.0 0 Research method 68.0 2.0 70.0 86.0 20.0 0 Objective/Goal 70.0 0.0 70.0 78.0 8.0 0 Prediction 68.0 0.0 68.0 68.0 0.0 0 Method source code 10.0 8.0 18.0 46.0 44.0 0 Hypothesis 32.0 6.0 38.0 42.0 16.0 0 Training data 12.0 38.0 50.0 36.0 62.0 0 Experimental setup 0.0 44.0 44.0 30.0 74.0"},{"location":"evaluation_results/#case-studies","title":"Case studies\u00b6","text":""},{"location":"evaluation_results/#gold-standard","title":"Gold standard\u00b6","text":"<p>This dataset contains the 50 most recent articles from arxiv.org in both the cs.LG and stat.ML categories, between the dates 2022-10-24 and 2022-10-25 which had 570 search results. We select articles that belong to cs.LG <code>or</code> (cs.LG <code>and</code> stat.ML) category.</p> <p>\"Repository evaluation\" is performed on articles that provided links to their code repository and \"Paper evaluation\" is performed on all 50 articles by parsing the <code>.tex</code> files from their corresponding arXiv links. <code>reproscreener</code> is evaluated this <code>gold_standard</code> dataset and the results are shown below.</p>"},{"location":"evaluation_results/#repo-evaluation","title":"Repo evaluation\u00b6","text":""},{"location":"evaluation_results/#tex-evaluation","title":"Tex Evaluation\u00b6","text":""},{"location":"evaluation_results/#comparision-with-manual-evaluation","title":"Comparision with manual evaluation\u00b6","text":""},{"location":"evaluation_results/#repo-evaluation-comparison","title":"Repo evaluation comparison\u00b6","text":""},{"location":"evaluation_results/#tex-evaluation-comparison","title":"Tex evaluation comparison\u00b6","text":""},{"location":"funding/","title":"Funding","text":"<p>We thank The Center for Research and Education in AI and Learning (REAL@USC) for their funding and support towards this project.</p> <p>Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of The Center for Research and Education in AI and Learning (REAL@USC).</p> <p>This material is based upon work supported by the National Science Foundation under Grant No. OAC 2138773</p>"},{"location":"installation/","title":"Installation","text":"<p>To install <code>reproscreener</code>, we recommend using a virtual environment such as pyenv or conda, or installing it using pipx. We recommend using python version 3.9.13.</p> <p>All you need is to pip install <code>reproscreener</code>! </p> <pre><code>pip install reproscreener\n</code></pre> pyenvcondapipx <pre><code>pyenv install 3.9.13\npyenv virtualenv 3.9.13 reproscreener\npyenv local reproscreener\npip install reproscreener\n</code></pre> <pre><code>conda create -n reproscreener python=3.9.13\nconda activate reproscreener\npip install reproscreener\n</code></pre> <pre><code>pipx install reproscreener\n</code></pre>"},{"location":"license/","title":"License","text":""},{"location":"license/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2022 Adhithya Bhaskar, Victoria Stodden</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"manual_eval_compare/","title":"Case studies","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom IPython.display import display\nfrom pathlib import Path\nimport sys\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsys.path.append(str(Path.cwd().parent / \"src\"))\n\npath_corpus = Path(\"../case-studies/arxiv-corpus/mine50-andor/\")\nmanual_path = path_corpus / \"manual_eval.csv\"\n\ndtypes_repro = {'id': str, 'link_count': float, 'found_links': str}\nrepro_eval = pd.read_csv(path_corpus / 'output/repro_eval_tex.csv', dtype=dtypes_repro)\nrepro_eval = repro_eval.drop(columns=['title', 'index', 'affiliation'])\n</pre> import pandas as pd import numpy as np from IPython.display import display from pathlib import Path import sys import seaborn as sns from matplotlib import pyplot as plt sys.path.append(str(Path.cwd().parent / \"src\"))  path_corpus = Path(\"../case-studies/arxiv-corpus/mine50-andor/\") manual_path = path_corpus / \"manual_eval.csv\"  dtypes_repro = {'id': str, 'link_count': float, 'found_links': str} repro_eval = pd.read_csv(path_corpus / 'output/repro_eval_tex.csv', dtype=dtypes_repro) repro_eval = repro_eval.drop(columns=['title', 'index', 'affiliation']) <p>The first 5 articles where ReproScreener found potential code/repository links:</p> In\u00a0[2]: Copied! <pre>repro_eval_links = repro_eval[repro_eval['link_count'] &gt; 0][['id', 'link_count', 'found_links']]\nrepro_eval_links.head()\n</pre> repro_eval_links = repro_eval[repro_eval['link_count'] &gt; 0][['id', 'link_count', 'found_links']] repro_eval_links.head() Out[2]: id link_count found_links 4 1909.00931 3.0 ['https://github.com/lanwuwei/Twitter-URL-Corp... 8 2009.01947 1.0 ['https://gitlab.com/luciacyx/nm-adaptive-code... 9 2010.04261 1.0 ['https://github.com/goodfeli/dlbook_notation/'] 11 2011.11576 5.0 ['https://github.com/jpbrooks/conjecturing.', ... 12 2012.09302 1.0 ['https://github.com/ain-soph/trojanzoo}.'] <p>Below are the scores from the manually labeled dataset of 50 articles.</p> <ul> <li><code>article_link_avail</code>: Whether ink to the code/repository was able to be found in the article.</li> <li><code>pwc_link_avail</code>: Whether ink to the code/repository was able to be found in the Papers With Code (<code>pwc</code>) website.</li> <li><code>pwc_link_match</code>: Whether ink to the code/repository found in the Papers With Code (<code>pwc</code>) website matches the link found in the article (whether the previous 2 columns match or not).</li> <li><code>result_replication_code_avail</code>: Whether code to replicate the specific experiments presented in the article was available. This to measure that the code is not just a generic implementation of the model (part of the tool/package) but is specific to the experiments in the article. If code is not available, this defaults to false.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>from reproscreener.gdrive_downloader import gdrive_get_manual_eval\nmanual = gdrive_get_manual_eval(overwrite=True, manual_path=manual_path)\n</pre> from reproscreener.gdrive_downloader import gdrive_get_manual_eval manual = gdrive_get_manual_eval(overwrite=True, manual_path=manual_path) In\u00a0[4]: Copied! <pre># found_repo_df = get_manual_eval_urls(manual)\nmanual = manual.merge(found_repo_df[['paper', 'found_repo_url']], on='paper', how='left')\nmanual['found_repo_url'] = manual['found_repo_url'].apply(lambda x: x[0] if len(x) &gt; 0 else np.nan)\nfound_repo_df.dropna().head()\n</pre> # found_repo_df = get_manual_eval_urls(manual) manual = manual.merge(found_repo_df[['paper', 'found_repo_url']], on='paper', how='left') manual['found_repo_url'] = manual['found_repo_url'].apply(lambda x: x[0] if len(x) &gt; 0 else np.nan) found_repo_df.dropna().head() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 found_repo_df = get_manual_eval_urls(manual)\n      2 manual = manual.merge(found_repo_df[['paper', 'found_repo_url']], on='paper', how='left')\n      3 manual['found_repo_url'] = manual['found_repo_url'].apply(lambda x: x[0] if len(x) &gt; 0 else np.nan)\n\nNameError: name 'get_manual_eval_urls' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>manual_df_numerical = manual[['paper', 'article_link_avail', 'pwc_link_avail', 'pwc_link_match', 'result_replication_code_avail']]\n# manual_df_numerical = manual_df_numerical.fillna(0) # fill NaN with 0\ndtypes_manual = {'paper': str, 'article_link_avail': float, 'pwc_link_avail': float, 'pwc_link_match': float, 'result_replication_code_avail': float}\nmanual_df_numerical = manual_df_numerical.astype(dtypes_manual) # convert to int\nmanual_df_numerical[9:15]\n</pre> manual_df_numerical = manual[['paper', 'article_link_avail', 'pwc_link_avail', 'pwc_link_match', 'result_replication_code_avail']] # manual_df_numerical = manual_df_numerical.fillna(0) # fill NaN with 0 dtypes_manual = {'paper': str, 'article_link_avail': float, 'pwc_link_avail': float, 'pwc_link_match': float, 'result_replication_code_avail': float} manual_df_numerical = manual_df_numerical.astype(dtypes_manual) # convert to int manual_df_numerical[9:15] <p>Tally of manual evaluation of the 50 articles:</p> In\u00a0[\u00a0]: Copied! <pre>manual_df_numerical.sum(axis=0, numeric_only=True)\n</pre> manual_df_numerical.sum(axis=0, numeric_only=True) In\u00a0[\u00a0]: Copied! <pre>manual_vs_repro = manual_df_numerical.merge(repro_eval_links, left_on='paper', right_on='id', how='left')\n# manual_df_numerical.article_link_avail.sum(), manual_df_numerical.result_replication_code_avail.sum()\nprint(f\"Manual evaluation found links in {manual_vs_repro.article_link_avail.sum()} papers, ReproScreener found links in {(manual_vs_repro.link_count&gt;0).sum()} papers\")\n</pre> manual_vs_repro = manual_df_numerical.merge(repro_eval_links, left_on='paper', right_on='id', how='left') # manual_df_numerical.article_link_avail.sum(), manual_df_numerical.result_replication_code_avail.sum() print(f\"Manual evaluation found links in {manual_vs_repro.article_link_avail.sum()} papers, ReproScreener found links in {(manual_vs_repro.link_count&gt;0).sum()} papers\") In\u00a0[\u00a0]: Copied! <pre>from repo_checker import tally_checks_by_paper, get_downloaded_repos, dict_files_to_list, all_checks_by_paper\n\ndownloaded_repos = get_downloaded_repos(path_corpus)\nmanual['repo_downloaded'] = manual['paper'].isin(downloaded_repos)\nmanual_disp = manual[manual['repo_downloaded'] == True][['paper', 'repo_downloaded', 'found_repo_url']]\n# manual_disp_all = manual_disp.copy()\nmanual_disp[\"checks\"] = manual_disp[\"paper\"].apply(lambda x: tally_checks_by_paper(path_corpus, x, only_found=True, verbose=False))\nmanual_disp[[\"tally\", \"found\"]] = pd.DataFrame(manual_disp[\"checks\"].tolist(), index=manual_disp.index)\nmanual_disp = manual_disp.join(manual_disp[\"tally\"].apply(pd.Series, dtype=float).rename(columns=lambda x: f\"tally_{x}\"))\nmanual_disp = manual_disp.join(manual_disp[\"found\"].apply(pd.Series, dtype=object).apply(dict_files_to_list))\nmanual_disp = manual_disp.drop(columns=[\"tally\", \"found\", \"checks\"])\n</pre> from repo_checker import tally_checks_by_paper, get_downloaded_repos, dict_files_to_list, all_checks_by_paper  downloaded_repos = get_downloaded_repos(path_corpus) manual['repo_downloaded'] = manual['paper'].isin(downloaded_repos) manual_disp = manual[manual['repo_downloaded'] == True][['paper', 'repo_downloaded', 'found_repo_url']] # manual_disp_all = manual_disp.copy() manual_disp[\"checks\"] = manual_disp[\"paper\"].apply(lambda x: tally_checks_by_paper(path_corpus, x, only_found=True, verbose=False)) manual_disp[[\"tally\", \"found\"]] = pd.DataFrame(manual_disp[\"checks\"].tolist(), index=manual_disp.index) manual_disp = manual_disp.join(manual_disp[\"tally\"].apply(pd.Series, dtype=float).rename(columns=lambda x: f\"tally_{x}\")) manual_disp = manual_disp.join(manual_disp[\"found\"].apply(pd.Series, dtype=object).apply(dict_files_to_list)) manual_disp = manual_disp.drop(columns=[\"tally\", \"found\", \"checks\"]) In\u00a0[\u00a0]: Copied! <pre>manual_disp\n</pre> manual_disp In\u00a0[\u00a0]: Copied! <pre>df = manual_disp[['paper','wrapper_script', 'parsed_readme', 'software_dependencies']].copy()\n# Add a new column to identify the original column\ndf['original_column'] = df.index\n\n# Melt the dataframe to create a new row for each unique value in the current column\ndf_melted = pd.melt(df, id_vars=['original_column'])\n\n# Drop any rows with NaN values\ndf_melted.dropna(inplace=True)\n\n# Create a new column for each unique value in the current column\ndf_melted = pd.concat([df_melted.drop('value', axis=1),\n                       df_melted['value'].str.split(',', expand=True)],\n                      axis=1)\nif 'value' in df_melted.columns:\n    df_melted.drop('value', axis=1, inplace=True)\n# Drop the 'value' column\n# df_melted.drop('value', axis=1, inplace=True)\n\n# Pivot the dataframe to transpose it\ndf_pivoted = pd.pivot_table(df_melted, index=['variable', 'original_column'],\n                            aggfunc=lambda x: ' '.join(str(v) for v in x))\n\n# Reset the index to create a new column for each original column\ndf_pivoted.reset_index(level=1, inplace=True)\n\n# Create a new column for the original column\ndf_pivoted['original_column'] = df_pivoted['original_column'].astype(str).astype(int)\n\n# Pivot the dataframe again to put the original column in its own column\ndf_final = pd.pivot_table(df_pivoted, index='original_column',\n                          columns='variable', values=0)\n\n# Rename the columns\ndf_final.columns = [f\"{col}_val\" for col in df_final.columns]\n\n# Reset the index\ndf_final.reset_index(inplace=True)\ndf_final\n</pre> df = manual_disp[['paper','wrapper_script', 'parsed_readme', 'software_dependencies']].copy() # Add a new column to identify the original column df['original_column'] = df.index  # Melt the dataframe to create a new row for each unique value in the current column df_melted = pd.melt(df, id_vars=['original_column'])  # Drop any rows with NaN values df_melted.dropna(inplace=True)  # Create a new column for each unique value in the current column df_melted = pd.concat([df_melted.drop('value', axis=1),                        df_melted['value'].str.split(',', expand=True)],                       axis=1) if 'value' in df_melted.columns:     df_melted.drop('value', axis=1, inplace=True) # Drop the 'value' column # df_melted.drop('value', axis=1, inplace=True)  # Pivot the dataframe to transpose it df_pivoted = pd.pivot_table(df_melted, index=['variable', 'original_column'],                             aggfunc=lambda x: ' '.join(str(v) for v in x))  # Reset the index to create a new column for each original column df_pivoted.reset_index(level=1, inplace=True)  # Create a new column for the original column df_pivoted['original_column'] = df_pivoted['original_column'].astype(str).astype(int)  # Pivot the dataframe again to put the original column in its own column df_final = pd.pivot_table(df_pivoted, index='original_column',                           columns='variable', values=0)  # Rename the columns df_final.columns = [f\"{col}_val\" for col in df_final.columns]  # Reset the index df_final.reset_index(inplace=True) df_final In\u00a0[\u00a0]: Copied! <pre>manual_disp\n</pre> manual_disp In\u00a0[\u00a0]: Copied! <pre># Merge manual_df_numerical with repro_eval using a left join\nall_papers_df = manual_df_numerical.merge(repro_eval, left_on='paper', right_on='id', how='left')\n\n# Fill NaN values in link_count with 0 and found_links with an empty string\nall_papers_df['link_count'] = all_papers_df['link_count'].fillna(0)\nall_papers_df['found_links'] = all_papers_df['found_links'].fillna(\"\")\n\n# Set the index to 'paper' and remove unnecessary columns\nrepro_eval_plot = all_papers_df.drop(columns=['id', 'title', 'index', 'affiliation', 'found_links', 'pwc_link_match', 'result_replication_code_avail', 'article_link_avail', 'pwc_link_avail', 'results']).set_index('paper').T\n\n# Plot the heatmap\nplt.figure(figsize=(20, 5), tight_layout={\"pad\": 1.5})\nsns.heatmap(repro_eval_plot, cbar=False, cmap=sns.cubehelix_palette(start=2, rot=0, dark=0, light=.85, as_cmap=True))\nsns.set_style(\"whitegrid\")\nsns.set(font_scale=1.5)\nplt.subplots_adjust(top=0.95, left=0.15, right=0.95)\nplt.xlabel(\"\")\nplt.savefig(\"../heatmap_manual_eval.png\", dpi=900, bbox_inches=\"tight\")\nplt.show()\n</pre> # Merge manual_df_numerical with repro_eval using a left join all_papers_df = manual_df_numerical.merge(repro_eval, left_on='paper', right_on='id', how='left')  # Fill NaN values in link_count with 0 and found_links with an empty string all_papers_df['link_count'] = all_papers_df['link_count'].fillna(0) all_papers_df['found_links'] = all_papers_df['found_links'].fillna(\"\")  # Set the index to 'paper' and remove unnecessary columns repro_eval_plot = all_papers_df.drop(columns=['id', 'title', 'index', 'affiliation', 'found_links', 'pwc_link_match', 'result_replication_code_avail', 'article_link_avail', 'pwc_link_avail', 'results']).set_index('paper').T  # Plot the heatmap plt.figure(figsize=(20, 5), tight_layout={\"pad\": 1.5}) sns.heatmap(repro_eval_plot, cbar=False, cmap=sns.cubehelix_palette(start=2, rot=0, dark=0, light=.85, as_cmap=True)) sns.set_style(\"whitegrid\") sns.set(font_scale=1.5) plt.subplots_adjust(top=0.95, left=0.15, right=0.95) plt.xlabel(\"\") plt.savefig(\"../heatmap_manual_eval.png\", dpi=900, bbox_inches=\"tight\") plt.show()"},{"location":"manual_eval_compare/#case-studies","title":"Case studies\u00b6","text":"<ol> <li><p>Gold standard: <code>mine-50-andor</code> contains the 50 most recent articles from arxiv.org in both the cs.LG and stat.ML categories, between the dates 2022-10-24 and 2022-10-25 and contained 570 search results at the time of the dataset creation. We select articles that belong to cs.LG <code>or</code> (cs.LG <code>and</code> stat.ML) category.</p> </li> <li><p><code>mine50</code> contains the 50 most recent articles from arxiv.org in both the cs.LG and stat.ML categories, between the dates 2022-10-24 and 2022-10-25 and contained 570 search results at the time of the dataset creation. The search result is sorted by date in descending order. (Note: The date being queried for is the last updated date and not the date of paper submission)</p> </li> <li><p><code>mine50-csLG</code> contains the results using the same method as <code>mine50</code> but without looking for articles in both cs.LG and stat.ML.</p> </li> </ol>"},{"location":"manual_eval_compare/#evaluating-reproscreener-on-the-manually-labeled-gold-standard-dataset","title":"Evaluating ReproScreener on the manually labeled (gold standard) dataset\u00b6","text":""},{"location":"manual_eval_compare/#performing-file-structure-and-dependency-checks-on-downloaded-repositories-of-the-manually-labeled-articles","title":"Performing file structure and dependency checks on downloaded repositories of the manually labeled articles\u00b6","text":""},{"location":"manual_eval_compare/#visualizations","title":"Visualizations\u00b6","text":""},{"location":"manual_eval_compare/#paper-evaluation-results","title":"Paper evaluation results\u00b6","text":""},{"location":"manual_evaluation_datasets/","title":"Manual Evaluation Explorer","text":"<p>The manual evaluation datasets are available on Hugging Face - adbX/reproscreener_manual_evaluations.</p>"},{"location":"manual_evaluation_explorer/","title":"Manual Evaluation Explorer","text":"In\u00a0[1]: import Copied! <pre>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom textwrap import wrap\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nimport sys\nimport os\n\n# Add src to path for importing colors module\nsys.path.append(os.path.abspath('../src'))\nfrom reproscreener.colors import (\n    CATEGORICAL_3_LIGHT_OPTION_1, CATEGORICAL_3_LIGHT_OPTION_2, CATEGORICAL_3_LIGHT_OPTION_3, \n    CATEGORICAL_3_LIGHT_OPTION_4, CATEGORICAL_3_LIGHT_OPTION_5, CATEGORICAL_2_LIGHT_OPTION_1, \n    CATEGORICAL_2_LIGHT_OPTION_2, CATEGORICAL_2_LIGHT_OPTION_3, CATEGORICAL_2_LIGHT_OPTION_4, \n    CATEGORICAL_2_LIGHT_OPTION_5, CATEGORICAL_3_DARK_OPTION_1, CATEGORICAL_3_DARK_OPTION_2, \n    CATEGORICAL_3_DARK_OPTION_3, CATEGORICAL_3_DARK_OPTION_4, CATEGORICAL_3_DARK_OPTION_5\n)\nall_columns = [\n  \"problem\",\n  \"problem_agreement\",\n  \"objective\",\n  \"objective_agreement\",\n  \"research_method\",\n  \"research_method_agreement\",\n  \"research_questions\",\n  \"research_questions_agreement\",\n  \"pseudocode\",\n  \"pseudocode_agreement\",\n  \"dataset\",\n  \"dataset_agreement\",\n  \"hypothesis\",\n  \"hypothesis_agreement\",\n  \"prediction\",\n  \"prediction_agreement\",\n  \"code_available\",\n  \"code_available_agreement\",\n  \"software_dependencies\",\n  \"software_dependencies_agreement\",\n  \"experiment_setup\",\n  \"experiment_setup_agreement\"\n]\nmetric_columns = [col for col in all_columns if not col.endswith(\"_agreement\")]\n\nrepro_manuscript_metrics = [\"problem\", \"objective\", \"research_method\", \"research_questions\", \"dataset\", \"hypothesis\", \"prediction\", \"code_available\", \"experiment_setup\"]\nrepro_manuscript_metrics_display_map = {\n    \"problem\": \"Research problem\",\n    \"objective\": \"Objective/ Goal\",\n    \"research_method\": \"Research method\",\n    \"research_questions\": \"Research questions\",\n    \"hypothesis\": \"Hypothesis\",\n    \"prediction\": \"Prediction\",\n    \"code_available\": \"Code available\",\n    \"dataset\": \"Dataset\",\n    \"experiment_setup\": \"Experimental setup\",\n    # \"software_dependencies\": \"Software dependencies\",\n}\ngundersen_metrics = [\"problem\", \"objective\", \"research_method\", \"research_questions\", \"pseudocode\", \"training_data\", \"validation_data\", \"test_data\", \"hypothesis\", \"prediction\", \"method_source_code\", \"hardware_specifications\",\"software_dependencies\", \"experiment_setup\"]\ngundersen_metrics_display_map = {\n    \"problem\": \"Research problem\",\n    \"objective\": \"Objective/ Goal\",\n    \"research_method\": \"Research method\",\n    \"research_questions\": \"Research questions\",\n    \"pseudocode\": \"Pseudocode\",\n    \"training_data\": \"Training data\",\n    \"validation_data\": \"Validation data\",\n    \"test_data\": \"Test data\",\n    \"hypothesis\": \"Hypothesis\",\n    \"prediction\": \"Prediction\",\n    \"method_source_code\": \"Method source code\",\n    \"hardware_specifications\": \"Hardware specifications\",\n    \"software_dependencies\": \"Software dependencies\",\n    \"experiment_setup\": \"Experimental setup\",\n}\n</pre> import pandas as pd import numpy as np import seaborn as sns from matplotlib.colors import ListedColormap from textwrap import wrap import matplotlib.pyplot as plt from matplotlib.patches import Patch import sys import os  # Add src to path for importing colors module sys.path.append(os.path.abspath('../src')) from reproscreener.colors import (     CATEGORICAL_3_LIGHT_OPTION_1, CATEGORICAL_3_LIGHT_OPTION_2, CATEGORICAL_3_LIGHT_OPTION_3,      CATEGORICAL_3_LIGHT_OPTION_4, CATEGORICAL_3_LIGHT_OPTION_5, CATEGORICAL_2_LIGHT_OPTION_1,      CATEGORICAL_2_LIGHT_OPTION_2, CATEGORICAL_2_LIGHT_OPTION_3, CATEGORICAL_2_LIGHT_OPTION_4,      CATEGORICAL_2_LIGHT_OPTION_5, CATEGORICAL_3_DARK_OPTION_1, CATEGORICAL_3_DARK_OPTION_2,      CATEGORICAL_3_DARK_OPTION_3, CATEGORICAL_3_DARK_OPTION_4, CATEGORICAL_3_DARK_OPTION_5 ) all_columns = [   \"problem\",   \"problem_agreement\",   \"objective\",   \"objective_agreement\",   \"research_method\",   \"research_method_agreement\",   \"research_questions\",   \"research_questions_agreement\",   \"pseudocode\",   \"pseudocode_agreement\",   \"dataset\",   \"dataset_agreement\",   \"hypothesis\",   \"hypothesis_agreement\",   \"prediction\",   \"prediction_agreement\",   \"code_available\",   \"code_available_agreement\",   \"software_dependencies\",   \"software_dependencies_agreement\",   \"experiment_setup\",   \"experiment_setup_agreement\" ] metric_columns = [col for col in all_columns if not col.endswith(\"_agreement\")]  repro_manuscript_metrics = [\"problem\", \"objective\", \"research_method\", \"research_questions\", \"dataset\", \"hypothesis\", \"prediction\", \"code_available\", \"experiment_setup\"] repro_manuscript_metrics_display_map = {     \"problem\": \"Research problem\",     \"objective\": \"Objective/ Goal\",     \"research_method\": \"Research method\",     \"research_questions\": \"Research questions\",     \"hypothesis\": \"Hypothesis\",     \"prediction\": \"Prediction\",     \"code_available\": \"Code available\",     \"dataset\": \"Dataset\",     \"experiment_setup\": \"Experimental setup\",     # \"software_dependencies\": \"Software dependencies\", } gundersen_metrics = [\"problem\", \"objective\", \"research_method\", \"research_questions\", \"pseudocode\", \"training_data\", \"validation_data\", \"test_data\", \"hypothesis\", \"prediction\", \"method_source_code\", \"hardware_specifications\",\"software_dependencies\", \"experiment_setup\"] gundersen_metrics_display_map = {     \"problem\": \"Research problem\",     \"objective\": \"Objective/ Goal\",     \"research_method\": \"Research method\",     \"research_questions\": \"Research questions\",     \"pseudocode\": \"Pseudocode\",     \"training_data\": \"Training data\",     \"validation_data\": \"Validation data\",     \"test_data\": \"Test data\",     \"hypothesis\": \"Hypothesis\",     \"prediction\": \"Prediction\",     \"method_source_code\": \"Method source code\",     \"hardware_specifications\": \"Hardware specifications\",     \"software_dependencies\": \"Software dependencies\",     \"experiment_setup\": \"Experimental setup\", } In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\n\nfrom highlight_text import fig_text\n# font_dir = '/mnt/c/Users/adb/Desktop/stuff/fonts/'\n# font_dir = '/Users/adb/Library/Mobile Documents/com~apple~CloudDocs/fonts/'\n# font_name = 'IBMPlexSans-Regular.ttf'\n# font_name = 'Inter-Bold.ttf'\n# # font_name = 'FiraSans-Regular.ttf'\n# from matplotlib import font_manager\n# font_manager.fontManager.addfont('/Users/adb/Library/Fonts/AlegreyaSans-Medium.ttf')\n# prop = font_manager.FontProperties(fname='/Users/adb/Library/Fonts/AlegreyaSans-Medium.ttf')\n# plt.rcParams['font.sans-serif'] = prop.get_name()\n\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Inter','Helvetica','IBM Plex Sans', 'Inter', 'Fira Sans']\n# plt.rcParams['font.weight'] = 'medium'\n# plt.rcParams['font.size'] = 12\nplt.rcParams['text.color'] = '#1E1E1E'\n\nthesis_figures_dir = \"../../USCthesis/figures/\"\npresentation_figures_dir = \"../../qualifying_exam_slides/present/figures/\"\ncurrent_figures_dir = \"../reports/figures/\"\n</pre> import matplotlib.pyplot as plt  from highlight_text import fig_text # font_dir = '/mnt/c/Users/adb/Desktop/stuff/fonts/' # font_dir = '/Users/adb/Library/Mobile Documents/com~apple~CloudDocs/fonts/' # font_name = 'IBMPlexSans-Regular.ttf' # font_name = 'Inter-Bold.ttf' # # font_name = 'FiraSans-Regular.ttf' # from matplotlib import font_manager # font_manager.fontManager.addfont('/Users/adb/Library/Fonts/AlegreyaSans-Medium.ttf') # prop = font_manager.FontProperties(fname='/Users/adb/Library/Fonts/AlegreyaSans-Medium.ttf') # plt.rcParams['font.sans-serif'] = prop.get_name()  plt.rcParams['font.family'] = 'sans-serif' plt.rcParams['font.sans-serif'] = ['Inter','Helvetica','IBM Plex Sans', 'Inter', 'Fira Sans'] # plt.rcParams['font.weight'] = 'medium' # plt.rcParams['font.size'] = 12 plt.rcParams['text.color'] = '#1E1E1E'  thesis_figures_dir = \"../../USCthesis/figures/\" presentation_figures_dir = \"../../qualifying_exam_slides/present/figures/\" current_figures_dir = \"../reports/figures/\" In\u00a0[3]: Copied! <pre>df_manuscript_manual = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/manuscript.csv\")\ndf_manuscript_manual.set_index(\"paper_id\", inplace=True)\ndf_manuscript_manual = df_manuscript_manual.drop(\n    columns=[\"evaluation_type\", \"source_file\"]\n    + [col for col in df_manuscript_manual.columns if \"_description\" in col]\n)\ndf_manuscript_manual = df_manuscript_manual.rename(columns={\"code_available_in_article\": \"code_available\"})\ndf_manuscript_manual.head()\n</pre> df_manuscript_manual = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/manuscript.csv\") df_manuscript_manual.set_index(\"paper_id\", inplace=True) df_manuscript_manual = df_manuscript_manual.drop(     columns=[\"evaluation_type\", \"source_file\"]     + [col for col in df_manuscript_manual.columns if \"_description\" in col] ) df_manuscript_manual = df_manuscript_manual.rename(columns={\"code_available_in_article\": \"code_available\"}) df_manuscript_manual.head() Out[3]: paper_url notes empirical_dataset code_available papers_with_code_link_available papers_with_code_link_matches result_replication_code_available is_package has_wrapper_scripts hardware_specifications_provided ... will_it_reproduce_desc problem objective research_method research_questions pseudocode dataset hypothesis prediction experiment_setup paper_id 1606.04671 https://arxiv.org/pdf/1606.04671.pdf No code or data released NaN False False False NaN NaN False NaN ... NaN False False False False False False False False True 1903.09668 https://arxiv.org/pdf/1903.09668.pdf No code or data released NaN False False False NaN NaN False NaN ... NaN False False False False True False False False True 1904.10554 https://arxiv.org/pdf/1904.10554.pdf No code or data released NaN False False False NaN NaN False NaN ... NaN False False False False True False False False True 1908.05659 https://arxiv.org/pdf/1908.05659.pdf 90 page review, no experiments, mostly math NaN False False False NaN NaN False NaN ... NaN False False False False False False False False False 1909.00931 https://arxiv.org/pdf/1909.00931.pdf NaN NaN True True True NaN NaN False True ... NaN True False False False True True True False True <p>5 rows \u00d7 28 columns</p> In\u00a0[4]: Copied! <pre>df_manuscript_regex = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\")\ndf_manuscript_regex = df_manuscript_regex.rename(columns={\"method_source_code\": \"code_available\", \"id\": \"paper_id\"})\ndf_manuscript_regex.set_index(\"paper_id\", inplace=True)\ndf_manuscript_regex['dataset'] = df_manuscript_regex['training_data'].astype(bool) + df_manuscript_regex['test_data'].astype(bool) + df_manuscript_regex['validation_data'].astype(bool) + df_manuscript_regex['training_data'].astype(bool)\ndf_manuscript_regex = df_manuscript_regex.drop(columns=[\"index\",\"training_data\", \"test_data\", \"validation_data\", \"title\"])\ndf_manuscript_regex[metric_columns] = df_manuscript_regex[metric_columns].astype(bool)\n</pre> df_manuscript_regex = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\") df_manuscript_regex = df_manuscript_regex.rename(columns={\"method_source_code\": \"code_available\", \"id\": \"paper_id\"}) df_manuscript_regex.set_index(\"paper_id\", inplace=True) df_manuscript_regex['dataset'] = df_manuscript_regex['training_data'].astype(bool) + df_manuscript_regex['test_data'].astype(bool) + df_manuscript_regex['validation_data'].astype(bool) + df_manuscript_regex['training_data'].astype(bool) df_manuscript_regex = df_manuscript_regex.drop(columns=[\"index\",\"training_data\", \"test_data\", \"validation_data\", \"title\"]) df_manuscript_regex[metric_columns] = df_manuscript_regex[metric_columns].astype(bool) In\u00a0[5]: Copied! <pre>common_idx = df_manuscript_manual.index.intersection(df_manuscript_regex.index)\nmanual_bool = df_manuscript_manual.loc[common_idx, metric_columns].astype(bool)\nregex_bool = df_manuscript_regex.loc[common_idx, metric_columns].astype(bool)\n\nresults_manuscript = {}\nfor manuscript_metric in metric_columns:\n    manuscript_regex_vals = regex_bool[manuscript_metric]\n    manuscript_manual_vals = manual_bool[manuscript_metric]\n\n    results_manuscript[manuscript_metric] = {\n        'regex_sum': int(manuscript_regex_vals.sum()),\n        'manual_sum': int(manuscript_manual_vals.sum()),\n        'regex_proportion': float(manuscript_regex_vals.mean()),\n        'regex_manual_agreement': float((manuscript_regex_vals == manuscript_manual_vals).mean()),\n        'manual_proportion': float(manuscript_manual_vals.mean()),\n        'total_n': int(len(manuscript_regex_vals))\n    }\n\nmanuscript_results_df = pd.DataFrame(results_manuscript).T\n\ntab_decimal_manu = manuscript_results_df\ntab_percent_manu = manuscript_results_df.copy()\ntab_percent_manu['regex_proportion'] = tab_percent_manu['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent_manu['manual_proportion'] = tab_percent_manu['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent_manu['regex_manual_agreement'] = tab_percent_manu['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent_manu\n</pre> common_idx = df_manuscript_manual.index.intersection(df_manuscript_regex.index) manual_bool = df_manuscript_manual.loc[common_idx, metric_columns].astype(bool) regex_bool = df_manuscript_regex.loc[common_idx, metric_columns].astype(bool)  results_manuscript = {} for manuscript_metric in metric_columns:     manuscript_regex_vals = regex_bool[manuscript_metric]     manuscript_manual_vals = manual_bool[manuscript_metric]      results_manuscript[manuscript_metric] = {         'regex_sum': int(manuscript_regex_vals.sum()),         'manual_sum': int(manuscript_manual_vals.sum()),         'regex_proportion': float(manuscript_regex_vals.mean()),         'regex_manual_agreement': float((manuscript_regex_vals == manuscript_manual_vals).mean()),         'manual_proportion': float(manuscript_manual_vals.mean()),         'total_n': int(len(manuscript_regex_vals))     }  manuscript_results_df = pd.DataFrame(results_manuscript).T  tab_decimal_manu = manuscript_results_df tab_percent_manu = manuscript_results_df.copy() tab_percent_manu['regex_proportion'] = tab_percent_manu['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent_manu['manual_proportion'] = tab_percent_manu['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent_manu['regex_manual_agreement'] = tab_percent_manu['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent_manu Out[5]: regex_sum manual_sum regex_proportion regex_manual_agreement manual_proportion total_n problem 44.0 15.0 88% 38% 30% 50.0 objective 39.0 4.0 78% 30% 8% 50.0 research_method 43.0 10.0 86% 30% 20% 50.0 research_questions 45.0 3.0 90% 16% 6% 50.0 pseudocode 7.0 22.0 14% 62% 44% 50.0 dataset 19.0 31.0 38% 48% 62% 50.0 hypothesis 21.0 8.0 42% 62% 16% 50.0 prediction 34.0 0.0 68% 32% 0% 50.0 code_available 23.0 22.0 46% 82% 44% 50.0 software_dependencies 0.0 14.0 0% 72% 28% 50.0 experiment_setup 14.0 37.0 28% 54% 74% 50.0 In\u00a0[6]: Copied! <pre># Load regex results for abstracts computed locally\ndf_abstract_regex = pd.read_csv(\"../reports/tables/abstract_regex_gs.csv\")\ndf_abstract_regex = df_abstract_regex.set_index(\"paper_id\")\n# Ensure boolean dtype for metrics\navailable_cols = [c for c in metric_columns if c in df_abstract_regex.columns]\ndf_abstract_regex[available_cols] = df_abstract_regex[available_cols].astype(bool)\ndf_abstract_regex.head()\n</pre> # Load regex results for abstracts computed locally df_abstract_regex = pd.read_csv(\"../reports/tables/abstract_regex_gs.csv\") df_abstract_regex = df_abstract_regex.set_index(\"paper_id\") # Ensure boolean dtype for metrics available_cols = [c for c in metric_columns if c in df_abstract_regex.columns] df_abstract_regex[available_cols] = df_abstract_regex[available_cols].astype(bool) df_abstract_regex.head() Out[6]: problem objective research_method research_questions pseudocode dataset hypothesis prediction code_available software_dependencies experiment_setup paper_id 1606.04671 False False False False False False False False False False False 1903.09668 False False True True False False False True False False False 1904.10554 False False False True False False False False False False False 1908.05659 False False False True False False False False False False False 1909.00931 False False True True False False False False False False False In\u00a0[7]: Copied! <pre># Load GPT agreement for abstracts and derive manual columns from agreement\ndf_abs_gpt_agreement = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")\n\n# Clean up columns - exclude metadata and description columns\ndf_abs_gpt_agreement = df_abs_gpt_agreement.set_index(\"paper_id\")\ndf_abs_gpt_agreement = df_abs_gpt_agreement.drop(\n    columns=[\"evaluation_type\", \"source_file\"]\n    + [col for col in df_abs_gpt_agreement.columns if \"_description\" in col]\n)\n\n# Remove gpt_ prefix from column names\ndf_abs_gpt_agreement = df_abs_gpt_agreement.rename(\n    columns={\n        col: col.replace(\"gpt_\", \"\")\n        for col in df_abs_gpt_agreement.columns\n        if col.startswith(\"gpt_\")\n    }\n)\n\n# Compute manual_&lt;metric&gt; using agreement flip rule\nfor abs_metric in metric_columns:\n    abs_agreement_col = f\"{abs_metric}_agreement\"\n    if abs_metric in df_abs_gpt_agreement.columns and abs_agreement_col in df_abs_gpt_agreement.columns:\n        abs_gpt_vals = df_abs_gpt_agreement[abs_metric].astype(bool)\n        abs_agreement_vals = df_abs_gpt_agreement[abs_agreement_col]\n        abs_manual_vals = np.where(abs_agreement_vals == 1, abs_gpt_vals, ~abs_gpt_vals)\n        df_abs_gpt_agreement[f\"manual_{abs_metric}\"] = abs_manual_vals.astype(bool)\n</pre> # Load GPT agreement for abstracts and derive manual columns from agreement df_abs_gpt_agreement = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")  # Clean up columns - exclude metadata and description columns df_abs_gpt_agreement = df_abs_gpt_agreement.set_index(\"paper_id\") df_abs_gpt_agreement = df_abs_gpt_agreement.drop(     columns=[\"evaluation_type\", \"source_file\"]     + [col for col in df_abs_gpt_agreement.columns if \"_description\" in col] )  # Remove gpt_ prefix from column names df_abs_gpt_agreement = df_abs_gpt_agreement.rename(     columns={         col: col.replace(\"gpt_\", \"\")         for col in df_abs_gpt_agreement.columns         if col.startswith(\"gpt_\")     } )  # Compute manual_ using agreement flip rule for abs_metric in metric_columns:     abs_agreement_col = f\"{abs_metric}_agreement\"     if abs_metric in df_abs_gpt_agreement.columns and abs_agreement_col in df_abs_gpt_agreement.columns:         abs_gpt_vals = df_abs_gpt_agreement[abs_metric].astype(bool)         abs_agreement_vals = df_abs_gpt_agreement[abs_agreement_col]         abs_manual_vals = np.where(abs_agreement_vals == 1, abs_gpt_vals, ~abs_gpt_vals)         df_abs_gpt_agreement[f\"manual_{abs_metric}\"] = abs_manual_vals.astype(bool) In\u00a0[8]: Copied! <pre># Build manual abstract evaluations from df_abs_gpt_agreement manual_ columns\nabs_manual_cols_map = {\n    f\"manual_{m}\": m for m in metric_columns if f\"manual_{m}\" in df_abs_gpt_agreement.columns\n}\ndf_abstract_manual = df_abs_gpt_agreement[list(abs_manual_cols_map.keys())].rename(columns=abs_manual_cols_map)\n# Ensure boolean dtype\ndf_abstract_manual = df_abstract_manual.astype(bool)\n</pre> # Build manual abstract evaluations from df_abs_gpt_agreement manual_ columns abs_manual_cols_map = {     f\"manual_{m}\": m for m in metric_columns if f\"manual_{m}\" in df_abs_gpt_agreement.columns } df_abstract_manual = df_abs_gpt_agreement[list(abs_manual_cols_map.keys())].rename(columns=abs_manual_cols_map) # Ensure boolean dtype df_abstract_manual = df_abstract_manual.astype(bool) In\u00a0[9]: Copied! <pre># Align indices and compute agreement for abstracts\ncommon_idx_abs = df_abstract_manual.index.intersection(df_abstract_regex.index)\nmanual_bool_abs = df_abstract_manual.loc[common_idx_abs]\nregex_bool_abs = df_abstract_regex.loc[common_idx_abs]\n\n# abstract_metrics = [m for m in metric_columns if m in manual_bool_abs.columns and m in regex_bool_abs.columns]\n\nresults_abs = {}\nfor metric in metric_columns:\n    regex_vals = regex_bool_abs[metric].astype(bool)\n    manual_vals = manual_bool_abs[metric].astype(bool)\n\n    results_abs[metric] = {\n        'regex_sum': int(regex_vals.sum()),\n        'manual_sum': int(manual_vals.sum()),\n        'regex_proportion': float(regex_vals.mean()),\n        'regex_manual_agreement': float((regex_vals == manual_vals).mean()),\n        'manual_proportion': float(manual_vals.mean()),\n        'total_n': int(len(regex_vals)),\n    }\n\nabstract_results_regex_df = pd.DataFrame(results_abs).T\n\ntab_decimal_abs = abstract_results_regex_df\ntab_percent_abs = abstract_results_regex_df.copy()\ntab_percent_abs['regex_proportion'] = tab_percent_abs['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent_abs['manual_proportion'] = tab_percent_abs['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent_abs['regex_manual_agreement'] = tab_percent_abs['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\n\ntab_percent_abs\n</pre> # Align indices and compute agreement for abstracts common_idx_abs = df_abstract_manual.index.intersection(df_abstract_regex.index) manual_bool_abs = df_abstract_manual.loc[common_idx_abs] regex_bool_abs = df_abstract_regex.loc[common_idx_abs]  # abstract_metrics = [m for m in metric_columns if m in manual_bool_abs.columns and m in regex_bool_abs.columns]  results_abs = {} for metric in metric_columns:     regex_vals = regex_bool_abs[metric].astype(bool)     manual_vals = manual_bool_abs[metric].astype(bool)      results_abs[metric] = {         'regex_sum': int(regex_vals.sum()),         'manual_sum': int(manual_vals.sum()),         'regex_proportion': float(regex_vals.mean()),         'regex_manual_agreement': float((regex_vals == manual_vals).mean()),         'manual_proportion': float(manual_vals.mean()),         'total_n': int(len(regex_vals)),     }  abstract_results_regex_df = pd.DataFrame(results_abs).T  tab_decimal_abs = abstract_results_regex_df tab_percent_abs = abstract_results_regex_df.copy() tab_percent_abs['regex_proportion'] = tab_percent_abs['regex_proportion'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent_abs['manual_proportion'] = tab_percent_abs['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent_abs['regex_manual_agreement'] = tab_percent_abs['regex_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')  tab_percent_abs Out[9]: regex_sum manual_sum regex_proportion regex_manual_agreement manual_proportion total_n problem 12.0 41.0 24% 38% 82% 50.0 objective 4.0 44.0 8% 20% 88% 50.0 research_method 20.0 22.0 40% 56% 44% 50.0 research_questions 13.0 4.0 26% 70% 8% 50.0 pseudocode 0.0 0.0 0% 100% 0% 50.0 dataset 1.0 6.0 2% 86% 12% 50.0 hypothesis 0.0 6.0 0% 88% 12% 50.0 prediction 4.0 9.0 8% 82% 18% 50.0 code_available 2.0 4.0 4% 96% 8% 50.0 software_dependencies 0.0 1.0 0% 98% 2% 50.0 experiment_setup 1.0 12.0 2% 74% 24% 50.0 In\u00a0[10]: Copied! <pre>df_gpt_agreement_manu = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")\n\n# Clean up columns - exclude metadata and description columns\n# Make \"paper_id\" the index\ndf_gpt_agreement_manu = df_gpt_agreement_manu.set_index(\"paper_id\")\n\ndf_gpt_agreement_manu = df_gpt_agreement_manu.drop(\n    columns=[\"evaluation_type\", \"source_file\"]\n    + [col for col in df_gpt_agreement_manu.columns if \"_description\" in col]\n)\n\n# Remove gpt_ prefix from column names\ndf_gpt_agreement_manu = df_gpt_agreement_manu.rename(\n    columns={\n        col: col.replace(\"gpt_\", \"\")\n        for col in df_gpt_agreement_manu.columns\n        if col.startswith(\"gpt_\")\n    }\n)\n\nresults = {}\nfor manu_metric in metric_columns:\n    gpt_col = manu_metric\n    agreement_col = f\"{manu_metric}_agreement\"\n\n    if agreement_col in df_gpt_agreement_manu.columns:\n        gpt_vals = df_gpt_agreement_manu[gpt_col].astype(bool)\n        agreement_vals = df_gpt_agreement_manu[agreement_col]\n\n        # Calculate revised manual evaluation: keep GPT when agreement=1, invert when agreement=0\n        manual_vals_gpt_abs = np.where(agreement_vals == 1, gpt_vals, ~gpt_vals)\n\n        # Add manual_vals_gpt_abs to the agreement_gpt DataFrame\n        df_gpt_agreement_manu[f\"manual_{manu_metric}\"] = manual_vals_gpt_abs.astype(bool)\n\n        results[manu_metric] = {\n            'gpt_sum': gpt_vals.sum(),\n            'manual_sum': manual_vals_gpt_abs.sum(),\n            'gpt_proportion': gpt_vals.mean(),\n            'gpt_manual_agreement': agreement_vals.mean(),\n            'manual_proportion': manual_vals_gpt_abs.mean(),\n            'total_n': len(gpt_vals)\n        }\nabstract_results_gpt4_df = pd.DataFrame(results).T\n\ntab_decimal = abstract_results_gpt4_df\ntab_percent = abstract_results_gpt4_df.copy()\ntab_percent['gpt_proportion'] = tab_percent['gpt_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent['manual_proportion'] = tab_percent['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent['gpt_manual_agreement'] = tab_percent['gpt_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\n\n# tabs = mo.ui.tabs({\"percent\": tab_percent, \"decimal\": tab_decimal})\ntab_percent\n</pre> df_gpt_agreement_manu = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/agreement_gpt.csv\")  # Clean up columns - exclude metadata and description columns # Make \"paper_id\" the index df_gpt_agreement_manu = df_gpt_agreement_manu.set_index(\"paper_id\")  df_gpt_agreement_manu = df_gpt_agreement_manu.drop(     columns=[\"evaluation_type\", \"source_file\"]     + [col for col in df_gpt_agreement_manu.columns if \"_description\" in col] )  # Remove gpt_ prefix from column names df_gpt_agreement_manu = df_gpt_agreement_manu.rename(     columns={         col: col.replace(\"gpt_\", \"\")         for col in df_gpt_agreement_manu.columns         if col.startswith(\"gpt_\")     } )  results = {} for manu_metric in metric_columns:     gpt_col = manu_metric     agreement_col = f\"{manu_metric}_agreement\"      if agreement_col in df_gpt_agreement_manu.columns:         gpt_vals = df_gpt_agreement_manu[gpt_col].astype(bool)         agreement_vals = df_gpt_agreement_manu[agreement_col]          # Calculate revised manual evaluation: keep GPT when agreement=1, invert when agreement=0         manual_vals_gpt_abs = np.where(agreement_vals == 1, gpt_vals, ~gpt_vals)          # Add manual_vals_gpt_abs to the agreement_gpt DataFrame         df_gpt_agreement_manu[f\"manual_{manu_metric}\"] = manual_vals_gpt_abs.astype(bool)          results[manu_metric] = {             'gpt_sum': gpt_vals.sum(),             'manual_sum': manual_vals_gpt_abs.sum(),             'gpt_proportion': gpt_vals.mean(),             'gpt_manual_agreement': agreement_vals.mean(),             'manual_proportion': manual_vals_gpt_abs.mean(),             'total_n': len(gpt_vals)         } abstract_results_gpt4_df = pd.DataFrame(results).T  tab_decimal = abstract_results_gpt4_df tab_percent = abstract_results_gpt4_df.copy() tab_percent['gpt_proportion'] = tab_percent['gpt_proportion'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent['manual_proportion'] = tab_percent['manual_proportion'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent['gpt_manual_agreement'] = tab_percent['gpt_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')  # tabs = mo.ui.tabs({\"percent\": tab_percent, \"decimal\": tab_decimal}) tab_percent Out[10]: gpt_sum manual_sum gpt_proportion gpt_manual_agreement manual_proportion total_n problem 49.0 41.0 98% 80% 82% 50.0 objective 49.0 44.0 98% 86% 88% 50.0 research_method 47.0 22.0 94% 46% 44% 50.0 research_questions 4.0 4.0 8% 96% 8% 50.0 pseudocode 0.0 0.0 0% 100% 0% 50.0 dataset 14.0 6.0 28% 68% 12% 50.0 hypothesis 6.0 6.0 12% 88% 12% 50.0 prediction 25.0 9.0 50% 52% 18% 50.0 code_available 4.0 4.0 8% 100% 8% 50.0 software_dependencies 1.0 1.0 2% 100% 2% 50.0 experiment_setup 27.0 12.0 54% 46% 24% 50.0 In\u00a0[11]: Copied! <pre># Load LLaMA 3.2 abstract results\ndf_abstract_llama32 = pd.read_csv(\"../../llama3/outputs_json/20250829-235938/analysis_summary_reproscreener.csv\")\ndf_abstract_llama32 = df_abstract_llama32.set_index(\"paper_id\")\n# Ensure boolean dtype for metrics present in this dataframe\navailable_cols_llama32 = [c for c in metric_columns if c in df_abstract_llama32.columns]\ndf_abstract_llama32[available_cols_llama32] = df_abstract_llama32[available_cols_llama32].astype(bool)\n# df_abstract_llama32.apply(lambda x: x.value_counts())\ncommon_idx_abs_llama32 = df_abstract_manual.index.intersection(df_abstract_llama32.index)\nmanual_bool_abs_llama32 = df_abstract_manual.loc[common_idx_abs_llama32]\nllama32_bool_abs = df_abstract_llama32.loc[common_idx_abs_llama32]\n\nmetrics_llama32_shared = [\n    m for m in metric_columns\n    if m in manual_bool_abs_llama32.columns and m in llama32_bool_abs.columns\n]\n\nresults_abs_llama32 = {}\nfor metric_llama32 in metrics_llama32_shared:\n    llama32_vals = llama32_bool_abs[metric_llama32].astype(bool)\n    manual_vals_llama32 = manual_bool_abs_llama32[metric_llama32].astype(bool)\n\n    results_abs_llama32[metric_llama32] = {\n        'llama32_sum': int(llama32_vals.sum()),\n        'manual_sum_llama32': int(manual_vals_llama32.sum()),\n        'llama32_proportion': float(llama32_vals.mean()),\n        'llama32_manual_agreement': float((llama32_vals == manual_vals_llama32).mean()),\n        'manual_proportion_llama32': float(manual_vals_llama32.mean()),\n        'total_n_llama32': int(len(llama32_vals)),\n    }\n\nabstract_results_llama32_df = pd.DataFrame(results_abs_llama32).T\n\ntab_decimal_abs_llama32 = abstract_results_llama32_df\ntab_percent_abs_llama32 = abstract_results_llama32_df.copy()\ntab_percent_abs_llama32['llama32_proportion'] = tab_percent_abs_llama32['llama32_proportion'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent_abs_llama32['manual_proportion_llama32'] = tab_percent_abs_llama32['manual_proportion_llama32'].mul(100).round(0).astype(int).astype(str).add('%')\ntab_percent_abs_llama32['llama32_manual_agreement'] = tab_percent_abs_llama32['llama32_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')\n\ntab_percent_abs_llama32\n</pre> # Load LLaMA 3.2 abstract results df_abstract_llama32 = pd.read_csv(\"../../llama3/outputs_json/20250829-235938/analysis_summary_reproscreener.csv\") df_abstract_llama32 = df_abstract_llama32.set_index(\"paper_id\") # Ensure boolean dtype for metrics present in this dataframe available_cols_llama32 = [c for c in metric_columns if c in df_abstract_llama32.columns] df_abstract_llama32[available_cols_llama32] = df_abstract_llama32[available_cols_llama32].astype(bool) # df_abstract_llama32.apply(lambda x: x.value_counts()) common_idx_abs_llama32 = df_abstract_manual.index.intersection(df_abstract_llama32.index) manual_bool_abs_llama32 = df_abstract_manual.loc[common_idx_abs_llama32] llama32_bool_abs = df_abstract_llama32.loc[common_idx_abs_llama32]  metrics_llama32_shared = [     m for m in metric_columns     if m in manual_bool_abs_llama32.columns and m in llama32_bool_abs.columns ]  results_abs_llama32 = {} for metric_llama32 in metrics_llama32_shared:     llama32_vals = llama32_bool_abs[metric_llama32].astype(bool)     manual_vals_llama32 = manual_bool_abs_llama32[metric_llama32].astype(bool)      results_abs_llama32[metric_llama32] = {         'llama32_sum': int(llama32_vals.sum()),         'manual_sum_llama32': int(manual_vals_llama32.sum()),         'llama32_proportion': float(llama32_vals.mean()),         'llama32_manual_agreement': float((llama32_vals == manual_vals_llama32).mean()),         'manual_proportion_llama32': float(manual_vals_llama32.mean()),         'total_n_llama32': int(len(llama32_vals)),     }  abstract_results_llama32_df = pd.DataFrame(results_abs_llama32).T  tab_decimal_abs_llama32 = abstract_results_llama32_df tab_percent_abs_llama32 = abstract_results_llama32_df.copy() tab_percent_abs_llama32['llama32_proportion'] = tab_percent_abs_llama32['llama32_proportion'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent_abs_llama32['manual_proportion_llama32'] = tab_percent_abs_llama32['manual_proportion_llama32'].mul(100).round(0).astype(int).astype(str).add('%') tab_percent_abs_llama32['llama32_manual_agreement'] = tab_percent_abs_llama32['llama32_manual_agreement'].mul(100).round(0).astype(int).astype(str).add('%')  tab_percent_abs_llama32 Out[11]: llama32_sum manual_sum_llama32 llama32_proportion llama32_manual_agreement manual_proportion_llama32 total_n_llama32 problem 41.0 41.0 82% 72% 82% 50.0 objective 14.0 44.0 28% 36% 88% 50.0 research_method 7.0 22.0 14% 54% 44% 50.0 research_questions 7.0 4.0 14% 86% 8% 50.0 pseudocode 0.0 0.0 0% 100% 0% 50.0 dataset 1.0 6.0 2% 90% 12% 50.0 hypothesis 2.0 6.0 4% 84% 12% 50.0 prediction 5.0 9.0 10% 80% 18% 50.0 code_available 3.0 4.0 6% 98% 8% 50.0 software_dependencies 0.0 1.0 0% 98% 2% 50.0 experiment_setup 0.0 12.0 0% 76% 24% 50.0 In\u00a0[12]: Copied! <pre>def plot_heatmap_regex(metrics_display_map):\n    # Filter to only include metrics in gundersen_metrics\n    df_manuscript_regex_gundersen = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\")\n    df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.rename(columns={\"id\": \"paper_id\"})\n    df_manuscript_regex_gundersen.set_index(\"paper_id\", inplace=True)\n    df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.drop(columns=[\"index\", \"title\"])\n    df_manuscript_regex_gundersen[gundersen_metrics] = df_manuscript_regex_gundersen[gundersen_metrics].astype(bool)\n\n    available_gundersen_metrics = [m for m in gundersen_metrics if m in df_manuscript_regex.columns]\n    \n    # Metrics on rows, papers on columns\n    heatmap_df = df_manuscript_regex[available_gundersen_metrics].astype(float).T\n    heatmap_df.index = [gundersen_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]\n\n    # custom_cmap = ListedColormap([\"#FFF0F0\", \"#E74C3C\"])\n    # custom_cmap = ListedColormap([\"#DFF3E3\", \"#3D9963\"])\n    custom_cmap = ListedColormap([\"#bae6ff\", \"#00539a\"])\n\n    fig, ax = plt.subplots(figsize=(14, 5), tight_layout={\"pad\": 1.5})\n\n    # Black frame\n    ax.axhline(y=0, color=\"k\", linewidth=1.5)\n    ax.axvline(x=0, color=\"k\", linewidth=1.5)\n    ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)\n    ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)\n\n    sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1.5, ax=ax)\n\n    ax.set_ylabel(\"Gundersen et al. metrics\", fontsize=12)\n    ax.set_xlabel(\"arXiv articles (n=50)\",  rotation=360, loc=\"right\")\n\n    plt.subplots_adjust(top=0.95, left=0.15, right=0.95)\n    plt.tight_layout()\n    plt.tick_params(axis='both', size=0, pad=5)\n    \n    for tick in ax.get_xticklabels():\n        tick.set_fontsize(11)\n    for tick in ax.get_yticklabels():\n        tick.set_fontsize(14)\n    plt.savefig(current_figures_dir + \"hm_manuscript_regex.png\", dpi=2560, bbox_inches=\"tight\")\n    plt.savefig(thesis_figures_dir + \"hm_manuscript_regex.png\", dpi=2560, bbox_inches=\"tight\")\n    plt.savefig(presentation_figures_dir + \"hm_manuscript_regex.png\", dpi=2560, bbox_inches=\"tight\")\n\n    plt.show()\n\nplot_heatmap_regex(gundersen_metrics_display_map)\n</pre> def plot_heatmap_regex(metrics_display_map):     # Filter to only include metrics in gundersen_metrics     df_manuscript_regex_gundersen = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\")     df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.rename(columns={\"id\": \"paper_id\"})     df_manuscript_regex_gundersen.set_index(\"paper_id\", inplace=True)     df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.drop(columns=[\"index\", \"title\"])     df_manuscript_regex_gundersen[gundersen_metrics] = df_manuscript_regex_gundersen[gundersen_metrics].astype(bool)      available_gundersen_metrics = [m for m in gundersen_metrics if m in df_manuscript_regex.columns]          # Metrics on rows, papers on columns     heatmap_df = df_manuscript_regex[available_gundersen_metrics].astype(float).T     heatmap_df.index = [gundersen_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]      # custom_cmap = ListedColormap([\"#FFF0F0\", \"#E74C3C\"])     # custom_cmap = ListedColormap([\"#DFF3E3\", \"#3D9963\"])     custom_cmap = ListedColormap([\"#bae6ff\", \"#00539a\"])      fig, ax = plt.subplots(figsize=(14, 5), tight_layout={\"pad\": 1.5})      # Black frame     ax.axhline(y=0, color=\"k\", linewidth=1.5)     ax.axvline(x=0, color=\"k\", linewidth=1.5)     ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)     ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)      sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1.5, ax=ax)      ax.set_ylabel(\"Gundersen et al. metrics\", fontsize=12)     ax.set_xlabel(\"arXiv articles (n=50)\",  rotation=360, loc=\"right\")      plt.subplots_adjust(top=0.95, left=0.15, right=0.95)     plt.tight_layout()     plt.tick_params(axis='both', size=0, pad=5)          for tick in ax.get_xticklabels():         tick.set_fontsize(11)     for tick in ax.get_yticklabels():         tick.set_fontsize(14)     plt.savefig(current_figures_dir + \"hm_manuscript_regex.png\", dpi=2560, bbox_inches=\"tight\")     plt.savefig(thesis_figures_dir + \"hm_manuscript_regex.png\", dpi=2560, bbox_inches=\"tight\")     plt.savefig(presentation_figures_dir + \"hm_manuscript_regex.png\", dpi=2560, bbox_inches=\"tight\")      plt.show()  plot_heatmap_regex(gundersen_metrics_display_map) In\u00a0[13]: Copied! <pre>def plot_heatmap_llama(metrics_display_map):\n    # Filter to only include metrics in repro_manuscript_metrics\n    available_repro_metrics = [m for m in repro_manuscript_metrics if m in df_abstract_llama32.columns]\n    \n    # Metrics on rows, papers on columns\n    heatmap_df = df_abstract_llama32[available_repro_metrics].astype(float).T\n    heatmap_df.index = [repro_manuscript_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]\n\n    # Two-color scheme (empty, filled)\n    # custom_cmap = ListedColormap([\"#FFF0F0\", \"#E74C3C\"])\n    # custom_cmap = ListedColormap([\"#DFF3E3\", \"#3D9963\"])\n    custom_cmap = ListedColormap([\"#e8daff\", \"#6929c4\"])\n\n    fig, ax = plt.subplots(figsize=(12, 4), tight_layout={\"pad\": 1.5})\n\n    # Black frame\n    ax.axhline(y=0, color=\"k\", linewidth=1.5)\n    ax.axvline(x=0, color=\"k\", linewidth=1.5)\n    ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)\n    ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)\n\n    sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1.5, ax=ax)\n\n    ax.set_ylabel(\"ReproManuscriptMetrics\", fontsize=12)\n    ax.set_xlabel(\"arXiv articles (n=50)\",  rotation=360, loc=\"right\")\n    \n    plt.subplots_adjust(top=0.95, left=0.15, right=0.95)\n    plt.tight_layout()\n    plt.tick_params(axis='both', size=0, pad=5)\n    \n    for tick in ax.get_xticklabels():\n        tick.set_fontsize(11)\n    for tick in ax.get_yticklabels():\n        tick.set_fontsize(14)\n    plt.savefig(current_figures_dir + \"hm_abstract_llama32.png\", dpi=2560, bbox_inches=\"tight\")\n    plt.savefig(thesis_figures_dir + \"hm_abstract_llama32.png\", dpi=2560, bbox_inches=\"tight\")\n    plt.savefig(presentation_figures_dir + \"hm_abstract_llama32.png\", dpi=2560, bbox_inches=\"tight\")\n\n    plt.show()\n# plot_heatmap_llama(repro_manuscript_metrics_display_map)\n</pre> def plot_heatmap_llama(metrics_display_map):     # Filter to only include metrics in repro_manuscript_metrics     available_repro_metrics = [m for m in repro_manuscript_metrics if m in df_abstract_llama32.columns]          # Metrics on rows, papers on columns     heatmap_df = df_abstract_llama32[available_repro_metrics].astype(float).T     heatmap_df.index = [repro_manuscript_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]      # Two-color scheme (empty, filled)     # custom_cmap = ListedColormap([\"#FFF0F0\", \"#E74C3C\"])     # custom_cmap = ListedColormap([\"#DFF3E3\", \"#3D9963\"])     custom_cmap = ListedColormap([\"#e8daff\", \"#6929c4\"])      fig, ax = plt.subplots(figsize=(12, 4), tight_layout={\"pad\": 1.5})      # Black frame     ax.axhline(y=0, color=\"k\", linewidth=1.5)     ax.axvline(x=0, color=\"k\", linewidth=1.5)     ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)     ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)      sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1.5, ax=ax)      ax.set_ylabel(\"ReproManuscriptMetrics\", fontsize=12)     ax.set_xlabel(\"arXiv articles (n=50)\",  rotation=360, loc=\"right\")          plt.subplots_adjust(top=0.95, left=0.15, right=0.95)     plt.tight_layout()     plt.tick_params(axis='both', size=0, pad=5)          for tick in ax.get_xticklabels():         tick.set_fontsize(11)     for tick in ax.get_yticklabels():         tick.set_fontsize(14)     plt.savefig(current_figures_dir + \"hm_abstract_llama32.png\", dpi=2560, bbox_inches=\"tight\")     plt.savefig(thesis_figures_dir + \"hm_abstract_llama32.png\", dpi=2560, bbox_inches=\"tight\")     plt.savefig(presentation_figures_dir + \"hm_abstract_llama32.png\", dpi=2560, bbox_inches=\"tight\")      plt.show() # plot_heatmap_llama(repro_manuscript_metrics_display_map) In\u00a0[14]: Copied! <pre># plot all 3 agreement metrics\nabstract_results_regex_df.regex_manual_agreement\nabstract_results_gpt4_df.gpt_manual_agreement\nabstract_results_llama32_df.llama32_manual_agreement\n\nmerged_agreement_results = pd.DataFrame({\n    'Regex': abstract_results_regex_df['regex_manual_agreement'],\n    'GPT-4': abstract_results_gpt4_df['gpt_manual_agreement'],\n    'LLaMA 3.2': abstract_results_llama32_df['llama32_manual_agreement']\n})\nmerged_agreement_results.index.name = 'Metric'\nmerged_agreement_results_melt = merged_agreement_results.reset_index().melt(id_vars='Metric', var_name='Method', value_name='Agreement')\nmerged_agreement_results\n</pre> # plot all 3 agreement metrics abstract_results_regex_df.regex_manual_agreement abstract_results_gpt4_df.gpt_manual_agreement abstract_results_llama32_df.llama32_manual_agreement  merged_agreement_results = pd.DataFrame({     'Regex': abstract_results_regex_df['regex_manual_agreement'],     'GPT-4': abstract_results_gpt4_df['gpt_manual_agreement'],     'LLaMA 3.2': abstract_results_llama32_df['llama32_manual_agreement'] }) merged_agreement_results.index.name = 'Metric' merged_agreement_results_melt = merged_agreement_results.reset_index().melt(id_vars='Metric', var_name='Method', value_name='Agreement') merged_agreement_results Out[14]: Regex GPT-4 LLaMA 3.2 Metric problem 0.38 0.80 0.72 objective 0.20 0.86 0.36 research_method 0.56 0.46 0.54 research_questions 0.70 0.96 0.86 pseudocode 1.00 1.00 1.00 dataset 0.86 0.68 0.90 hypothesis 0.88 0.88 0.84 prediction 0.82 0.52 0.80 code_available 0.96 1.00 0.98 software_dependencies 0.98 1.00 0.98 experiment_setup 0.74 0.46 0.76 In\u00a0[15]: Copied! <pre>merged_agreement_results_manual = pd.DataFrame({\n    'Regex': tab_percent_abs['regex_manual_agreement'],\n    'GPT-4': tab_percent['gpt_manual_agreement'],\n    'LLaMA 3.2': tab_percent_abs_llama32['llama32_manual_agreement'],\n    '% human eval. found (n=50)': tab_percent.manual_proportion,\n    '# human eval. found (n=50)': tab_percent.manual_sum\n})\nmerged_agreement_results_manual\n</pre> merged_agreement_results_manual = pd.DataFrame({     'Regex': tab_percent_abs['regex_manual_agreement'],     'GPT-4': tab_percent['gpt_manual_agreement'],     'LLaMA 3.2': tab_percent_abs_llama32['llama32_manual_agreement'],     '% human eval. found (n=50)': tab_percent.manual_proportion,     '# human eval. found (n=50)': tab_percent.manual_sum }) merged_agreement_results_manual Out[15]: Regex GPT-4 LLaMA 3.2 % human eval. found (n=50) # human eval. found (n=50) problem 38% 80% 72% 82% 41.0 objective 20% 86% 36% 88% 44.0 research_method 56% 46% 54% 44% 22.0 research_questions 70% 96% 86% 8% 4.0 pseudocode 100% 100% 100% 0% 0.0 dataset 86% 68% 90% 12% 6.0 hypothesis 88% 88% 84% 12% 6.0 prediction 82% 52% 80% 18% 9.0 code_available 96% 100% 98% 8% 4.0 software_dependencies 98% 100% 98% 2% 1.0 experiment_setup 74% 46% 76% 24% 12.0 In\u00a0[16]: Copied! <pre>def plot_heatmap_regex_grouped(metrics_display_map, color_palette=CATEGORICAL_3_LIGHT_OPTION_5, values_cmap=ListedColormap([\"#edf5ff\", \"#002d9c\"])):\n    # Group colors from provided palette\n    colors_3group = {\n        \"Method\": color_palette[0],\n        \"Data\": color_palette[1],\n        \"Experiment\": color_palette[2],\n    }\n\n    df_manuscript_regex_gundersen = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\")\n    df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.rename(columns={\"id\": \"paper_id\"})\n    df_manuscript_regex_gundersen.set_index(\"paper_id\", inplace=True)\n    df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.drop(columns=[\"index\", \"title\"])\n    df_manuscript_regex_gundersen[gundersen_metrics] = df_manuscript_regex_gundersen[gundersen_metrics].astype(bool)\n\n    available_gundersen_metrics = [m for m in gundersen_metrics if m in df_manuscript_regex_gundersen.columns]\n\n    # Metric groups for Gundersen metrics\n    gundersen_metric_groups = {\n        \"Method\": [\"problem\", \"objective\", \"research_method\", \"research_questions\", \"pseudocode\"],\n        \"Data\": [\"training_data\", \"validation_data\", \"test_data\"],\n        \"Experiment\": [\"hypothesis\", \"prediction\", \"method_source_code\", \"hardware_specifications\", \"software_dependencies\", \"experiment_setup\"],\n    }\n\n    # Metrics on rows, papers on columns\n    heatmap_df = df_manuscript_regex_gundersen[available_gundersen_metrics].astype(float).T\n    heatmap_df.index = [gundersen_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]\n\n    fig, ax = plt.subplots(figsize=(12, 5), tight_layout={\"pad\": 1.5})\n\n    # Black frame\n    ax.axhline(y=0, color=\"k\", linewidth=1.5)\n    ax.axvline(x=0, color=\"k\", linewidth=1.5)\n    ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)\n    ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)\n\n    sns.heatmap(heatmap_df, cmap=values_cmap, cbar=False, linewidths=1.5, ax=ax)\n\n    # Color code the y-axis labels by metric group\n    for i, (metric_key, display_name) in enumerate(zip(available_gundersen_metrics, heatmap_df.index)):\n        for group_name, group_metrics in gundersen_metric_groups.items():\n            if metric_key in group_metrics:\n                ax.get_yticklabels()[i].set_color(colors_3group[group_name])\n                ax.get_yticklabels()[i].set_weight(\"medium\")\n                break\n\n    # Axis label styles aligned with llama version\n    ax.set_ylabel(\"Gundersen et al. metrics\", fontsize=13, fontweight=\"medium\", labelpad=15)\n    ax.set_xlabel(\"arXiv identifier (n=50)\", fontsize=11, fontweight=\"medium\", rotation=360, loc=\"center\", labelpad=10)\n\n    # Style to match llama: hide spines and adjust ticks\n    ax.spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)\n    ax.tick_params(axis=\"both\", size=0, pad=5)\n\n    for tick in ax.get_xticklabels():\n        tick.set_fontsize(11)\n    for tick in ax.get_yticklabels():\n        tick.set_fontsize(13)\n\n    legend_string = \"&lt;Factor: &gt;\" + \" \".join([f\"\\n&lt;{group}&gt;\" for group in gundersen_metric_groups.keys()])\n    fig_text(\n        x=.11,\n        y=0.20,\n        s= legend_string,\n        highlight_textprops=[{\"fontweight\":\"bold\"}]\n            + [{\"color\": colors_3group[g], \"fontweight\": \"medium\"} for g in gundersen_metric_groups.keys()],\n        annotationbbox_kw={'frameon': True, 'pad': .4, \n                           'bboxprops': {'linewidth': .8}},\n    )\n\n    plt.subplots_adjust(top=0.95, left=0.15, right=0.85)\n\n    plt.savefig(current_figures_dir + \"hm_manuscript_regex_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)\n    plt.savefig(thesis_figures_dir + \"hm_manuscript_regex_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)\n    plt.savefig(presentation_figures_dir + \"hm_manuscript_regex_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)\n\n    plt.show()\nplot_heatmap_regex_grouped(\n    gundersen_metrics_display_map,\n    color_palette=[\"#8a3800\",\"#005d5d\", \"#002d9c\"],  # or any 3-color palette [m, d, e]\n    values_cmap=ListedColormap([\"#e8daff\", \"#6929c4\"])  # for a purple theme\n)\n</pre> def plot_heatmap_regex_grouped(metrics_display_map, color_palette=CATEGORICAL_3_LIGHT_OPTION_5, values_cmap=ListedColormap([\"#edf5ff\", \"#002d9c\"])):     # Group colors from provided palette     colors_3group = {         \"Method\": color_palette[0],         \"Data\": color_palette[1],         \"Experiment\": color_palette[2],     }      df_manuscript_regex_gundersen = pd.read_csv(\"https://huggingface.co/datasets/adbX/reproscreener_manual_evaluations/resolve/main/repro_eval_tex.csv\")     df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.rename(columns={\"id\": \"paper_id\"})     df_manuscript_regex_gundersen.set_index(\"paper_id\", inplace=True)     df_manuscript_regex_gundersen = df_manuscript_regex_gundersen.drop(columns=[\"index\", \"title\"])     df_manuscript_regex_gundersen[gundersen_metrics] = df_manuscript_regex_gundersen[gundersen_metrics].astype(bool)      available_gundersen_metrics = [m for m in gundersen_metrics if m in df_manuscript_regex_gundersen.columns]      # Metric groups for Gundersen metrics     gundersen_metric_groups = {         \"Method\": [\"problem\", \"objective\", \"research_method\", \"research_questions\", \"pseudocode\"],         \"Data\": [\"training_data\", \"validation_data\", \"test_data\"],         \"Experiment\": [\"hypothesis\", \"prediction\", \"method_source_code\", \"hardware_specifications\", \"software_dependencies\", \"experiment_setup\"],     }      # Metrics on rows, papers on columns     heatmap_df = df_manuscript_regex_gundersen[available_gundersen_metrics].astype(float).T     heatmap_df.index = [gundersen_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]      fig, ax = plt.subplots(figsize=(12, 5), tight_layout={\"pad\": 1.5})      # Black frame     ax.axhline(y=0, color=\"k\", linewidth=1.5)     ax.axvline(x=0, color=\"k\", linewidth=1.5)     ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)     ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)      sns.heatmap(heatmap_df, cmap=values_cmap, cbar=False, linewidths=1.5, ax=ax)      # Color code the y-axis labels by metric group     for i, (metric_key, display_name) in enumerate(zip(available_gundersen_metrics, heatmap_df.index)):         for group_name, group_metrics in gundersen_metric_groups.items():             if metric_key in group_metrics:                 ax.get_yticklabels()[i].set_color(colors_3group[group_name])                 ax.get_yticklabels()[i].set_weight(\"medium\")                 break      # Axis label styles aligned with llama version     ax.set_ylabel(\"Gundersen et al. metrics\", fontsize=13, fontweight=\"medium\", labelpad=15)     ax.set_xlabel(\"arXiv identifier (n=50)\", fontsize=11, fontweight=\"medium\", rotation=360, loc=\"center\", labelpad=10)      # Style to match llama: hide spines and adjust ticks     ax.spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)     ax.tick_params(axis=\"both\", size=0, pad=5)      for tick in ax.get_xticklabels():         tick.set_fontsize(11)     for tick in ax.get_yticklabels():         tick.set_fontsize(13)      legend_string = \"\" + \" \".join([f\"\\n&lt;{group}&gt;\" for group in gundersen_metric_groups.keys()])     fig_text(         x=.11,         y=0.20,         s= legend_string,         highlight_textprops=[{\"fontweight\":\"bold\"}]             + [{\"color\": colors_3group[g], \"fontweight\": \"medium\"} for g in gundersen_metric_groups.keys()],         annotationbbox_kw={'frameon': True, 'pad': .4,                             'bboxprops': {'linewidth': .8}},     )      plt.subplots_adjust(top=0.95, left=0.15, right=0.85)      plt.savefig(current_figures_dir + \"hm_manuscript_regex_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)     plt.savefig(thesis_figures_dir + \"hm_manuscript_regex_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)     plt.savefig(presentation_figures_dir + \"hm_manuscript_regex_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)      plt.show() plot_heatmap_regex_grouped(     gundersen_metrics_display_map,     color_palette=[\"#8a3800\",\"#005d5d\", \"#002d9c\"],  # or any 3-color palette [m, d, e]     values_cmap=ListedColormap([\"#e8daff\", \"#6929c4\"])  # for a purple theme ) In\u00a0[17]: Copied! <pre>def plot_heatmap_llama_grouped(metrics_display_map, color_palette=CATEGORICAL_2_LIGHT_OPTION_1):\n    available_repro_metrics = [m for m in repro_manuscript_metrics if m in df_abstract_llama32.columns]\n    \n    # Define metric groups for ReproManuscript metrics\n    repro_manuscript_metric_groups = {\n        \"Method\": [\"problem\", \"objective\", \"research_method\", \"research_questions\"],\n        \"Experiment\": [\"hypothesis\", \"prediction\", \"code_available\", \"dataset\", \"experiment_setup\"]\n    }\n    \n    # Color mapping for groups using input palette\n    colors_2group = {\"Method\": color_palette[0], \"Experiment\": color_palette[1]}\n    \n    # Metrics on rows, papers on columns\n    heatmap_df = df_abstract_llama32[available_repro_metrics].astype(float).T\n    heatmap_df.index = [repro_manuscript_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]\n\n    # Custom colormap\n    custom_cmap = ListedColormap([\"#FBDAE6\", \"#ee538b\"])\n\n    fig, ax = plt.subplots(figsize=(12, 4), tight_layout={\"pad\": 1.5})\n\n    # Black frame\n    ax.axhline(y=0, color=\"k\", linewidth=1.5)\n    ax.axvline(x=0, color=\"k\", linewidth=1.5)\n    ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)\n    ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)\n\n    sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1.5, ax=ax)\n\n    # Color code the y-axis labels by metric group\n    for i, (metric_key, display_name) in enumerate(zip(available_repro_metrics, heatmap_df.index)):\n        # Find which group this metric belongs to\n        for group_name, group_metrics in repro_manuscript_metric_groups.items():\n            if metric_key in group_metrics:\n                color = colors_2group[group_name]\n                ax.get_yticklabels()[i].set_color(color)\n                ax.get_yticklabels()[i].set_weight('medium')\n                break\n\n    ax.set_ylabel(\"ReproManuscriptMetrics\", fontsize=13, fontweight=\"medium\", labelpad=15)\n    ax.set_xlabel(\"arXiv identifier (n=50)\", fontsize=11, fontweight=\"medium\", rotation=360, loc=\"center\", labelpad=10)\n    ax.spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)\n    ax.tick_params(axis='both', size=0, pad=5)\n    \n    for tick in ax.get_xticklabels():\n        tick.set_fontsize(11)\n    for tick in ax.get_yticklabels():\n        tick.set_fontsize(13)\n    \n    legend_string = \"&lt;Factor: &gt;\" + \" \".join([f\"\\n&lt;{group}&gt;\" for group in repro_manuscript_metric_groups.keys()])\n    fig_text(\n        x=.08,\n        y=0.25,\n        s= legend_string,\n        highlight_textprops=[{\"fontweight\":\"bold\"}]\n            + [{\"color\": colors_2group[g], \"fontweight\": \"medium\"} for g in repro_manuscript_metric_groups.keys()],\n        annotationbbox_kw={'frameon': True, 'pad': .4, \n                           'bboxprops': {'linewidth': .8}},\n    )\n    \n    plt.subplots_adjust(top=0.95, left=0.15, right=0.85)\n    plt.tight_layout()\n        \n    plt.savefig(current_figures_dir + \"hm_abstract_llama32_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)\n    plt.savefig(thesis_figures_dir + \"hm_abstract_llama32_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)\n    plt.savefig(presentation_figures_dir + \"hm_abstract_llama32_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)\n\n    plt.show()\nplot_heatmap_llama_grouped(repro_manuscript_metrics_display_map, color_palette=[\"#8a3800\",\"#005d5d\"])\n</pre> def plot_heatmap_llama_grouped(metrics_display_map, color_palette=CATEGORICAL_2_LIGHT_OPTION_1):     available_repro_metrics = [m for m in repro_manuscript_metrics if m in df_abstract_llama32.columns]          # Define metric groups for ReproManuscript metrics     repro_manuscript_metric_groups = {         \"Method\": [\"problem\", \"objective\", \"research_method\", \"research_questions\"],         \"Experiment\": [\"hypothesis\", \"prediction\", \"code_available\", \"dataset\", \"experiment_setup\"]     }          # Color mapping for groups using input palette     colors_2group = {\"Method\": color_palette[0], \"Experiment\": color_palette[1]}          # Metrics on rows, papers on columns     heatmap_df = df_abstract_llama32[available_repro_metrics].astype(float).T     heatmap_df.index = [repro_manuscript_metrics_display_map.get(m, m.replace(\"_\", \" \").title()) for m in heatmap_df.index]      # Custom colormap     custom_cmap = ListedColormap([\"#FBDAE6\", \"#ee538b\"])      fig, ax = plt.subplots(figsize=(12, 4), tight_layout={\"pad\": 1.5})      # Black frame     ax.axhline(y=0, color=\"k\", linewidth=1.5)     ax.axvline(x=0, color=\"k\", linewidth=1.5)     ax.axhline(y=heatmap_df.shape[0], color=\"k\", linewidth=1.5)     ax.axvline(x=heatmap_df.shape[1], color=\"k\", linewidth=1.5)      sns.heatmap(heatmap_df, cmap=custom_cmap, cbar=False, linewidths=1.5, ax=ax)      # Color code the y-axis labels by metric group     for i, (metric_key, display_name) in enumerate(zip(available_repro_metrics, heatmap_df.index)):         # Find which group this metric belongs to         for group_name, group_metrics in repro_manuscript_metric_groups.items():             if metric_key in group_metrics:                 color = colors_2group[group_name]                 ax.get_yticklabels()[i].set_color(color)                 ax.get_yticklabels()[i].set_weight('medium')                 break      ax.set_ylabel(\"ReproManuscriptMetrics\", fontsize=13, fontweight=\"medium\", labelpad=15)     ax.set_xlabel(\"arXiv identifier (n=50)\", fontsize=11, fontweight=\"medium\", rotation=360, loc=\"center\", labelpad=10)     ax.spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)     ax.tick_params(axis='both', size=0, pad=5)          for tick in ax.get_xticklabels():         tick.set_fontsize(11)     for tick in ax.get_yticklabels():         tick.set_fontsize(13)          legend_string = \"\" + \" \".join([f\"\\n&lt;{group}&gt;\" for group in repro_manuscript_metric_groups.keys()])     fig_text(         x=.08,         y=0.25,         s= legend_string,         highlight_textprops=[{\"fontweight\":\"bold\"}]             + [{\"color\": colors_2group[g], \"fontweight\": \"medium\"} for g in repro_manuscript_metric_groups.keys()],         annotationbbox_kw={'frameon': True, 'pad': .4,                             'bboxprops': {'linewidth': .8}},     )          plt.subplots_adjust(top=0.95, left=0.15, right=0.85)     plt.tight_layout()              plt.savefig(current_figures_dir + \"hm_abstract_llama32_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)     plt.savefig(thesis_figures_dir + \"hm_abstract_llama32_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)     plt.savefig(presentation_figures_dir + \"hm_abstract_llama32_grouped.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.3)      plt.show() plot_heatmap_llama_grouped(repro_manuscript_metrics_display_map, color_palette=[\"#8a3800\",\"#005d5d\"]) In\u00a0[18]: Copied! <pre>def plot_agreement_by_metric_mpl(metrics_display_map, methods, color_palette=CATEGORICAL_3_LIGHT_OPTION_4):\n    df_sorted = merged_agreement_results_melt.sort_values([\"Metric\",\"Method\"])\n    colors = {method: color_palette[i] for i, method in enumerate(methods)}\n\n    groups = [\n        [\"problem\", \"objective\", \"research_method\", \"research_questions\"],\n        [\"dataset\", \"hypothesis\", \"prediction\",\"code_available\",\"experiment_setup\"],\n    ]\n    width = 0.8 / len(methods)\n    for idx, group in enumerate(groups, start=1):\n        x = np.arange(len(group))\n        fig, ax = plt.subplots(figsize=(6, 3))\n        for i, m in enumerate(methods):\n            sub = df_sorted[df_sorted[\"Method\"] == m].set_index(\"Metric\").reindex(group)\n            ax.bar(x + i*width - (len(methods)-1)*width/2, sub[\"Agreement\"].values, width, label=m, color=colors[m])\n\n        ax.set_xticks(x)\n        ax.set_xticklabels([ \"\\n\".join(wrap(metrics_display_map[m], 12)) for m in group ])\n        ax.set_ylim(0, 1.05)\n        ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n        ax.set_yticklabels(['0', '20', '40', '60', '80', '100'])\n        ax.set_ylabel('Agreement (%)', fontweight=\"medium\", labelpad=10)\n        ax.yaxis.grid(True, linestyle=\"-\", linewidth=0.2, color=\"#1E1E1E\", alpha=1)\n        ax.tick_params(axis=\"x\", size=0, pad=5)\n        ax.spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)\n        ax.tick_params(axis=\"y\", size=0, pad=5)\n\n        legend_string = \"&lt;Model: &gt;\\n\" + \" \".join([f\"&lt;{group}&gt;\" for group in methods])\n        fig_text(\n        x=.65,\n        y=.98,\n        s= legend_string,\n        highlight_textprops=[{\"fontweight\":\"bold\"}]\n            + [{\"color\": colors[g], \"fontweight\": \"medium\"} for g in methods],\n        annotationbbox_kw={'frameon': True, 'pad': .4, \n                           'bboxprops': {'linewidth': .0}},\n        )\n\n        plt.tight_layout()\n        plt.savefig(current_figures_dir + f\"agreements_on_abstracts_row{idx}.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.0)\n        plt.savefig(thesis_figures_dir + f\"agreements_on_abstracts_row{idx}.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.0)\n        plt.savefig(presentation_figures_dir + f\"agreements_on_abstracts_row{idx}.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.0)\n        plt.show()\n        \nmethod_order = [\"LLaMA 3.2\", \"GPT-4\", \"Regex\"]\nplot_agreement_by_metric_mpl(repro_manuscript_metrics_display_map, method_order, color_palette=CATEGORICAL_3_LIGHT_OPTION_4)\n</pre> def plot_agreement_by_metric_mpl(metrics_display_map, methods, color_palette=CATEGORICAL_3_LIGHT_OPTION_4):     df_sorted = merged_agreement_results_melt.sort_values([\"Metric\",\"Method\"])     colors = {method: color_palette[i] for i, method in enumerate(methods)}      groups = [         [\"problem\", \"objective\", \"research_method\", \"research_questions\"],         [\"dataset\", \"hypothesis\", \"prediction\",\"code_available\",\"experiment_setup\"],     ]     width = 0.8 / len(methods)     for idx, group in enumerate(groups, start=1):         x = np.arange(len(group))         fig, ax = plt.subplots(figsize=(6, 3))         for i, m in enumerate(methods):             sub = df_sorted[df_sorted[\"Method\"] == m].set_index(\"Metric\").reindex(group)             ax.bar(x + i*width - (len(methods)-1)*width/2, sub[\"Agreement\"].values, width, label=m, color=colors[m])          ax.set_xticks(x)         ax.set_xticklabels([ \"\\n\".join(wrap(metrics_display_map[m], 12)) for m in group ])         ax.set_ylim(0, 1.05)         ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])         ax.set_yticklabels(['0', '20', '40', '60', '80', '100'])         ax.set_ylabel('Agreement (%)', fontweight=\"medium\", labelpad=10)         ax.yaxis.grid(True, linestyle=\"-\", linewidth=0.2, color=\"#1E1E1E\", alpha=1)         ax.tick_params(axis=\"x\", size=0, pad=5)         ax.spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)         ax.tick_params(axis=\"y\", size=0, pad=5)          legend_string = \"\\n\" + \" \".join([f\"&lt;{group}&gt;\" for group in methods])         fig_text(         x=.65,         y=.98,         s= legend_string,         highlight_textprops=[{\"fontweight\":\"bold\"}]             + [{\"color\": colors[g], \"fontweight\": \"medium\"} for g in methods],         annotationbbox_kw={'frameon': True, 'pad': .4,                             'bboxprops': {'linewidth': .0}},         )          plt.tight_layout()         plt.savefig(current_figures_dir + f\"agreements_on_abstracts_row{idx}.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.0)         plt.savefig(thesis_figures_dir + f\"agreements_on_abstracts_row{idx}.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.0)         plt.savefig(presentation_figures_dir + f\"agreements_on_abstracts_row{idx}.png\", dpi=2560, bbox_inches=\"tight\", pad_inches=0.0)         plt.show()          method_order = [\"LLaMA 3.2\", \"GPT-4\", \"Regex\"] plot_agreement_by_metric_mpl(repro_manuscript_metrics_display_map, method_order, color_palette=CATEGORICAL_3_LIGHT_OPTION_4)"},{"location":"manual_evaluation_explorer/#manual-evaluation-explorer","title":"Manual Evaluation Explorer\u00b6","text":""},{"location":"manual_evaluation_explorer/#reproscreener-regex-vs-manual-evaluations-of-full-manuscripts","title":"Reproscreener (regex) vs. Manual evaluations of full manuscripts\u00b6","text":""},{"location":"manual_evaluation_explorer/#reproscreener-regex-vs-manual-evaluations-of-abstracts","title":"Reproscreener (regex) vs. Manual evaluations of abstracts\u00b6","text":""},{"location":"manual_evaluation_explorer/#gpt-4-vs-manual-evaluations-of-manuscript-abstracts","title":"GPT-4 vs. Manual evaluations of manuscript abstracts\u00b6","text":""},{"location":"manual_evaluation_explorer/#abstract-evaluation-comparison","title":"Abstract evaluation comparison\u00b6","text":""},{"location":"manual_evaluation_explorer/#initial-gundersen-heatmap-grouped","title":"Initial gundersen heatmap (grouped)\u00b6","text":""},{"location":"manual_evaluation_explorer/#llama-grouped-heatmap","title":"LLama grouped heatmap\u00b6","text":""},{"location":"manual_evaluation_explorer/#agreement-bar-plots","title":"Agreement bar plots\u00b6","text":""},{"location":"todo/","title":"Todo","text":""},{"location":"todo/#features","title":"Features","text":"<ul> <li>[x] In Progress: Automatically check specific guidances to improve correctness of ML models</li> <li>[] Predict, capture and identify differences in model output at scale (due to architecture, non-determinism, etc.)</li> <li>[] Enable comparison of model code through</li> <li>Checks for modularity, file structure, dependencies</li> <li>Checks for steps/scripts to create figures &amp; visualizations</li> <li>Track model benchmarks and provenance</li> <li>[x] Progress bar (scrape): (https://rich.readthedocs.io/en/latest/progress.html)</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>The tool has two primary arguments:</p> <ul> <li><code>--arxiv</code>: This is the Arxiv URL to download and evaluate.</li> <li><code>--repo</code>: This is the Git repository to evaluate.</li> </ul> <p>You can also set the logging level using the <code>--log-level</code> argument.</p>"},{"location":"usage/#examples","title":"Examples","text":"<pre><code># Paper 2111.12673 from the gold standard dataset \nreproscreener --arxiv https://arxiv.org/e-print/2111.12673 --repo https://github.com/nicolinho/acc\n\n# Paper 2106.07704 from the gold standard dataset\nreproscreener --arxiv https://arxiv.org/e-print/2106.07704 --repo https://github.com/HanGuo97/soft-Q-learning-for-text-generation\n\n# Paper 2203.06735 from the gold standard dataset\nreproscreener --arxiv https://arxiv.org/e-print/2203.06735 --repo https://github.com/ghafeleb/Private-NonConvex-Federated-Learning-Without-a-Trusted-Server\n\n# Run the tool with logging level set to debug\nreproscreener --arxiv https://arxiv.org/e-print/2111.12673 --repo https://github.com/nicolinho/acc --log-level debug\n</code></pre> <p>By default, the logging level is set to <code>warning</code>. This means that only warnings, errors, and critical issues will be logged.</p> <p>If you want to see more detailed logs, you can set the logging level to <code>debug</code>.</p>"},{"location":"usage/#project-structure","title":"Project structure","text":"<ul> <li><code>case-studies</code> contains the papers that <code>reproscreener</code> is developed and tested on</li> <li><code>guidance</code> contains the set of metrics that <code>reproscreener</code> will check for</li> <li><code>tests</code> contains the unit tests for <code>reproscreener</code></li> <li><code>src/reproscreener</code> contains the main python scripts</li> </ul>"},{"location":"z_evaluation_results/","title":"Case studies","text":"In\u00a0[20]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom IPython.display import display\nfrom pathlib import Path\nimport sys\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\nsys.path.append(str(Path.cwd().parent / \"src/reproscrener\"))\n\nfrom reproscreener.plots.repo_eval_heatmaps import prepare_repo_heatmap_df, plot_repo_heatmap, plot_repo_clustermap\nfrom reproscreener.plots.tex_eval_heatmaps import prepare_tex_heatmap_df, plot_tex_heatmap\nfrom reproscreener.repo_eval import get_all_repo_eval_dict\nfrom reproscreener.tex_eval import get_all_tex_eval_dict\n# from reproscreener.gold_standard import get_gold_standard_ids_from_manual\nfrom reproscreener.gdrive_downloader import gdrive_get_manual_eval\nfrom reproscreener.utils import reverse_mapping\n\ndef summary_table(df, column, number_of_papers):\n    variable_counts = df[column].value_counts()\n    percentage = variable_counts / number_of_papers * 100\n\n    summary_table = pd.DataFrame({\"Count\": variable_counts, \"Percentage\": percentage})\n    summary_table = summary_table.sort_values(by=\"Count\", ascending=False)\n    summary_table[\"Percentage\"] = summary_table[\"Percentage\"].map(\"{:.2f}%\".format)\n    return summary_table\n</pre> import pandas as pd import numpy as np from IPython.display import display from pathlib import Path import sys import seaborn as sns from matplotlib import pyplot as plt from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score  sys.path.append(str(Path.cwd().parent / \"src/reproscrener\"))  from reproscreener.plots.repo_eval_heatmaps import prepare_repo_heatmap_df, plot_repo_heatmap, plot_repo_clustermap from reproscreener.plots.tex_eval_heatmaps import prepare_tex_heatmap_df, plot_tex_heatmap from reproscreener.repo_eval import get_all_repo_eval_dict from reproscreener.tex_eval import get_all_tex_eval_dict # from reproscreener.gold_standard import get_gold_standard_ids_from_manual from reproscreener.gdrive_downloader import gdrive_get_manual_eval from reproscreener.utils import reverse_mapping  def summary_table(df, column, number_of_papers):     variable_counts = df[column].value_counts()     percentage = variable_counts / number_of_papers * 100      summary_table = pd.DataFrame({\"Count\": variable_counts, \"Percentage\": percentage})     summary_table = summary_table.sort_values(by=\"Count\", ascending=False)     summary_table[\"Percentage\"] = summary_table[\"Percentage\"].map(\"{:.2f}%\".format)     return summary_table In\u00a0[21]: Copied! <pre>path_repo = Path(\"../case-studies/arxiv-corpus/gold_standard/repo\")\npath_tex = Path(\"../case-studies/arxiv-corpus/gold_standard/source\")\npath_manual = Path(\"../case-studies/arxiv-corpus/manual_eval.csv\")\n\nmanual_eval = gdrive_get_manual_eval(overwrite=False, manual_path=path_manual)\ngold_standard_ids = manual_eval[\"paper\"].unique()\n</pre> path_repo = Path(\"../case-studies/arxiv-corpus/gold_standard/repo\") path_tex = Path(\"../case-studies/arxiv-corpus/gold_standard/source\") path_manual = Path(\"../case-studies/arxiv-corpus/manual_eval.csv\")  manual_eval = gdrive_get_manual_eval(overwrite=False, manual_path=path_manual) gold_standard_ids = manual_eval[\"paper\"].unique() <pre>Manual eval file already exists, use the overwrite flag to download\n</pre> In\u00a0[22]: Copied! <pre>repo_evaluation_dict = get_all_repo_eval_dict(path_repo)\nrepo_heatmap_df = prepare_repo_heatmap_df(repo_evaluation_dict, gold_standard_ids)\nplot_repo_heatmap(repo_heatmap_df, filename=\"heatmap_repo_eval.png\", path_plots=None, sort_x=True, sort_y=True)\n</pre> repo_evaluation_dict = get_all_repo_eval_dict(path_repo) repo_heatmap_df = prepare_repo_heatmap_df(repo_evaluation_dict, gold_standard_ids) plot_repo_heatmap(repo_heatmap_df, filename=\"heatmap_repo_eval.png\", path_plots=None, sort_x=True, sort_y=True) In\u00a0[23]: Copied! <pre>plot_repo_clustermap(repo_heatmap_df, filename=\"clustermap_repo_eval.png\", path_plots=None)\n</pre> plot_repo_clustermap(repo_heatmap_df, filename=\"clustermap_repo_eval.png\", path_plots=None) In\u00a0[24]: Copied! <pre>repo_heatmap_df.head(10).drop(columns=[\"Display_Label\"])\n</pre> repo_heatmap_df.head(10).drop(columns=[\"Display_Label\"]) Out[24]: Paper_ID Matched_File Category 0 1606.04671 No code provided Others 1 1903.09668 No code provided Others 2 1904.10554 No code provided Others 3 1908.05659 No code provided Others 4 1909.00931 Code provided but no matches Others 5 1911.03867 No code provided Others 6 2002.05905 No code provided Others 7 2004.05258 No code provided Others 8 2009.01947 readme_dependencies Parsed Readme 9 2010.04261 No code provided Others In\u00a0[25]: Copied! <pre>number_of_papers = len(repo_heatmap_df[\"Paper_ID\"].unique())\nprint(f\"Total number of papers in the gold standard: {len(gold_standard_ids)}\")\n</pre> number_of_papers = len(repo_heatmap_df[\"Paper_ID\"].unique()) print(f\"Total number of papers in the gold standard: {len(gold_standard_ids)}\") <pre>Total number of papers in the gold standard: 50\n</pre> In\u00a0[26]: Copied! <pre>summary_table(repo_heatmap_df, \"Matched_File\", number_of_papers)\n</pre> summary_table(repo_heatmap_df, \"Matched_File\", number_of_papers) Out[26]: Count Percentage No code provided 27 54.00% Code provided but no matches 10 20.00% requirements.txt 6 12.00% readme_install 4 8.00% readme_requirements 3 6.00% readme_setup 3 6.00% readme_dependencies 2 4.00% environment.yml 1 2.00% conda_reqs.txt 1 2.00% pip_reqs.txt 1 2.00% run_experiments.py 1 2.00% main.py 1 2.00% <p>The variables are grouped by the following categories defined in <code>reverse_mapping</code>:</p> <ul> <li>Dependencies: Files related to the dependencies of the repository.</li> <li>Wrapper Scripts: Files that combine various stages of the workflow.</li> <li>Parsed Readme: Headers present in the README file of the repository that provide instructions about the code/data.</li> <li>Others: Contains <code>No code provided</code> or <code>Code provided but no matches</code>. The latter is used when the code is provided but files from any of the other categories were found in the repository.</li> </ul> In\u00a0[27]: Copied! <pre>reverse_mapping_df = pd.DataFrame.from_dict(reverse_mapping, orient='index', columns=['Category'])\nreverse_mapping_df.index.name = 'Matched_File'\nreverse_mapping_df\n</pre> reverse_mapping_df = pd.DataFrame.from_dict(reverse_mapping, orient='index', columns=['Category']) reverse_mapping_df.index.name = 'Matched_File' reverse_mapping_df Out[27]: Category Matched_File requirements.txt Dependencies setup.py Dependencies environment.yml Dependencies pyproject.toml Dependencies pip_reqs.txt Dependencies conda_reqs.txt Dependencies run.py Wrapper Scripts run.sh Wrapper Scripts main.py Wrapper Scripts main.sh Wrapper Scripts run_all.py Wrapper Scripts run_all.sh Wrapper Scripts run_experiments.py Wrapper Scripts run_experiments.sh Wrapper Scripts readme_requirements Parsed Readme readme_dependencies Parsed Readme readme_setup Parsed Readme readme_install Parsed Readme No code provided Others Code provided but no matches Others In\u00a0[28]: Copied! <pre>summary_table(repo_heatmap_df, \"Category\", number_of_papers)\n</pre> summary_table(repo_heatmap_df, \"Category\", number_of_papers) Out[28]: Count Percentage Others 37 74.00% Parsed Readme 12 24.00% Dependencies 9 18.00% Wrapper Scripts 2 4.00% In\u00a0[29]: Copied! <pre>no_code_provided_counts = len(repo_heatmap_df[repo_heatmap_df[\"Matched_File\"] == \"No code provided\"])\ncode_provided_counts = number_of_papers - no_code_provided_counts\ncode_provided_percentage = (code_provided_counts / number_of_papers) * 100\nprint(f\"{code_provided_counts}/{number_of_papers} ({code_provided_percentage:.2f}%) of the papers have provided some code\")\n</pre> no_code_provided_counts = len(repo_heatmap_df[repo_heatmap_df[\"Matched_File\"] == \"No code provided\"]) code_provided_counts = number_of_papers - no_code_provided_counts code_provided_percentage = (code_provided_counts / number_of_papers) * 100 print(f\"{code_provided_counts}/{number_of_papers} ({code_provided_percentage:.2f}%) of the papers have provided some code\") <pre>23/50 (46.00%) of the papers have provided some code\n</pre> In\u00a0[30]: Copied! <pre>tex_evaluation_dict = get_all_tex_eval_dict(path_tex)\ntex_heatmap_df = prepare_tex_heatmap_df(tex_evaluation_dict, gold_standard_ids)\n</pre> tex_evaluation_dict = get_all_tex_eval_dict(path_tex) tex_heatmap_df = prepare_tex_heatmap_df(tex_evaluation_dict, gold_standard_ids) <pre>Output()</pre> <pre></pre> <pre>\n</pre> In\u00a0[31]: Copied! <pre>plot_tex_heatmap(tex_heatmap_df, filename=\"heatmap_tex_eval.png\", path_plots=None, sort_x=True, sort_y=True)\n</pre> plot_tex_heatmap(tex_heatmap_df, filename=\"heatmap_tex_eval.png\", path_plots=None, sort_x=True, sort_y=True) In\u00a0[32]: Copied! <pre>tex_heatmap_df.head(10)\n</pre> tex_heatmap_df.head(10) Out[32]: Paper_ID Found_Variable 0 1606.04671 Research method 1 1606.04671 Hypothesis 2 1606.04671 Prediction 3 1606.04671 Research questions 4 1606.04671 Objective/Goal 5 1606.04671 Research problem 6 1606.04671 Experimental setup 7 1606.04671 Training data 8 1903.09668 Research method 9 1903.09668 Objective/Goal In\u00a0[33]: Copied! <pre>summary_table(tex_heatmap_df, \"Found_Variable\", number_of_papers)\n</pre> summary_table(tex_heatmap_df, \"Found_Variable\", number_of_papers) Out[33]: Count Percentage Research questions 44 88.00% Research problem 44 88.00% Research method 43 86.00% Objective/Goal 39 78.00% Prediction 34 68.00% Method source code 23 46.00% Hypothesis 21 42.00% Training data 18 36.00% Experimental setup 15 30.00% Test data 7 14.00% Pseudocode 6 12.00% Validation data 2 4.00% No variables found 1 2.00% In\u00a0[40]: Copied! <pre># Prepare mapping dictionary for 'parsed_readme'\nrepo_map_dict = {\n    \"Dependencies\": \"software_dependencies\",\n    \"Wrapper Scripts\": \"wrapper_scripts\",\n    # Add other mappings if necessary. You may not have direct mappings for 'Parsed Readme' and 'Others'\n}\n\nrepo_heatmap_df['Mapped_Category'] = repo_heatmap_df['Category'].map(repo_map_dict)\nrepo_heatmap_df['Value'] = 1  # Assign 1 to found categories\nrepo_heatmap_pivot = repo_heatmap_df.pivot_table(values='Value', index='Paper_ID', columns='Mapped_Category', fill_value=0)\n\n# For 'Parsed Readme' category, we have to adjust values according to 'Matched_File' column\nparsed_readme_mask = repo_heatmap_df['Mapped_Category'] == 'parsed_readme'\nrepo_heatmap_pivot.loc[repo_heatmap_df[parsed_readme_mask]['Paper_ID'], 'parsed_readme'] = repo_heatmap_df[parsed_readme_mask]['Matched_File']\n\n# Convert the 'parsed_readme' column in manual_eval to the same format\nmanual_eval['parsed_readme'] = manual_eval['parsed_readme'].apply(lambda x: 'Parsed Readme' if x else 'No Readme')\n\nrepo_heatmap_pivot['no_code_provided'] = ((repo_heatmap_df['Category'] == 'Others') &amp; (repo_heatmap_df['Matched_File'] == 'No code provided')).astype(int)\nrepo_heatmap_pivot['code_provided_no_match'] = ((repo_heatmap_df['Category'] == 'Others') &amp; (repo_heatmap_df['Matched_File'] == 'Code provided but no matches')).astype(int)\n\n\nfor col in repo_map_dict.values():\n    print(f'\\nMetrics for: {col}')\n    if col in repo_heatmap_pivot.columns:\n        if col != 'parsed_readme':\n            manual_eval[col] = pd.to_numeric(manual_eval[col], errors='coerce')\n            \n            y_auto = repo_heatmap_pivot[col]\n            y_manual = manual_eval.set_index('paper')[col]\n            \n            eval_df = pd.concat([y_auto, y_manual], axis=1, join='inner')\n            eval_df.columns = ['auto', 'manual']\n            eval_df.dropna(inplace=True)\n            \n            precision = precision_score(eval_df['manual'], eval_df['auto'])\n            recall = recall_score(eval_df['manual'], eval_df['auto'])\n            f1 = f1_score(eval_df['manual'], eval_df['auto'])\n            accuracy = accuracy_score(eval_df['manual'], eval_df['auto'])\n            \n            print(f'Precision: {precision}\\nRecall: {recall}\\nF1-score: {f1}\\nAccuracy: {accuracy}')\n        else:\n            # Now, you can calculate the agreement percentage\n            agreement = (repo_heatmap_pivot['parsed_readme'] == manual_eval.set_index('paper')['parsed_readme']).mean()\n            print(f'Agreement percentage: {agreement*100:.2f}%')\n    else:\n        print('This column is not evaluated by RepoScreener.')\n</pre> # Prepare mapping dictionary for 'parsed_readme' repo_map_dict = {     \"Dependencies\": \"software_dependencies\",     \"Wrapper Scripts\": \"wrapper_scripts\",     # Add other mappings if necessary. You may not have direct mappings for 'Parsed Readme' and 'Others' }  repo_heatmap_df['Mapped_Category'] = repo_heatmap_df['Category'].map(repo_map_dict) repo_heatmap_df['Value'] = 1  # Assign 1 to found categories repo_heatmap_pivot = repo_heatmap_df.pivot_table(values='Value', index='Paper_ID', columns='Mapped_Category', fill_value=0)  # For 'Parsed Readme' category, we have to adjust values according to 'Matched_File' column parsed_readme_mask = repo_heatmap_df['Mapped_Category'] == 'parsed_readme' repo_heatmap_pivot.loc[repo_heatmap_df[parsed_readme_mask]['Paper_ID'], 'parsed_readme'] = repo_heatmap_df[parsed_readme_mask]['Matched_File']  # Convert the 'parsed_readme' column in manual_eval to the same format manual_eval['parsed_readme'] = manual_eval['parsed_readme'].apply(lambda x: 'Parsed Readme' if x else 'No Readme')  repo_heatmap_pivot['no_code_provided'] = ((repo_heatmap_df['Category'] == 'Others') &amp; (repo_heatmap_df['Matched_File'] == 'No code provided')).astype(int) repo_heatmap_pivot['code_provided_no_match'] = ((repo_heatmap_df['Category'] == 'Others') &amp; (repo_heatmap_df['Matched_File'] == 'Code provided but no matches')).astype(int)   for col in repo_map_dict.values():     print(f'\\nMetrics for: {col}')     if col in repo_heatmap_pivot.columns:         if col != 'parsed_readme':             manual_eval[col] = pd.to_numeric(manual_eval[col], errors='coerce')                          y_auto = repo_heatmap_pivot[col]             y_manual = manual_eval.set_index('paper')[col]                          eval_df = pd.concat([y_auto, y_manual], axis=1, join='inner')             eval_df.columns = ['auto', 'manual']             eval_df.dropna(inplace=True)                          precision = precision_score(eval_df['manual'], eval_df['auto'])             recall = recall_score(eval_df['manual'], eval_df['auto'])             f1 = f1_score(eval_df['manual'], eval_df['auto'])             accuracy = accuracy_score(eval_df['manual'], eval_df['auto'])                          print(f'Precision: {precision}\\nRecall: {recall}\\nF1-score: {f1}\\nAccuracy: {accuracy}')         else:             # Now, you can calculate the agreement percentage             agreement = (repo_heatmap_pivot['parsed_readme'] == manual_eval.set_index('paper')['parsed_readme']).mean()             print(f'Agreement percentage: {agreement*100:.2f}%')     else:         print('This column is not evaluated by RepoScreener.') <pre>\nMetrics for: software_dependencies\nPrecision: 1.0\nRecall: 1.0\nF1-score: 1.0\nAccuracy: 1.0\n\nMetrics for: wrapper_scripts\nPrecision: 1.0\nRecall: 0.2857142857142857\nF1-score: 0.4444444444444445\nAccuracy: 0.375\n</pre> In\u00a0[36]: Copied! <pre># Define a mapping from tex heatmap to manual eval\nmap_dict = {\n    \"Research questions\": \"research_questions\",\n    \"Research problem\": \"problem\",\n    \"Research method\": \"research_method\",\n    \"Objective/Goal\": \"objective\",\n    \"Prediction\": \"prediction\",\n    \"Method source code\": \"code_avail_article\",\n    \"Hypothesis\": \"hypothesis\",\n    \"Training data\": \"dataset\",\n    \"Experimental setup\": \"experiment_setup\",\n}\n\n# Apply mapping and pivot the dataframe\ntex_heatmap_df['Mapped_Variable'] = tex_heatmap_df['Found_Variable'].map(map_dict)\ntex_heatmap_df['Value'] = 1  # Assign 1 to found variables\ntex_heatmap_pivot = tex_heatmap_df.pivot_table(values='Value', index='Paper_ID', columns='Mapped_Variable', fill_value=0)\n\ntex_heatmap_pivot\n</pre> # Define a mapping from tex heatmap to manual eval map_dict = {     \"Research questions\": \"research_questions\",     \"Research problem\": \"problem\",     \"Research method\": \"research_method\",     \"Objective/Goal\": \"objective\",     \"Prediction\": \"prediction\",     \"Method source code\": \"code_avail_article\",     \"Hypothesis\": \"hypothesis\",     \"Training data\": \"dataset\",     \"Experimental setup\": \"experiment_setup\", }  # Apply mapping and pivot the dataframe tex_heatmap_df['Mapped_Variable'] = tex_heatmap_df['Found_Variable'].map(map_dict) tex_heatmap_df['Value'] = 1  # Assign 1 to found variables tex_heatmap_pivot = tex_heatmap_df.pivot_table(values='Value', index='Paper_ID', columns='Mapped_Variable', fill_value=0)  tex_heatmap_pivot Out[36]: Mapped_Variable code_avail_article dataset experiment_setup hypothesis objective prediction problem research_method research_questions Paper_ID 1606.04671 0 1 1 1 1 1 1 1 1 1903.09668 0 1 0 0 1 1 1 1 1 1904.10554 0 0 0 1 1 0 1 1 1 1908.05659 0 1 0 1 1 1 1 1 1 1909.00931 1 0 1 1 0 1 1 1 1 1911.03867 0 1 0 0 1 1 1 1 1 2002.05905 0 0 1 1 1 0 1 1 1 2004.05258 0 1 0 0 0 0 1 1 1 2009.01947 1 0 1 0 1 0 1 1 1 2010.04261 1 0 1 1 1 1 1 1 1 2010.04855 0 0 0 1 1 1 1 1 1 2011.11576 1 1 1 1 1 1 1 1 1 2012.09302 1 1 0 0 1 1 0 1 1 2101.07354 0 0 0 0 1 1 1 1 1 2102.11887 0 1 0 0 1 1 1 0 1 2104.11893 0 0 1 1 1 1 1 1 0 2104.12546 1 0 0 1 1 1 1 1 1 2105.01099 0 0 0 0 1 1 1 1 1 2105.01937 1 0 0 0 1 1 0 1 1 2105.15197 0 0 0 1 1 0 1 0 0 2106.01528 1 1 0 1 1 0 1 1 1 2106.03157 1 1 1 0 1 1 1 1 0 2106.03725 0 0 0 0 1 0 1 1 0 2106.06927 1 0 1 0 1 1 1 1 1 2106.07704 1 1 0 1 1 0 1 1 1 2106.10898 0 0 0 1 1 1 1 1 1 2106.12177 0 0 0 0 1 1 1 1 1 2106.12936 0 0 0 1 0 1 1 1 1 2106.13823 0 0 0 0 0 0 1 0 1 2107.01131 1 1 1 1 1 1 1 1 1 2108.09779 1 0 1 0 1 0 1 1 1 2109.01372 1 1 0 1 1 1 1 1 1 2109.12784 1 0 0 0 1 0 1 1 1 2110.02343 0 1 0 0 1 1 1 0 0 2110.02474 0 0 0 1 1 1 1 1 1 2110.03135 0 0 0 0 0 0 0 1 1 2110.05169 1 0 0 0 0 0 0 0 1 2110.08255 0 0 0 0 0 1 0 0 1 2110.08432 1 1 1 0 1 1 1 1 1 2110.09902 1 1 0 0 0 1 1 1 1 2110.11688 0 0 0 0 0 0 1 1 1 2110.14241 0 0 1 1 1 1 1 1 1 2111.02997 1 0 0 1 1 1 1 1 1 2111.03664 1 0 1 0 1 1 1 1 1 2111.08356 0 1 0 1 1 1 1 1 1 2111.12673 0 0 0 0 0 0 1 1 1 2111.15449 1 0 0 0 1 1 1 1 1 2112.00016 1 1 0 0 1 1 1 1 1 2112.04871 1 0 1 0 1 1 1 1 1 In\u00a0[37]: Copied! <pre># Convert the columns in manual_eval to numeric values, if they are not already\nfor col in map_dict.values():\n    manual_eval[col] = pd.to_numeric(manual_eval[col], errors='coerce')\n\n# Iterate over all relevant columns and calculate metrics\nfor col in map_dict.values():\n    print(f'\\nMetrics for: {col}')\n    if col in tex_heatmap_pivot.columns:\n        y_auto = tex_heatmap_pivot[col]\n        y_manual = manual_eval.set_index('paper')[col]\n        \n        # Merge manual and automatic evaluations\n        eval_df = pd.concat([y_auto, y_manual], axis=1, join='inner')\n        eval_df.columns = ['auto', 'manual']\n        eval_df.dropna(inplace=True)\n        \n        # Calculate metrics\n        precision = precision_score(eval_df['manual'], eval_df['auto'])\n        recall = recall_score(eval_df['manual'], eval_df['auto'])\n        f1 = f1_score(eval_df['manual'], eval_df['auto'])\n        accuracy = accuracy_score(eval_df['manual'], eval_df['auto'])\n        \n        print(f'Precision: {precision}\\nRecall: {recall}\\nF1-score: {f1}\\nAccuracy: {accuracy}')\n    else:\n        print('This column is not evaluated by Reproscreener.')\n</pre> # Convert the columns in manual_eval to numeric values, if they are not already for col in map_dict.values():     manual_eval[col] = pd.to_numeric(manual_eval[col], errors='coerce')  # Iterate over all relevant columns and calculate metrics for col in map_dict.values():     print(f'\\nMetrics for: {col}')     if col in tex_heatmap_pivot.columns:         y_auto = tex_heatmap_pivot[col]         y_manual = manual_eval.set_index('paper')[col]                  # Merge manual and automatic evaluations         eval_df = pd.concat([y_auto, y_manual], axis=1, join='inner')         eval_df.columns = ['auto', 'manual']         eval_df.dropna(inplace=True)                  # Calculate metrics         precision = precision_score(eval_df['manual'], eval_df['auto'])         recall = recall_score(eval_df['manual'], eval_df['auto'])         f1 = f1_score(eval_df['manual'], eval_df['auto'])         accuracy = accuracy_score(eval_df['manual'], eval_df['auto'])                  print(f'Precision: {precision}\\nRecall: {recall}\\nF1-score: {f1}\\nAccuracy: {accuracy}')     else:         print('This column is not evaluated by Reproscreener.')  <pre>\nMetrics for: research_questions\nPrecision: 0.06818181818181818\nRecall: 1.0\nF1-score: 0.1276595744680851\nAccuracy: 0.16326530612244897\n\nMetrics for: problem\nPrecision: 0.3181818181818182\nRecall: 0.9333333333333333\nF1-score: 0.47457627118644075\nAccuracy: 0.3673469387755102\n\nMetrics for: research_method\nPrecision: 0.20930232558139536\nRecall: 0.9\nF1-score: 0.339622641509434\nAccuracy: 0.2857142857142857\n\nMetrics for: objective\nPrecision: 0.10256410256410256\nRecall: 1.0\nF1-score: 0.18604651162790695\nAccuracy: 0.2857142857142857\n\nMetrics for: prediction\nPrecision: 0.0\nRecall: 0.0\nF1-score: 0.0\nAccuracy: 0.30612244897959184\n\nMetrics for: code_avail_article\nPrecision: 0.782608695652174\nRecall: 0.8181818181818182\nF1-score: 0.8\nAccuracy: 0.8163265306122449\n\nMetrics for: hypothesis\nPrecision: 0.23809523809523808\nRecall: 0.625\nF1-score: 0.3448275862068965\nAccuracy: 0.6122448979591837\n\nMetrics for: dataset\nPrecision: 0.6666666666666666\nRecall: 0.3870967741935484\nF1-score: 0.4897959183673469\nAccuracy: 0.4897959183673469\n\nMetrics for: experiment_setup\nPrecision: 1.0\nRecall: 0.40540540540540543\nF1-score: 0.5769230769230769\nAccuracy: 0.5510204081632653\n</pre> <pre>/home/adb/.pyenv/versions/3.9.13/envs/repro-screener/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre>"},{"location":"z_evaluation_results/#case-studies","title":"Case studies\u00b6","text":""},{"location":"z_evaluation_results/#gold-standard","title":"Gold standard\u00b6","text":"<p>This dataset contains the 50 most recent articles from arxiv.org in both the cs.LG and stat.ML categories, between the dates 2022-10-24 and 2022-10-25 which had 570 search results. We select articles that belong to cs.LG <code>or</code> (cs.LG <code>and</code> stat.ML) category.</p> <p>\"Repository evaluation\" is performed on articles that provided links to their code repository and \"Paper evaluation\" is performed on all 50 articles by parsing the <code>.tex</code> files from their corresponding arXiv links. <code>reproscreener</code> is evaluated this <code>gold_standard</code> dataset and the results are shown below.</p>"},{"location":"z_evaluation_results/#repo-evaluation","title":"Repo evaluation\u00b6","text":""},{"location":"z_evaluation_results/#tex-evaluation","title":"Tex Evaluation\u00b6","text":""},{"location":"z_evaluation_results/#comparision-with-manual-evaluation","title":"Comparision with manual evaluation\u00b6","text":""},{"location":"z_evaluation_results/#repo-evaluation-comparison","title":"Repo evaluation comparison\u00b6","text":""},{"location":"z_evaluation_results/#tex-evaluation-comparison","title":"Tex evaluation comparison\u00b6","text":""},{"location":"z_notes/","title":"reproscreener","text":"<pre><code>https://arxiv.org/pdf/2306.00622\nhttps://arxiv.org/pdf/2411.03417 \nhttps://arxiv.org/abs/2106.07704\n\nuv add requests pandas pathlib flashtext exrex rich gitpython urlextract feedparser watchdog docling streamlit\n\n# Paper 2111.12673 from the gold standard dataset \nreproscreener --arxiv https://arxiv.org/e-print/2111.12673 --repo https://github.com/nicolinho/acc\n\n# Paper 2106.07704 from the gold standard dataset\nreproscreener --arxiv https://arxiv.org/e-print/2106.07704 --repo https://github.com/HanGuo97/soft-Q-learning-for-text-generation\n\n# Paper 2203.06735 from the gold standard dataset\nreproscreener --arxiv https://arxiv.org/e-print/2203.06735 --repo https://github.com/ghafeleb/Private-NonConvex-Federated-Learning-Without-a-Trusted-Server\n\n# Run the tool with logging level set to debug\nreproscreener --arxiv https://arxiv.org/e-print/2111.12673 --repo https://github.com/nicolinho/acc --log-level debug\n</code></pre>"},{"location":"z_notes/#notes","title":"Notes","text":""},{"location":"z_notes/#june-4-2025","title":"June 4, 2025","text":"<ul> <li>The <code>manual</code> evals for the abstracts were not used, the agreement was used instead. So the new derived/revised manual col is <code>manual_rev</code>.</li> </ul>"},{"location":"z_references/","title":"Bibliography","text":"<ol> <li> <p>Bhaskar, A. and Stodden, V. 2024. Reproscreener: Leveraging LLMs for Assessing Computational Reproducibility of Machine Learning Pipelines. Proceedings of the 2nd ACM Conference on Reproducibility and Replicability (New York, NY, USA, Jul. 2024), 101--109.\u00a0\u21a9</p> </li> <li> <p>Bhaskar, A. and Stodden, V. 2022. ReproScreen: Enabling Robustness in Machine Learning at Scale via Automated Knowledge Verification. Zenodo.\u00a0\u21a9</p> </li> <li> <p>Krafczyk, M.S. et al. 2021. Learning from reproducing computational results: Introducing three principles and the Reproduction Package. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 379, 2197 (May 2021), rsta.2020.0069, 20200069. https://doi.org/10.1098/rsta.2020.0069.\u00a0\u21a9</p> </li> <li> <p>Krafczyk, M. et al. 2019. Scientific Tests and Continuous Integration Strategies to Enhance Reproducibility in the Scientific Software Context. Proceedings of the 2nd International Workshop on Practical Reproducible Evaluation of Computer Systems - P-RECS '19 (Phoenix, AZ, USA, 2019), 23--28.\u00a0\u21a9</p> </li> <li> <p>Stodden, V. et al. 2018. Enabling the Verification of Computational Results: An Empirical Evaluation of Computational Reproducibility. Proceedings of the First International Workshop on Practical Reproducible Evaluation of Computer Systems (Tempe AZ USA, 2018), 1--5.\u00a0\u21a9</p> </li> <li> <p>Conference, N.I.P.S. 2021. Introducing the NeurIPS 2021 Paper Checklist. Medium.\u00a0\u21a9</p> </li> <li> <p>Gundersen, O.E. and Kjensmo, S. 2018. State of the Art: Reproducibility in Artificial Intelligence. Proceedings of the AAAI Conference on Artificial Intelligence. 32, 1 (2018).\u00a0\u21a9</p> </li> <li> <p>Isdahl, R. and Gundersen, O.E. 2019. Out-of-the-Box Reproducibility: A Survey of Machine Learning Platforms. 2019 15th International Conference on [eScience]{.nocase} ([eScience]{.nocase}) (San Diego, CA, USA, 2019), 86--95.\u00a0\u21a9</p> </li> <li> <p>Stodden, V. et al. 2018. AIM: AN ABSTRACTION FOR IMPROVING MACHINE LEARNING PREDICTION. 2018 IEEE Data Science Workshop (DSW) (Lausanne, Switzerland, 2018), 1--5.\u00a0\u21a9</p> </li> <li> <p>Midwinter, M. et al. 2021. Resolution adaptive networks for efficient inference.\u00a0\u21a9</p> </li> <li> <p>Pineau, J. et al. 2020. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program). arXiv:2003.12206 [cs, stat]. (2020).\u00a0\u21a9</p> </li> <li> <p>Raghupathi, W. et al. 2022. Reproducibility in Computing Research: An Empirical Study. IEEE Access. 10, (2022), 29207--29223. https://doi.org/10.1109/ACCESS.2022.3158675.\u00a0\u21a9</p> </li> <li> <p>Sadjadi, M. 2017. Arxivscraper.\u00a0\u21a9</p> </li> </ol>"}]}