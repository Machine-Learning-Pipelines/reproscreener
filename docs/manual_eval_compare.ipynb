{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case studies\n",
    "\n",
    "1. **Gold standard**: `mine-50-andor` contains the 50 most recent articles from [arxiv.org in both the cs.LG and stat.ML categories](https://arxiv.org/list/cs.LG/recent), between the dates 2022-10-24 and 2022-10-25 and contained 570 search results at the time of the dataset creation. We select articles that belong to cs.LG `or` (cs.LG `and` stat.ML) category.\n",
    "\n",
    "2. `mine50` contains the 50 most recent articles from [arxiv.org in both the cs.LG and stat.ML categories](https://arxiv.org/list/cs.LG/recent), between the dates 2022-10-24 and 2022-10-25 and contained 570 search results at the time of the dataset creation. The search result is sorted by date in descending order. (*Note:* The date being queried for is the last updated date and not the date of paper submission)\n",
    "\n",
    "1. `mine50-csLG` contains the results using the same method as `mine50` but without looking for articles in both cs.LG and stat.ML."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating ReproScreener on the manually labeled (gold standard) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "path_corpus = Path(\"../case-studies/arxiv-corpus/mine50-andor/\")\n",
    "manual_path = path_corpus / \"manual_eval.csv\"\n",
    "\n",
    "dtypes_repro = {'id': str, 'link_count': float, 'found_links': str}\n",
    "repro_eval = pd.read_csv(path_corpus / 'output/repro_eval_tex.csv', dtype=dtypes_repro)\n",
    "repro_eval = repro_eval.drop(columns=['title', 'index', 'affiliation'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 articles where ReproScreener found potential code/repository links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_count</th>\n",
       "      <th>found_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1909.00931</td>\n",
       "      <td>3.0</td>\n",
       "      <td>['https://github.com/lanwuwei/Twitter-URL-Corp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2009.01947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['https://gitlab.com/luciacyx/nm-adaptive-code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010.04261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['https://github.com/goodfeli/dlbook_notation/']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011.11576</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['https://github.com/jpbrooks/conjecturing.', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012.09302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['https://github.com/ain-soph/trojanzoo}.']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  link_count                                        found_links\n",
       "4   1909.00931         3.0  ['https://github.com/lanwuwei/Twitter-URL-Corp...\n",
       "8   2009.01947         1.0  ['https://gitlab.com/luciacyx/nm-adaptive-code...\n",
       "9   2010.04261         1.0   ['https://github.com/goodfeli/dlbook_notation/']\n",
       "11  2011.11576         5.0  ['https://github.com/jpbrooks/conjecturing.', ...\n",
       "12  2012.09302         1.0        ['https://github.com/ain-soph/trojanzoo}.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repro_eval_links = repro_eval[repro_eval['link_count'] > 0][['id', 'link_count', 'found_links']]\n",
    "repro_eval_links.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the scores from the manually labeled dataset of 50 articles.\n",
    "- `article_link_avail`: Whether ink to the code/repository was able to be found in the article.\n",
    "- `pwc_link_avail`: Whether ink to the code/repository was able to be found in the Papers With Code (`pwc`) website.\n",
    "- `pwc_link_match`: Whether ink to the code/repository found in the Papers With Code (`pwc`) website matches the link found in the article (whether the previous 2 columns match or not).\n",
    "- `result_replication_code_avail`: Whether code to replicate the specific experiments presented in the article was available. This to measure that the code is not just a generic implementation of the model (part of the tool/package) but is specific to the experiments in the article. If code is not available, this defaults to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reproscreener.gdrive_downloader import gdrive_get_manual_eval\n",
    "manual = gdrive_get_manual_eval(overwrite=True, manual_path=manual_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_manual_eval_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m found_repo_df \u001b[39m=\u001b[39m get_manual_eval_urls(manual)\n\u001b[1;32m      2\u001b[0m manual \u001b[39m=\u001b[39m manual\u001b[39m.\u001b[39mmerge(found_repo_df[[\u001b[39m'\u001b[39m\u001b[39mpaper\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfound_repo_url\u001b[39m\u001b[39m'\u001b[39m]], on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpaper\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m manual[\u001b[39m'\u001b[39m\u001b[39mfound_repo_url\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m manual[\u001b[39m'\u001b[39m\u001b[39mfound_repo_url\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mnan)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_manual_eval_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# found_repo_df = get_manual_eval_urls(manual)\n",
    "manual = manual.merge(found_repo_df[['paper', 'found_repo_url']], on='paper', how='left')\n",
    "manual['found_repo_url'] = manual['found_repo_url'].apply(lambda x: x[0] if len(x) > 0 else np.nan)\n",
    "found_repo_df.dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_df_numerical = manual[['paper', 'article_link_avail', 'pwc_link_avail', 'pwc_link_match', 'result_replication_code_avail']]\n",
    "# manual_df_numerical = manual_df_numerical.fillna(0) # fill NaN with 0\n",
    "dtypes_manual = {'paper': str, 'article_link_avail': float, 'pwc_link_avail': float, 'pwc_link_match': float, 'result_replication_code_avail': float}\n",
    "manual_df_numerical = manual_df_numerical.astype(dtypes_manual) # convert to int\n",
    "manual_df_numerical[9:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tally of manual evaluation of the 50 articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_df_numerical.sum(axis=0, numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_vs_repro = manual_df_numerical.merge(repro_eval_links, left_on='paper', right_on='id', how='left')\n",
    "# manual_df_numerical.article_link_avail.sum(), manual_df_numerical.result_replication_code_avail.sum()\n",
    "print(f\"Manual evaluation found links in {manual_vs_repro.article_link_avail.sum()} papers, ReproScreener found links in {(manual_vs_repro.link_count>0).sum()} papers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing file structure and dependency checks on downloaded repositories of the manually labeled articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repo_checker import tally_checks_by_paper, get_downloaded_repos, dict_files_to_list, all_checks_by_paper\n",
    "\n",
    "downloaded_repos = get_downloaded_repos(path_corpus)\n",
    "manual['repo_downloaded'] = manual['paper'].isin(downloaded_repos)\n",
    "manual_disp = manual[manual['repo_downloaded'] == True][['paper', 'repo_downloaded', 'found_repo_url']]\n",
    "# manual_disp_all = manual_disp.copy()\n",
    "manual_disp[\"checks\"] = manual_disp[\"paper\"].apply(lambda x: tally_checks_by_paper(path_corpus, x, only_found=True, verbose=False))\n",
    "manual_disp[[\"tally\", \"found\"]] = pd.DataFrame(manual_disp[\"checks\"].tolist(), index=manual_disp.index)\n",
    "manual_disp = manual_disp.join(manual_disp[\"tally\"].apply(pd.Series, dtype=float).rename(columns=lambda x: f\"tally_{x}\"))\n",
    "manual_disp = manual_disp.join(manual_disp[\"found\"].apply(pd.Series, dtype=object).apply(dict_files_to_list))\n",
    "manual_disp = manual_disp.drop(columns=[\"tally\", \"found\", \"checks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = manual_disp[['paper','wrapper_script', 'parsed_readme', 'software_dependencies']].copy()\n",
    "# Add a new column to identify the original column\n",
    "df['original_column'] = df.index\n",
    "\n",
    "# Melt the dataframe to create a new row for each unique value in the current column\n",
    "df_melted = pd.melt(df, id_vars=['original_column'])\n",
    "\n",
    "# Drop any rows with NaN values\n",
    "df_melted.dropna(inplace=True)\n",
    "\n",
    "# Create a new column for each unique value in the current column\n",
    "df_melted = pd.concat([df_melted.drop('value', axis=1),\n",
    "                       df_melted['value'].str.split(',', expand=True)],\n",
    "                      axis=1)\n",
    "if 'value' in df_melted.columns:\n",
    "    df_melted.drop('value', axis=1, inplace=True)\n",
    "# Drop the 'value' column\n",
    "# df_melted.drop('value', axis=1, inplace=True)\n",
    "\n",
    "# Pivot the dataframe to transpose it\n",
    "df_pivoted = pd.pivot_table(df_melted, index=['variable', 'original_column'],\n",
    "                            aggfunc=lambda x: ' '.join(str(v) for v in x))\n",
    "\n",
    "# Reset the index to create a new column for each original column\n",
    "df_pivoted.reset_index(level=1, inplace=True)\n",
    "\n",
    "# Create a new column for the original column\n",
    "df_pivoted['original_column'] = df_pivoted['original_column'].astype(str).astype(int)\n",
    "\n",
    "# Pivot the dataframe again to put the original column in its own column\n",
    "df_final = pd.pivot_table(df_pivoted, index='original_column',\n",
    "                          columns='variable', values=0)\n",
    "\n",
    "# Rename the columns\n",
    "df_final.columns = [f\"{col}_val\" for col in df_final.columns]\n",
    "\n",
    "# Reset the index\n",
    "df_final.reset_index(inplace=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_disp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge manual_df_numerical with repro_eval using a left join\n",
    "all_papers_df = manual_df_numerical.merge(repro_eval, left_on='paper', right_on='id', how='left')\n",
    "\n",
    "# Fill NaN values in link_count with 0 and found_links with an empty string\n",
    "all_papers_df['link_count'] = all_papers_df['link_count'].fillna(0)\n",
    "all_papers_df['found_links'] = all_papers_df['found_links'].fillna(\"\")\n",
    "\n",
    "# Set the index to 'paper' and remove unnecessary columns\n",
    "repro_eval_plot = all_papers_df.drop(columns=['id', 'title', 'index', 'affiliation', 'found_links', 'pwc_link_match', 'result_replication_code_avail', 'article_link_avail', 'pwc_link_avail', 'results']).set_index('paper').T\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 5), tight_layout={\"pad\": 1.5})\n",
    "sns.heatmap(repro_eval_plot, cbar=False, cmap=sns.cubehelix_palette(start=2, rot=0, dark=0, light=.85, as_cmap=True))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font_scale=1.5)\n",
    "plt.subplots_adjust(top=0.95, left=0.15, right=0.95)\n",
    "plt.xlabel(\"\")\n",
    "plt.savefig(\"../heatmap_manual_eval.png\", dpi=900, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repro-screener",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20a99d91287664821558953b6dd64860c99c1cbc0b07e06fad4c963774f71786"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
