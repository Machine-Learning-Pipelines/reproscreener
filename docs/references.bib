@inproceedings{bhaskarReproscreenerLeveragingLLMs2024,
  title = {Reproscreener: {{Leveraging LLMs}} for {{Assessing Computational Reproducibility}} of {{Machine Learning Pipelines}}},
  shorttitle = {Reproscreener},
  booktitle = {Proceedings of the 2nd {{ACM Conference}} on {{Reproducibility}} and {{Replicability}}},
  author = {Bhaskar, Adhithya and Stodden, Victoria},
  date = {2024-07-11},
  series = {{{ACM REP}} '24},
  pages = {101--109},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3641525.3663629},
  url = {https://doi.org/10.1145/3641525.3663629},
  urldate = {2025-02-07},
  abstract = {The increasing reliance on machine learning models in scientific research and day-to-day applications -- and the near-opacity of their associated computational methods -- creates a widely recognized need to enable others to verify results coming from Machine Learning Pipelines. In this work we use an empirical approach to build on efforts to define and deploy structured publication standards that allow machine learning research to be automatically assessed and verified, enabling greater reliability and trust in results. To automate the assessment of a set of publication standards for Machine Learning Pipelines we developed Reproscreener; a novel, open-source software tool (see https://reproscreener.org/). We benchmark Reproscreener's automatic reproducibility assessment against a novel manually labeled ``gold standard'' dataset of machine learning arXiv preprints. Our empirical evaluation has a dual goal: to assess Reproscreener's performance; and to uncover gaps and opportunities in current reproducibility standards. We develop reproducibility assessment metrics we called the Repo Metrics to provide a novel overall assessment of the re-executability potential of the Machine Learning Pipeline, called the ReproScore. We used two approaches to the automatic identification of reproducibility metrics, keywords and LLM tools, and found the reproducibility metric evaluation performance of Large Language Model (LLM) tools superior to keyword associations.},
  isbn = {979-8-4007-0530-4}
}

@misc{bhaskaradhithyaReproScreenEnablingRobustness2022,
  title = {{{ReproScreen}}: {{Enabling Robustness}} in {{Machine Learning}} at {{Scale}} via {{Automated Knowledge Verification}}},
  shorttitle = {{{ReproScreen}}},
  author = {Bhaskar, Adhithya and Stodden, Victoria},
  year = {2022},
  month = jun,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.6601019},
  abstract = {This poster summarizes our motivation and proposals to build on computational reproducibility metrics in machine learning at scale for the REAL@USC-Meta poster presentation.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  keywords = {computational reproducibility}
}

@article{krafczykLearningReproducingComputational2021,
  title = {Learning from Reproducing Computational Results: Introducing Three Principles and the {{{\emph{Reproduction Package}}}}},
  shorttitle = {Learning from Reproducing Computational Results},
  author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
  year = {2021},
  month = may,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {rsta.2020.0069, 20200069},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2020.0069},
  abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel               Reproduction Package               , a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar               Reproduction Packages               .                                         This article is part of the theme issue `Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification               in silico               '.},
  langid = {english},
  file = {C\:\\Users\\adb\\Zotero\\storage\\5NLKLHWT\\Krafczyk et al_2021_Learning from reproducing computational results.pdf}
}

@inproceedings{krafczykScientificTestsContinuous2019,
  title = {Scientific {{Tests}} and {{Continuous Integration Strategies}} to {{Enhance Reproducibility}} in the {{Scientific Software Context}}},
  booktitle = {Proceedings of the 2nd {{International Workshop}} on {{Practical Reproducible Evaluation}} of {{Computer Systems}}  - {{P-RECS}} '19},
  author = {Krafczyk, Matthew and Shi, August and Bhaskar, Adhithya and Marinov, Darko and Stodden, Victoria},
  year = {2019},
  pages = {23--28},
  publisher = {{ACM Press}},
  address = {{Phoenix, AZ, USA}},
  doi = {10.1145/3322790.3330595},
  isbn = {978-1-4503-6756-1},
  langid = {english},
  file = {C\:\\Users\\adb\\Zotero\\storage\\PGQZPWZ6\\Krafczyk et al_2019_Scientific Tests and Continuous Integration Strategies to Enhance.pdf}
}

@inproceedings{stoddenEnablingVerificationComputational2018,
  title = {Enabling the {{Verification}} of {{Computational Results}}: {{An Empirical Evaluation}} of {{Computational Reproducibility}}},
  shorttitle = {Enabling the {{Verification}} of {{Computational Results}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Practical Reproducible Evaluation}} of {{Computer Systems}}},
  author = {Stodden, Victoria and Krafczyk, Matthew S. and Bhaskar, Adhithya},
  year = {2018},
  month = jun,
  pages = {1--5},
  publisher = {{ACM}},
  address = {{Tempe AZ USA}},
  doi = {10.1145/3214239.3214242},
  isbn = {978-1-4503-5861-3},
  langid = {english},
  file = {C\:\\Users\\adb\\Zotero\\storage\\UXNHSZAY\\Stodden et al_2018_Enabling the Verification of Computational Results.pdf}
}

@misc{conferenceIntroducingNeurIPS20212021,
  title = {Introducing the {{NeurIPS}} 2021 {{Paper Checklist}}},
  author = {Conference, Neural Information Processing Systems},
  year = {2021},
  month = mar,
  journal = {Medium},
  abstract = {Alina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan NeurIPS 2021 Program Chairs},
  langid = {english}
}

@article{gundersenStateArtReproducibility2018,
  title = {State of the {{Art}}: {{Reproducibility}} in {{Artificial Intelligence}}},
  author = {Gundersen, Odd Erik and Kjensmo, Sigbj{\o}rn},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  abstract = {\&lt;p\&gt;        Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.      \&lt;/p\&gt;},
  chapter = {AAAI Technical Track: Humans and AI}
}

@inproceedings{isdahlOutoftheBoxReproducibilitySurvey2019,
  title = {Out-of-the-{{Box Reproducibility}}: {{A Survey}} of {{Machine Learning Platforms}}},
  shorttitle = {Out-of-the-{{Box Reproducibility}}},
  booktitle = {2019 15th {{International Conference}} on {{eScience}} ({{eScience}})},
  author = {Isdahl, Richard and Gundersen, Odd Erik},
  year = {2019},
  month = sep,
  pages = {86--95},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/eScience.2019.00017},
  isbn = {978-1-72812-451-3}
}

@inproceedings{stoddenAIMABSTRACTIONIMPROVING2018,
  title = {{{AIM}}: {{AN ABSTRACTION FOR IMPROVING MACHINE LEARNING PREDICTION}}},
  shorttitle = {{{AIM}}},
  booktitle = {2018 {{IEEE Data Science Workshop}} ({{DSW}})},
  author = {Stodden, Victoria and Wu, Xiaomian and Sochat, Vanessa},
  year = {2018},
  month = jun,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Lausanne, Switzerland}},
  doi = {10.1109/DSW.2018.8439914},
  isbn = {978-1-5386-4410-2}
}

@misc{midwinter2021resolution,
  title = {Resolution Adaptive Networks for Efficient Inference},
  author = {Midwinter, Max and {Al-Sabbag}, Zaid Abbas and Bajaj, Rishabh},
  year = {2021}
}

@article{pineauImprovingReproducibilityMachine2020,
  title = {Improving {{Reproducibility}} in {{Machine Learning Research}} ({{A Report}} from the {{NeurIPS}} 2019 {{Reproducibility Program}})},
  author = {Pineau, Joelle and {Vincent-Lamarre}, Philippe and Sinha, Koustuv and Larivi{\`e}re, Vincent and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily and Larochelle, Hugo},
  year = {2020},
  month = dec,
  journal = {arXiv:2003.12206 [cs, stat]},
  eprint = {2003.12206},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{raghupathiReproducibilityComputingResearch2022,
  title = {Reproducibility in {{Computing Research}}: {{An Empirical Study}}},
  shorttitle = {Reproducibility in {{Computing Research}}},
  author = {Raghupathi, Wullianallur and Raghupathi, Viju and Ren, Jie},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {29207--29223},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3158675}
}

@misc{msadjadi,
  author       = {Mahdi Sadjadi},
  title        = {arxivscraper},
  year         = 2017,
  doi          = {10.5281/zenodo.889853},
  url          = {https://doi.org/10.5281/zenodo.889853}
}




