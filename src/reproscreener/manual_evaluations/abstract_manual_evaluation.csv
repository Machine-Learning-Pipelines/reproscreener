paper,problem,problem_phrase,objective,objective_phrase,research_method,research_method_phrase,research_questions,research_questions_phrase,pseudocode,pseudocode_phrase,dataset,dataset_phrase,hypothesis,hypothesis_phrase,prediction,prediction_phrase,code_avail,code_avail_phrase,software_dependencies,software_dependencies_phrase,experiment_setup,experiment_setup_phrase
Paper,Problem,Column1,Objective,Column2,Research method,Column3,Research questions,Column4,Pseudocode,Column5,Train/test/validation dataset,Column6,Hypothesis,Column7,Prediction,Column10,Code,Column8,Software dependencies,Column11,Experiment setup,Column9
1606.04671,0,Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence.,0,"Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",0,,0,,0,,0,,0,,0,,0,,0,,0,
1903.09668,0," While the representation properties of
DL have been well studied, uncertainty quantification remains challenging and largely unexplored.",0,The purpose of our paper is to show that training DL architectures with data augmentation leads to efficiency gains.,1,"We use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning. This allows variants of the expectation-maximization and MCMC algorithms to be brought to bear on these high dimensional nonlinear deep learning models. To demonstrate our methodology, we develop data augmentation algorithms for a variety of commonly used activation functions: logit, ReLU, leaky ReLU and SVM. Our methodology is compared to traditional stochastic gradient descent with back-propagation. Our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods providing substantial improvement in speed. We illustrate our methodology on a number of standard datasets. Finally, we conclude with directions for future research.",0,,0,,0,,0,,0,,0,,0,,0,
1904.10554,0,"Model-free learning for multi-agent stochastic games is an active area of
research. Existing reinforcement learning algorithms, however, are often
restricted to zero-sum games, and are applicable only in small state-action
spaces or other simplified settings.",0,"Here, we develop a new data efficient
Deep-Q-learning methodology for model-free learning of Nash equilibria for
general-sum stochastic games.",0,"The algorithm uses a local linear-quadratic
expansion of the stochastic game, which leads to analytically solvable optimal
actions. The expansion is parametrized by deep neural networks to give it
sufficient flexibility to learn the environment without the need to experience
all state-action pairs. We study symmetry properties of the algorithm stemming
from label-invariant stochastic games and as a proof of concept, apply our
algorithm to learning optimal trading strategies in competitive electronic
markets.",0,,0,,0,,0,,0,,0,,0,,0,
1908.05659,0,,0,"This paper surveys
main concepts and contributions to DRO, and its relationships with robust
optimization, risk-aversion, chance-constrained optimization, and function
regularization.",0,,0,,0,,0,,0,,0,,0,,0,,0,
1909.00931,0,"While BERT's performance improves by increasing its model
size, the required computational power is an obstacle preventing practical
applications from adopting the technology.",0,"Herein, we propose to inject phrasal
paraphrase relations into BERT in order to generate suitable representations
for semantic equivalence assessment instead of increasing the model size.",0,"Herein, we propose to inject phrasal
paraphrase relations into BERT in order to generate suitable representations
for semantic equivalence assessment instead of increasing the model size.
Experiments on standard natural language understanding tasks confirm that our
method effectively improves a smaller BERT model while maintaining the model
size. ",0,,0,,0,,0," Herein, we propose to inject ",0,,0,,0,,0,
1911.03867,0,The absence of large quantities of representative data from current astronomical surveys motivates the development,0,"The absence of large quantities of
representative data from current astronomical surveys motivates the development
of a robust forward-modeling approach using synthetic lensing images. Using a
mock sample of strong lenses created upon a state-of-the-art extragalactic
catalogs, we train a modular deep learning pipeline for uncertainty-quantified
detection and modeling with intermediate image processing components for
denoising and deblending the lensing systems.",0," Using a mock sample of strong lenses created upon a state-of-the-art extragalactic catalogs, we train a modular deep learning pipeline for uncertainty-quantified detection and modeling with intermediate image processing components for denoising and deblending the lensing systems. We demonstrate a high degree of interpretability and controlled systematics due to domain-specific task modules trained with different stages of synthetic image generation. For lens detection and modeling, we obtain semantically meaningful latent spaces that separate classes of strong lens images and yield uncertainty estimates that explain the origin of misclassified images and provide probabilistic predictions for the lens parameters. Validation of the inference pipeline has been carried out using images from the Subaru telescope's Hyper Suprime-Cam camera, and LSST DESC simulated DC2 sky survey catalogues.",0,,0,,0,,0,,0,,0,,0,,0,
2002.05905,0,"However, USB flash drives are also one of the most
common attack vectors used to gain unauthorized access to host devices. For
instance, it is possible to replace a USB drive so that when the USB key is
connected, it would install passwords stealing tools, root-kit software, and
other disrupting malware. In such a way, an attacker can steal sensitive
information via the USB-connected devices, as well as inject any kind of
malicious software into the host.",0,"To thwart the above-cited raising threats, we propose MAGNETO",0,,0,,0,,0,,0,,0,"MAGNETO can also identify the
specific USB Flash drive, with a minimum classification accuracy of 91.2%",0,,0,,0,
2004.05258,0,"Analyzing a huge amount of malware is a major burden for security analysts. Since emerging malware is often a variant of existing malware, automatically classifying malware into known families greatly reduces a part of their burden.",0,"In this paper, we conducted an exhaustive survey of deep learning models using 24 ImageNet pre-trained models and five fine-tuning parameters, totaling 120 combinations, on two platforms.",0,,0,,0,,0,,0,,0,,0,,0,,0,"In this paper, we conducted an exhaustive survey of deep learning models using 24 ImageNet pre-trained models and five fine-tuning parameters, totaling 120 combinations, on two platforms."
2009.01947,0,"We present combinatorial and parallelizable algorithms for maximization of a submodular function, not necessarily monotone, with respect to a size constraint.",0,"In this
version, we propose a fixed and improved subroutine to add a set with high
average marginal gain, \threseq, which returns a solution in $O( \log(n) )$
adaptive rounds with high probability. ",0,,0,,0,,0,,0,,0,,0,,0,,0,"In this version, we propose a fixed and improved subroutine to add a set with high average marginal gain, \threseq, which returns a solution in $O( \log(n) )$ adaptive rounds with high probability. Moreover, we provide two approximation algorithms. The first has approximation ratio"
2010.04261,0,,0," In this paper, we propose a decoupling conjecture that decomposes the layer-wise Hessians of a network as the Kronecker product of two smaller matrices.",0,,0,,0,,0,,0," In this paper, we propose a decoupling conjecture that decomposes the layer-wise Hessians of a network as the Kronecker product of two smaller matrices.",0,,0,,0,,0,
2010.04855,0,"We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves.",0,We prove uniform consistency with finite sample rates ,0,We prove uniform consistency with finite sample rates via original analysis of generalized kernel ridge regression. We extend our main results to counterfactual distributions and to causal functions identified by front and back door criteria.,0,,0,,0,,0,,0,,0,,0,,0,
2011.11576,0,We propose the use of a conjecturing machine that generates feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features. ,0,,0,We then compare the method to a previously-proposed framework for symbolic regression and demonstrate that it can also be used to recover equations that are satisfied among features in a dataset. The framework is then applied to patient-level data regarding COVID-19 outcomes to suggest possible risk factors that are confirmed in medical literature.,0,,0,,0,,0,,0,,0,,0,,0,
2012.09302,0,"Neural backdoors represent one primary threat to the security of deep learning systems. The intensive research has produced a plethora of backdoor attacks/defenses, resulting in a constant arms race. ",0,"To bridge this gap, we design and implement TROJANZOO, the first open-source
platform for evaluating neural backdoor attacks/defenses in a unified,
holistic, and practical manner.",0,"Leveraging TROJANZOO, we conduct a systematic study on the existing attacks/defenses, unveiling their complex design spectrum: both manifest intricate trade-offs among multiple desiderata (e.g., the effectiveness, evasiveness, and transferability of attacks). We further explore improving the existing attacks/defenses, leading to a number of interesting findings",1,many critical questions remain under-explored: (i) what are the strengths and limitations of different attacks/defenses? (ii) what are the best practices to operate them? and (iii) how can the existing attacks/defenses be further improved? ,0,,0,,0,,0,,0,"To bridge this gap, we design and implement TROJANZOO, the first open-source",0,,0,
2101.07354,0,"Random-walk based network embedding algorithms like DeepWalk and node2vec are widely used to obtain Euclidean representation of the nodes in a network prior to performing downstream inference tasks. However, despite their impressive empirical performance, there is a lack of theoretical results explaining their large-sample behavior.",0,"In this paper, we study node2vec and DeepWalk through the perspective of matrix factorization. In particular, we analyze these algorithms in the setting of community detection for stochastic blockmodel graphs (and their degree-corrected variants).",0,"By exploiting the row-wise uniform perturbation bound for leading singular vectors, we derive high-probability error bounds between the matrix factorization-based node2vec/DeepWalk embeddings and their true counterparts, uniformly over all node embeddings.",0,,0,,0,,0,,0,,0,,0,,0,
2102.11887,0,"We conclude that to achieve the goal of full quantum machine learning, it is crucial to utilize the deferred measurement principle.",0,,0,"We define its quantum generalization, the quantum cross entropy, prove its lower bounds, and investigate its relation to quantum fidelity. ",0,,0,,0,,0,,0,,0,,0,,0,
2104.11893,0,"However, it relies on disentangling information heavily from a local range (i.e., a node and its 1-hop neighbors), while the local information in many cases can be uneven and incomplete, hindering the interpretabiliy power and model performance of DisenGCN.",0," In this paper, we introduce a novel Local and Global Disentangled Graph Convolutional Network (LGD-GCN) to capture both local and global information for graph disentanglement.",0,"GD-GCN performs a statistical mixture modeling to derive a factor-aware latent continuous space, and then constructs different structures w.r.t. different factors from the revealed space. In this way, the global factor-specific information can be efficiently and selectively encoded via a message passing along these built structures, strengthening the intra-factor consistency",0,,0,,0,,0,,0,,0,,0,,0,
2104.12546,0,"The lack of knowledge about the virus, the extension of this phenomenon, and the speed of the evolution of the infection are all factors that highlight the necessity of employing new approaches to study these events. ",1,"The aim of this work is to investigate any possible relationships between air quality and confirmed cases of COVID-19 in Italian districts. Specifically, we report an analysis of the correlation between daily COVID-19 cases and environmental factors, such as temperature, relative humidity, and atmospheric pollutants.",0,,0,,0,,0,,0,"Our analysis confirms a significant
association of some environmental parameters with the spread of the virus.",0,,0,,0,,0,
2105.01099,0,"Most of
the literature has appeared in the last few years, and several core challenges
are to continue to be tackled: model complexity, agent coordination, and joint
optimization of multiple levers.",0,"In this paper, we present a comprehensive, in-depth survey of the literature
on reinforcement learning approaches to decision optimization problems in a
typical ridesharing system.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2105.01937,0," Yet, multi-view algorithms strongly depend on camera parameters; particularly, the relative transformations between the cameras. Such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings.",0,"We introduce FLEX (Free muLti-view rEconstruXion), an
end-to-end extrinsic parameter-free multi-view model. FLEX is extrinsic
parameter-free (dubbed ep-free) in the sense that it does not require extrinsic
camera parameters. ",0,,0,,0,,1," Code, trained models, and other materials are available on our project page.",0,,0,,1," Code, trained models, and other materials are available on our project page.",0,,0,
2105.15197,0,"Debiased machine learning is a meta algorithm based on bias correction and
sample splitting to calculate confidence intervals for functionals, i.e. scalar
summaries, of machine learning algorithms.",0,"We provide a nonasymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions.",0,"Formally, we prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments.",0,,0,,0,,0,,0,,0,,0,,0,
2106.01528,0,"Controlled feature selection aims to discover the features a response depends on while limiting the false discovery rate (FDR) to a predefined level. Recently, multiple deep-learning-based methods have been proposed to perform controlled feature selection through the Model-X knockoff framework. We demonstrate, however, that these methods often fail to control the FDR for two reasons",0," We propose a new procedure called FlowSelect to
perform controlled feature selection that does not suffer from either of these
two problems. ",0,,0,"We demonstrate, however, that these methods often fail to control the FDR for two reasons. First, these methods often learn inaccurate models of features. Second, the ""swap"" property, which is required for knockoffs to be valid, is often not well enforced.",0,,0,,0,,0,,0,,0,,0,
2106.03157,0,"Existing combinatorial search methods are often complex and require some level of expertise. This work introduces a simple and efficient deep learning method for solving combinatorial problems with a predefined goal, represented by Rubik's Cube.",0,"This work introduces a simple and efficient deep learning
method for solving combinatorial problems with a predefined goal, represented
by Rubik's Cube. We demonstrate that, for such problems, training a deep neural network on random scrambles branching from the goal state is sufficient to achieve near-optimal solutions.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2106.03725,0,,0,The paper defines and studies manifold (M) convolutional filters and neural networks (NNs). \emph{Manifold} filters and MNNs are defined in terms of the Laplace-Beltrami operator exponential and are such that \emph{graph} (G) filters and neural networks (NNs) are recovered as discrete approximations when the manifold is sampled. ,0,,0,,0,,0,,0,,0,,0,,0,,0,
2106.06927,0,"Despite unconditional feature inversion being the foundation of many image synthesis applications, training an inverter demands a high computational budget, large decoding capacity and imposing conditions such as autoregressive priors.",0,"To address these limitations, we propose the use of adversarially robust representations as a perceptual primitive for feature inversion. We train an adversarially robust encoder to extract disentangled and perceptually-aligned image representations, making them easily invertible.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2106.07704,0," Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences.",0," In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2106.10898,1,"For collaborative filtering, the classical method is training the model offline, then perform the online testing, but this approach can no longer handle the dynamic changes in user preferences which is the so-called cold start. So how to effectively recommend items to users in the absence of effective information?",0,"To address the aforementioned problems, a multi-armed bandit based collaborative filtering recommender system has been proposed, named BanditMF",0,,0,"BanditMF is designed to address two challenges in the multi-armed bandits algorithm and collaborative filtering: (1) how to solve the cold start problem for collaborative filtering under the condition of scarcity of valid information, (2) how to solve the sub-optimal problem of bandit algorithms in strong social relations domains caused by independently estimating unknown parameters associated with each user and ignoring correlations between users.",0,,0,,0,,0,,0,,0,,0,
2106.12177,1,"However, this replicating process could be problematic, such as the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments.",0,"In this survey, we provide a systematic review on imitation learning.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2106.12936,0,,0,We study the frontier between learnable and unlearnable hidden Markov models (HMMs),0,,0,,0,,0,,0,,0,,0,,0,,0,
2106.13823,0,,0,"In this paper, we present one operational interpretation of this quantity, that the quantum cross entropy is the compression rate for sub-optimal quantum source coding",0,,0,,0,,0,,0,,0,,0,,0,,0,
2107.01131,0,"Successful applications of InfoNCE and its variants have popularized the use of contrastive variational mutual information (MI) estimators in machine learning. While featuring superior stability, these estimators crucially depend on costly large-batch training, and they sacrifice bound tightness for variance reduction.",0,"To overcome these limitations, we revisit the mathematics of popular variational MI bounds from the lens of unnormalized statistical modeling and convex optimization.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2108.09779,0,We present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary 6-DoF pose with only 3-fingers trained with NVIDIA's IsaacGym simulator.,0,"We present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary 6-DoF pose with only 3-fingers trained with NVIDIA's IsaacGym simulator. We show empirical benefits, both in
simulation and sim-to-real transfer, of using keypoints as opposed to
position+quaternion representations for the object pose in 6-DoF for policy
observations and in reward calculation to train a model-free reinforcement
learning agent.",0,,0,,0,,0,,0,,0,,1," With the aim of assisting further research in learning in-hand manipulation, we make the codebase of our system, along with trained checkpoints that come with billions of steps of experience available, at https://s2r2-ig.github.io",0,,0,
2109.01372,0,"This work explores the effect of noisy sample selection in active learning
strategies. ",0,"We show on both synthetic problems and real-life use-cases that
knowledge of the sample noise can significantly improve the performance of
active learning strategies.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2109.12784,1,"Motivated by the problem of learning with small sample sizes, this paper shows how to incorporate into support-vector machines (SVMs) those properties that have made convolutional neural networks (CNNs) successful.",0,"We address this lacuna and show that positive definiteness indeed holds \textit{with high probability} for kernels based on the maximum similarity in the small training sample set regime of interest, and that they do yield the best results in that regime. We also show how additional properties such as their ability to incorporate local features at multiple spatial scales, e.g., as done in CNNs through max pooling, and to provide the benefits of composition through the architecture of multiple layers, can also be embedded into SVMs.",0,,0,,0,,0,We verify through experiments on widely available image sets that the resulting SVMs do provide superior accuracy in comparison to well-established deep neural network benchmarks for small sample sizes.,0,,0,,0,,0,,0," We
address this lacuna and show that positive definiteness indeed holds
\textit{with high probability} for kernels based on the maximum similarity in
the small training sample set regime of interest, and that they do yield the
best results in that regime. We also show how additional properties such as
their ability to incorporate local features at multiple spatial scales, e.g.,
as done in CNNs through max pooling, and to provide the benefits of composition
through the architecture of multiple layers, can also be embedded into SVMs. We
verify through experiments on widely available image sets that the resulting
SVMs do provide superior accuracy in comparison to well-established deep neural
network benchmarks for small sample sizes."
2110.02343,0,"Quantum machine learning promises to efficiently solve important problems. There are two persistent challenges in classical machine learning: the lack of labeled data, and the limit of computational power.",0," We propose a novel framework that resolves both issues: quantum semi-supervised learning. Moreover, we provide a protocol in systematically designing quantum machine learning algorithms with quantum supremacy, which can be extended beyond quantum semi-supervised learning.",0," We showcase two concrete quantum semi-supervised learning algorithms: a quantum self-training algorithm named the propagating nearest-neighbor classifier, and the quantum semi-supervised K-means clustering algorithm. By doing time complexity analysis, we conclude that they indeed possess quantum supremacy.",0,,0,,0,,0,,0,"By doing time complexity analysis, we conclude that they indeed
possess quantum supremacy.",0,,0,,0,
2110.02474,0,I model the belief formation and decision making processes of economic agents during a monetary policy regime change (an acceleration in the money supply) with a deep reinforcement learning algorithm in the AI literature. ,0,"I show that
when the money supply accelerates, the learning agents only adjust their
actions, which include consumption and demand for real balance, after gathering
learning experience for many periods.",0,,0,"I also show that, 1. the AI agents who
explores their environment more adapt to the policy regime change quicker,
which leads to welfare improvements and less inflation volatility, and 2. the
AI agents who have experienced a structural change adjust their beliefs and
behaviours quicker than an inexperienced learning agent.",0,,0,,0,,0,,0,,0,,0,
2110.03135,0,"Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples - the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples.",0,"We show that label noise exists in adversarial training. Guided by our analyses, we
proposed a method to automatically calibrate the label to address the label
noise and robust overfitting. Our method achieves consistent performance
improvements across various models and datasets without introducing new
hyper-parameters or additional tuning.",0,"Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting.",0,,0,,0,,0,,0,,0,,0,,0,
2110.05169,0,"Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ. For instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. It can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). There is a need to develop RL methods that generalize well to variations of the training conditions.",0,"In this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. ",0,,0,,0,,0,,0,,0,,0,,0,,0,
2110.08255,0,"However, the current state of the art sparse Transformer architectures fail to couple down- and upsampling procedures to produce outputs in a similar resolution as the input.",0,"We propose the Yformer model, based on a novel Y-shaped encoder-decoder architecture that (1) uses direct connection from the downscaled encoder layer to the corresponding upsampled decoder layer in a U-Net inspired architecture, (2) Combines the downscaling/upsampling with sparse attention to capture long-range effects, and (3) stabilizes the encoder-decoder stacks with the addition of an auxiliary reconstruction loss.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2110.08432,1,"a critical challenge in MAML is to calculate the gradient w.r.t. the initialization of a long training trajectory for the sampled tasks, because the computation graph can rapidly explode and the computational cost is very expensive.",0,"To address this problem, we propose Adjoint MAML (A-MAML). We view gradient descent in the inner optimization as the evolution of an Ordinary Differential Equation (ODE).",0,"To efficiently compute the gradient of the validation loss w.r.t. the initialization, we use the adjoint method to construct a companion, backward ODE. To obtain the gradient w.r.t. the initialization, we only need to run the standard ODE solver twice -- one is forward in time that evolves a long trajectory of gradient flow for the sampled task; the other is backward and solves the adjoint ODE.",0,,0,,0,,0,,0,,0,,0,,0,
2110.09902,0,"We make an attempt to understanding convolutional neural network by exploring
the relationship between (deep) convolutional neural networks and Volterra
convolutions",0,"We propose a novel approach to explain and study the overall
characteristics of neural networks without being disturbed by the horribly
complex architectures.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2110.11688,1,"Machine learning models can leak information about the data used to train them. To mitigate this issue, Differentially Private (DP) variants of optimization algorithms like Stochastic Gradient Descent (DP-SGD) have been designed to trade-off utility for privacy in Empirical Risk Minimization (ERM) problems.",0,"In this paper, we propose Differentially Private proximal Coordinate Descent (DP-CD), a new method to solve composite DP-ERM problems. We derive utility guarantees through a novel theoretical analysis of inexact coordinate descent.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2110.14241,0,"Previous work using a single set of agents has shown great progress in generalizing to known partners, however it struggles when coordinating with unfamiliar agents. ",0,"In this work, our goal is to train agents that can coordinate with seen, unseen as well as human partners in a multi-agent communication environment involving natural language.",0," We perform a holistic evaluation of our method on two different referential games, and show that our agents outperform all prior work when communicating with seen partners and humans. Furthermore, we analyze the natural language generation skills of our agents, where we find that our agents also outperform strong baselines. Finally, we test the robustness of our agents when communicating with out-of-population agents and carefully test the importance of each component of our method through ablation studies.",0,,0,,0,,0,,0,"We perform a
holistic evaluation of our method on two different referential games, and show
that our agents outperform all prior work when communicating with seen partners
and humans.",0,,0,,0,
2111.02997,0,"In this paper, we establish the global optimality and convergence rate of an
off-policy actor critic algorithm in the tabular setting without using density
ratio to correct the discrepancy between the state distribution of the behavior
policy and that of the target policy",0,"In this paper, we establish the global optimality and convergence rate of an off-policy actor critic algorithm in the tabular setting without using density ratio to correct the discrepancy between the state distribution of the behavior policy and that of the target policy. ",0,,0,,0,,0,,0,,0,,0,,0,,0,
2111.03664,0,,0,"Conventional KD methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2111.08356,0," In both FL and PFL, all clients participate in the training process and their labeled data are used for training. However, in reality, novel clients may wish to join a prediction service after it has been deployed, obtaining predictions for their own \textbf{unlabeled} data.",0,"Here, we introduce a new learning setup, On-Demand Unlabeled PFL (OD-PFL), where a system trained on a set of clients, needs to be later applied to novel unlabeled clients at inference time. We propose a novel approach to this problem, ODPFL-HN, which learns to produce a new model for the late-to-the-party client.",0,"Specifically, we train an encoder network that learns a representation for a client given its unlabeled data. That client representation is fed to a hypernetwork that generates a personalized model for that client. Evaluated on five benchmark datasets, we find that ODPFL-HN generalizes better than the current FL and PFL methods, especially when the novel client has a large shift from training clients. We also analyzed the generalization error for novel clients, and showed analytically and experimentally how novel clients can apply differential privacy.",0,,0,,0,,0,,0,,0,,0,,0,
2111.12673,0,Accurate value estimates are important for off-policy reinforcement learning. Algorithms based on temporal difference learning typically are prone to an over- or underestimation bias building up over time.,0,"In this paper, we propose a general method called Adaptively Calibrated Critics (ACC) that uses the most recent high variance but unbiased on-policy rollouts to alleviate the bias of the low variance temporal difference targets.",0,,0,,0,,0,,0,,0,,0,,0,,0,
2111.15449,0,"In the case of end-to-end learning, there is usually no effective loss function that completely relies on the features of the middle layer to restrict learning, resulting in the distribution of sample latent features is not optimal, so there is still room for improvement in classification accuracy.",0," Based on the concept of Predefined Evenly-Distributed Class Centroids (PEDCC), this article proposes a Softmax-free loss function based on predefined optimal-distribution of latent features-POD Loss",0,,0,,0,,0,,0,,0,,1,Code is available in https://github.com/TianYuZu/POD-Loss.,0,,0,
2112.00016,0,,0,We use deep neural networks to machine learn correlations between knot invariants in various dimensions.,0,,0,,0,,0,,0,,0,,0,,0,,0,
2112.03432,0,"Obtaining first-order regret bounds -- regret bounds scaling not as the worst-case but with some measure of the performance of the optimal policy on a given instance -- is a core question in sequential decision-making. While such bounds exist in many settings, they have proven elusive in reinforcement learning with large state spaces.",0,"In this work we address this gap, and show that it is possible to obtain regret scaling as $\widetilde{\mathcal{O}}(\sqrt{d^3 H^3 \cdot V_1^\star \cdot K} + d^{3.5}H^3\log K )$ in reinforcement learning with large state spaces, namely the linear MDP setting.",0,"In this work we address this gap, and show that it is possible to obtain regret scaling as $\widetilde{\mathcal{O}}(\sqrt{d^3 H^3 \cdot V_1^\star \cdot K} + d^{3.5}H^3\log K )$ in reinforcement learning with large state spaces, namely the linear MDP setting. Here $V_1^\star$ is the value of the optimal policy and $K$ is the number of episodes. We demonstrate that existing techniques based on least squares estimation are insufficient to obtain this result, and instead develop a novel robust self-normalized concentration bound based on the robust Catoni mean estimator, which may be of independent interest.",0,,0,,0,,0,,0,,0,,0,,0,
2112.04871,1,"However, most previous KGE methods ignore the semantic similarity between the related entities and entity-relation couples in different triples since they separately optimize each triple with the scoring function.",0,"To address this problem, we propose a simple yet efficient contrastive learning framework for tensor decomposition based (TDB) KGE, which can shorten the semantic distance of the related entities and entity-relation couples in different triples and thus improve the performance of KGE.",0,,0,,0,,0,,0,,0,,0,,0,,0,
,6,,1,,1,,1,,0,,1,,0,,0,,3,,0,,0,