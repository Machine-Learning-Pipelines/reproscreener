Note: The 3rd column of each metric is the comparision with the manually evaluated abstract. 1 signifies true/correct judgement by GPT. It doesn't necessarily have to agree with the manual evaluation as there were a couple of papers where GPT correctly identified metrics that were missed in the manual evaluation. Highlighted cells indicate interesting findings (GPT was mistaken/Manual evaluation was incorrect/etc.),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
paper,problem,problem_phrase,problem_phrase_comparision,objective,objective_phrase,objective_phrase_comparision,research_method,research_method_phrase,research_method_phrase_comparision,research_questions,research_questions_phrase,research_questions_phrase_comparision,pseudocode,pseudocode_phrase,pseudocode_phrase_comparision,dataset,dataset_phrase,dataset_phrase_comparision,hypothesis,hypothesis_phrase,hypothesis_phrase_comparision,prediction,prediction_phrase,prediction_phrase_comparision,code_avail,code_avail_phrase,code_avail_phrase_comparision,software_dependencies,software_dependencies_phrase,software_dependencies_phrase_comparision,experiment_setup,experiment_setup_phrase,experiment_setup_phrase_comparision,
Paper,Problem,Column1,Column12,Objective,Column2,Column22,Research method,Column3,Column32,Research questions,Column4,Column42,Pseudocode,Column5,Column52,Train/test/validation dataset,Column6,Column62,Hypothesis,Column7,Column72,Prediction,Column10,Column102,Code,Column8,Column82,Software dependencies,Column11,Column112,Experiment setup,Column9,Column92,Full abstract
1606.04671,1,Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence.,1,0.7,"The objective is not explicitly stated in the abstract, but it can be implied from """"The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features.",0,1,"Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",0,0,Not mentioned in the abstract,1,0,Not mentioned in the abstract,1,0.5,"We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games)"""" implies usage of dataset, but it is not explicitly mentioned if the dataset is shared or made available",1,0,Not mentioned in the abstract,1,0.7,"""and show that it outperforms common baselines based on pretraining and finetuning. """" implies a prediction but not a very explicit one.",1,0,Not mentioned in the abstract,1,0,Not mentioned in the abstract,1,0.5,"""""We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games)"""" implies an experimental setup but it is not detailed.",1,
1903.09668,1,"While the representation properties of DL have been well studied, uncertainty quantification remains challenging and largely unexplored.",1,1,The purpose of our paper is to show that training DL architectures with data augmentation leads to efficiency gains.,1,1,We use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning,1,0,Not explicitly mentioned ,1,0,Not mentioned ,1,0.5,We illustrate our methodology on a number of standard datasets.,0,0.5,"The hypothesis isn't directly stated, though it can be implied from """"The purpose of our paper is to show that training DL architectures with data augmentation leads to efficiency gains.",0,0,Not mentioned ,1,0,Not mentioned ,1,0,Not mentioned ,1,1,Our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods providing substantial improvement in speed,0,
1904.10554,1,"Model-free learning for multi-agent stochastic games is an active area of research. Existing reinforcement learning algorithms, however, are often restricted to zero-sum games, and are applicable only in small state-action spaces or other simplified settings.",1,1,"Here, we develop a new data efficient Deep-Q-learning methodology for model-free learning of Nash equilibria for general-sum stochastic games.",1,1,"The algorithm uses a local linear-quadratic expansion of the stochastic game, which leads to analytically solvable optimal actions. The expansion is parametrized by deep neural networks to give it sufficient flexibility to learn the environment without the need to experience all state-action pairs.",1,0,N/A,1,0,N/A,1,0,N/A,1,0,N/A,1,0,N/A,1,0,N/A,1,0,N/A,1,0.5,"as a proof of concept, apply our algorithm to learning optimal trading strategies in competitive electronic markets.",0,
1908.05659,0.8,"The concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade.",0,1,"This paper surveys main concepts and contributions to DRO, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.",1,0,The specific research method isn't mentioned in the given abstract.,1,0,Research questions aren't explicitly mentioned in the given abstract.,1,0,No pseudocode is described in the provided abstract.,1,0,The data set's availability is not mentioned in the provided abstract.,1,0,No explicit hypotheses are made in the given abstract.,1,0,There isn't an explicit prediction in the provided abstract.,1,0,The source code isn't mentioned in the provided abstract.,1,0,No mention of software packages / dependencies used to run the code in the given abstract.,1,0,Experimental setup isn't described in the provided abstract.,1,
1909.00931,1,"""While BERT's performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology",1,1,"""Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size.",1,1,"we propose to inject phrasal paraphrase relations into BERT"""", """"Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size.",1,0,Not Mentioned,1,0,Not Mentioned,1,0.5,"Mention of """"Experiments on standard natural language understanding tasks"""" implies some type of dataset was used but not specifically shared.",1,1,"Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size.",1,1,"The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks."""" and Â """"Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.",0,0,Not Mentioned,1,1,Not Mentioned,1,1,Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size.,0,
1911.03867,1,Upcoming large astronomical surveys are expected to capture an unprecedented number of strong gravitational lensing systems.,0,1,Deep learning is emerging as a promising practical tool for the detection and quantification of these galaxy-scale image distortions.,0,1,"Using a mock sample of strong lenses created upon a state-of-the-art extragalactic catalogs, we train a modular deep learning pipeline for uncertainty-quantified detection and modeling with intermediate image processing components for denoising and deblending the lensing systems.",1,0,,1,0,,1,0.75,"Validation of the inference pipeline has been carried out using images from the Subaru telescope's Hyper Suprime-Cam camera, and LSST DESC simulated DC2 sky survey catalogues",0,0,,1,0.5,We demonstrate a high degree of interpretability and controlled systematics due to domain-specific task modules trained with different stages of synthetic image generation,0,0,,1,0,,1,0.7,We obtain semantically meaningful latent spaces that separate classes of strong lens images and yield uncertainty estimates that explain the origin of misclassified images and provide probabilistic predictions for the lens parameters.,0,
2002.05905,1,"However, USB flash drives are also one of the most common attack vectors used to gain unauthorized access to host devices. For instance, it is possible to replace a USB drive so that when the USB key is connected, it would install passwords stealing tools, root-kit software, and other disrupting malware.",1,1,"To thwart the above-cited raising threats, we propose MAGNETO, an efficient, non-interactive, and privacy-preserving framework to verify the authenticity of a USB flash drive, rooted in the analysis of its unintentional magnetic emissions.",1,1,"We show that the magnetic emissions radiated during boot operations on a specific host are unique for each device, and sufficient to uniquely fingerprint both the brand and the model of the USB flash drive, or the specific USB device, depending on the used equipment.",0,0,,1,0,,1,1,"Our investigation on 59 different USB flash drives---belonging to 17 brands, including the top brands purchased on Amazon in mid-2019---reveals a minimum classification accuracy of 98.2% in the identification of both brand and model, accompanied by a negligible time and computational overhead.",0,0,,1,1,"MAGNETO can also identify the specific USB Flash drive, with a minimum classification accuracy of 91.2%",1,0,,1,0,,1,1,"""Our investigation on 59 different USB flash drives---belonging to 17 brands, including the top brands purchased on Amazon in mid-2019---, reveals a minimum classification accuracy of 98.2% in the identification of both brand and model, accompanied by a negligible time and computational overhead",0,
2004.05258,1,Analyzing a huge amount of malware is a major burden for security analysts.,1,1,"In this paper, we conducted an exhaustive survey of deep learning models using 24 ImageNet pre-trained models and five fine-tuning parameters, totaling 120 combinations, on two platforms",1,1,"we conducted an exhaustive survey of deep learning models using 24 ImageNet pre-trained models and five fine-tuning parameters, totaling 120 combinations, on two platforms.",0,0,,1,0,,1,1,"on the Malimg and Drebin datasets', 'we also confirmed that this trend holds true for the recent malware variants using the VirusTotal 2020 Windows and Android datasets",0,0,,1,0,,1,0,,1,0,,1,1,"using 24 ImageNet pre-trained models and five fine-tuning parameters, totaling 120 combinations, on two platforms.",1,
2009.01947,1,"We present combinatorial and parallelizable algorithms for maximization of a submodular function, not necessarily monotone, with respect to a size constraint",1,0,N/A ,0,1,"We improve the best approximation factor achieved by an algorithm that has optimal adaptivity and nearly optimal query complexity to $0.193 - \varepsilon$.' and also 'In this version, we propose a fixed and improved subroutine to add a set with high average marginal gain, \threseq, which returns a solution in $O( \log(n) )$ adaptive rounds with high probability.",0,0,N/A ,1,0,N/A ,1,0,N/A ,1,0,N/A ,1,0,N/A ,1,0,N/A ,1,0,N/A ,1,1,"Our algorithms are empirically validated to use a low number of adaptive rounds and total queries while obtaining solutions with high objective value in comparison with state-of-the-art approximation algorithms, including continuous algorithms that use the multilinear extension.",1,
2010.04261,1,'Hessian captures important properties of the deep neural network loss landscape.' and 'Previous works have observed low rank structure in the Hessians of neural networks.',1,1,"In this paper, we propose a decoupling conjecture that decomposes the layer-wise Hessians of a network as the Kronecker product of two smaller matrices.",1,1,We can analyze the properties of these smaller matrices and prove the structure of top eigenspace random 2-layer networks.' and 'All of these can be verified empirically for deeper networks.,1,0,,1,0,,1,0,,1,0.5,"The decoupling conjecture has several other interesting implications - top eigenspaces for different models have surprisingly high overlap, and top eigenvectors form low rank matrices when they are reshaped into the same shape as the corresponding weight matrix.",1,1,"Finally, we use the structure of layer-wise Hessian to get better explicit generalization bounds for neural networks",0,0,,1,0,,1,0.5,All of these can be verified empirically for deeper networks.,1,"Hessian captures important properties of the deep neural network loss
landscape. Previous works have observed low rank structure in the Hessians of
neural networks. In this paper, we propose a decoupling conjecture that
decomposes the layer-wise Hessians of a network as the Kronecker product of two
smaller matrices. We can analyze the properties of these smaller matrices and
prove the structure of top eigenspace random 2-layer networks. The decoupling
conjecture has several other interesting implications - top eigenspaces for
different models have surprisingly high overlap, and top eigenvectors form low
rank matrices when they are reshaped into the same shape as the corresponding
weight matrix. All of these can be verified empirically for deeper networks.
Finally, we use the structure of layer-wise Hessian to get better explicit
generalization bounds for neural networks."
2010.04855,0.75,"We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves.",1,0.65,"We propose estimators based on kernel ridge regression for nonparametric causal functions such as dose, heterogeneous, and incremental response curves",1,0.85,"Due to a decomposition property specific to the RKHS, our estimators have simple closed form solutions. We prove uniform consistency with finite sample rates via original analysis of generalized kernel ridge regression",1,0,,1,0,,1,0.5,and conduct a policy evaluation of the US Job Corps training program for disadvantaged youths' (Indirect reference to a dataset used for policy evaluation),1,0,,1,0,,1,0,,1,0,,1,0.5,We prove uniform consistency with finite sample rates via original analysis of generalized kernel ridge regression.' (Indirect reference to the experimental setup which involves the original analysis of generalized kernel ridge regression).,1,
2011.11576,1,We propose the use of a conjecturing machine that generates feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features.,1,1,The proposed \textsc{Conjecturing} framework recovers known nonlinear and boolean relationships among features from data.,1,1,We propose the use of a conjecturing machine that...generates feature relationships in the form of bounds involving nonlinear terms for numerical features and boolean expressions for categorical features.,1,0,,1,0,,1,1,The framework is then applied to patient-level data regarding COVID-19 outcomes...,0,0,,1,1,"We then compare the method to a previously-proposed framework for symbolic regression and demonstrate that it can also be used to recover equations...', 'The framework is then applied to patient-level data regarding COVID-19 outcomes to suggest possible risk factors that are confirmed in medical literature.'",0,0,,1,0,,1,0,,1,
2012.09302,1,Neural backdoors represent one primary threat to the security of deep learning systems.,1,1,"To bridge this gap, we design and implement TROJANZOO, the first open-source platform for evaluating neural backdoor attacks/defenses in a unified, holistic, and practical manner",1,1,"Leveraging TROJANZOO, we conduct a systematic study on the existing attacks/defenses, unveiling their complex design spectrum: both manifest intricate trade-offs among multiple desiderata (e.g., the effectiveness, evasiveness, and transferability of attacks).",1,0,Not provided,0,0,Not provided,1,0,Not provided,1,0,Not provided,1,0.5,"We further explore improving the existing attacks/defenses, leading to a number of interesting findings",1,1,"TROJANZOO, the first open-source platform for evaluating neural backdoor attacks/defenses in a unified, holistic, and practical manner.",1,0,Not provided,1,1,"Focusing on the computer vision domain, it has incorporated 8 representative attacks, 14 state-of-the-art defenses, 6 attack performance metrics, 10 defense utility metrics, as well as rich tools for in-depth analysis of the attack-defense interactions.",0,"Neural backdoors represent one primary threat to the security of deep
learning systems. The intensive research has produced a plethora of backdoor
attacks/defenses, resulting in a constant arms race. However, due to the lack
of evaluation benchmarks, many critical questions remain under-explored: (i)
what are the strengths and limitations of different attacks/defenses? (ii) what
are the best practices to operate them? and (iii) how can the existing
attacks/defenses be further improved?
  To bridge this gap, we design and implement TROJANZOO, the first open-source
platform for evaluating neural backdoor attacks/defenses in a unified,
holistic, and practical manner. Thus far, focusing on the computer vision
domain, it has incorporated 8 representative attacks, 14 state-of-the-art
defenses, 6 attack performance metrics, 10 defense utility metrics, as well as
rich tools for in-depth analysis of the attack-defense interactions. Leveraging
TROJANZOO, we conduct a systematic study on the existing attacks/defenses,
unveiling their complex design spectrum: both manifest intricate trade-offs
among multiple desiderata (e.g., the effectiveness, evasiveness, and
transferability of attacks). We further explore improving the existing
attacks/defenses, leading to a number of interesting findings: (i) one-pixel
triggers often suffice; (ii) training from scratch often outperforms perturbing
benign models to craft trojan models; (iii) optimizing triggers and trojan
models jointly greatly improves both attack effectiveness and evasiveness; (iv)
individual defenses can often be evaded by adaptive attacks; and (v) exploiting
model interpretability significantly improves defense robustness. We envision
that TROJANZOO will serve as a valuable platform to facilitate future research
on neural backdoors."
2101.07354,1,"Random-walk based network embedding algorithms like DeepWalk and node2vec are widely used to obtain Euclidean representation of the nodes in a network prior to performing downstream inference tasks. However, despite their impressive empirical performance, there is a lack of theoretical results explaining their large-sample behavior",1,0.9,"In this paper, we study node2vec and DeepWalk through the perspective of matrix factorization.",1,0.9,"Specifically, as the network becomes sparser, our results guarantee that with large enough window size and vertices number, applying $K$-means/medians on the matrix factorization-based node2vec embeddings can, with high probability, correctly recover the memberships of all vertices in a network generated from the stochastic blockmodel (or its degree-corrected variants).",1,0,,1,0,Not directly mentioned,1,0.8,"The theoretical justifications are mirrored in the numerical experiments and real data applications, for both the original node2vec and its matrix factorization variant.",0,0,Not explicitly mentioned,1,0.9,"By exploiting the row-wise uniform perturbation bound for leading singular vectors, we derive high-probability error bounds between the matrix factorization-based node2vec/DeepWalk embeddings and their true counterparts, uniformly over all node embeddings.",0,0,Not mentioned,1,0,Not mentioned directly,1,0.9,"Based on strong concentration results, we further show the perfect membership recovery by node2vec/DeepWalk, followed by $K$-means/medians algorithms. Specifically, as the network becomes sparser, our results guarantee that with large enough window size and vertices number, applying $K$-means/medians on the matrix factorization-based node2vec embeddings can, with high probability, correctly recover the memberships of all vertices in a network generated from the stochastic blockmodel (or its degree-corrected variants).",0,
2102.11887,0.6,"Based on strong concentration results, we further show the perfect membership recovery by node2vec/DeepWalk, followed by $K$-means/medians algorithms. Specifically, as the network becomes sparser, our results guarantee that with large enough window size and vertices number, applying $K$-means/medians on the matrix factorization-based node2vec embeddings can, with high probability, correctly recover the memberships of all vertices in a network generated from the stochastic blockmodel (or its degree-corrected variants).",1,0.8,"We conclude that to achieve the goal of full quantum machine learning, it is crucial to utilize the deferred measurement principle",1,0.6,"We define its quantum generalization, the quantum cross entropy, prove its lower bounds, and investigate its relation to quantum fidelity",1,0,,1,0,,1,0,,1,0,,1,0.2,These two different scenarios illustrate the information loss when making quantum measurements.,1,0,,1,0,,1,0,,1,
2104.11893,1,"Disentangled Graph Convolutional Network (DisenGCN) is an encouraging framework to disentangle the latent factors arising in a real-world graph. However, it relies on disentangling information heavily from a local range (i.e., a node and its 1-hop neighbors), while the local information in many cases can be uneven and incomplete, hindering the interpretabiliy power and model performance of DisenGCN",1,1,"In this paper, we introduce a novel Local and Global Disentangled Graph Convolutional Network (LGD-GCN) to capture both local and global information for graph disentanglement",1,1,"LGD-GCN performs a statistical mixture modeling to derive a factor-aware latent continuous space, and then constructs different structures w.r.t. different factors from the revealed space.",1,0,,1,0,,1,0.5,Evaluations of the proposed LGD-GCN on the synthetic and real-world datasets,1,0,,1,0,,1,0,,1,0,,1,0,,1,
2104.12546,1,"The COVID-19 pandemic considerably affects public health systems around the world. The lack of knowledge about the virus, the extension of this phenomenon, and the speed of the evolution of the infection are all factors that highlight the necessity of employing new approaches to study these events.",1,1,The aim of this work is to investigate any possible relationships between air quality and confirmed cases of COVID-19 in Italian districts.,1,1,"Specifically, we report an analysis of the correlation between daily COVID-19 cases and environmental factors, such as temperature, relative humidity, and atmospheric pollutants.",0,0,N/A,1,0,N/A,1,0,N/A,1,1,Our analysis confirms a significant association of some environmental parameters with the spread of the virus.,1,1,This suggests that machine learning models trained on the environmental parameters to predict the number of future infected cases may be accurate.,0,0,N/A,1,0,N/A,1,1,"Specifically, we report an analysis of the correlation between daily COVID-19 cases and environmental factors, such as temperature, relative humidity, and atmospheric pollutants",0,
2105.01099,0.8,"Several core challenges are to continue to be tackled: model complexity, agent coordination, and joint optimization of multiple levers",1,1,"In this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to decision optimization problems in a typical ridesharing system.",1,1,"In this paper, we present a comprehensive, in-depth survey of the literature...",0,0,,1,0,,1,1,we also introduce popular data sets and open simulation environments to facilitate further research and development.,0,0,,1,0,,1,0,,1,0,,1,0,,1,
2105.01937,1,"The increasing availability of video recordings made by multiple cameras has offered new means for mitigating occlusion and depth ambiguities in pose and motion reconstruction methods. Yet, multi-view algorithms strongly depend on camera parameters; particularly, the relative transformations between the cameras. Such a dependency becomes a hurdle once shifting to dynamic capture in uncontrolled settings.",1,0.9,"We introduce FLEX (Free muLti-view rEconstruXion), an end-to-end extrinsic parameter-free multi-view model.",1,1,"""Our network takes multiple video streams, learns fused deep features through a novel multi-view fusion layer, and reconstructs a single consistent skeleton with temporally coherent joint rotations.",0,0,,1,0,,1,1,"""We demonstrate quantitative and qualitative results on three public datasets, and on synthetic multi-person video streams captured by dynamic cameras.",1,0,,1,0.5,"We compare our model to state-of-the-art methods that are not ep-free and show that in the absence of camera parameters, we outperfom them by a large margin while obtaining comparable results when camera parameters are available.",0,1,"Code, trained models, and other materials are available on our project page.",1,0,,1,0.7,"We compare our model to state-of-the-art methods that are not ep-free and show that in the absence of camera parameters, we outperfom them by a large margin while obtaining comparable results when camera parameters are available.",0,
2105.15197,1,"Debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e. scalar summaries, of machine learning algorithms.",1,1,"'For example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network.",0,1,"We provide a nonasymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions.",0,0,,1,0,,1,0,,1,0.5,"The rate of convergence is $n^{-1/2}$ for global functionals, and it degrades gracefully for local functionals.",0,0,,1,0,,1,0,,1,1,"Formally, we prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments.",0,
2106.01528,1,Controlled feature selection aims to discover the features a response depends on while limiting the false discovery rate (FDR),1,1,We propose a new procedure called FlowSelect to perform controlled feature selection that does not suffer from either of these two problems.,1,1,"To more accurately model the features, FlowSelect uses normalizing flows, the state-of-the-art method for density estimation. Instead of enforcing the """"swap"""" property, FlowSelect uses a novel MCMC-based procedure to calculate p-values for each feature directly.",0,0,,1,0,,1,0.7,"'Empirically, FlowSelect consistently controls the FDR on both synthetic and semi-synthetic benchmarks, whereas competing knockoff-based approaches do not.",0,0,,1,0.7,"Empirically, FlowSelect consistently controls the FDR on both synthetic and semi-synthetic benchmarks, whereas competing knockoff-based approaches do not. FlowSelect also demonstrates greater power on these benchmarks.",0,0,,1,0,,1,0.7,"Empirically, FlowSelect consistently controls the FDR on both synthetic and semi-synthetic benchmarks, whereas competing knockoff-based approaches do not. FlowSelect also demonstrates greater power on these benchmarks.",0,
2106.03157,1,Existing combinatorial search methods are often complex and require some level of expertise.,1,1,"This work introduces a simple and efficient deep learning method for solving combinatorial problems with a predefined goal, represented by Rubik's Cube.'",1,1,"We demonstrate that, for such problems, training a deep neural network on random scrambles branching from the goal state is sufficient to achieve near-optimal solutions.",0,0,,1,0,,1,0,,1,0,,1,1,"When tested on Rubik's Cube, 15 Puzzle, and 7$\times$7 Lights Out, our method outperformed the previous state-of-the-art method DeepCubeA, improving the trade-off between solution optimality and computational cost, despite significantly less training data",0,0,,1,0,,1,0,,1,
2106.03725,0.7,The paper defines and studies manifold (M) convolutional filters and neural networks (NNs).,1,0.8,The main technical contribution of the paper is to analyze the stability of manifold filters and MNNs to smooth deformations of the manifold.,1,0,Not explicitly mentioned,1,0,Not explicitly mentioned,1,0,Not explicitly mentioned,1,0,Not explicitly mentioned,1,0.6,"manifold filters, same as graph filters and standard continuous time filters, have difficulty discriminating high frequency components in the presence of deformations.",0,0,Not explicitly mentioned,1,0,Not explicitly mentioned,1,0,Not explicitly mentioned,1,0,Not explicitly mentioned,1,
2106.06927,1,"Despite unconditional feature inversion being the foundation of many image synthesis applications, training an inverter demands a high computational budget, large decoding capacity and imposing conditions such as autoregressive priors.",1,1,"To address these limitations, we propose the use of adversarially robust representations as a perceptual primitive for feature inversion.",1,1,"We train an adversarially robust encoder to extract disentangled and perceptually-aligned image representations, making them easily invertible",0,0,,1,0,,1,0,,1,0,,1,1,"By training a simple generator with the mirror architecture of the encoder, we achieve superior reconstruction quality and generalization over standard models.",0,0,,1,0,,1,1,"Based on this, we propose an adversarially robust autoencoder and demonstrate its improved performance on style transfer, image denoising and anomaly detection tasks.",0,
2106.07704,1,"This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models.",0,1,"Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward."""" and """"In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward.",1,1,"soft Q-learning (SQL) perspective"""" and """"path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward",0,0,,1,0,,1,0,,1,0,,1,0.5,Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods.,1,0,,1,0,,1,1,"We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation.",0,
2106.10898,0,,0,1,"To address the aforementioned problems, a multi-armed bandit based collaborative filtering recommender system has been proposed, named BanditMF.",1,0.8,"a multi-armed bandit based collaborative filtering recommender system has been proposed, named BanditMF.",0,1,"(1) how to solve the cold start problem for collaborative filtering under the condition of scarcity of valid information, (2) how to solve the sub-optimal problem of bandit algorithms in strong social relations domains caused by independently estimating unknown parameters associated with each user and ignoring correlations between users",1,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,"Multi-armed bandits (MAB) provide a principled online learning approach to
attain the balance between exploration and exploitation. Due to the superior
performance and low feedback learning without the learning to act in multiple
situations, Multi-armed Bandits drawing widespread attention in applications
ranging such as recommender systems. Likewise, within the recommender system,
collaborative filtering (CF) is arguably the earliest and most influential
method in the recommender system. Crucially, new users and an ever-changing
pool of recommended items are the challenges that recommender systems need to
address. For collaborative filtering, the classical method is training the
model offline, then perform the online testing, but this approach can no longer
handle the dynamic changes in user preferences which is the so-called cold
start. So how to effectively recommend items to users in the absence of
effective information? To address the aforementioned problems, a multi-armed
bandit based collaborative filtering recommender system has been proposed,
named BanditMF. BanditMF is designed to address two challenges in the
multi-armed bandits algorithm and collaborative filtering: (1) how to solve the
cold start problem for collaborative filtering under the condition of scarcity
of valid information, (2) how to solve the sub-optimal problem of bandit
algorithms in strong social relations domains caused by independently
estimating unknown parameters associated with each user and ignoring
correlations between users."
2106.12177,1,"Imitation learning aims to extract knowledge from human experts' demonstrations or artificially created agents in order to replicate their behaviors."""" and """"the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments.",1,1,"In this survey, we provide a systematic review on imitation learning. We first introduce the background knowledge from development history and preliminaries, followed by presenting different taxonomies within Imitation Learning and key milestones of the field.",1,1,"In this survey, we provide a systematic review on imitation learning.",0,0,None.,1,0,None.,1,0,None.,1,0,None.,1,0,None.,1,0,None.,1,0,None.,1,0,None.,1,
2106.12936,0.7,HMMs are flexible tools for clustering dependent data coming from unknown populations.,0,0.75,We study the frontier between learnable and unlearnable hidden Markov models (HMMs).,1,0.85,"For a chain with two hidden states we prove nonasymptotic minimax upper and lower bounds, matching up to constants, which exhibit thresholds at which the parameters become learnable",0,0,N/A,1,0,N/A,1,0,N/A,1,0,N/A,1,0.6,We also provide an upper bound on the relative entropy rate for parameters in a neighbourhood of the unlearnable region which may have interest in itself.',0,0,N/A,1,0,N/A,1,0,N/A,1,
2106.13823,1,Quantum machine learning is an emerging field at the intersection of machine learning and quantum computing.,0,1,"In this paper, we present one operational interpretation of this quantity, that the quantum cross entropy...",1,1,"we give a simple, universal quantum data compression protocol, which is developed based on quantum generalization of variable-length coding, as well as quantum strong typicality",0,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,
2107.01131,1,"Successful applications of InfoNCE and its variants have popularized the use of contrastive variational mutual information (MI) estimators in machine learning. While featuring superior stability, these estimators crucially depend on costly large-batch training, and they sacrifice bound tightness for variance reduction.",1,1,"To overcome these limitations, we revisit the mathematics of popular variational MI bounds from the lens of unnormalized statistical modeling and convex optimization. Our investigation not only yields a new unified theoretical framework encompassing popular variational MI bounds but also leads to a novel, simple, and powerful contrastive MI estimator named as FLO.",1,1,"To overcome these limitations, we revisit the mathematics of popular variational MI bounds from the lens of unnormalized statistical modeling and convex optimization.",0,0,,1,0,,1,0,,1,0,,1,1,"Empirically, our FLO estimator overcomes the limitations of its predecessors and learns more efficiently.",0,0,,1,0,,1,0.5,The utility of FLO is verified using an extensive set of benchmarks',0,
2108.09779,1,We present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary 6-DoF pose with only 3-fingers trained with NVIDIA's IsaacGym simulator.,1,1,"We show empirical benefits, both in simulation and sim-to-real transfer, of using keypoints as opposed to position+quaternion representations for the object pose in 6-DoF for policy observations and in reward calculation to train a model-free reinforcement learning agent.",1,1,"By utilizing domain randomization strategies along with the keypoint representation of the pose of the manipulated object, we achieve a high success rate.",0,0,Not Found,1,0,Not Found,1,0,Not Found,1,0,Not Found,1,1,we achieve a high success rate of 83% on a remote TriFinger system maintained by the organizers of the Real Robot Challenge.,0,1,"we make the codebase of our system, along with trained checkpoints that come with billions of steps of experience available, at https://s2r2-ig.github.io",1,0,Not Found,1,0,Not Found,1,
2109.01372,0.9,This work explores the effect of noisy sample selection in active learning strategies.,1,0.8,that knowledge of the sample noise can significantly improve the performance of active learning strategies.,1,0.7,"Building on prior work, we propose a robust sampler, Incremental Weighted K-Means",1,0.7,We hope that the questions raised in this paper are of interest to the community and could open new paths for active learning research.,0,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,
2109.12784,1,Motivated by the problem of learning with small sample sizes,1,1,this paper shows how to incorporate into support-vector machines (SVMs) those properties that have made convolutional neural networks (CNNs) successful.,1,1,Kernels based on the \textit{maximum} similarity over a group of transformations are not generally positive definite. Perhaps it is for this reason that they have not been studied theoretically. We address this lacuna,0,0,,1,0,,1,1,We verify through experiments on widely available image sets,1,1,"We address this lacuna and show that positive definiteness indeed holds \textit{with high probability} for kernels based on the maximum similarity in the small training sample set regime of interest, and that they do yield the best results in that regime",0,1,We verify through experiments on widely available image sets that the resulting SVMs do provide superior accuracy in comparison to well-established deep neural network benchmarks for small sample sizes,0,0,,1,0,,1,1,"We address this lacuna and show that positive definiteness indeed holds \textit{with high probability} for kernels based on the maximum similarity in the small training sample set regime of interest, and that they do yield the best results in that regime. We also show how additional properties such as their ability to incorporate local features at multiple spatial scales, e.g., as done in CNNs through max pooling, and to provide the benefits of composition through the architecture of multiple layers, can also be embedded into SVMs.",1,
2110.02343,1,"There are two persistent challenges in classical machine learning: the lack of labeled data, and the limit of computational power.",1,1,We propose a novel framework that resolves both issues: quantum semi-supervised learning.,1,1,"Moreover, we provide a protocol in systematically designing quantum machine learning algorithms with quantum supremacy, which can be extended beyond quantum semi-supervised learning",1,0,None,1,0,None,1,0,None,1,0,None,1,1,"""We conclude that they indeed possess quantum supremacy",1,0,None,1,0,None,1,1,"By doing time complexity analysis, we conclude that they indeed possess quantum supremacy",0,
2110.02474,1,I model the belief formation and decision making processes of economic agents during a monetary policy regime change (an acceleration in the money supply)...',1,1,"'I show that when the money supply accelerates, the learning agents only adjust their actions, which include consumption and demand for real balance, after gathering learning experience for many periods.",1,1,...with a deep reinforcement learning algorithm in the AI literature.,0,1,"'I also show that, 1. the AI agents who explore their environment more adapt to the policy regime change quicker, which leads to welfare improvements and less inflation volatility, and 2. the AI agents who have experienced a structural change adjust their beliefs and behaviours quicker than an inexperienced learning agent.",1,0,,1,0,,1,1,"I show that when the money supply accelerates, the learning agents only adjust their actions, which include consumption and demand for real balance, after gathering learning experience for many periods. This delayed adjustments leads to low returns during transition periods. Once they start adjusting to the new environment, their welfare improves.",1,1,Their changes in beliefs and actions lead to temporary inflation volatility.,0,0,,1,0,,1,0.5,I model the belief formation and decision making processes of economic agents during a monetary policy regime change (an acceleration in the money supply) with a deep reinforcement learning algorithm in the AI literature.,0,
2110.03135,1,label noise exists in adversarial training,1,1,we proposed a method to automatically calibrate the label to address the label noise and robust overfitting,1,1,"Guided by our analyses, we proposed a method",1,0,,1,0,,1,0.5,across various models and datasets,0,1,Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples,0,0,,1,0,,1,0,,1,0,,1,
2110.05169,1,"Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ",1,1,There is a need to develop RL methods that generalize well to variations of the training conditions.,0,1,"To do so, we propose an approach where we learn a subspace of policies within the parameter space.",0,0,None,1,0,None,1,0,None,1,0,None,1,0.8,"Our experiments carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods.",0,0,None,1,0,None,1,0.8,"In comparison, our approach is simple to tune, does not need any extra component (e.g., discriminator) and learns policies able to gather a high reward on unseen environments.",0,
2110.08255,0.9,Time series data is ubiquitous in research as well as in a wide variety of industrial applications. Effectively analyzing the available historical data and providing insights into the far future allows us to make effective decisions.,0,0.7,"Recent research has witnessed the superior performance of transformer-based architectures, especially in the regime of far horizon time series forecasting.",0,1,"We propose the Yformer model, based on a novel Y-shaped encoder-decoder architecture...",1,0,,1,0,,1,1,Extensive experiments have been conducted with relevant baselines on four benchmark datasets,0,0,,1,0,,1,0,,1,0,,1,0.9,"Extensive experiments have been conducted with relevant baselines on four benchmark datasets, demonstrating an average improvement of 19.82, 18.41 percentage MSE and 13.62, 11.85 percentage MAE in comparison to the current state of the art for the univariate and the multivariate settings respectively.",1,
2110.08432,1,"a critical challenge in MAML is to calculate the gradient w.r.t. the initialization of a long training trajectory for the sampled tasks, because the computation graph can rapidly explode and the computational cost is very expensive.",1,1,"To address this problem, we propose Adjoint MAML (A-MAML).",1,1,"To efficiently compute the gradient of the validation loss w.r.t. the initialization, we use the adjoint method to construct a companion, backward ODE",1,0,Not directly mentioned in the abstract,1,0,Not directly mentioned in the abstract,1,0,Not directly mentioned in the abstract,1,0,Not directly mentioned in the abstract,1,1,"Our approach is cheap, accurate, and adaptable to different trajectory lengths.",0,0,Not directly mentioned in the abstract,1,0,Not directly mentioned in the abstract,1,1,We demonstrate the advantage of our approach in both synthetic and real-world meta-learning tasks.,0,
2110.09902,1,We make an attempt to understanding convolutional neural network by exploring the relationship between (deep) convolutional neural networks and Volterra convolutions,1,1,We propose a novel approach to explain and study the overall characteristics of neural networks without being disturbed by the horribly complex architectures.,1,1,"Specifically, we attempt to convert the basic structures of a convolutional neural network (CNN) and their combinations to the form of Volterra convolutions.",1,0,None provided,1,0,None provided,1,0,None provided,11,0,None provided,1,1,"The results show that most of convolutional neural networks can be approximated in the form of Volterra convolution, where the approximated proxy kernels preserve the characteristics of the original network.",0,0,None provided,1,0,None provided,1,0.8,"Base on this setup, we presented methods to approximating the order-zero and order-one proxy kernels, and verified the correctness and effectiveness of our results.",0,
2110.11688,1,Machine learning models can leak information about the data used to train them.,1,1,"In this paper, we propose Differentially Private proximal Coordinate Descent (DP-CD), a new method to solve composite DP-ERM problems.",1,1,We derive utility guarantees through a novel theoretical analysis of inexact coordinate descent.,1,0,No specific research question is identified,1,0,No pseudocode is provided,1,0.5,'Experiments on real and synthetic data support our results',0,0.5,"Our results show that, thanks to larger step sizes, DP-CD can exploit imbalance in gradient coordinates to outperform DP-SGD.",0,0.5,"We also prove new lower bounds for composite DP-ERM under coordinate-wise regularity assumptions, that are nearly matched by DP-CD.",0,0,No mention of source code being shared,1,0,There is no mention of specific software or package dependency,1,1,"For practical implementations, we propose to clip gradients using coordinate-wise thresholds that emerge from our theory, avoiding costly hyperparameter tuning.",0,
2110.14241,1,"Previous work using a single set of agents has shown great progress in generalizing to known partners, however it struggles when coordinating with unfamiliar agents.",1,1,"In this work, our goal is to train agents that can coordinate with seen, unseen as well as human partners in a multi-agent communication environment involving natural language.",1,0.7,We attribute this to the use of static populations and instead propose a dynamic population-based meta-learning approach that builds such a population in an iterative manner. We perform a holistic evaluation of our method on two different referential games,1,0,,1,0,,1,0,,1,0,,1,0.8,"We show that our agents outperform all prior work when communicating with seen partners and humans. Furthermore, we analyze the natural language generation skills of our agents, where we find that our agents also outperform strong baselines.",1,0,,1,0,,1,0.5,"Finally, we test the robustness of our agents when communicating with out-of-population agents and carefully test the importance of each component of our method through ablation studies.",0,
2111.02997,1,"In this paper, we establish the global optimality and convergence rate of an off-policy actor critic algorithm in the tabular setting without using density ratio to correct the discrepancy between the state distribution of the behavior policy and that of the target policy.",1,1,"In this paper, we establish the global optimality and convergence rate of an off-policy actor critic algorithm in the tabular setting without using density ratio to correct the discrepancy between the state distribution of the behavior policy and that of the target policy.",1,0.9,"Our work goes beyond existing works on the optimality of policy gradient methods...' and 'Central to our work is the finite sample analysis of a generic stochastic approximation algorithm with time-inhomogeneous update operators on time-inhomogeneous Markov chains, based on its uniform contraction properties.",0,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,0,,1,0.6,"Our update step is not a gradient update because we do not use a density ratio to correct the state distribution, which aligns well with what practitioners do. Our update is approximate because we use a learned critic instead of the true value function. Our update is stochastic because at each step the update is done for only the current state action pair.",0,
2111.03664,1,"Knowledge distillation (KD), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student).",0,0.9,"Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input.",1,1,"Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input... Based on a many-to-one mapping property of the CTC algorithm, we present a training strategy that can effectively prevent the trivial solution and thus enables utilizing both source and target inputs for model training.",0,0,,1,0,,1,0.6,Extensive experiments are conducted on two sequence learning tasks: speech recognition and scene text recognition,0,0,,1,0.7,"From the experimental results, we empirically show that the proposed model improves the students across these tasks while achieving a considerable speed-up in the teacher model's training time.",0,0,,1,0,,1,0.7,"Based on a many-to-one mapping property of the CTC algorithm, we present a training strategy that can effectively prevent the trivial solution and thus enables utilizing both source and target inputs for model training.",0,
2111.08356,0.8,"In Federated Learning (FL), multiple clients collaborate to learn a shared model through a central server while keeping data decentralized. Personalized Federated Learning (PFL) further extends FL by learning a personalized model per client.",0,0.9,"we introduce a new learning setup, On-Demand Unlabeled PFL (OD-PFL), where a system trained on a set of clients, needs to be later applied to novel unlabeled clients at inference time.",1,1,we train an encoder network that learns a representation for a client given its unlabeled data. That client representation is fed to a hypernetwork that generates a personalized model for that client.,1,0,None found,1,0,None found,1,0.8,"Evaluated on five benchmark datasets, we find that ODPFL-HN generalizes better than the current FL and PFL methods, especially when the novel client has a large shift from training clients.",0,0,None found,1,0.6,we find that ODPFL-HN generalizes better than the current FL and PFL methods,0,0,None found,1,0,None found,1,0.6,"we find that ODPFL-HN generalizes better than the current FL and PFL methods, especially when the novel client has a large shift from training clients.",1,
2111.12673,1,Algorithms based on temporal difference learning typically are prone to an over- or underestimation bias building up over time.,1,1,"In this paper, we propose a general method called Adaptively Calibrated Critics (ACC).",1,1,Adaptively Calibrated Critics (ACC) that uses the most recent high variance but unbiased on-policy rollouts to alleviate the bias of the low variance temporal difference targets.,0,0,,1,0,,1,0.7,"sets a new state of the art on the OpenAI gym continuous control benchmark' and 'We apply ACC to Truncated Quantile Critics, which is an algorithm for continuous control that allows regulation of the bias with a hyperparameter tuned per environment.",0,0,,1,0.9,The resulting algorithm adaptively adjusts the parameter during training rendering hyperparameter search unnecessary and sets a new state of the art on the OpenAI gym continuous control benchmark among all algorithms that do not tune hyperparameters for each environment. ACC further achieves improved results on different tasks from the Meta-World robot benchmark,0,0,,1,0,,1,0.8,"We apply ACC to Truncated Quantile Critics, which is an algorithm for continuous control that allows regulation of the bias with a hyperparameter tuned per environment. The resulting algorithm adaptively adjusts the parameter during training rendering hyperparameter search unnecessary and sets a new state of the art on the OpenAI gym continuous control benchmark among all algorithms that do not tune hyperparameters for each environment.",0,
2111.15449,0.75,"In the field of pattern classification, the training of deep learning classifiers is mostly end-to-end learning, ... so there is still room for improvement in classification accuracy.",1,0.8,"Based on the concept of Predefined Evenly-Distributed Class Centroids (PEDCC), this article proposes a Softmax-free loss function based on predefined optimal-distribution of latent features-POD Loss.",1,0.8,"The loss function only restricts the latent features of the samples, including the norm-adaptive Cosine distance between the latent feature vector of the sample and the center of the predefined evenly-distributed class, and the correlation between the latent features of the samples.",0,0,,1,0,,1,0.5,experiments on several commonly used datasets on several typical deep learning classification networks,0,0,,1,0.5,the classification performance of POD Loss is always significant better and easier to converge.,0,1,Code is available in https://github.com/TianYuZu/POD-Loss.,1,0,,1,0.6,"Compared with the commonly used Softmax Loss, some typical Softmax related loss functions and PEDCC-Loss, experiments on several commonly used datasets on several typical deep learning classification networks",1,
2112.00016,1,We use deep neural networks to machine learn correlations between knot invariants in various dimensions.,0,1,"We find that a two-layer feed-forward neural network can predict $s$ from $\text{Kh}(q,-q^{-4})$ with greater than $99\%$ accuracy.",0,1,We use deep neural networks,0,1,"we discuss the extent to which the machine is learning $s$ as opposed to $g$, since there is a general inequality $|s| \leq 2g$.",1,0,,1,0,,1,0.5,"This suggests a relationship with $SU(2)$ Chern--Simons theory, and we review the gauge theory construction of Khovanov homology which may be relevant for explaining the network's performance.",1,1,"we find similar performance for the prediction of $s$ from $\text{Kh}(q,-q^{-2})$, which suggests a novel relationship between the Khovanov and Lee homology theories of a knot.",1,0,,1,0,,1,0.5,"We find that a two-layer feed-forward neural network can predict $s$ from $\text{Kh}(q,-q^{-4})$",0,
2112.03432,1,Obtaining first-order regret bounds -- regret bounds scaling not as the worst-case but with some measure of the performance of the optimal policy on a given instance -- is a core question in sequential decision-making.,1,0.7,In this work we address this gap,1,1,we... develop a novel robust self-normalized concentration bound based on the robust Catoni mean estimator,1,0,Not explicitly mentioned,1,0,Not mentioned ,1,0,Not mentioned ,1,0,Not mentioned ,1,0,Not mentioned ,1,0,Not mentioned ,1,0,Not mentioned ,1,0.8,"...show that it is possible to obtain regret scaling as $\widetilde{\mathcal{O}}(\sqrt{d^3 H^3 \cdot V_1^\star \cdot K} + d^{3.5}H^3\log K )$ in reinforcement learning with large state spaces, namely the linear MDP setting.",0,
2112.04871,1,"However, most previous KGE methods ignore the semantic similarity between the related entities and entity-relation couples in different triples since they separately optimize each triple with the scoring function.",1,1,"To address this problem, we propose a simple yet efficient contrastive learning framework for tensor decomposition based (TDB) KGE",1,0.5,"We evaluate our proposed method on three standard KGE datasets: WN18RR, FB15k-237 and YAGO3-10.",0,0,,1,0,,1,1,"We evaluate our proposed method on three standard KGE datasets: WN18RR, FB15k-237 and YAGO3-10.",0,0,,1,1,"Our method can yield some new state-of-the-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR dataset, 37.8% MRR, 28.6% Hits@1 on FB15k-237 dataset, and 59.1% MRR, 51.8% Hits@1 on the YAGO3-10 dataset.",0,0,,1,0,,1,0.5,"Our method can yield some new state-of-the-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR dataset, 37.8% MRR, 28.6% Hits@1 on FB15k-237 dataset, and 59.1% MRR, 51.8% Hits@1 on the YAGO3-10 dataset.",0,
,46.7,,40,46.3,,43,45.6,,23,3.7,,48,0,,50,16.35,,44,8.1,,44,25.9,,26,4,,50,1,,50,28.1,,23,